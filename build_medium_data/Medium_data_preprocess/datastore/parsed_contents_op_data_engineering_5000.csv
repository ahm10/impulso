UUID,sentences
eit72x9f7yyEb8G2oaFQ6b,This article describes a way to periodically move on-premise Cassandra data to S3 for analysis Were been using this approach successfully over the last few months in order to get the best of both worlds for an early-stage platform such as 1200aero: The cost effectiveness of on-premise hosting for a stable live workload and the on-demand scalability of AWS for data analysis and machine learningOur end goal was to upload data to S3 organized as follows: To that effect we performed the following steps described in detail in this article: For your reference we used Cassandra 311 and Spark 231 both straight open source versionsThe data we want to analyze consists of millions of low-altitude air traffic messages captured from ADS-B receivers at a number of regional airports The original table definition is: Notice the primary key is (icao gentime) Cassandra data models are designed with a specific data retrieval use case in mind Our applications query data by aircraft identifier so our partitioning column is the aircrafts icao (a 6-character hex transponder code) followed by gentime (a timestamp) as the clustering column that orders records chronologically within each partitionProblem: In order to periodically export a day/week/months worth of data we needed to be able to retrieve records by date Since such attribute did not originally exist we had to: Defining a new date column in Cassandra is straightforward: However populating it proved more complex than anticipated: We had to update several hundred million records and did not find a suitable CQL UPDATE statement to do so in place (ie deriving day from the gentime timestamp) After evaluating all alternatives we ended up doing this: Export the primary key for every record to a CSV file using CQL: Output file: Update the file in Unix to create a new field based on the existing timestamp: The above yielded a file with the new attribute: Import the modified file into the same table which updates every record with just the new column value leaving all other fields intact: The import took several hours but worked flawlessly In hindsight its still far from ideal and required some research to guarantee the safety of the data but given the lack of support in Cassandra its as good a solution we could findPrior to creating our first Cassandra materialized view we considered secondary indexes However the many caveats and words of caution from experienced users led us in the direction of materialized viewsMaterialized view creation ran in background for ~12 hours as it duplicated and organized the data This query reports any builds in progress: To date we have not observed a significant performance overhead associated with the materialized view DataStax found that every materialized view takes away ~10% of the insert performance on a given tableOnce we had the means to retrieve the data by date we had to ensure that the Spark Cassandra connector issued the right queries to retrieve the data efficientlySpark configuration parameters for the Cassandra connector (passed when creating a SparkConf via spark-submit sparkmagic in Jupyter etc): Import the right packages define a dataset on the Cassandra table: Filter data for a specific date: …or for a number of dates (discrete dates must be specified in an IN condition): To verify that your filter will be applied to the Cassandra table print the physical plan with dfexplain() There should be predicates (ie actual values) being pushed down to the data source as in these examples: Important: Attempts to filter the dataset that dont conform to the connector documentation will cause Spark to issue a full table scan to Cassandra (and thus defeat the purpose of reorganizing your data as done in Step 1)As a reminder our goal was to lay the data in S3 partitioned by date example: So rather than partitioning the dataset by the whole date string (eg 2018 11 06) we needed to partition by its components (year month day) Hence we had to first convert the day column into year month and day columns: At long last we had our data available in Spark with the necessary attributes and partitioned by the criteria we neededSetting up Spark to push data to S3 was pretty easy Required AWS libraries are included in the Hadoop distribution Spark depends upon All we had to configure were the following five entries in $SPARK_HOME/conf/spark-defaultsconf: Now we were ready to partition and push the data to S3 in a single instruction: Performance of the above statement is surprisingly fast (in part due to Parquet files being much smaller than their CSV equivalents) Loading a days worth of records (~500000 rows) completes in less than one minuteWe use SaveModeAppend to make it possible to load additional days without deleting existing dataNote: The s3a URL prefix has desirable performance and capacity implications for large file operations such as ParquetFrom here sky is the limit You can spin an EMR cluster and query S3 directly with an arbitrary number of cluster nodes Alternatively you can also launch a Sagemaker notebook and point it to EMR Spark following similar steps to those outlined in my previous article And as of the time of writing Boto3 the AWS SDK for Python now makes it possible to issue basic SQL queries against Parquet files in S3Future articles will describe how 1200aero is using these data to predict potentially hazardous situations for general aviation aircraft
FBbr4ipgZWYUJmvhk8iDX2,Betterment has built a highly available data processing platform to power new product features and backend processing needs using AirflowBetterments data platform is unique in that it not only supports offline needs such as analytics but also powers our consumer-facing product Features such as Time Weighted Returns and Betterment for Business balances rely on our data platform working throughout the day Additionally we have regulatory obligations to report complex data to third parties daily making data engineering a mission critical part of what we do at BettermentWe originally ran our data platform on a single machine in 2015 when we ingested far less data with fewer consumer-facing requirements However recent customer and data growth coupled with new business requirements require us to now scale horizontally with high availabilityOur single-server approach used Luigi a Python module created to orchestrate long-running batch jobs with dependencies While we could achieve high availability with Luigi its now 2017 and the data engineering landscape has shifted We turned to Airflow because it has emerged as a full-featured workflow management framework better suited to orchestrate frequent tasks throughout the dayTo migrate to Airflow were deprecating our Luigi solution on two fronts: cross-database replication and task orchestration Were using Amazons Database Migration Service (DMS) to replace our Luigi-implemented replication solution and re-building all other Luigi workflows in Airflow Well dive into each of these pieces below to explain how Airflow mediated this transitionWe used Luigi to extract and load source data from multiple internal databases into our Redshift data warehouse on an ongoing basis We recently adopted Amazons DMS for continuous cross-database replication to Redshift moving away from our internally-built solutionThe only downside of DMS is that we are not aware of how recent source data is in Redshift For example a task computing all of a prior days activity executed at midnight would be inaccurate if Redshift were missing data from DMS at midnight due to lagIn Luigi we knew when the data was pulled and only then would we trigger a task However in Airflow we reversed our thinking to embrace DMS using Airflows sensor operators to wait for rows to be pushed from DMS before carrying on with dependent tasksWhile Airflow doesnt claim to be highly available out of the box we built an infrastructure to get as close as possible Were running Airflows database on Amazons Relational Database Service and using Amazons Elasticache for Redis queuing Both of these solutions come with high availability and automatic failover as add-ons Amazon provides Additionally we always deploy multiple baseline Airflow workers in case one fails in which case we use automated deploys to stand up any part of the Airflow cluster on new hardwareThere is still one single point of failure left in our Airflow architecture though: the scheduler While we may implement a hot-standby backup in the future we simply accept it as a known risk and set our monitoring system to notify a team member of any deviancesSince our processing needs fluctuate throughout the day we were paying for computing power we didnt actually need during non-peak times on a single machine as shown in our Luigi servers loadDistributed workers used with Amazons Auto Scaling Groups allow us to automatically add and remove workers based on outstanding tasks in our queues Effectively this means maintaining only a baseline level of workers throughout the day and scaling up during peaks when our workload increasesAirflow queues allow us to designate certain tasks to run on particular hardware (eg CPU optimized) to further reduce costs We found just a few hardware type queues to be effective For instance tasks that saturate CPU are best run on a compute optimized worker with concurrency set to the number of cores Non-CPU intensive tasks (eg polling a database) can run on higher concurrency per CPU core to save overall resourcesAirflow tasks that pass data to each other can run on different machines presenting a new challenge versus running everything on a single machine For example one Airflow task may write a file and a subsequent task may need to email the file from the dependent task ran on another machine To implement this pattern we use Amazon S3 as a persistent storage tierFortunately Airflow already maintains a wide selection of hooks to work with remote sources such as S3 While S3 is great for production its a little difficult to work with in development and testing where we prefer to use the local filesystem We implemented a local fallback mixin for Airflow maintained hooks that uses the local filesystem for development and testing deferring to the actual hooks remote functionality only on productionWe mimic our production cluster as closely as possible for development & testing to identify any issues that may arise with multiple workers This is why we adopted Docker to run a production-like Airflow cluster from the ground up on our development machines We use containers to simulate multiple physical worker machines that connect to officially maintained local Redis and PostgreSQL containersDevelopment and testing also require us to stand up the Airflow database with predefined objects such as connections and pools for the code under test to function properly To solve this programmatically we adopted Alembicdatabase migrations to manage these objects through code allowing us to keep our development testing and production Airflow databases consistentUpon each deploy we use Ansible to launch new worker instances and terminate existing workers But what happens when our workers are busy with other work during a deploy? We dont want to terminate workers while theyre finishing something up and instead want them to terminate after the work is done (not accepting new work in the interim)Fortunately Celery supports this shutdown behavior and will stop accepting new work after receiving an initial TERM signal letting old work finish up We use Upstart to define all Airflow services and simply wrap the TERM behavior in our workers post-stop script sending the TERM signal first waiting until we see the Celery process stopped then finally poweroff the machineThe path to building a highly available data processing service was not straightforward requiring us to build a few specific but critical additions to Airflow Investing the time to run Airflow as a cluster versus a single machine allows us to run work in a more elastic manner saving costs and using optimized hardware for particular jobs Implementing a local fallback for remote hooks made our code much more testable and easier to work with locally while still allowing us to run with Airflow-maintained functionality in productionWhile migrating from Luigi to Airflow is not yet complete Airflow has already offered us a solid foundation We look forward to continuing to build upon Airflow and contributing back to the communityThis article is part of Engineering at BettermentOriginally published at wwwbettermentcom on August 14 2017
fwSKxXMWiBEE6gpLEnfupV,Talk about data driven company which means people should use data to prove their argument so the data itself must be rich enough Rich data can be measured by its size throughput (amount per time) or complexity of its value Distributed machine framework or simply we call it big data technology can be used to handle that Data engineers in my office uses Hadoop family ecosystem such as HBase Hive Impala HDFS etc Hadoop is convenient enough for us to handle large data the implementation of partition to address which data on which physical nodes makes us able to keep performance of query trough nearly infinite data set Its distributed architecture and replication makes it have high availability feature so we less to worry about data loss But replication itself comes with disadvantage the most disadvantage is increasing its storage size as much as number of replication that we setOn the other hand data value will most likely decrease over time especially for time based data like system log user behavior events etc Storing large amount of data in distributed manner (Hadoop) is so costly since most likely we implement replication factor to meet high standard of availability By default HDFS replication factor is 3 so our data multiplied by 3 and distributed among available nodes For some reasons this is too much especially for data which is no longer valuable ie user behavior data gathered last year So we need to find solution or technology that meet our needs such as able to handle large or nearly infinite data set since data is always increasing trough time have good performance when querying and efficient disk consumption Its not as simple as reducing replication factor because if we reduce replication factor to lets say 1 we cant achieve its high standard availability anymore since data that stored in dead node will be unreachableApache Hive has great solution for it which we can store our data on AWS S3 assuming that AWS S3 availability is beyond our standard In the context of persistence in my case(data engineers at my office) AWS S3 provide better persistence than HDFS since its premium and managed service so data will be safe even we failed to spin up our self hosted service So its wiser for data engineers to archive old less valuable data to S3 instead on HDFS In the context of technology when we pick to use S3 instead of HDFS on Hive it become external table So if we use S3 we just need to deploy as minimum as one node just for Hive related instances (minimum node specification still applied)In the context of cost data persistence and effort to maintain S3 is better than HDFS subjectively for our case But what about performance? When we need to consume data that stored in S3 by Hive it will be downloaded first decompress if any compression method is applied before it is ready to be used More or less it will affect the read performance different from HDFS which data is ready to be consumed every time So storing data on HDFS will provide better read throughput than S3 in the context of Hive Another issue is computation service different than Hive as a warehouse service If we need to consume large data set of course we still needs to deploy multiple nodes and uses computation service like SparkTo make it easy we differentiate data by its value first is the data will be frequently consumed or not how big the data is etc For example we can classified data by group such asSome of the data may have more than one type ie products stock we can put it as dynamic data on its own RDBMS but we can also gather its time based log or past activity to analyze it later Talk about big data the most relevant data stored to distributed system on my case is time based But occasionally dynamic and static data may be stored to distributed system as well such as HBase to leverage some specific needs (may discussed in different article)For time based data we should have the information about how people will use it how old is past data should be kept in HDFS etc After we understand how the behavior of the data is then we decide where to store it whether on HDFS or S3 how old we should keep on HDFS etcIn my case whether to use S3 or HDFS for storage is very flexible and highly influenced by its use case As a data engineer we wont restrict people ideas to implement his own flows to achieve better performance or value Some data are stored to HDFS first and S3 later some of them are directly to HDFS and S3 and some of them only on S3 without HDFS All is possible and have its own advantages and disadvantages the most important part is engineer who used it must be understand and responsible whats best for their caseFor data engineers who uses big data technology to do data warehousing most likely most of expensive data is time based ie users behaviors events logs or systems logs which are very expensive both on size and on the context of its high throughput To determine how to store some data at first we gather informations from the people who needed it and how those data will be used for to determine do we need to store it on HDFS and how long this data will be stored on HDFS Also we determine when this data is ready to store to S3 to leverage efficiency Some of the data stored in HDFS will eventually be deleted to eliminate storage costs
XtYdtvd9J6rFMH7SwZ3Gcj,New York City has 59 Community Boards each representing the residents living within a geographic area of the city The members of these boards also live within its bounds and play an integral role connecting citizens with City agencies which they do in many ways One way Community Boards connect with City agencies is by what we call Community Board Budget RequestsCommunity Board Budget Requests well call them Budget Requests for short are submitted by each Community Board every October and contain a list of that communitys priority capital and programmatic investments intended to better that neighborhood These lists are submitted to the Office of Management and Budget who then distributes these Budget Requests to the respective City agency The City agencies then take these Budget Requests into consideration when developing the upcoming Capital BudgetBy sharing the neighborhoods priority needs for services and infrastructure with City agencies the Community Boards via Budget Requests can influence and shape investments across the cityHistorically these Budget Requests have been shared with City agencies via spreadsheets which is not always the easiest way to consume this information City agencies know where their assets are so seeing the locations of the requests on a map is a more intuitive and helpful way for them to explore the requests Additionally by viewing the requests spatially agencies can better understand what requests are being made within a given community what is being requested of them across the city and what is being requested of partner agenciesOur problem was that we couldnt create the maps since the submitted Budget Requests do not contain spatial data so we had to generate our ownTo geolocate these Budget Requests we used three methods: automated georeferencing fuzzy string matching and manual data creationAfter Community Boards submit the Budget Requests our colleagues on the Planning Coordination team receive the requests in Excel spreadsheets The requests occasionally include addresses and often include descriptive information that can be matched to a location but again no spatial dataFor records with free form address intersection or on-to-from information (eg on 5th Avenue between 42nd and 48th Street) we leveraged City Plannings geocoder whose web API is called Geoclient Geoclient returns the latitude and longitude (lat/long) of an address or intersection and the beginning and ending lat/long for any street segment (Unfortunately we werent able to link the information returned by the Geoclient API to LION the citys street centerline database but thats another blog postUsing the lat/long data returned by the Geoclient API we made spatial data! This method got some requests mapped but there were more requests to goThis method may be the most funFor records that couldnt automatically be georeferenced via the method described above we had to get creative since we needed to decipher location information from the description of the requested projectFor context here are some sample descriptions: Now you (maybe with the help of Google) can read Handicap Accessibility in Front of the 46th Precinct and know where that project should be mapped to The trick is getting a computer to know to map Handicap Accessibility in Front of the 46th Precinct to NYPDs 46th Precinct station house Inconsistent site name patterns such as 46 PCT vs 46th Precinct added to the challenge and complexity of automating the mapping of projects based on descriptionGiven these factors we developed a series of SQL LIKE statements and Fuzzy string matching algorithms that matched words in the descriptions to names of places in other spatial datasets The two reference datasets we used to compare against were City Plannings Facilities Database which maps +36k government facilities or program sites throughout NYC and NYC Parks Properties which has polygon geometries for all 1969 NYC parks These two datasets capture the majority of the Citys building or park based fixed assetsThis process mapped a fair number of projects by matching Build the Green Outlook Riverside Park (DPR) to Riverside Park in the Park Properties dataset and Upgrade the FDNY Engine 307 kitchen to Eng 307 Lad 154 in the Facilities Database for exampleWithout over engineering our solution we could not automatically map all requests such as this one: So we had to do some manual workBudget Requests are classified into two categories: Capital and Expense Generally Capital requests are for large scale investments in infrastructure and impact the built environment while Expense requests are for funding programs and other repeating costs of government servicesAdditionally Budget Requests are grouped into two impact types: Site specific and Non-site specific Non-site specific requests are for projects that are not necessarily tied to a discrete fixed asset such as: Whereas Site specific requests can be associated with a known location like: We focused our manual mapping efforts on Site specific Capital requestsTo map these records we used a simple Leaflet Draw GeoJSON creator built by our colleague Chris Whong which outputs code for a geojson file The tool is simple enough for most people to create spatial data regardless of skill in GISIn the end we mapped 95% of all site specific records which we published on an online map currently accessible to City employees Using this map Planners from all City agencies can filter and search requests to explore projects requested by Community Boards across the cityOur goal is that by making Community Board Budget Requests more accessible and easier to explore through a map City agencies will discover synergies between existing capital projects and these Budget Requests so that in the end more Budget Requests come to fruitionWe plan to make the data and map public after City agencies have reviewed and responded to all Community Board Budget Requests
JvT3qFqkpb679mvs3rxJ5g,These days docker and Cloudera are garnering a lot of attention I have written this blog on how to deploy the Cloudera Quickstart VM using Docker on Mac OSInstall Docker Desktop using these official steps: https://docsdockerShow the list of repositories available in dockerExecute the `docker run` command with the following arguments: hostname  quickstartcloudera and interactive mode-p describes a process running on a given port$ docker run --hostname=quickstartTo manage the Cloudera cluster with Cloudera Manager it will give you a UI to manage your cluster and configurationNow the Cloudera Manager is running$ docker stop  express=60 <Container ID> | <Name>eg
PGmabj7aMotHnmG7uhSWkZ,Apache Spark has quickly become one of the most heavily used processing engines in the Big Data space since it became a Top-Level Apache Project in February of 2014 Not only can it run in a variety of environments (locally Standalone Spark Cluster Apache Mesos YARN etc) but it can also provide a number of libraries that can help you solve just about any problem on Hadoop This includes running SQL Queries Streaming and Machine Learning to name a few All running on an optimized execution engineWe at Clairvoyant have built many Data Pipelines with Apache Spark including Batch and Streaming over the years You can find out more information here After having built so many pipelines weve found some simple ways to improve the performance of Spark ApplicationsBy using the DataFrame API and not reverting to using RDDs you enable Spark to use the Catalyst Optimizer to improve the execution plan of your Spark JobJava Regex is a great process for parsing data in an expected structure Unfortunately the Regex process is generally a slow process and when you have to process millions of rows a little bit of increase in parsing a single row can cause the entire job to increase in processing time If at all possible avoid using Regexs and try to ensure your data is loaded in a more structured formatWhen youre joining together two datasets where one is smaller than the other put the larger dataset on the Left: When Spark shuffles data for the join it keeps the data you specified on the left static on the executors and transfers the data you designed on the right between the executors If the data thats on the right thats being transferred is larger then the serialization and transfer of the data will take longerIn many cases we will be joining smaller data sets (a couple dozen or so rows maybe a bit more) with larger data sets In this case its more performant to use a Broadcast Join: If you find you are constantly using the same DataFrame on multiple queries its recommended to implement Caching or Persistence: Note: Avoid overusing this Due to Sparks caching strategy (in-memory then swap to disk) the cache can end up in slightly slower storage Also using that storage space for caching purposes means that its not available for processing In the end caching might cost more than simply reading the DataFrameBefore querying a series of tables it can be helpful to tell spark to Compute the Statistics of those tables so that the Catalyst Optimizer can come up with a better plan on how to process the tablesIn some cases Spark doesnt get everything it needs from just the above broad COMPUTE STATISTICS call It also helps to tell Spark to check specific columns so the Catalyst Optimizer can better check those columns Its recommended to COMPUTE STATISTICS for any columns that are involved in filtering and joiningThe default value for this is 200 which can be too high for some jobs Set this configuration to the number of cores you have available across all your executorsReferences • https://medium
a2oTgWdBnm2rgBFDzcyqaj,"Pre-requisites:1 Docker installed 2Follow this link to install the docker and the Cloudera image Follow this link authored by Akash Patel to upgrade it to CDH 516This article focuses on installing the Cloudera services on your local system with the help of Docker Docker as we all know is better in so many ways than the usual Virtual Box Both of them essentially do the same job but the docker uses fewer resources from your computer thus delivering better performance and avoiding the usual headaches during the deploymentConsidering that you have followed the blog in the link above and started the docker services I am going to proceed with my explanation to install JDK 18 This is because by default the CDH image of docker comes with JDK 17 and if you are someone who wants to learn spark2 along with CDH and its other services then you should know the importance of JDK18 alreadyOnce the CDH image has been installed you can check the processes running in the docker using the below command: As you can see the names now there are two ways to start your CDH; the first one is manually starting all the ports using the below command: If you followed the previous blog on how to upgrade to CDH 516 you can use another easy way to start the service by using the name which here is youthful_haslett Yes I know it sounds weird But there should be some reason behind these naming conventions The command to perform this is given below: Now use the container ID of your process which is fa825ad78e9f here To start the container use the below command: Once it executes the above command you are going to see something like this on your system: As shown below the java version inside the docker will be JDK17Now to upgrade the JDK to 1Before starting we need to stop all the Cloudera services for which we can use the commands shown below: Install JDK18 using the below command Doing this is going to update all your CDH services but we should also make sure that we are changing the environment variables in your bash profile so that your terminal picks up the path by defaultNow this is one of the most important steps to avoid calling JDK18 manually every time Edit the bash profile as shown below Do not forget to source the bash profile once its editedNow change your working directory to /etc/bashrc and add the following pathsAdd the following path to all the files shown belowTo see all the changes reflected we need to start the services againCheck for the version of Java installed nowOnce all the above steps are done we can log in to http://localhost:7180/ to see the UI of Cloudera Manager The next step is to go to Hosts → All hosts →configuration(on right top) and search for java which is going to look like the image belowHere you need to enter the path of the new JDK18 that has been installed""Take the path /usr/lib/jvm/jre-180-openjdkx86_64 and paste it in the java home directory in the Cloudera hosts configuration Once this is done restart the Cloudera management services and the quickstart on the CDH manager's home page This will help to restart all the stale configurations Once all these are done your services are going to glow greenUse this command to see if your services are picking up the right JDKYour service should start with JDK1So now if you check Cloudera manager for the JDK information Support → About you will find the information as shown below"
FuiFWWhFxJRUjFYSgbcysr,Elastic Map Reduce one of many Amazons Big Data Analytics products has been out for a while now It was first released on 2009 with automated Hadoop+Hive provisioning basic job management and S3 transfer optimizations Since then it has been actively developed to support more and more Big Data technologies The latest release of EMR is 580 as of todayOur workloads consist of several dependent tasks to compose Data Pipelines These Pipelines are represented as DAGs in order to control execution order inputs outputs resources etcMost of those tasks are written in the Spark Python API (PySpark) whose dependencies vary greatly between them That said our fondness for Spark and the Hadoop ecosystem is no secretTherefore the decision whether to use EMR Hadoop on EC2 or bare metal Hadoop on premises came down to the characteristics of our workloads: With the goal of optimizing resources EMRs Hadoop is a modified version of the plain vanilla Hadoop highly integrated with the AMIs OS and using their own Linux repositories for dependencies like pythonHere is a screenshot of the Linux version of EMR 570: And the default yum repositories: By default Amazons python 26 27 and 34 are installed: So even though this works for most cases it was an important issue for us due to the heavy use of specific python versions and dependencies that may not be compatible if installed on a global scopeOne option to work around this issue was to simply create and bootstrap each cluster differently for each specific workflow requirements This didnt sound like a bad idea due to the effortlessness of provisioning clusters on demand But a closer look showed a big increase in complexity when chaining several tasks into a single pipeline Not to mention the costs of having multiple clusters for the same work This would look like this: A better solution that we ended up implementing was to isolate each task environment using pyenv This made it possible to have multiple python versions and dependencies that will not conflict with each otherNow each pipeline can have its own EMR cluster or even share a single one (with enough resources for all the tasks) The tricky part was to install pyenv on each node of the cluster and correctly set up the spark options to use it: After some trial and error we managed to install pyenv using the bootstrap option that EMR expose for your cluster customization The following is a snippet of a bootstrap script used to install and configure pyenv with python 35 and some dependencies: Now the final step is to add spark options to a spark-submit script when launching our PySpark tasks: EMR has proven to be a cost-effective easy yet powerful solution to most Big Data Analytics tasks The simplicity to provision clusters combined with the flexibility that pyenv gave us to isolate python environments and dependencies has been invaluable to our products developmentThe solution shown above may not fit all use cases but certainly could be used as a reference of the things that can be done with EMR
AAyU6T8xHJDZbcPfVAUSPm,2019 was another exciting year for Emumba which brought a lot of wins challenges opportunities frowns and smiles Overall we kept our heads down and continued to run the marathon grew our team won new customers increased our global footprint and most importantly expanded our leadership team More than the destination it is the journey that counts and I am happy to share some exciting moments from 2019Creating a place where the delineation between learning and fun diminishes has been one of the guiding principles of setting up the workplace at Emumba We ran an intensely contested photography competition celebrating the culture of twin cities Islamabad and Rawalpindi At the risk of displeasing the winners I am sharing the shot that I liked the bestWe successfully set-up Emumba Toastmasters club to nourish and hone public communication and leadership skills in the team I look forward to some of our shy people becoming prolific speakers in coming months Im a personal beneficiary and a fan of this amazing program and am thrilled to have this going at EmumbaWe had our first football tournament  7 a side  where one of the teams was kind enough to include a 45 years old wannabe like me into the mix To my teams credit we made it to the finals and to my credit I lasted that long! Irrespective of who won and who lost it was a beautiful team building activity for the 6 participating teams lasting 16 matches I was particularly happy with the sportsmanship demonstrated by all players throughout the tournamentAnd while I missed accompanying the team I could not stop drooling over the photos that came out of the trip to the beautiful North of Pakistan I am not allowed to share any stories since what happens in the North stays in the NorthIn the spirit of learning relentless learning that is we continued to have internal SIGs (Special Interest Groups) on Design/UX Management & Leadership Networking Back-end Engineering and Quality Assurance It is really satisfying to see these self-organized groups maturing in their approach towards regular meets and the appetite to learn share and grow together as a group We took another step in our attempt to engage community outside of Emumba and arranged many Technology SIGs in partnership with OPEN Islamabad We brought some amazing speakers and conducted sessions on cutting edge topics covering DevOps FinTech IOT OPEN Stack and SDN I want to extend special thanks to some our international visitors who made guests appearances and shared wealth of information Gracias Affan Dar Kamran Kundi & Zoaib KhanI am ecstatic to have welcomed top leaders and engineering minds who joined Emumba in 2019 Surrounding myself with people who are much more capable than myself is a dream come true Dr Affan brings wealth of knowledge experience and fan-following to the company I continue to discover amazing accomplishments by Faisal Ghias Mir every time I meet him half of which I only understand only partially! I hope he has finally found a lasting home after his return from Germany at NEC labs Waqqas Jabbar is a technical authority in the areas that he has worked in I have lost count of the times when peoples jaw dropped when they learned that he works for Emumba Its amazing how one individual brings so much credibility to an organizationOn the same note I want to welcome the super advisors in the US and the UK who joined Emumba in 2019 They are all inspired by the mission Emumba is pursuing and I cannot be happier to call them our team Aamir Waheed brings wealth of experience in the space of networks Adnan Shafi specializes in DevOps BI & Analytics Weqaar Janjua is a lead researcher in DPDK and we were ecstatic to have him advise our FinTech and DPDK groupsWe grew our Engineering Design and Products team by 40% and are now a constellation of 150 stars We added 6 new customers during the year and continued to serve the many that we were already working with And while the Silicon Valley continues to be our favorite destination we have added new geographies to the mix including Dallas Cambridge and LondonWe are very close to becoming Amazon Partner at the Select Tier where we start getting leads and possibly reselling AWS We felt proud to graduate our FinTech group from Emumba-Labs Led by Ali Rizvi this team is now 20 strong and doing some amazing work in accelerated software DPDK and low latency optimization We have in parallel continued to invest in the back-end and data engineering groups under the leadership of the impeccable Haris Hasan Amongst many accomplishment the team deployed enterprise grade data pipeline systems with one of our very large enterprise customers in the US We have continued to invest in our IP and have developed some amazing libraries that can help build stunningly beautiful dashboards at the speed of light And finally our journey in microservices and Kubernetes is becoming increasingly enjoyable as the team continues to win certificationsThere are many more things to share but this has all been possible because of the top talent Islamabad produces that we are able to acquire and retainWe welcome 2020 with hope positivity commitment and best wishes for everyone Cheers to Pakistan and Emumba
QwzUYUiFmavv65Zznd2PH6,The natural state of the universe is chaos: entropy tends to increase in closed systems and theres really nothing that we can do about that So too is the nature of data warehouses: unless action is taken to maintain orderMore posts by Claire CarrollThe natural state of the universe is chaos: entropy tends to increase in closed systems and theres really nothing that we can do about that So too is the nature of data warehouses: unless action is taken to maintain orderin your data warehouse it will inevitably spiral into a hard to navigate hard to operate collection of objects that youre too afraid to delete While maintaining databases was once the job of dedicated Database Administrators these days its common for no one (or rather everyone) to be directly responsible for maintaining order in the databaseAn actively maintained warehouse is a critical component of the highest functioning data teams In a poorly maintained warehouse: And in such warehouses it becomes difficult to answer seemingly simple questions such as: A well maintained warehouse makes is easy for users to find relevant datasets and answers their own questions While theres no replacement forreally good data documentation the application of consistent conventions goes a long way towards empowering end-usersThis post reflects our best-practices for maintaining analytical data warehouses based on years of experience working with data across many organizations and data stacks Weve distilled our experiences into five principles that we feel to be true in any well maintained warehouse: For each principle weve also shared our practices that help us implement these principles Like much of what we write these practices are opinionated and wont be right for every organization! We think theyre a great starting point for anyone who has suddenly found themselves in charge of administering a warehouse without having solved these problems beforeSchemas should be used to logically group together objects similar to using directories when organizing files Within a single schema: We tend to: Its been said many times over: naming things is hardIn a data warehouse you have a lot of objects to name  databases schemas relations columns users and shared roles On Snowflake you have even more things to name warehouses (ie compute resources) stages and pipes for instanceUsing consistent naming patterns helps reduce the number of decisions to be made when creating objects and can make it easier for a user to understand the structure of your warehouse That being said they can go too far! Ive seen some warehouses that implement so many conventions that they become harder to navigate unless you know the rules (for example d_account_vf_active instead of accountsis_active)Some of the patterns we use when naming objects in a warehouse include: Once youve settled on your naming practices we recommend codifying them into conventions and sharing them with anyone using your warehouse Weve written down our specific naming conventions in our dbt coding conventionsBefore we dive into this its worth noting that different data warehouses treat users groups and roles in very different ways (for anyone else that wants to fall down this rabbit hole I wrote my findings up on Discourse)As such its pretty hard to give advice here that isnt tied to one specific data warehouse! For the remainder of this article Im going to abstract users groups and roles into two entities: We always use a separate user for each human being and application connecting to a warehouse We name these users in a way that makes sense with names like claire drew stitch fivetran and looker This makes it easier to manage access debug when something goes wrong and understand data lineage Credentials should be securely stored and only admins should have access to the passwords for application usersWith so many users it can become difficult to manage privileges Instead we grant privileges to shared roles (more on this below) with users inheriting their privileges via their role membership We tend to create a small number of shared roles named loader transformer and reporter  even if your data warehouse allows them multi-level hierarchies for shared roles should be avoided as they often become confusing to manageData warehouses provide fine-grained privileges you can grant any combination of the following privileges to a user or a shared role (in fact these are just a subset of your options!): Theres a balance to be found between being too generous with privileges where you grant all privileges to the public group and being too restrictive with privileges where a super user has to run a series of complex grant statements for every user that wants to run a select statementAn overly permissive privilege scheme can lead to irreversible mistakes being made while an overly restrictive privilege scheme can become a blocker for database users that just want to get their work doneAt the same time you should consider the complexity of your privilege scheme  complex schemes can be difficult to maintain With too much complexity it becomes unclear who has access to what and which grant statements should be run when a new user or schema is addedWeve found that a simplified privilege structure applied to shared roles provides a balanced level of control and is easy to maintain In our data warehouses we implement two privilege styles: We then grant the following privileges to each of the shared roles in our warehouse: Ive done a very implementation-focused writeup of the exact statements involved in setting up a warehouse in this way over on DiscourseUsers should inherit their privileges via their memberships of these shared roles This scheme is extremely easy to maintain  when a new user is added to our data warehouse we only need to ensure they are members of the appropriate shared roles Similarly when a new schema is added to our data warehouse we only need to grant privileges to the correct shared rolesSuperuser privileges (for example the ability to create users or drop relations that do not belong to you) should be limited Unless running a command that specifically requires these elevated privileges users should use a user/role that does not have these privilegesFirst we limit access to the root user on Postgres and Redshift or the AccountAdmin role on Snowflake We only share this password on a need-to-know basis (but make sure it wont be lost if someone leaves the organization!)Further we ensure users use a non-superuser as their default The implementation of this varies across data warehouses based on how the warehouse handles inheritance: In short good database administration can be summarized by two key themes: If youre getting started with a data warehouse spending some time upfront thinking about database administration will pay huge dividends for the future
72YPLfL6Cr372cQMPvWjuT,"""The hardest thing about scaling a company is communication Here's how our latest release will make data communication just a little bit easierBuilding the modern analytics engineering workflow Founder & CEO @ Fishtown Analytics builders of dbtMore posts by Tristan HandyThe more people that work at a company the more nodes in the network the more links between them In a completely decentralized network the number of connections is proportional to the square of the number of nodes In a hierarchical network each node is only connected to X peers which limits complexity but also significantly decreases the flow of information The reason why messaging platforms like Slack are so important today is that they reduce communication friction allowing companies greater flexibility and/or greater throughput in how they design their communications network architectures Or in people-speak: their org chartsIf youve ever been at a company that has grown from 20 to 500 employees you will have felt this process in your lived experience What were once impromptu conversations turn into scheduled meetings Your calendar starts to come under pressure You simply spend a greater percentage of your time saying or typing words to other peopleThere are three types of knowledge that people within a company need to communicate to one anotherFirst there is tactical knowledge What is the status of this project? Who is working on that activity? When will that dependency un-block me? What are our financial goals this quarter? There is a huge industry of software products (ie Trello) and collection of methodologies (ie scrum) that are dedicated to collecting and disseminating this kind of knowledgeSecond there is cultural knowledge Culture isnt a mission statement or a set of values: culture is the way a group of people make decisions in support of the companys values Founders cant be present for every decision but the best ones build cultures that allow their employees to make decisions from the same core values that they wouldThe best post Ive ever read on this topic is by Brian Chesky CEO of Airbnb called Dont Fuck Up the Culture The whole post is beyond phenomenal but the payload is here: People can be independent and autonomous because they all share the same knowledge about how to make decisionsThe third type of knowledge and ultimately the subject of this post is factual knowledgeAs companies grow there is tremendous pressure to produce more factual knowledge Growth begets bigger budgets new people new processes new technology all aligned around new and bigger initiativesUndertaking new initiatives without supporting factual knowledge doesnt necessitate failure but it significantly lowers your chances Growing a company is like a poker tournament: you might win a couple of lucky hands early but ultimately youre going to have to make good decisions if you want to win the whole thing And good decisions are founded on a solid understanding of factsWith new and cheaper fact-generating technologies like data pipelines data warehouses and BI tools companies today are generating more facts than ever But what weve come to realize over the past two years is that almost no companies are good at disseminating factual knowledge Whats more most companies dont even realize they suck at it Im even going to go out on a limb and suggest\u200a\u200abegging your pardon\u200a\u200athat your company probably sucks at disseminating factual knowledgeIf youre a data analyst or engineer you might not be aware of this problem specifically because your entire job is to be a librarian of facts Of course you know where to find them! And even if you dont you know the arcane internal systems you have to use to look them up But you dont make day-to-day decisions in product marketing finance or ops And most of your counterparts who do (aka business users) have a common experience repeated multiple times a day: This is a communication failure In order to disseminate factual knowledge it is insufficient to simply disseminate data Factual knowledge must include the data themselves as well as the knowledge about how those data were produced This is why scientists dont just upload CSVs they write lengthy papers describing how other scientists can reproduce their resultsThe answer that hierarchical companies give their employees to the trust question is: because I said so Have you ever heard the phrase Are those the blessed numbers? Who blessed them\u200a\u200aa corporate data guru?! Blessed means You dont have to worry about how these facts were produced just trust usThis answer wouldnt support the distributed entrepreneurial culture that Brian Chesky wanted at Airbnb so theyve worked hard to build something betterAirbnb has been thinking about this problem since at least early 2016 when they released the post Scaling Knowledge at Airbnb The post like Brians is singular\u200a\u200ait described the problem with incredible clarity Heres my favorite line: AmenThe post also demonstrates the investments that the company had made in solving it their own knowledge scaling problem At the time Airbnbs solution was a collection of R Jupyter Markdown and Git and the result was a Shiny app that looked like this: The project has matured since then and all code is available under the Apache 20 license Its core features\u200a\u200aa) data and narrative colocated and versioned together b) universal knowledge search\u200a\u200aare prescient But unlike other Airbnb data projects like Airflow and Superset Knowledge Repo hasnt taken off My personal guess is that its too idiosyncratic to Airbnbs particular needsOther companies with impressive data credentials have worked on this problem as theyve experienced it themselves Linkedin (originator of Kafka) built WhereHows Uber just released Databook Companies scaling fast are experiencing this problem and they are devoting real resources towards solving it But none of their solutions has yet broken out(need a refresher on dbt? see thisdbt 0110 includes a massive new feature called Docs Docs is the most requested feature from our community ever Its a browser GUI that helps users\u200a\u200aany users\u200a\u200aunderstand the provenance of their data Weve invested heavily in its UX and in the past month during the beta weve gotten wildly positive reactions If you want to see what the fuss is about check out Drews detailed release postI wanted to write this post though to plant a flag in the ground dbt today is used by 250 companies to transform raw data in their data warehouses But most downstream users of this data dont have any access to the knowledge of the workings of this data transformation process dbt is opaque to them To say that another way: With dbt 0110 were saying that this is a problem\u200a\u200adbt needs to promote not hinder knowledge dissemination Transforming raw data into useful information is a part of that process but its not sufficient on its own dbt Docs explicitly promotes transparency throughout the entire DAG helping data engineers and analysts expose the provenance of data to others on their team and throughout the broader organizationWith Docs users can understand where your data comes fromWe have big goals for dbt Docs many of them shaped by the smart folks at Airbnb Linkedin and Uber who have trodden this path before us The version weve released today is a huge step forwards but its just the beginning As more users get their hands on Docs well be learning and evolving with you so pleasethe more feedback the betterOne final note because I know youll ask: weve released Docs under the Apache 20 license making all of its code fully open source This is a big deal to us There are a lot of proprietary tools out there that purport to be your single source of truth Well\u200a\u200aI dont want anyone else to own my truth If your pricing strategy impacts my ability to know basic factual knowledge about my business thats a risk Im just not OK withYou already invest tens of thousands of person-hours every year maintaining this knowledge base; it should live within software that you controlThanks for reading Ill see you in Slack"
Gb3CzFTB7aHcfBj63xbH7h,A quick guide on what (and how) to monitor to keep your workflows running smoothlyIf your organisation deals with a significant amount of data and has huge data pipelines chances are you must have used or heard about Apache Airflow already Airflow is an open-source workflow management platform that enables scheduling and monitoring workflows programmaticallyAt Gojek our products generate a tremendous amount of data but thats only step one Were constantly making use of that data and give value back to our customers merchants and partners  in the form of recommendations and other customisations The Data Engineering (DE) Team is responsible for building the platform and products to manage the entire lifecycle of dataNeedless to say Airflow is one of our most heavily used tools We cater over a thousand pipelines and an enormous amount of data using Airflow Monitoring all these pipelines is not easy  especially considering that Airflow is still in its early phaseLike any production application it becomes crucial to monitor the Airflow jobs and of course Airflow itself It has a very resilient architecture and the design is highly scalable It has multiple components to enable this viz Scheduler Webserver Workers Executor and so on At Gojek we have a few additional processes as well to enable flexibility for our workflowsFor example we have a separate process running to sync our DAGs with GCS/git and a separate process to sync custom Airflow variables We know very well that the more components you have higher the chances of failure Hence this requires a thorough monitoring and alerting systemAt a high-level we have multiple Airflow processes running in our different Kubernetes Pods and each of them has a statsd client enabled using airflowcfg The statsd client will send all the metrics to Telegraf over UDP Our custom processes are also emitting those heartbeats and other data in the same wayWeve configured InfluxDB as an output for Telegraf configuration (telegrafconf) which will send the data over HTTP You can add InfluxDB as a data source in Grafana as well as in Kapacitor The alerts can now be configured in Kapacitor using TICK scripts which well cover in the next sectionsAirflows implementation and documentation of metrics are not the best things about it and its still in the early stages In the first attempt the measurements created by Airflow in InfluxDB were not how we wanted them to be We solved that by writing some custom statsd telegraf templates based on the metrics name Some of them are: These templates will create some meaningful measurements named prefix_dag_duration prefix_dagrun_schedule_delay prefix_dag_loading-duration etc in InfluxDB which can be easily queried using Grafana/Kapacitor Following is a sample of how the fields (or tags) are parsed in InfluxDB: The tags enable higher flexibility for querying and filtering the data We built a Grafana dashboard on top of it as the first goal to see whats happening under-the-hood A sample query to generate a time series trend in Grafana for the above measurement is: The following list contains some of the important areas that you should monitor which could also be helpful for debugging and finding bottlenecks for resources: Its important to track these metrics at an overall level as well as individual tasks and the DAG level You should also consider tracking your specific operators and tasks that you think have higher chances of failure and/or consume more resourcesNow that we have data in InfluxDB and the monitoring is in place we can use Kapacitor to write the TICK script to trigger alerts based on the checks and thresholds The following snippets show some sample alerts that can be setApart from this it is also important to monitor the health of InfluxDB Kapacitor and Grafana as wellThank you everyone at the DE team  we build and manage the components described above and special thanks to Sravan for the discussions and pairing If youve questions let us know in the comments or tweet meLiked what you read? Get our stories delivered straight to your inbox by signing up for our newsletter
HJbeYpFGiMNomckEPhfrff,When in-person technical interviews are no longer an option hiring managers still have a wealth of online resources at their disposal With a properly designed interview process teams can effectively and efficiently assess whether an applicant has the right set of data engineering skills without having to physically be in the same roomSince 2014 Insight been successfully running a fully distributed and fully remote interviewing process that has helped us sift through thousands of applications and identify top-tier candidates who have joined our Fellowship programs and gone on to work as data engineers at Netflix Facebook Vanguard Apple Bosch and others We recently shared some of our best practices during a live webinar (you can watch the video here) These are some of our main takeaways: Cassie Stover Insights Director of Admissions has spent several years fine-tuning a large-scale application and interviewing process that allows her and the rest of the Insight team to review more than 12000 applications and conduct upwards of 3700 technical interviews annuallyWith such a high volume of interviews standardizing the interview process helps keep everyone organized and provides a consistent candidate experience to maintain a level of fairness and integrity in the process Stover recommends streamlining the many different touch points in the process for instance creating email templates so that all applicants receive the same amount of information in the same mannerWhen there are many candidates going through the interview process with multiple steps at each stage its important to pick tools that integrate with one another automatically For instance having tools that automatically integrate interview scheduling (eg ClaraAI) with a companys calendaring system (eg Google Calendar) remote video platform (eg Zoom) and applicant tracking system (eg Greenhouse) will help ease the interview processConsistently communicating with your applicants on what they can expect at each stage of the process such as setting the tone the format of the interview and the timeline is also important In some cases its better to over-communicate expectations with applicants because it puts them at ease and allows them to feel better preparedAs Director of Engineering at Insight Hoa Nguyen has personally interviewed and evaluated hundreds of applicants for our Fellows programs Shes also worked closely with many companies to design and improve their interviewing processes to find the best possible engineering talent for their teamsBefore starting the process of evaluating your applicants its important to identify who you are hiring and what skills youd like to have That information is key to creating and tailoring your technical assessments toward those skillsHaving run a data engineering program at Insight for several years weve identified three broad categories of data engineers: Based on the work data engineers do they must have certain technical skills such as the ability to code and work on backend software development interest and experience working with diverse and growing data stores as well as the knowledge and ability to deal with engineering challenges around scalability optimization resilience and maintainability And finally because data engineers often work with different user groups they must be communicative and collaborativeAt Insight our Data Engineering Fellows program is a collaborative learning space for people with strong computer science fundamentals who want to transition to data engineering through hands-on experience with distributed computing technologies That means Fellows must be able to code and put together a project using version control in a Linux environment They must also have the potential to grow and benefit from the Insight Fellowship experience and finally fit well into our collaborative and congenial environmentOur interview process at Insight consists of a coding assessment application review technical video interview and any necessary follow-upsWhen it comes to coding assessments there are may different types widely used in industry One of the advantages of using a coding assessment is the evidence they can provide regarding candidates technical abilities Also some candidates may be unable to share past projects they have worked on due to non-disclosure agreements Having an independent technical assessment that allows candidates to showcase their skills can help Providing a coding assessment also allows the interviewer and the candidate to be on the same page during any subsequent interviews so they can discuss the work in more depth and dig a little deeper into some of the technical detailsSome of the disadvantages of using a coding assessment are that it does take time and in some cases if you use an off-the-shelf assessment tool (such as HackerRank) it will cost money Another major disadvantage with using an assessment tool is that if you use the same one for multiple candidates theres a high likelihood it will have been shared on the internet (eg these are common on Glassdoor) so there is a possibility that cheating may occurWhile many companies use coding assessments with computer science algorithm questions (commonly found on platforms such as LeetCode and HackerRank) other companies may opt for a more open-ended take-home problem For example they may give applicants access to an API and ask them to query data that satisfies some criteria or they may share a large dataset and asking applicants to perform some sort of data transformationAt Insight we use a hybrid approach where we give applicants a link to a problem statement detailed on a Github The problem requires use of one or two foundational data structures and details some sort of analysis that wed like performed on a dataset Each submission is run through a series of tests to ensure that the desired output is produced Each submission is also manually reviewed by a program director who is looking for code quality and software best practicesCoding assessments are reviewed first  even before the application is read That allows us to reduce bias and fairly evaluate the coding assessment The results of the coding assessment can help overcome gaps in a candidates technical background while a poorly executed assessment can temper the experience an applicant lists on a resumeAfter the coding assessment we review a candidates application in advance of a technical interview so that we can be prepared for the areas we want to dig into during the technical video interview The agenda for our video interviews are high-level and open-ended primarily consisting of introductions walk-through of the coding assessment and then a conversation with the applicantReviewing the coding assessment during a live video interview is generally about ensuring that the applicant wrote the code and put some thought into how they solved the problem We will also use the coding assessment as a starting point in assessing their interest in system design and working with data as challenges such as scalability become a factorAfter the coding assessment we use the rest of the time during the video interview chatting with applicants about their career aspirations and what they hope to get out of InsightFollowing the video interview we put together our notes on how the interview went Most importantly if we ran out of time and there were questions that were left unanswered or there was more to discuss its important to note all of that information At that point we will sometimes recommend that a second colleague conduct a follow-up interview to address any remaining questionsFor additional resources to help you develop your own remote interview process check out our video as well as our Guide to Conducting Remote Interviews In this guide weve compiled advice based on Insights own experience operating a 100% remote interview process for our Fellows programs as well as best practices amassed from working with hundreds of hiring teams ranging from small startups to Fortune 500 enterprises These tips will help you build a remote interviewing process that fits your needs and enables you to make sound decisions as you grow your teamThinking about adapting your existing processing to remote? Sign up for our monthly newsletter or get in touch with our team to learn more
kyY5Y5tE242syE7uQtoJ8o,Want to learn Streaming technologies and other big data tools from top data engineers in Silicon Valley or New York? The Insight Data Engineering Fellows Program is free 7-week professional training where you can build cutting edge big data platforms and transition to a career in data engineering at top teams like Facebook Uber Slack and SquarespaceLearn more about the program and apply todayDuring the seven-week Insight Data Engineering Fellows Program recent grads and experienced software engineers learn the latest open source technologies by building a data platform to handle large real-time datasets Ryan Walker (now a Data Engineer at Casetext) discusses his project of building a streaming search platformOn average Twitter users worldwide generate about 6000 tweets per second Obviously there is much interest in extracting real-time signal from this rich but noisy stream of data More generally there are many open and interesting problems in using high-velocity streaming text sources to track real-time events In this post I describe the key components of a platform that will allow for near real-time search of a streaming text data source such as the Twitter firehoseSuch a platform can have many applications far beyond monitoring Twitter For example a network of speech to text monitors could transcribe radio and television feeds and pass the transcriptions to the platform When key phrases or features are found in the feeds the platform could be configured to trigger real-time event management This application is potentially relevant to finance marketing and other domains that depend on real-time information processingAll code for the platform I describe here can be found on my github repository Straw The code base includes: I completed this project as a Fellow in the Insight Data Engineering Program The original inspiration for for this project came from two excellent blog posts on streaming search: The key data structure for solving a traditional text search problem is an inverted index built from the collection of documents you want to be able to query In its simplest form an inverted index is just a map whose keys are the set of all unique terms in the documents The value associated to a particular term in the map is a list of all the documents which use that termAfter the index has been built users can submit queries to run against the index For example we can have a query that should return all the documents that contain both words in the phrase llama pajamas The query engine will split the input phrase into the tokens llama and pajamas then it will check the inverted index to get the list of all documents that contain the word llamas and the list of all documents that contain the word pajamas The engine will then return the intersection of these two lists ie the list of the documents that are present in both listsIn the streaming case documents arrive at a very fast rate (eg average of 6000 per second in the case of Twitter) and with this kind of velocity and volume it is impractical to build the inverted document index in real-time Moreover the goal is not to create a static index of tweets  rather it is to scan the tweets as they arrive in real-time and determine if they match a registered query Heres where we can play a clever trick Instead of building our inverted index from the documents we can instead build the index from the queries themselvesAs a simple example suppose a user wants to see all the tweets that contain the word llama and pajamas To add this query to the inverted index we would: As tweets arrive in the stream the query engine will break the text into tokens and then query engine would return the intersection of all the list values whose key is a token in the inverted indexFortunately there are already several existing tools which can be used to build an inverted query index: Now that weve got the basic tools for streaming search (Elasticsearch-Percolators or Lucene-Luwak) lets describe the architecture for the platform The Straw platform is made up of the following components: The Twitter streaming API does not offer access to the firehose without special permission To see how Straw would perform under firehose level load we can instead use the sample endpoint to collect a large corpus of tweets We can either store these tweets in a file or alternatively send them directly to the Kafka clusters documents topic: Alternatively we can load tweets from a file into Kafka with a simple producer script: To maintain a high load we can run multiple instances of this script and restart the script as soon as it finishes reading the file using for example a supervisorThough the Straw project was designed for handling discrete JSON documents by change of the internal parsers it could be very easy to use other formats like XML A more interesting challenge is handling continuous stream data such as audio transcriptions In this case several strategies could be tried For example we could detect sentence breaks and treat each detected break as a separate document in the streamThe Kafka cluster has two topics: documents and queries The producer script above can be used to populate the documents topic The frontend client populates the query topic with user subscriptions In production I found a 5 node Kafka cluster could easily accommodate Twitter level volume For the documents topic I used a partition factor of 5 and a replication factor of 2 While high availability is very important to accommodate the volume of the stream document loss may not be a big concern For queries I used only 2 partitions with a replication factor of 3 Queries are infrequent so availability may not be important but query loss is not acceptable Note that the partition factor should be less than or equal to the number of KafkaSpouts in our Storm topology since each spout will consume from exactly one partitionOne other important Kafka configuration is in kafkaserverproperties: The Kafka default is 168 hours  far too big since you can easily fill a modestly sized disk under load As messages should ideally be consumed in real-time I recommend using the minimum value which is 1 hour Note however that you may still need to ensure that you have a sufficiently large volume for the Kafka log In production I gave each Kafka node a 64GB volume with a 1 hour retentionThe Storm topology implements KafkaSpouts for the documents and queries topics In production I used 5 document spouts and 3 query spouts (consistent with the Kafka partitioning) The bolts in the topology search the document stream and publish any matches to Redis In production I allocated a total of 6 workers Sizing the cluster correctly proved to be somewhat challenging I highly recommend this post which explains the key concepts of Storm parallelism Also the Storm built-in UI can be helpful for monitoring and understanding how the cluster is performingIn the most basic scenario we assume that the number of queries is small and can fit easily into memory on a single machine Then scaling to the volume of the stream is quite easy The idea is to give each bolt a complete copy of the in memory Lucene-Luwak index (remember that the queries are whats being indexed here) So each time a user registers a new query we must broadcast it to all of the bolts in the topology to maintain the local query index When a document arrives from the stream we can then randomly assign it to any bolt since each bolt has a full copy of the query index To handle failover we can also keep a global copy of the all the queries so that if a bolt dies we can replace it with a new one and populate its index from the global store This Java snippet defines such a topology: Since this platform is intended to be multiuser and multitenant we can easily imagine a situation where the number of queries cant practically fit in memory on a single bolt In this case we can add another layer of bolts to the Storm topology: Here the complete query index is partitioned across a small cluster of bolts Incoming queries are broadcast to the fan bolts Each fan bolt will then randomly choose one Lucene worker to index that query Documents from the stream can be shuffled among the fan bolts Each fan bolt must then broadcast the document so that each Lucene bolt can check the document against its partition of the indexIf we use Percolators instead of Luwak then each bolt contains an Elasticsearch client In this case it is a good idea to collocate the Elasticsearch cluster with the search bolts and to use high replication so as to minimize network overhead Note that Percolator queries are also stored in-memory so we still face difficulties as the number of queries becomes largeRedis is most commonly used as an in-memory application cache but it also has a simple and elegant publish-subscribe framework Heres an example of pubsub using the Redis-cli: In a terminal A listeners subscribe to a topic: In a separate terminal B the publisher publishes to the topic: Back in terminal A the subscriber receives the message: Thats all there is to it All standard Redis clients expose an API to interact with the PUBSUB frameworkWhen a user registers a query in the Straw platform heres what happens: Using the hash as the query ID allows two or more users to subscribe to the same query while only needing to actually index a single queryA client for Straw has the following duties: The Straw platform comes packaged with a default client which is a simple Python Flask webserver The webserver is sessionized so that users can follow particular queries The server implements a basic Kafka producer to publish queries to Kafka and Redis keeps track of the list of subscribed query IDs for each user The listening is handled by a single background thread that holds a Redis client subscribed to all unique queries across the entire set of active users When a query ID and document pair are found the background thread queries Redis to find which users are subscribed to that query ID It will then copy the document text to a result pool for each subscribed user The user interface checks the users pool for updates every half-second so that results stream into the console Here is a video of UI in action: One goal of the Straw project was to compare and measure performance of Elasticsearch-Percolators vs Lucene-Luwak Measuring this performance is not completely straightforward I used the following very basic approach to measuring throughput: By monitoring the benchmark channel in Redis we can then track the search throughput of the system Pictured below are density plots for estimated total throughput per second obtained by running this procedure for several hours: Some comments and conclusion about these preliminary estimates are in order: Interested in transitioning to career in data engineering? Find out more about the Insight Data Engineering Fellows Program in New York and Silicon Valley apply today or sign up for program updatesAlready a data scientist or engineer?Find out more about our Advanced Workshops for Data Professionals Register for two-day workshops in Apache Spark and Data Visualization or sign up for workshop updates
hNSHpihJgbyt94DLXBmVXu,"Distributed computing frameworks often require the creation of a set of machines that work with one another to perform some sort of large-scale data processingThis cluster of computing resources can be provided through a variety of services For instance Amazon Web Services offers a service called EMR which allows the user to quickly spin up a Spark cluster and configure how many machines will make up the cluster In other cases programmers might use tools such as Terraform to programmatically create and configure a set of machines that can communicate with one anotherFor the beginner it might be helpful to learn how to create a cluster of instances using user-friendly web console utilities provided by AWS then access those machines from your laptop and finally configure them to communicate with one anotherThis blog will take you through the steps for how to create a cluster of four machines on AWS and then ensure that from one of those machines (designated the master) youll be able to connect via ssh to the other (worker) machinesTo complete the steps you must have a Unix laptop with ssh access and be familiar with ssh (and ssh-keygen) public and private IP addresses DNS VPCs subnets AWS EC2 key-pairs (or pem files) and other concepts You dont need to have a deep understanding of each and every concept to complete the steps but it would be helpfulAlso note that because this blog relies heavily on the AWS web console as it exists now in May 2020 theres a chance AWS could make modifications to its web console that would render some of the details listed here obsoleteOnce you have this cluster set up you can proceed to installing open-source distributed computing frameworks such as Apache Hadoop and Apache SparkLogin in to AWS and navigate to the EC2 Dashboard and click on Launch Instance Youll then be prompted to choosing an AMI In our case well scroll down until we get to an Ubuntu AMI and select thatNext youll have to choose the instance type You can peruse the specs and the costs here and choose whats best for you For this exercise Im assuming this cluster would be used to do a fairly large amount of data processing so I chose m5alarge instances which has 2 virtual CPUs and 8GB in memoryNext because Im planning on creating a 4-node Hadoop cluster with one master and three workers I set the number of (ec2) instances to 4 and designate a VPC and subnetNext youll have to add storage and you can take a rough guess of how much you might need per machine at one time If as an example you think youll be storing around 100GB at any one time it might be enough to provision around 30GB of storage for each instance Keep in mind that youll be paying per GB-month that youll be using it  as of this writing that was about $10 for a SSD volume and about half that for an older generation magnetic volumeThe next step involves adding key-value tags if you want a way to quickly identify the four machines you are about to spin up (eg Hadoop cluster Spark cluster)After that youll be asked to associate a security group with your cluster of machines For the purposes of this section lets set up a brand new security group and associate that with our instances Its important for you to correctly set up your security group so that you can connect to the new machines you are about to spin up while still not making your cluster publicly accessible to the hackers of the worldWhen you choose My IP under the Source column AWS will automatically identify the IP address that youre coming from and only allow traffic coming from that IP address to access machines associated with that security group (note if you move from work to home or a new location you may need to update that IP address or add a new line)Later well update this newly created security group to allow for the machines in your cluster to communicate with each other  thatll be very important for the machines in your cluster to talk to one anotherOnce youve configured your security group youre ready to launch your instances Note the message at the top which is letting you know that what you are about to do is going to cost you moneyAfter pressing Launch the next screen should allow you to associate an existing key pair or create a new one A key pair is a file ending in pem that holds credentials allowing you to ssh into the instances that you just spun up If you created a key pair previously you can go ahead and use that otherwise create a new one  just be sure and download the pem file to your laptop and make sure that it has the right file permissions so that youll be able to connect to that instance from your laptopIn the case illustrated above the key pair that I associated with my new instances is named hoa-nguyenpem If that pem file was new Id save the file and move it to the ~/ssh directory on my laptop and ensure it has the right read-only permissions (egchmod 400 ~/ssh/hoa-nguyenpem) If I dont change the permissions the next time I try to access my new instances via ssh with that pem file Id most likely receive permission errorsReturning to the AWS web console once you press Launch instances you can navigate to your EC2 Dashboard and Instances you should see the four instances that you just createdTo make it easier for you to identify which machine will be your master and which ones are the workers for the later stages go ahead and name each instance as a master or worker as seen belowFinally we want to go back and update the security group we created in the previous step to allow machines in the same security group to communicate with one another See below for where you can find the security group you associated with your instances (circled in red)Click on that security group and edit your inbound rules Add a new rule that allows All traffic between the same security group (ie Mentally note the security group id listed at the top  it starts with sg  and start typing it in the text box and AWS should autocomplete for you) Remember to press Save rules for your changes to take effectBe advised that limiting traffic to machines in the same security group (as well as your laptop) should be adequate for the purposes of this walk-through but would not be secure enough for productionNow that the machines have been provisioned were going to want to ssh into all four machines and set up passwordless ssh which essentially is a mechanism for the master instance to communicate with the workers without having to pass around the key pair (or pem file) Setting up passwordless ssh is a requirement for using distributed computing frameworks such as Hadoop and SparkBelow is an example of connecting to your master machine using ssh and the key pair (pem) file you associated with those machines that were just createdSubstitute your own pem file and public DNS of the instance you marked as the master You should be able to find the public DNS of your master instance from your Amazon web console details""So if my pem file was named hoa-nguyenpem (make sure that it is indeed saved in your ~/ssh directory) and my MASTER DNS was ec2 35 170 172 230compute-1amazonawsIf you are unable to ssh into your machine some things to double check are that you have the right MASTER DNS your pem file has the right file permissions (eg chmod 400 pem-file) and your security group is allowing access from the laptop that you are executing ssh Note that the first time you log into a machine from your laptop ssh will warn you that the authenticity of the machine cant be established and ask if you want to continue connecting  youll want to answer yesIf you are unsure where to find the DNS for your machine refer to the below picture and to find that information on the AWS web console: 2Youll be prompted to enter the file in which you want to save the key  just press enter and itll automatically choose /home/ubuntu/ssh/id_rsaOnce you do this you should be able to ssh into your current (localhost) machine with the following command: Now youre going to want to copy the credentials stored in id_rsapub on your master instance to the other three machines in your clusterThe easiest way is to go back to your laptop and use the pem file on your laptop to copy the master instances keys to the workers by issuing the following commands making sure that you replace PEM_FILE MASTER_PUBLIC_DNS and WORKER_PUBLIC_DNS with your own values: Once youve copied your public keys to your workers ssh back into your master and from there try to ssh into each of your workers (eg ssh ubuntu@WORKER_PUBLIC_DNS) and you should be able to connect Make sure you can reach all of your workers before you go on to the next stepsIf you encounter errors and various permission issues with trying to copy around your public keys you may spend a lot of time trying to debug the issue Often the easiest workaround would be to copy-and-paste the contents of the ~/ssh/id_rsapub file on the master instance and append it to the bottom of the ~/ssh/authorized_keys file on your worker instance This method is prone to copy-and-paste mishaps so its not foolproof but it might be a simpler way than debugging permission issuesAt this point you can move to installing distributed computing frameworks and skip the rest of this section Proceed through the rest of this section if you want to reserve an elastic IP that can be associated with your four machines regardless of whether they are running or stoppedRemember that if you stop and start your instances via the AWS web console the elastic IPs that was associated with your instances at the time you created will generally be released When you re-start your instances new elastic IPs will be assigned and any configuration files where you wrote in the previous public DNS must be updated to reflect those new assignments (This will be an important lesson to remember if you proceed to installing Hadoop and Spark) Note that private IPs will all remain unchanged and luckily you wont have to re-do passwordless sshIf you anticipate wanting to stop and start your instances frequently you can allocate your own IP addresses and persistently associate it to your instances or you can use a tool such as Pegasus or build your own Terraform scripts to bring up and down your machinesBelow is a manual way to allocate and associate an elastic IP Start by navigating your web console to the section pertaining to elastic IP addresssesOnce youve allocated you should now Associate this Elastic IP address: On the next page youll want to click onto the Instance search bar for one of the instances you just recently created Once youve picked the instance click on the Private IP address search bar and it should auto populate  select that and also choose Allow this Elastic IP address to be reassociated in case you want to reuse it then click AssociateNow repeat the same action for your three other instancesCongratulations you now have four elastic IP addresses that will continue to persist even when your instances are down Keep in mind that AWS does charge a daily fee for any allocated elastic IP address that is not associated with an instanceIf you are unable to allocate enough elastic IP addresses you can contact AWS support to increase the limit available to youInterested in seeing more content like this? Sign up for our newsletter and get updates on the latest resources and upcoming events"
697M6LtcoaFYEUuwCwR6JG,A Data Engineers view on his favorite Linux and Open Source conferenceThe SCaLE Linux conference has been a regular part of my life for the past 9 years and it holds a special place in my heart SCaLE isnt like most tech conferences; its the kind of gathering that encourages you to reach out and meet your fellow Open Source enthusiasts and they mean it You wont find speakers holed up in the speaker lounge; instead youll find yourself chatting with them in the halls As an all-volunteer effort SCaLE is absolutely best-of-breed I could not have said it better than Dave Stokes MySQL Community Manager @ Oracle and long-time SCaLE speaker who shared: SCaLE is the show other conferences wish they wereThis was the first year I attended SCaLE with my Data Engineering hat on and I was not disappointed With the wealth of talks on containerization deployment and programming languages populating the schedule my 4-day weekend was packed with sessions that left me inspired and eager to learn moreBased on Colonel Chris Hadfields book An Astronauts Guide to Life on Earth Guy Martin gives a compelling talk that summarizes Colonel Hadfields philosophy with an eye towards how to become a trusted member of an Open Source communityFor a thorough introduction to the Go language pitched at seasoned engineers see Kelly Hellers talk on the best features added to Go and the best decisions around features intentionally left outChris Smith VP of Engineering and Data Science at Ticketmaster gave a fun talk on the pipeline that performs streaming fraud detection at massive scale for Ticketmasters purchasing process Interestingly only the last ~10 minutes of the talk were about their ML engine of choice: Vowpal Wabbit while the majority of the talk covered the considerations surrounding the design of their distributed pipeline from both engineering and business-need perspectivesBack in Silicon Valley one week later thinking about what I can do for the next batch of Insight Data Engineering fellows Im still energized by all the stellar people I got to hang out with and the great work they shared at SCaLE In the next few months Ill be digging in to large-scale machine learning and contributing to our in-house deployment and configuration management tools continuing our constant effort to integrate the best of industry into our program and push the envelope of what brilliant people can learn to do in 7 weeksWant to get hands-on experience with Amazon AWS and big data tools in Silicon Valley or New York? The Insight Data Engineering Fellows Program is a free 7-week professional training program where you build cutting edge big data platforms and transition to a career in data engineering at top teams like LinkedIn Facebook Slack and Yelp
ZhDuuUhL7fr56EZiHLX9vP,Want to learn Lambda Architecture and other big data tools from top data engineers in Silicon Valley or New York? The Insight Data Engineering Fellows Program is free 7-week professional training where you can build cutting edge big data platforms and transition to a career in data engineering at top teams like Facebook Uber Slack and SquarespaceLearn more about the program and apply todayAndy Chu (now a Data Engineer at Netflix) discusses his project of implementing Lambda Architecture to track real-time updatesTodays traders on the stock market have a vast wealth of information available to them From financial news to traditional newspapers and magazines to blogs and social media posts there is more data than is possible to manually filter for news relevant to specific stocks of interest For my project at Insight I wanted to build a news service that can hook into a traders portfolio and customize news specific to the stocks a trader is holdingThis news service named AutoNews Financial reads in financial news from various sources as well as individual trades that users are making in real time Whenever a piece of news arrives that relates to a company that makes up a significant percent of a users portfolio it will be shown on the users dashboard With a large number of users making many trades and news coming in it would be ideal to have a big data system holding the history of trades for all users as a source of truth However processing that source of truth of large data sets is too slow to maintain real-time updates of a users holdings The two requirements for real-time tracking and keeping results accurately up to date can be satisfied by building a lambda architectureIn a traditional SQL system updates to a table change the existing value of the field This works well for datasets that fit on a small number of servers which can be vertically scaled with slave servers and backups However as the dataset scales to more servers when hardware fails it becomes more and more difficult to restore the data to the failure point and will likely require downtime In addition since history is not kept in the database only in logs data corruptions that result in incorrect data may go unnoticedA big data system with a distributed replicated messaging queue ensures that once data is entered into the system it would not be lost even in the case of hardware/network failures Storing the entire history of updates allows recalculating the results from this source of truth and results are guaranteed to be correct after each batch processing run However reprocessing the entire historical set of data would take too long for results to be presented in real-time To bridge the gap the design of lambda architecture adds a real-time component that works like traditional SQL systems  by only storing the current value after updates results are fast enough to be presented in real-time Errors in data in the real-time layer are resolved by overwriting the results with each run of batch processing This allows accurate results in a highly available eventually consistent system : any errors in the current value reported by the real-time layer caused by hardware/network failure data corruption or software bugs will be corrected by the next automated batch processing run which processes the entire dataset from the beginning of timeThe diagram below illustrates the data pipelineThe input data comes in as JSON messages with trades synthesized from a normal distribution and news coming from the Twitter API These JSON messages are pushed into Kafka and consumed by both the batch layer and the real-time layerBy using Kafka at the beginning of the pipeline to accept inputs it can be guaranteed that messages will be delivered as long as they enter the system regardless of hardware or network failureIn the batch layer Camus is used to consume all messages from Kafka and save them into HDFS then Spark sums through the transaction history to get an accurate count of stocks held by each user The aggregate results are then written to a Cassandra database tableIn the streaming layer Kafka messages are consumed in real time using Spark Streaming Spark Streaming is not fully real-time like Storm in that it micro-batches stream data into RDDs with a resolution of up to 500ms However it allows for reusing the Spark code in the batch layer and the micro-batch latency is small enough that its not significant for this use case of updating news articlesResults from both the batch and real-time layers are written out to Cassandra and the data is served to a web interface via Flask With the high number of trades being written to the system the write-fast capability of Cassandra is most suitable for this use caseThe results served on the web interface are always up to date with the latest messages coming into the system and this is achieved by combining the results from batch and real-time layer Illustrating how the real-time and batch results are used is most easily done with an exampleIn the diagrams below there are three database tables: one that stores the result of the batch processing one that stores only the trades made since the last batch processing run was finished and one that stores the correct up to date values which is the highlighted tableBy using a separate database table to record only deltas and having the full counts replaced by the results of the batch processing any incorrect results caused by software hardware or network problems will be resolved after a successful run of the batch processing ensuring accurate results after each runAnd at t0 batch processing begins When this finishes the result will reflect the totals as oft0 The real-time table Real Time 1 with the up to date values also correctly shows the current value of 5000During the batch processing the user sells 1000 shares of 3M The Real Time 1 table will correctly update itself to hold 4000 shares while the Real Time 2 table stores -1000 the delta from t0 as shown belowWhen the batch processing finishes these are the values in the database tables: 5000 4000 -1000At this time I swap my active database table to Real Time 2 and sum the batch result with the delta to get a consistent up-to-date count I then reset table Real Time 1 to 0 so it can start recording deltas from t1And the the next round begins with table Real Time 2 storing the consistent up-to-date valuesCorrectness of the data being displayed to the user requires that each of the above steps are performed only after the previous step has been fully completed and the results have been written out Rather than scheduling these steps based on time of day scheduling them sequentially allows for scalability as processing time increases with dataset size Workflows in a system like this can become very complex as when more features are added the various jobs scheduled have dependencies and use shared resources in a production system Hence platforms which can programmatically schedule and monitor workflows are used to manage these systems In this project I used Airflow  a scheduling and monitoring platform by Airbnb which models the sequence of tasks and upstream dependencies as a directed acyclic graph Airflow is a Python implementation that can model individual jobs as Bash operators This makes Airflow simple to use since anything that can be called by Bash can be called directly by Airflow The programmatic interface is less verbose than XML configuration based tools such as Oozie and the use of the Bash operator results in less coding than with a tool like Luigi which models each job as a Python object By wrapping each task as a Bash command each step in the process of combining the real-time and batch results are performed only after the previous step exits successfullyIn summary Lambda Architecture involves having both a batch layer and a real-time layer processing historical data and real-time updates respectively to get robust real-time results on top of a horizontally scalable hardware platform where failures are expected For Lambda Architecture to be a viable solution the data processing in the two layers must be designed such that the results of the batch layer can be combined with the live data streams that were read into the real-time layer For this project a second database table holding only the sums for inputs not yet read into the batch layer allows for a simple sum to aggregate the batch and real-time counts This is how I track real-time updates in a highly available eventually consistent system using Lambda ArchitectureInterested in transitioning to career in data engineering? Find out more about the Insight Data Engineering Fellows Program in New York and Silicon Valley apply today or sign up for program updates
dFPhRMDLmmH2NbXTYKoVk2,Want to learn Kafka Kinesis and other big data tools from top data engineers in Silicon Valley or New York? The Insight Data Engineering Fellows Program is free 7-week professional training where you can build cutting edge big data platforms and transition to a career in data engineering at top teams like Facebook Uber Slack and SquarespaceLearn more about the program and apply todayThomas Schreiter (now a Data Engineer at Microsoft/Yammer) discusses his project of comparing two ingestion technologies: Open source Kafka and AWS KinesisSelecting an appropriate tool for the task at hand is a recurring theme for an engineers work When multiple competing tools are available for the same task selecting the right tool is non-trivial In some small applications any tool might do the trick and spending a lot of effort selecting the right one would be a waste of time But for a reasonably large application selecting the right tool might save days weeks or even months down the road because of high data throughput low latency and spared maintenance headachesInsight provided me with a great opportunity to compare multiple tools For my project I decided to take a close look at ingestion technologies which are responsible for storing external raw data and making it available for batch or stream processing Ingestion is commonly the first part of the data pipeline The first contestant was Kafka which is open-sourced under Apache very popular and widely used in the industry The second contestant was Kinesis which is proprietary to Amazon Web Services and fairly new in the game as it was released in 2013The metric the contestants were evaluated against was throughput which is an important one in practical applications next to latency To set up a fair experiment a producer script was generating messages in the form of plain strings of a few dozen bytes each and shooting them as fast as possible to either the Kafka cluster or the Kinesis stream These stored the messages on their respective disks and waited for the next message to be sent This was the test environment and would theoretically run forever To measure the throughput a logger was attached to the producer that kept a count of the number of messages Every few seconds the throughput was calculated and stored in a MySQL database I also built a front-end to visualize the results in a web browser (Since the portion of Insight Data Engineering Program when we went to present our projects at various companies is long over the website does not exist anymoreSo who won? The short answer is that Kafka consistently achieved a higher throughput than Kinesis Kafka reached a throughput of 30k messages per second whereas the throughput of Kinesis was substantially lower but still solidly in the thousands While Kinesisâ€™ throughput improved when parallelizing the producers in the sense that multiple producers scripts were running in parallel on one machine it maxed out at about 20k msg/sec I suppose that Kafka achieved the higher throughput because it has been open-sourced for much longer and has a strong community As Kinesis will be improved over time I am sure that its throughput will increase as it evolvesBesides varying the number of parallel producers there is another variable that has a strong influence on the throughput; both Kafka and Kinesis support the notion of bulking multiple messages and sending them together in one go The messages are still considered as single messages and their order is preserved but sending them together greatly reduces the communication overhead The graph below shows the vast impact of the bulk size on the throughput Whereas Kafka flattens out at around 200 bulked messages the curve for Kinesis seems to increase even after 500 messages Unfortunately Kinesis is currently capped at that size So if that throttle would be released higher throughputs are likely (For completeness the numbers from the diagram above come from a setup where 500 messages were sent in bulkAs said before throughput is one of the most important metrics when evaluating technologies in a Big Data pipeline During my project I also had the chance to look at other metrics although in lesser scrutiny so the results in the following paragraphs should be considered more as anecdotal than as hard evidence First the above results focused on the producer side ie sending messages to the ingestion system Retrieving messages from the ingestion system ie consuming messages turned out to be a lot faster in Kafkas case For Kinesis it depends how many messages are bulked on the consumer side If only a single message is read at the time then the consumption tends to be very slowNext is latency which is another important metric Latency is defined as the duration between sending the message to the ingestion system and consuming it on the other end Kafka turns out to be very fast in terms of latency While I didnt measure the time with a stopwatch just watching the producer in one terminal and the consumer in another showed that the messages were piped through seemingly instantly For Kinesis the latency is visibly higher That is because the consumer has to ping the Kinesis stream to check whether a new message is available to be consumed The rate at which the consumer is allowed to pull a message out is forced to be at least about a tenth of a second This mechanism introduces a lower bound on the average latency of a twentieth of a second (If there is a way to circumvent this pull-mechanism ie if the Kinesis stream pushes a message to the consumer as soon as it receives it then the latency would drop significantly However I am not aware if this functionality existsFinally a somewhat softer but nonetheless relevant metric is maintainability ie ease of installation and difficulty to keep it up and running After a few hiccups Kinesis was fairly easy to get running thanks to pythons boto package Once installed Kinesis kept happily running and was stable Kafka on the other hand caused some trouble I could not identify the underlying cause but Kafka broke down multiple times during my project This led to a couple of long evenings but luckily most of it could be fixed within hours Still it is quite surprising when a system crashes down seemingly randomly (I want to point out that my fellow Insight Fellows who used Kafka in their projects did not have these problems so there is a good chance that this is not typical for Kafka but instead might have been a result of human error on my partNow I dont consider this project a full-blown benchmarking study since there are many more important variables to consider eg the size of the messages the specs of the nodes the location of the data center or parallelizing the producers across multiple nodes to just name a few However just taking the technologies out of the box playing around with them and stress-testing them in a simple yet reasonable test environment provides valuable insights of which tool to use for a given application Given the results in a production environment I would choose Kafka due to its high throughput If on the other hand the underlying task is exploratory then its easy setup makes Kinesis my first choiceFor more information about the implementation the code for this project is available at: githubcom/thomas-schreiter/ingestion_benchmarkInterested in transitioning to career in data engineering? Find out more about the Insight Data Engineering Fellows Program in New York and Silicon Valley apply today or sign up for program updatesAlready a data scientist or engineer?Find out more about our Advanced Workshops for Data Professionals Register for two-day workshops in Apache Spark and Data Visualization or sign up for workshop updates
h7tFUTHxHUofccfSE8j7ft,Insight has helped over 500 engineers transition into advanced roles in the tech industry over the past six years Today were announcing the expansion of both our Data Engineering and DevOps Engineering Fellows programs to the vibrant engineering community in SeattleAs one of the fastest growing cities in the US for tech jobs Seattle is home to top companies like Microsoft and Amazon and more tech companies like Airbnb eBay Google Lyft and Facebook continue to grow their presence Weve seen the same trend amongst our alumni  dozens of our engineers from across the country have joined the growing community of data scientists that launched our Seattle program last summerIn Seattle engineers drive the culture and community of the tech industry Recognizing this strong engineering presence in Seattle Insights launch of the Data Engineering and DevOps Engineering Fellows Programs will help connect companies to talented engineersHeres how Insights unique model works  we select highly-skilled programmers seeking advanced careers from a competitive pool of applicants During the free 7-week full-time in-person program we provide a collaborative space and resources to build distributed platforms Our engineers cultivate their skills with the latest open source technologies and distributed systems by building an industry-standard platform Fellows join a lifelong network by connecting with Insight alumni and mentors from top engineering and data teams in the community After taking a deep dive into the latest technologies Fellows showcase their knowledge to engineering leads at top companies who hire engineers from the sessionThe rise of Cloud Infrastructure and Infrastructure as CodeOver the past few years cloud computing has evolved from an exciting idea to one of the most transformational trends in the tech industry From the smallest startups to the largest enterprises more teams are leveraging Infrastructure as a Service (IaaS) from cloud providers Much of this growth has been pioneered by Seattle-based companies like Amazon and MicrosoftAs building software on cloud infrastructure has become commonplace the demand for secure and robust platforms has driven an equally high demand for engineers with cloud expertise Cloud providers need engineers who can build out these platforms and companies of all sizes need those who understand the ever-growing features and intricacies offered by those cloud platforms The days of spinning up VMs on a single network in a data center are long gone  todays engineer must learn how to deploy modern systems and microservices across a secure and fault-tolerant mesh of data centersThis rise in cloud infrastructure dovetails with another revolution in the way software is developed and deployed Trends like Infrastructure as Code (IaC) and containerization allow teams to go from design to deployment in hours and days instead of weeks and months Advances in continuous deployment feature flags canary testing distributed tracing and observability allow engineering teams to develop and operate with high agility and velocityWhile these tools were born in the data centers of Amazon Microsoft and Google theyve matured in the open source community as projects like Terraform Puppet Docker Kubernetes Mesos Zipkin and Prometheus Now any engineering team has the technology to turn their vision into reality but their bottleneck is the engineering knowledge to use this increasingly sophisticated tool setFor this reason engineers who can automate the industry-leading best practices and processes are incredibly valuable They empower teams to move quickly and scale But the demand for these engineers in Seattle has outpaced the supply  our new engineering programs will help solve this shortage by training the next generation of advanced engineers in SeattleNow that the smallest startups can scale to petabytes of data on hundreds of servers in as little as a month The growth of these companies is no longer limited by the time to provision and configure servers but instead by the engineers who can work with large volumes of data and develop against distributed systems The demand for engineers who can use the latest open source data technologies like Spark Kafka and Cassandra as well as cloud services like S3 Kinesis Lambda Cosmos DB and Redshift has continued to accelerate as data becomes ubiquitous in softwareIndeed what many teams in other cities call data engineering is a core part of the role of a software engineer in Seattle Working with data and distributed scalable systems is a fact of life for engineers in the community Engineers must stay on top of the latest developments in data processing and storage They should understand the internals and appreciate the engineering tradeoffs of the modern tech stack that supports the advanced data products at the core of the apps and services we use everyday Engineering teams at top companies are using data to build exciting products and hiring Insight Fellows to build themIts no secret that Microsoft and Amazon have contributed several engineering advances cementing Seattle as a home for world-class engineering This has attracted other tech leaders like Facebook Airbnb Lyft and many others to expand their footprint to this strong engineering communityWhile established companies have set the foundation for tech in Seattle new startups are also building innovative data-driven products LiveStories works to build state-of-the-art data solutions with civic data promoting data literacy and positively impacting public health Convoy is transforming the trucking industry matching reliable trucks with companies that need to ship freight iSpot is a leader in real-time TV analytics connecting TV ad impressions with web and mobile data to enable rapid actionable insights Pioneer Square Labs is partnered with Insight to help grow the community in Seattle turning the best ideas into funded companies with rapid customer adoptionPartnering companies in Seattle sponsor the Insight Data Engineering and DevOps Engineering Fellows Programs allowing Fellows to participate completely tuition-free Additionally Fellows get access to cloud resources and financial support to help cover their cost of livingWhile our engineering programs will be new in Seattle Insight Fellows are already working as data engineers site reliability engineers and data scientists at top Seattle teams like Amazon Microsoft Zillow and Facebook Throughout the fellowship engineers in both programs will receive mentorship from practicing engineers who have hands-on experience with the relevant technologies Fellows learn the same way as engineers in the industry  from collaborating with their peers and getting guidance from a strong network of experienced engineersAs with all our programs our engineering Fellows will not just learn about Data Engineering and DevOps Engineering but will learn by building Fellows dont sit through lectures or classes  they build and maintain world-class engineering platforms in a real-world setting with industry-standard tools They gain the experience with these tools and build the evidence to show top teams they will thrive in the field of data engineering DevOps site reliability or infrastructure engineeringThe DevOps Engineering program is designed for engineers with a passion for systems operations and programming Our Data Engineering program enables experienced programmers to apply their existing software engineering skills by building efficient and scalable data platforms with distributed systemsBoth engineering Fellows Programs will start on March 18 2019 Applications are open today  apply by Monday Oct 22 to receive an early decisionInterested in transitioning to an advanced career in engineering or data?Learn more about the Data Engineering and DevOps Engineering programs in Seattle apply today or sign up for updates
UMm5qJzfPM3fckoqKUQcvW,Applications are open for our next session of Insight Data Engineering in New York and Silicon Valley and close on November 3rdThe Insight Data Engineering Program is: All Insight Data Engineering Fellows are provided with: During the free seven week full-time program Insight Data Engineering Fellows work with technologies like Hadoop Cassandra Storm Spark and others to build robust scalable data platforms while meeting data engineers from top technology companies and interview with them immediately following the programInsight Fellows have the opportunity to learn from industry mentors including Nathan Marz creator of Apache Storm and author of the book Big Data Nathan has joined Insight as an advisor and active mentor and has been working with the Insight Fellows on a regular basis running workshops on Storm Lambda Architecture and Clojure in addition to giving project feedbackJoin our alumni network of 750 Data Scientists and Data Engineers who are now working at top tech companies including Apple The New York Times NBC Universal Capital One Facebook Microsoft Uber Memorial Sloan Kettering Cancer Center Yelp and many Silicon Valley and New York startupsFind out more about the Insight Data Engineering Fellows Program apply today or sign up for program updates
9FeWyUeHUEN6UpN99LURWg,Were excited to announce that the Insight Data Engineering Fellows Program is expanding to Boston starting April 2018 This fully-sponsored fellowship is a continuation of the Data Engineering programs currently ongoing in New York and Silicon Valley which have successfully helped software engineers and academic programmers make the transition to a career in data engineeringWe have witnessed the growing need in a front-row seat through building the Insight data community in Boston since 2015 We are particularly excited about the expansion to Boston a city with an unique blend of academia and industry to make innovations in technology This year Boston ranked as the #1 biotech hub with $29 billion venture funding in the past year giving professional engineers immense opportunities to make a positive impact through technology Many top-tier data teams are drawn to Boston thanks to the burgeoning supply of talented studentsStill the demand for hiring data engineers has increased dramatically due to a rapid growth of data in volume and complexity Companies are looking for experienced software engineers to build scalable distributed data architecture to process massive real-world data However there is an increasing number of specialized open-source technologies in the data engineering ecosystem It is extremely challenging to find candidates with hands-on experience in big data engines (eg Hadoop Spark and Cassandra) as well as an ability to collaborate with other team members such as data scientists or product managers Thats where Insight comes inThe Insight Data Engineering Fellows Program is a 7 week full-time professional training program It enables experienced engineers and programmers to apply their existing software development skills by building efficient and scalable data platforms with distributed systems Throughout the fellowship Fellows receive mentorship from practicing data engineers at companies such as Amazon Microsoft Tamr Wayfair Boston Consulting Group Boston Health Economics MassMutual Kayak iRobot TempAlert among others Several of these companies will be looking to hire Fellows for positions in BostonIn addition to learning data engineering tools and building projects under the mentorship of Boston-based engineers Data Engineering Fellows will have opportunities to gain practical machine learning experience through collaboration with our data science Fellows Moreover they will join an active network of over 1000 Insight alumni now working as data scientists and engineers at over 350 companies across the USWe are looking forward to helping engineers and programmers transition into data engineering in the Greater Boston Area Applications for the April 2018 Boston session are now open as well as for upcoming sessions in Silicon Valley and New York City  Learn more and apply at the Insight Data Engineering websiteInterested in transitioning to a career in data? Find out more about the Insights Data Science Data Engineering Health Data and Artificial Intelligence Fellows Programs in New York Boston Seattle and Silicon Valley apply today or sign up for program updates
jypWGaXj8fRtq3yaSyCeyy,Drew Conway is a leader in the New York City data science community founder & CEO of Alluvium and Insight advisor and mentorAs the technology industry continues to come to terms with the lack of diversity in its workforce there is one group that is often overlooked: veterans I started my career as a civilian working inside the US Defense and Intelligence communities and had the privilege of building data science products alongside active and retired service members Their technical expertise and aptitude not only accelerated my own development but taught me how real mission critical data science is meant to workYet very few veterans retire from their service and embark on a second career in the startup community The barriers however are primarily informational In my experience most veterans are unaware of the opportunities that exist in startups Likewise most startups are unaware of the applicability of veteran experience to their workThis is why I am proud to announce that Insight is teaming up with the Iraq and Afghanistan Veterans of America (IAVA) to continue the work others have done in paving a path for veterans interested in working in the tech and startup spaceIAVA is the most diverse and rapidly growing veteran empowerment organization serving 28 million members through education advocacy and community building Some of their most recent efforts to support veterans have focused on their employment transition to civilian jobs Insight hopes to expand the range of opportunities open to veterans and facilitate their career transitions by partnering with IAVA to identify candidates for the Insight Data Engineering Fellows ProgramThe unique perspective of veteran experience combined with the world class training of Insight will be something quite powerfulTo learn more about the IAVA head to their website To apply for our upcoming Fall 2016 session visit the Insight Data Engineering Fellows Program websiteInterested in transitioning to career in data engineering? Find out more about the Insight Data Engineering Fellows Program in New York and Silicon Valley apply today or sign up for program updatesAlready a data scientist or engineer?Find out more about our Advanced Workshops for Data Professionals Register for two-day workshops in Apache Spark and Data Visualization or sign up for workshop updates
PEowLvvhcEjv2ozWC8eVtk,During the seven-week Insight Data Engineering Fellows Program experienced software engineers and recent grads learn the latest open source technologies by building a data platform to handle large real-time datasets Daniel Blazevski (now a Program Director and Data Engineer at Insight) discusses his project of building a quadtree data structure to improve the performance of Apache Flinks k-nearest neighbors algorithmA young and exciting open source tool for distributed data processing known as Apache Flink has recently emerged as a player in the data engineering ecosystem Similar data processing tools do indeed already exist most notably Spark Storm and Hadoop MapReduce Compared to existing technologies Flink has a unique framework placing batch and streaming into a unified streaming framework In contrast Spark is a batch processing tool and the Spark Streaming lumps relatively small amounts of data into micro-batches Storm is able to process data one-by-one in a purely streaming way though does not have a batch processing frameworkFlink on the other hand operates in a purely streaming framework and instantiates the vision of Jay Kreps of the kappa architecture The quick rise in popularity and development of Flink should be noted: Flink started as a university project in Berlin and in a matter of a mere eight months Flink went from Incubator status to becoming a Top-Level Apache project in December 2014 Even more recently Yahoo wrote a blog about their experiences in using Flink in comparison to other toolsFor my Insight project I improved on an initial brute-force approach to add an exact k-nearest neighbors (kNN) algorithm by writing a quadtree data structure I have since made a pull-request and the Flink community intends to soon merge the work with its main body of source codeThe kNN algorithm is one of the fundamental classification algorithms and has an endless amount of applications in data science Given a training set A and test set B the kNN query can formally be stated as follows: form the set (b A_b) of tuples of points b in B and the k geometrically closest points A_b in A to bThe brute-force method computes the distance between a given test point and all the points in the training setEven for a modest number of points  eg 50000 test and training points  the brute-force method can take hours to run on a single CPUA quadtree is a dynamically constructed object on the training set and one starts by forming a bounding box on the training set Once the bounding box has more than some specified value maxPerBox of training set points the box is partitioned into equal sub-boxesA quadtree is a dynamically constructed object on the training set…once the bounding box has more than some specified value of points the box is partitioned into equal sub-boxesThe intuitive idea of partitioning the training set into smaller sub-boxes is appealing though there are some notable challenges in using the quadtree for the kNN query namely some of the k-nearest neighbors may not be in the minimal bounding box of the gold star The following diagram for example shows red points in the training set that are closest to the test pointA clean efficient way to search both in a test points minimal bounding box and surrounding area is needed Defining the surrounding area of a test point is in fact the most delicate part
EYZhARnkC5XAVsK7GnrXnw,Interested in getting hands-on experience with tools like Kafka Spark etc on Amazon Web Services and transition to data engineering? The Insight Data Engineering Fellows Program is a 7-week professional training program where you can build cutting edge big data platforms and transition to a career in data engineering at top teams like Facebook Airbnb Slack and SquarespaceLearn more about the program in New York and Silicon Valley apply today or sign up for updatesData Engineers are specialized software engineers that enable others to answer questions on datasets within latency constraintsSince 2014 Insight has helped 300+ of the brightest software engineers and academic programmers transition into top Data Engineering roles and weve learned a lot about how to make the transition efficient Weve received many requests over the last two years from people wanting to learn what they can do to improve their data engineering skills and chances of getting into the Fellows Program so we wanted to share some of our thoughts hereAt Insight Fellows learn to think as a data engineer and are exposed to many open source distributed tools used in the industry through building a scalable data platform Below you will find suggestions and resources that have helped our Fellows prepare for this transitionThe number of data engineers more than doubled from 2013 2015 And based on the job posting data from earlier this growth isnt about to slow downOne of the best ways to begin understanding data engineering is to familiarize yourself with real-world challenges by learning from data engineers in the industryWe also highly recommend Big Data the book from Apache Storm and Lambda Architecture creator Nathan Marz Our Fellows have found it really helpful and the first two chapters are available free onlineWhy?: Action Item: Read -at least- the first two chapters of Nathan Marzs book on Big DataOnce youre familiar with the challenges of a data engineer the next step is to get acquainted with the components of a data pipeline and the technologies used to handle large real-time data sets: During the program our Fellows take a deep dive into some of these technologies but it is very helpful to spend some time exploring a few of these tools at a high-level to understand how they fit into data engineering ecosystemWhy?: Action Items: One of the hurdles in learning data engineering is setting up a distributed cluster to develop on Amazon provides a free-tier which can be used to learn the distributed technologies rather than just using your local systemDuring the session Insight fellows commonly use AWS to run more complex applications on distributed clustersWhy?: Action Items: While it is good to know the basics of some popular technologies it is better to know the underlying computer science fundamentals behind them For example it is better to know that many modern NoSQL databases are implementations of a distributed hash table and understand why these are usedTip: Do not underestimate the importance of computer science fundamentals whose concepts are re-used and developed upon in distributed systemsWhy?: Action Items: As a data engineer you will most likely be part of a team of software engineers and data scientists working on shared projects and code bases Using software development best practices will help you to become an efficient team memberWhy?: Action Items: Many of you are familiar with Python Java or perhaps C++ but youll find that learning a new language may be to your advantage Data engineering tools such as Hadoop Spark and graph libraries are written in Java and Scala Some tools have a Python wrapper so its good to be familiar with Python but the newest updates often occur in the native language firstWhy?: Action Items: Bonus Item: Get a sense of Scala with some of the problems in 99 Scala Problems or re-write one of your favorite algorithms in Scala You may also enjoy Twitters Scala School as a referenceIf youve familiarized yourself with all the materials above here are some extra useful topics you should continue withAction Items: Here are some of the primary news sources read by people in tech We recommend starting to skim these resources every few days: Interested in transitioning to a career in data engineering? Find out more about the Insight Data Engineering Fellows Program in New York and Silicon Valley apply today or sign up for program updates
dywJ7CrMoRWabog28T6orP,Hadoop is one of the most mature and well-known open-source big data frameworks on the market Sprung from the concepts described in a paper about a distributed file system created at Google and implementing the MapReduce algorithm made famous by Google Hadoop was first released by the open-source community in 2006Fourteen years later there are quite a number of Hadoop clusters in operation across many companies though fewer companies are probably creating new Hadoop clusters  instead opting for other choices such as cloud-based object stores like AWS S3 buckets newer distributed computing tools such as Apache Spark or managed services such as AWS Athena and EMRBut there are definite advantages to installing and working with the open-source version of Hadoop even if you dont actually use its MapReduce features For instance if you really want to understand the concept of map and reduce learning how Hadoop does it will give you a fairly deep understanding of it Youll also find that if you are running a Spark job putting data in the Hadoop Distributed File System and giving your Spark workers access to HDFS can come in handyInstalling and getting Hadoop working on a cluster of computers is quite straightforward Well start at the beginning and assume you have access to a Unix laptop and Amazon Web Services This blog will assume that you know how to find the public DNS and private IP addresses for your instances and that you have access to your AWS EC2 key-pair (or pem file) and AWS credentials (secret key and access key id)By the end of this blog you will have created a 4-node Hadoop cluster and run a sample word count map-reduce job on a file that you pulled from an AWS S3 bucket and saved the results to your clusters HDFSIf you were going to programmatically provision your instances you might use a tool such as Terraform For this exercise if you dont know how to provision your instances follow the instructions on this blog to create a cluster of four machines on AWS with one of the machines capable of communicating with the other threeFor the next set of instructions youll need to log on (via ssh  ssh -i ~/ssh/PEM_FILE ubuntu@DNS where the PEM_FILE is your AWS key pair file and DNS refers to your instances public DNS) to all four of your machines and perform the same commands on each machineFirst youll want to update apt then install Java and double-check that you have the latest Java 8 version which is what Hadoop 27 is compatible withIf youre planning to use Hadoop in conjunction with Spark 24 (at least as of May 2020) youll want to download an older Hadoop 27 version If you arent planning to use Hadoop with Spark you can choose a stable and more recent version (eg Hadoop 3x) Download decompress and move the files into position which in our case will be to the /usr/local/hadoop directoryYoull also want to make sure the Hadoop binaries are in your path so edit the ~/profile file and update the PATH environment variable by adding this somewhere in the file: To ensure that your PATH is updated be sure and update your environment by executing source ~/profileRemember that once you are on your master instance and if you correctly installed passwordless ssh as described in the instructions detailed in the blog links provided up above you should be able to get to all of your workers from your master instance (eg ssh ubuntu@WORKER_PUBLIC_DNS where WORKER_PUBLIC_DNS refers to the public DNS for the particular worker instance)Once you have completed the above steps on all four of your instances return to your master instance and complete the following steps • Configure Hadoop hadoop-envYoull now want to edit your Hadoop configuration files starting with the /usr/local/hadoop/etc/hadoop/hadoop-envsh file First youll want to find the line that starts export JAVA_HOME and change the assignment (expression after =) so that it points to the directory where java was just installed In our case Java can be found at/usr/bin/java but JAVA_HOME is always listed with the /bin/java suffix removedAdd a new line to the file which augments the Hadoop classpath to take advantage of additional tool libraries that come with the Hadoop distribution See below for what those two lines should look like (ie youll want to modify the export JAVA_HOME line and add the export HADOOP_CLASSPATH line to the file)If you are planning to run the example listed at the end of this blog which accesses S3 buckets that require a certain type of AWS access signature youll also want to search the same file for a line that mentions HADOOP_OPTS and make sure it includes both of the -D configuration settings (usually its the second one that is missing) If you dont plan on running the example you can skip this modification • Configure Hadoop core-siteThe /usr/local/hadoop/etc/hadoop/core-sitexml file holds information on how to access HDFS as well as ways of accessing S3 buckets on AWS from Hadoop See below for what properties to add the file above the ending</configuration> xml tagBe sure and replace the below bold and italicized items such as the MASTER_DNS with the DNS of your master instance If you wont require access to S3 buckets you can skip the next three configuration lines However if you also want to access AWS S3 buckets via your Hadoop or Spark jobs you can add your AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY credentials here But be aware that this is highly insecure because you are listing your credentials in plain text A better way might be to store credentials in environment variables and then pull them into your code when trying to access S3(VERY IMPORTANT WARNING: If you add your keys here do not add this file core-sitexml to your Github or Bitbucket repository otherwise you will compromise your AWS account by revealing these credentials to hackers and public in general) • Configure Hadoop yarn-siteEdit the /usr/local/hadoop/etc/hadoop/yarn-sitexml and add the following lines above the ending</configuration> xml tag Be sure to change the MASTER_DNS to the DNS of your master instance • Configure Hadoop mapred-siteFinally copy /usr/local/hadoop/etc/hadoop/mapred-sitexmltemplate to /usr/local/hadoop/etc/hadoop/mapred-sitexml and add the following lines above the ending</configuration> xml tag The expanded classpath allows you to call Hadoop libraries to successfully access libraries such as those to connect to AWSBe sure and change the MASTER_DNS to the DNS of your master instance • Configure Hadoop hdfs-siteEdit /usr/local/hadoop/etc/hadoop/hdfs-sitexml and add the following lines above the ending</configuration> tag to set the replication factor and identify where in the normal file system the HDFS metadata would be savedPrior to this section weve been mainly referring to the master instances public DNS primarily because for processes that need to interact with your Hadoop daemons and other services theyll communicate with your master instance via its public DNS However for communication between instances in your cluster Hadoop will rely on private IP addresses See below for where you might find the private IP address for your instances on the AWS web console: Create a /usr/local/hadoop/etc/hadoop/masters file and add the private IP address of the master instanceEdit the /usr/local/hadoop/etc/hadoop/slaves file remove localhost and add each worker instances private IP address  one takes up its own lineHDFS requires a namenode directory to be created and it needs to be owned by the Hadoop user which in this case is ubuntu and not rootCreate the datanode directory and make sure that its not owned by rootNow that youve edited your configuration file on your master instance youll want to make sure all of your workers also get the same changes Get a list of the public DNS (or private IP addresses) for all of your workers handy and while logged on to your master instance rsync the Hadoop directory with each of your workers(Alternatively you can manually ssh into each of your workers and redo Configure Hadoops steps 1 7 in the previous section Yes rsync is our friendOn your master instance issue the command to format HDFSNow you can start firing off scripts to start the Hadoop daemons and Yarn services To begin with start the Distributed Filesystem: You may get Are you sure you want to continue connecting (yes/no)? messages  go ahead and answer yesCongratulations youve successfully started HDFS If you open your web browser and navigate to http://MASTER DNS:50070 (replacing MASTER DNS with the DNS of your own master instance) you should see something like the following (if you do not there is most likely an issue with your AWS security group settings and/or it is not providing access to your laptops IP)Next start the Hadoop Job TrackerIf you navigate to your web browser and type http://MASTER DNS:8088/cluster/nodes replacing MASTER DNS with the DNS of your master instance You should see a similar image as this showing three worker nodes: Now that youve gotten your Hadoop cluster up and running lets check that the installation went smoothly by going through an example that will copy a file from a SWS S3 bucket to your clusters HDFS and run a word count map-reduce job on the fileIf you were to use another batch processing framework such as Spark you probably could read the file directly from the S3 bucket but because we want to double check that our Hadoop and HDFS installation work well use Hadoops distcp command (or distributed copy command that uses Hadoops map-reduce under the covers) to copy the file from S3 to HDFS in this exampleFirst lets create a new directory on our clusters HDFS to hold the results of this test You can do this on any of the machines in your clusterThe S3 bucket that were going to use is a recently published covid19-lake that is publicly available and accessible in the us-east-2 region Because this is a recently created bucket on AWS accessing it relies on a newer authentication method referred to as V4  which requires the changes we made to our hadoop-envsh configuration file earlier (see above section)-Dmapreducemapjavaopts=-Dcomamazonawsservicess3-Dmapreducereducejavaopts=-Dcomamazonawsservicess3Put together the following command will copy the contents of the states_abvcsv file on the S3 covid19-lake bucket in the static-datasets/csv/state-abv directory to your clusters HDFS in the /my_test directory: After you execute this command you can double check that the file exists in your HDFS and look at its contents with the following two commands: Now lets run a map-reduce job that will count the words in that state abbreviation file and put it into another file on HDFS: The above command should create a new directory on HDFS at /my_test/states_abv_output that will hold the results of the word count job in a file named /my_test/states_abv_output/part-r-00000 If you then print the contents of that file each line should hold the state name state abbreviation and the number 1 because each state is only listed onceCongratulations youve successfully installed Hadoop pulled a file from a S3 bucket and run a Hadoop map-reduce job to count the words in the fileObviously theres no good reason to do a word count on a lookup file but its shown here to give you an idea of how to interact with Hadoop and HDFS From here you can go on to explore tools such as Presto which is designed to work with HDFS or Apache Spark which uses many similar map-reduce concepts from HadoopHeres an installation blog for Spark Note that if you have completed the instructions here for Hadoop and you plan to use the same cluster to install Spark youll be able to skip the Step 05: Setting up Keyless SSH section in that Spark blogInterested in seeing more content like this? Sign up for our newsletter and get updates on the latest resources and upcoming events
BMAkUKq3SVH3rrLMAPGDhX,A story about an Insight Data Engineering project that nearly wasnt and the beginning of an unexpected new careerI vaguely recall my first encounter with a trampoline  the physical acrobatic sort of trampoline not the Lispy control flow concept I remember the joy I found in the novelty of such a gigantic toy The exhilaration of launching myself into the air unsupported defying gravity The discovery that I can get a lot higher with some well-timed help from a friend And noticing that I wanted to keep jumping long after I got tired because it was fun And I wasnt about to let tired get in the way of funMy time as an Insight Data Engineering Fellow went much the same way Maybe its because I got to learn something new and exciting or maybe because I really enjoyed being around the other people who found themselves here too: through an intense 7 weeks theres something about the Insight experience that drove me to work very hard and to be grateful for the chance to do so Now as a Program Director I consider myself a custodian of that experience and hope to share it with all who come through our doorsI came to Insight in September 2016 with a Chromebook and a bad haircut  a new Fellow looking to show the big-data world the beauty that lies in simple efficient solutions Necessity is the mother of invention was my mantra Thats how I justified not upgrading to an actual laptop anyway I already had Arch Linux installed on my Chromebook and the 16GB SSD held the JDK with room to spare I had a machine and I was convinced I could make it work The hard part interestingly enough was figuring out what to work onI went through five project ideas before I settled on the one that I ended up presenting For context five ideas is not a lot for a Fellow these days; there are many things you need to understand before you can wrap your head around a modern data engineering challenge Five iterations on your ideas is fine and expected but for many reasons my ideas took a long time to invalidate and I was left without a project at the end of the second week of the program The end of week two is just beyond the point where we were expected to have our project MVP built and usable I burned two weeks of a three-week project and I had just buried my fourth ideaThere was a decision I had to make over and over again and it went a bit like this: Put in such simple terms the decision was easy: I was going to keep working until they kicked me out Gladly they didntThrough all my failed project ideas I ended up with a decent understanding of Flinks model and internals One of my Program Directors Ronak Nathani recommended I actually play around with the tool (as I hadnt yet) and see if anything inspires me I took interest in the streaming graph library gelly-streaming and learned it had an algorithm that could find connected components incrementally from the beginning of time but for instance couldnt show you the connected components that had developed in just the past 2 hours I didnt have a great application in mind but intuitively it seemed there must be value in isolating a view of the graph that captures change in the network over time with respect to disjoint setsTo get more concrete take Github for example You make pull requests to peoples repos comment on others issues and fork someones project to work on it yourself If you treat these interactions as connections in a network (and squint a bit) you can see a developing social network right? Flinks gelly-streaming could tell you if theres ever been a connection in your extended network of Github interactions to someone like Brendan Gregg but it couldnt tell you if such a connection happened recently or if it happens periodically It couldnt show the ephemeral nature of groups as novel repos come and go or identify the people that are most likely to bridge disparate groups as they are growing and changingI decided to implement this feature in the gelly-streaming framework The concept isnt too heavy-duty My Insight project and demo consisted of a pipeline that identified groups (connected components) in the Github event stream within a sliding window At ~20000 events per second this pipeline could chew through 5 years of Github data in about 8 hours I titled the project Network Pulse but Github offered the repo name literate-garbanzo and I just couldnt bring myself to change itLamentably my contribution wasnt in line with gelly-streamings goals I also bumbled the implementation at first and I didnt make a good case for its inclusion since this sort of algorithm deviates a bit from the restAnd throughout it all my workhorse was a $150 Acer C710 Chromebook In my late-night stupor I even spent hours building JARs locally swapping memory to a USB20 flash drive when my SSD ran out of space before I realized itd take mere minutes to build the same things on any of the AWS EC2 nodes I had up and running It was a hard lesson on the false economy of neglecting adequate rest Live and learnInsight was my trampoline into a new career I once called it the shortest path between me and a really cool job I didnt expect that job would be here at Insight but Im grateful that it isGiven some free AWS resources 7 dedicated weeks a bit of instruction and an amazing professional network … if you have the drive and perseverance to turn those ingredients into a career I highly encourage you to apply The deadline for applications is March 29th
L3Ngu73wVGXb63HatE6ppR,The industry demand for Data Engineers is constantly on the rise and with it more and more software engineers and recent graduates try to enter the field The biggest hurdle for newcomers lies in understanding the Data Engineering landscape and getting hands-on experience with relevant frameworksWe at Insight offer a 7 week tuition-free Fellowship to transition into Data Engineering and have worked with hundreds of Fellows who had to overcome this exact hurdleBefore jumping into a project or choosing frameworks to work with it is important to take a step and look at the big picture What is Data Engineering and what is the role of a Data Engineer? Which are the most important concepts and frameworks one should understand? Here is a collection of great articles that shed some light on these questions and have helped our Fellows in the past: This blog post by Richard Taylor starts with a discussion of what big data and data engineering really mean before delving into an overview of the current landscape The author strikes a great balance between being concise yet deep enough to cover topics such as the CAP theorem or resource managersThe article by Pranav Dar walks through the different technical skills a data engineer should acquire and lists useful resources to get started It starts very basic with introductory articles to Python but also includes links to courses covering the Hadoop ecosystem including Spark and HiveYou can think of this blog post by George Orosz as a notebook the author developed when transitioning into Data Engineering themself Rather than jumping into the details of specific frameworks it focuses on basic recurring concepts that are helpful for any tech stackThis cookbook by Andreas Kretz is not yet complete but already has gathered a huge following This is not surprising given that it already contains a lot of high quality content starting from a definition of Data Engineering (the author likes to refer to it as plumbing for Data Science) to agile development methodologies to in-depth discussions about Hadoop and Docker It is definitely worth bookmarking this cookbook as it rapidly evolves to one of the most comprehensive resources for Data EngineersNow that you have gotten an overview of the most relevant concepts and frameworks you might feel overwhelmed  you are not alone! Our Fellows found that it is extremely helpful to look at some case studies/projects from leading Data Engineering teams to understand how the different pieces can fit together to build a cohesive pipeline/platformIn this article by Andrey Gusev an engineer from Pinterests Content Quality team we get an in-depth tour on how Pinterest detects duplicate images using Spark and TensorFlowThis one is technically not a blog post but a slide show  but we still wanted to include it as it is an amazing representation of Lyfts data platform and how it has changed It does not just illustrate how technologies such as Spark Presto Kafka Hive Druid or Flink can work together  it also highlights how to build a platform that hosts both streaming and batch applicationsYou have an understanding of the big picture and you have learned about examples of cutting edge projects  it is time to build your very own pipeline! Once you are working on a Data Engineering project you will inevitably look for resources to understand the frameworks of your choice better Maybe you want to get started with Spark or deploy your first Elasticsearch cluster? Here are some articles that go into more depth about various popular technologies: When Fellows start working with their first NoSQL database they often need some time to get accustomed to the very different way of defining tables and running queries This article by Joe Chasinga is a great resource for anyone dabbling into Cassandra and in need of a gentle introduction into basic conceptsOnce you start scaling your platform writing to your database can become a huge bottleneck  Karl Düünas article is a very approachable journey into concurrency into MySQL and how to enable a high write and read throughout without running into a nightmare of locksAnyone who has ever written a Spark job knows the feeling: Your code runs just fine with your test data but the moment you unleash the full data set onto your Spark cluster things start to break! Sital Kedia Shuojie Wang and Avery Ching from Facebooks Core Data/Data Infrastructure team walk through a comprehensive case study on how to optimize a Spark jobApache Kafka is one of the leading message ingestion frameworks for realtime streaming applications and is a great tool to serve ML models in production to potentially millions of users If youre just getting started with serving ML models look no further than this article to learn best practices from Kai Waehner from Confluent (the company founded by the team that built Kafka)Are you interested in working on high-impact projects and transitioning to a career in data and tech? Sign up to learn more about the Insight Fellows programs and start your application today
VucmGS2c3MfzzQAq2XTrPB,The industry demand for Data Engineers is constantly on the rise and with it more and more software engineers and recent graduates try to enter the field Data Engineering is a discipline notorious for being framework-driven and it is often hard for newcomers to find the right ones to learnWe at Insight offer a 7-week tuition-free Fellowship to help programmers transition to Data Engineering and have helped guide hundreds of Fellows overcome this exact hurdle As part of their fellowship training and transition into data engineering Fellows spend three weeks working on data engineering projects where they dive deep into these frameworksHere are the six most important and useful ones our Fellows work on during their projectsSpark is one of the most popular tools in distributed computing and can be used for batch and streaming applications Sparks rich ecosystem and advanced APIs and libraries such as SparkSQL and SparkML make it one of the most powerful and flexible toolsIf you want to get started with Spark check out this blog on how to setup your very own Spark cluster on AWS hereAn alternative to Spark Flink has gotten a lot of traction in the Data Engineering community While its ecosystem is not as rich Flink shines with its different approach to unified stream/batch computations Fellows used Flink for instance to build a real-time fraud detection pipeline where the focus was on low latencyKafka started out as a fault-tolerant distributed messaging and real-time data ingestion platform However it has evolved to a complete streaming platform capable of real-time analytics and high-throughput processing of data Our Fellows love Kafka for its performance and ease of use  and have used it to build an ingestion platform for collecting data for autonomous driving and a real time app suggesting tags for your next StackOverflow postElasticSearch is a popular distributed search engine built on top of Apache Lucene ElasticSearch is also part of the so-called ELK Stack consisting of ElasticSearch Logstash and Kibana This stack is very popular to build a highly scalable logging framework to maintain web applicationsPostgreSQL is a popular open source relation database While NoSQL databases emerged with the advent of big data relational databases remain widely popular and are still the best solution for many use cases Our Fellows gravitate to PostgreSQL not just for its ease of use but also for extension PostGIS which adds unlocks powerful geospatial queries This has been used by our Fellows to built platforms such as AirAware an app monitoring air qualityAn accessible introduction into how to setup your own PostgreSQL database can be found hereRedshift is an analytical database/data warehousing solution from AWS It is originally based on Postgres (which is why we grouped these tools together) but has been greatly expanded and modified with a focus on support of performant analytical queries and advanced data warehouse features Our Fellows have used it in their projects often in conjunction with Spark for the exploration of Reddit dataAirflow is one of the most popular workflow automation and scheduling systems It manages all jobs in directed acyclic graphs (DAGs) enables it to be both accessible to new users while also supporting complex workloads Our Fellows have used Airflow to build to provide financial analytics for crypto assets or have hosted a fully fault tolerant Airflow clusterIf you want to get started with Airflow we got you covered with our Airflow 101 articleAre you interested in working on high-impact projects and transitioning to a career in data and tech? Sign up to learn more about the Insight Fellows programs and start your application today
ectUH4UcYseuEGnRXQNKCf,Lets talk about what big data is and what are the common types of big data architectureBig data is defined by the 3 Vs: According to the 3Vs model the challenges of big data management result from the expansion of all three properties rather than just the volume alone or the sheer amount of data to be managedIn Nathan Marzs famed book Big Data he defines the difference between traditional data systems and big data systemsA data system is an application that provides answers to questions about information that has been collected so farA big data system is a data system that on top of that can work on Terabyte and Petabyte scale of dataBefore we start thinking about how to architect and build such systems that are capable of processing large volumes of data lets formalize their requirementsCommon requirements that we would like to see in most systems are: Not every type of big data system is capable of fulfilling them all as well will see shortlyThe first type is real-time for a limited range and ad-hoc queries
E55BBtsiSVJEpUU4YQqYyu,A look at the most common questionsData engineers primarily focus on the following areasData pipelines encompass the journey and processes that data undergoes within a company Data engineers are responsible for creating those pipelinesCreating a data pipeline may sound easy or trivial but at big data scale this means bringing together 10 30 different big data technologies More importantly a data engineer is the one who understands and chooses the right tools for the job A data engineer is the one who understands the various technologies and frameworks in-depth and how to combine them to create solutions to enable a companys business processes with data pipelinesData engineers make sure the data the organization is using is clean reliable and prepped for whatever use cases may present themselves Data engineers wrangle data into a state that can then have queries run against it by data scientistsData wrangling is about taking a messy or unrefined source of data and turning it into something useful You begin by seeking out raw data sources and determining their value: How good are they as data sets? How relevant are they to your goal? Is there a better source? Once youve parsed and cleaned the data so that the data sets are usable you can utilize tools and methods (like Python scripts) to help you analyze them and present your findings in a report This allows you to take data no one would bother looking at and make it both clear and actionableData engineering is a specialization of software engineering The majority of data engineers working in the field used to be software engineersDue to the high demand other roles such as DBAs system admins data analysts and scientists are also transitioningData engineers should have the following skills and knowledge: A holistic understanding of data is also important That can mean thinking and acting like an engineer and sometimes that can mean thinking more like a traditional product managerSalaries vary greatly between Big Tech Fortune 500 other large tech companies and startupsThis is only considering cash compensation Stock grants could be a percentage of salary or multiples of salary depending on individual company performanceTraditionally data engineering is not an entry-level role Most employers want individuals with some professional software engineering experience However companies are growing too fast these days and are willing to take fresh CS/bootcamp graduatesIf you are in school I recommend getting a software or data engineering internship at a tech company that works with big dataIf you have work experience as a software engineer Insight Data Engineering is excellent for those in the US and ASIs Fellowship is the equivalent for UK/EuropeIf you cant quit your day job we are developing a data engineering course It will be released mid-2019Check out the following articles on Medium to learn more about the field: An unofficial manifesto for the field of data engineering It hits on the main challenges that data engineers faceA gentle introduction to some of the common tasks data engineers tackle with code examples as wellA step-by-step look at how Boxever settled on their data processing pipeline This gives great insight on how data engineers make decisions about technology based on the project requirements
95zgt5FsTyAuMvctmeJNsd,The best way to learn data engineering is to build data processing pipelines that handle big data Its not that easy to find free big dataBelow is a list of streaming data sets that are open for public consumption Ill keep updating as I find moreOne of the best sources for user generated content and processing textTwitter  You can use their official API or a third-party library to access a part of their stream For Python go with TweepyReddit  Normally you would have to call the REST API at a regular interval to pull down posts and comments for certain subreddits however the team at Pusher has built a Realtime API read more about it hereMeetup  They provide extensive developer endpoints plus 3 WebSockets You can stream event comments photos and RSVPsMoney never sleeps There are people buying and selling financial instruments every second of the dayStocks  IEX has an amazing developer platform Not only can you get stock information but you can get information on the order book cryptocurrencies sector performance and much moreBlockchain  The official Blockchain organization provides an API to get notifications on blocks and transactions for BitcoinCrypto  CryptoCompare offers a free API with a streaming component that allows you to get all currency pairs and pricing info on demand
FvwtoXif6zUS4nvyqcjE8G,There are various ways to do data engineering on Google Cloud Platform At ML6 we often get the question of what tool is the best solutionOver the next few months well have a look at the different tools and frameworks available on Google Cloud Platform Well focus on a simple use case and try to highlight the features that increase productivityA few weeks ago I attended the Google Cloud Data Fusion training so lets build a data set using the new kid on the block: Cloud Data FusionA few months ago I discovered on Bike Share Research its easy to access the Antwerp Velo API Ive deployed a scraper written using scrapy on Scrapinghub to fetch the bike availability every 5 minutes and store the results in a JSON line file Well clean combine the data with the latest master data and load the data into BigQuery and a Parquet fileSetting up a Cloud Data Fusion instance is an easy 3 step process as explained in the documentation It takes about 10 15 minutes for the instance to be availableOne of the task people with a database / ETL background struggle with is parsing JSON data Lets check how Cloud Data Fusion simplifies this using the Wrangler transformationAfter youve selected a file in storage Cloud Data Fusion samples a number of records and displays the data Click on the arrow in the column and select Parse + JSON Enter the depth and boom the data is transformed into columnsI added 6 more directives to drop certain columns and correct the data typesIn 7 clicks I transformed the raw JSON line file into the format needed for the next stepOne of the columns is a date timestamp in Epoch/UNIX formatThe Parse  Date directive in Wrangler doesnt support this yet but I found a plug-in in the hub that can do thisIn the Date Transform step you specify the unix time stamp column and the date format you want to outputTo parse the master data JSON I added another storage source and Wrangler transformation followed by a joiner to combine the dateIn this step I renamed the columns to define the final schemaUsing 2 sinks I write the data to a BigQuery table and Parquet file in storageIts easy to check that the transformations work as expected by running the job in preview modeAfter the preview I save and deployed the jobIt takes about 6 minutes for the job to provision and 25 minutes to process about 9 million records with 1 master node and 2 workers This can be improved without any code changes by adding more workersIn this blog post we only scratched the surface of Google Cloud Data Fusion Transforming JSON files is a smooth process and certainly beats having to write JSONPath or code manually Its not as visual as the excellent JSON and XML mapper in TalendManaging the schema across all the steps works as expectedI need to look into the capabilities of the custom transform in Wrangler because maybe the Unix timestamp to date conversion could be done without an additional transformation It will be great to see a Parse Unix timestamp functionality because Unix timestamps are used a lot in IoTThe BigQuery sink and sources need more functionality Its essential to be able to set the create and write dispositions I also would like to have a SQL query option in the source It would be great to have specific BigQuery actions to for example execute a SQL statement create/drop tablesAbout ML6: We are a team of AI experts and the fastest growing AI company in Belgium With offices in Ghent Amsterdam Berlin and London we build and implement self learning systems across different sectors to help our clients operate more efficiently We do this by staying on top of research innovation and applying our expertise in practice To find out more please visit wwwml6
ESPpiepSbzARNQ8zxedwf4,The process to extract data from Redshift can be as simple as running an UNLOAD command However the UNLOAD command has some limitations The biggest limitation is not allowing you to include a header row in your outputWe created a service that wraps the Redshift UNLOAD command with some extra capabilities One of these capabilities is that the service automatically retrieves and adds headers to the extract from the convenience of a Docker container Everything is built into the container including psycopg OS and other Python packages What you will need to have is the connection details to your Redshift host and the AWS credentials to write the UNLOADED data to an S3 bucketGet it here: The service requires a configuration file named configjson in the same directory The config file is comprised of the following parameters set here to obtain database connection info AWS credentials and any UNLOAD options you prefer to useA sample configuration file is belowThis command will unload the data in the table mytable using thedatecol to specify a export period It will then export to the specified S3 locationRunning inside a container: Running it via Docker command: The service can accept different runtime parameters: As mentioned previously it is possible to supply your own WHERE clause to be used in the UNLOAD command This is done via an external SQL fileHere is an example To use this functionality to UNLOAD only new users you createD a SQL file called new-userssql that contains WHERE is_new = true Pretty simpleNOTE: The -s option will only work for the WHERE clause If you try other clauses it will fail The script would need to be ehanced to do moreIt may be wise to create a read-only user on the database for the script to use This will improve the security of the script by further protecting against SQL injection For more information on how to do this check the manual for your database typeDid we miss anything? Do you have any questions about how to export data from Redshift? We can help! Feel free to leave a comment or contact us at hello@openbridgecom You can also visit us at https://wwwopenbridgecom to learn how we are helping other companies with their data efforts
Tjnr2HpwSdKWGsEV6NbkXN,With the increasing demand around Kafka Cluster monitoring and management a number of open-source and commercial graphical tools have reached the market offering a variety of administration and monitoring functionalitiesConfluent is the company founded by the original creators of Apache Kafka Confluent Enterprise is a -more complete- Kafka distribution for production environments The commercial licence of Confluent Platform comes with Confluent Control Centre which is a management system for Apache Kafka that enables cluster monitoring and management from a User InterfaceConfluent Control Center delivers understanding and insight about the inner workings of the Apache Kafka clusters and the data that flows through them Control Center gives the administrator monitoring and management capabilities through curated dashboards so that they can deliver optimal performance and meet SLAs for their Apache Kafka clustersLenses (ex Landoop) is a company that offers enterprise features and monitoring tools for Kafka Clusters More precisely it enhances Kafka with User Interface streaming SQL engine and Cluster monitoring It also enables faster monitoring of Kafka data pipelines by providing SQL and Connector visibility into your data flowsLenses works with any Kafka distribution delivers high quality enterprise features and monitoring SQL for ALL and self-serviced real-time data access and flows on KubernetesThe company also offers Lenses Box which is a free all-in-one docker that can serve a single broker for up to 25M messages Note that Lenses Box is recommended for development environmentsFurthermore Lenses also offers Kafka Topics UI which is a web tool for managing Kafka TopicsKafka Dashboard by Datadog is a comprehensive Kafka Dashboard that displays key metrics for Kafka Brokers Producers Consumers and Apache Zookeeper Kafka deployments often rely on external software which is not part of the Kafka like Apache Zookeeper Datadog enables a comprehensive monitoring on all the layers of your deployment including software components in your data pipeline which are not part of Kafka as suchSave time on setup and visualize your Kafka data in minutes with Datadogs out-of-the-box dashboardKafka in Cloudera Manager is clearly a less rich monitoring tool compared to Confluent Lenses and Datadog However it is very convenient for companies that are already customers of Cloudera and need their monitoring mechanisms under the same platformWant to read this story later? Save it in JournalYahoo Kafka Manager is an open-source managing tool for Apache Kafka clusters With Kafka Manager you can: KafDrop is an open-source UI for monitoring Apache Kafka clusters The tool displays information such as brokers topics partitions and even lets you view messages It is a lightweight application that runs on Spring Boot and requires very little configurationKafdrop 3 is a UI for navigating and monitoring Apache Kafka brokers The tool displays information such as brokers topics partitions consumers and lets you view messagesThis project is a reboot of Kafdrop 2x dragged kicking and screaming into the world of JDK 11+ Kafka 2x and KubernetesLinkedIn Burrow is an open-source monitoring companion for Apache Kafka that provides consumer lag checking as a service without the need for specifying thresholds It monitors committed offsets for all consumers and calculates the status of those consumers on demand An HTTP endpoint is provided to request status on demand as well as provide other Kafka cluster information There are also configurable notifiers that can send status out via email or HTTP calls to another service Burrow is written in Go so before you get started you should install and set up GoKafka Tool is a GUI application for managing and using Apache Kafka clusters It provides an intuitive UI that allows one to quickly view objects within a Kafka cluster as well as the messages stored in the topics of the cluster It contains features geared towards both developers and administrators Using Kafka Tool you can: The tool runs on Windows Linux and Mac OSIf you cannot afford commercial licenses then your options are Yahoo Kafka Manager LinkedIn Burrow KafDrop and Kafka Tool In my opinion the former is a comprehensive solution that should do the trick for most of the use-casesIf you are running relatively big Kafka Clusters then it is worth paying for a commercial license Confluent and Lenses offer more rich functionality compared to the other monitoring tools weve seen in this post and I would highly recommend both of themThere are many Black creators doing incredible work in Tech
aSAXMeQbP4eHGUWMURFrJa,A Workflow consists of steps configured to respect a predefined order and accomplish a specific business objective They vary from something as simple as defining an IT request process in a small company to complex data transformations aimed to deliver key business insightsLeaving complexity aside some characteristics are common across all of them: To address the above is where the concept of Workflow Automation comes in It can be thought as a framework that seeks to standardize and facilitate the development/deployment of workflows across an organization These frameworks are especially important in mid/large-size companies where multiple teams have similar requirements but dont necessarily communicate about their needsAt Trade Me we identified the following pain points that led us to build our own framework: As Data Engineers we were responsible for delivering data outputs from customers requests We soon realized that this was not sustainable going forward as the number of data analysts and scientists was growing but our team remained at the same sizeWhile we were spending most of our time going through the backlog we could not properly help the ones who wanted to write their own transformations This generated frustration from the consumers point of view - especially regarding the performance of running things locally - but also on our end as data that should be kept in the cloud was being downloaded to their laptopsData Analysts and Scientists are spread across the business working on problems related to their own area There was no transparency about the code written to interact with our data sources resulting in duplication of work Also disparate development practices resulted in recurring bugs and slow review processesWe kicked off this project with the goal to facilitate writing deploying and running workflows to anyone across the organizationWant to read this story later? Save it in JournalTo start simple and enable us to move faster we decided that our framework would support a single/primary programming language: Python Besides being the go-to language for data workloads it is also heavily used by analysts and data scientists at Trade MeThe next step was to figure out the Orchestration Execution Deployment and Visualization layersOrchestration: this is the layer that defines what the workflow looks like It is also what not only data engineers but also data consumers need to be comfortable with As we decided to go with Python and we had been experimenting with Prefect for a few months it made sense to incorporate this powerful workflow management system as part of itExecution: to better manage dependencies and treat workflows as single and separate units we decided to containerize them We push every workflow as an ECR image to AWS and use AWS Fargate to run them This setup works well as our data lives on AWS and Prefect integrates well with FargateDeployment: we use Gitlab as our web-based Git repository and take advantage of its CI/CD functionality to register build and push workflows to our environmentsVisualization: we signed up to use Prefect Cloud which is an interface on top of their open source engine It allows us to connect AWS execution environments with their web UI to monitor troubleshoot and interact with workflowsTheres just one small missing part the name Here I introduce you to: The image below illustrates the architecture from end to endDWAAS pushes workflows to Test and Production environments Both consist of a combination of Prefect Cloud Projects (where workflows are registered) and AWS accounts (where workflows run) This gives our consumers the ability to validate their code and the data before going to productionAnyone who wants to deploy a Workflow through DWAAS must follow these steps: These are the deployment stages: This step builds a prefect based docker image containing the workflow code and installs dependencies like external python libraries being usedAn AWS ECR Repository is created for the workflow and the image is pushed to either our Test or Production AWS accountThis step registers the flow to Prefect Cloud and creates a Task Definition on AWS ECS The Task Definition points to the previously built ECR image and has Memory and CPU requirements set based on what is defined in the workflow config filesThe image below illustrates a successful deployment pipeline on GitlabOnce the Pipeline finishes we can check the Prefect Cloud UI and the workflow will be available inside the projectThe following image shows a graphical representation of a workflow run: DWAAS is still growing as an internal product at Trade Me but the engagement and interest we have had from our consumers has been invaluable Analysts have achieved great results wrangling data from multiple sources and have sped up existing manual processes dramatically They now own their workflows and have full autonomy to make changes or fix bugs as soon as they occurWe also see an opportunity for this to be used for other business processes outside of data science and analytics In a world where were constantly looking to reduce manual intervention having automated workflows across the business could provide many opportunities and improvementsTo finalize we learned that a framework doesnt solve the problem itself Recurrent training sessions with consumers and constant involvement with teams across the business are key to identifying opportunities to bring people onboard and make use of it Also while technology is evolving fast in the data space we still find that off the shelf solutions usually solve only part of the problem Our job as engineers requires creativity to deliver valueThe following are three areas I find important to focus on: AWS Fargate is the current chosen execution environment and it handles our use cases very well However to accommodate workflows that require Tasks to run in parallel we might look into how Prefect leverages on-the-fly creation of temporary Dask clusters on top of Fargate and KubernetesAs more people start using DWAAS across the organization the more important documentation becomes to help developers get up to speed with the framework Also once the maturity level reaches an acceptable point we might want to allow reviews and approvals to happen without interference from the Data Engineering teamExplore ways to verify data and code quality as part of the workflow development process Publish python libraries and packages to be reused across workflowsI would like to give a big shout-out to Jessica Young Joshua Lowe and Alan Featherston that along with me have been building DWAAS📝 Save this story in Journal👩\u200d💻 Wake up every Sunday morning to the weeks most noteworthy stories in Tech waiting in your inbox Read the Noteworthy in Tech newsletter
PGBQKyUsU5TLB6wAibzoZe,Knowledge of where customers are located and the places they are looking to travel and stay is critical to the Data and Strategy team at HotelTonight It helps us segment customers better provide a personalized app experience plan market growth strategies and a lot moreClickstream data is tracked on our apps and mobile web using Segmentios SDK and is received as hourly compressed json dumps on Amazon S3 The files are ingested flattened and transformed using Spark a powerful open source distributed data processing platform The result is then loaded into our Snowflake data warehouse which is a popular cloud based columnar databaseOne significant challenge we recently faced in Data Engineering was figuring out how to include geographical metadata(in the form of postal codes) on every event that our app fires If GPS is enabled on the customers device (roughly 70% of customers have this turned on) then their lat/long coordinates are captured and fired out enabling us to derive postal codes If the lat/long coordinates are unavailable the postal code can be less accurately determined from the users IP addressesThe first solution was to compute this in the Snowflake Data Warehouse using good old SQL after the events had been loaded into the staging area A comprehensive list of postal codes with their centroid latitude-longitude coordinates is available online provided by GeoNames for download free of charge under a creative commons attribution license We loaded all 12+ million into a table stripping out zip codes associated with US military postingsIP address to zip code maps are provided by certain proprietary databases such as Maxmind or ip2location The IPs can only be mapped to a certain degree of accuracy and hence is a fallback plan for when the lat/long is unavailableOnce events were staged into the data warehouse a SQL statement was run to update the table with a range join against the postal codes using the haversine function over a range of up to 1500 kilometers (to account for sparsely coded regions or codes that represent a large area)The problem with the above query especially in columnar databases is that a cross join needs to be performed between the tables In other words for every event the distance to every postal code needs to be computed followed by ordering and choosing the minimum This operation proved to be very expensive as it ran for over 10 hours despite being given large compute resourcesThe IP location database contains ranges of IP addresses (35 million ranges) which are translated into numbers and mapped to zip codes In order to avoid the range join we expanded it to all 4 billion+ possible IPv4 number translated entries (since columnar stores have block level optimizations for their numeric fields) and stored it in a table A javascript user-defined function translated event IPv4 addresses to numbers which was then joined with the expanded IP address table to get the associated zip codeStarting with version 32 Redis added geospatial abilities with GeoSets which enabled us to store latitudes and longitudes as geospatial indexes A Redis GeoSet is simply a Redis SortedSet with latitude and longitude hashed into a Geohash using 52 bit integer encoding Geohashing is a technique to encode a pair of lat/long coordinates into a single ASCII string A description on GeoHashing its implementation and its efficiency in answering geospatial queries can be found in the wiki pagePython scripts scheduled through Airflow a workflow orchestrator developed by AirBnB load Geonames postal code data once a week (postal codes dont change that often) into Redis using the GEOADD command with the postal code as the member for each lat/long coordinateThe data is ingested into a Spark dataframe which is a distributed collection of data in memory organized like a table structure with named columnsA dataframe UDF was then built to to obtain the postal code from Redis using the georadius api for every event UDFs in Spark are executed as lambda function calls which operate once per dataframe record In order to maintain state across UDF calls (within an executor) such as database connection pools Singletons in Scala implemented through companion objects need to be usedWe use the Redis java client Jedis rather than the direct Spark connector or the scala-redis library because neither currently support the APIs necessary to query the GeoSetsA UDF was built to connect to Redis with the connector above and run a Georadius query The Georadius search has a time complexity of O(N+log(M)) where N is the number of postal codes inside the bounding box of the circular area within a 1500 kms radius and M is the number of items inside the index which is about 12 millions entiresOn a small Amazon EMR spark cluster comprising of 3 m4xlarge instance(1 master and 2 workers) and a single Elasticache(Redis) non replicated instance sized cacher3xlarge uncompressing and ingesting data processing it by accessing Redis and finally loading into the Data Warehouse took 2 hours for 100000 events A couple of hours on a small EMR cluster was way better than 10 hours on a large compute warehouse but the goal was to compute it within an hour We could analyze from the that most of the time spent was on getting information from RedisThe time taken to search for the associated postal code could only be brought down by reducing the number of postal codes in the index and the number of postal codes per bounding circle To get there we tried a few different strategiesThe one large GeoSet (representing 12 million postal codes) is split into 2 sets of contiguous regions  UCM (US Canada and Mexico) and ROW (Rest of the World) More than 90% of our events are generated by customers from within UCM and UCM accounts for less than 1% of all postal codes in the world The assumption is that searching the UCM GeoSet first would find matches for most of the events and only a small portion of the searches related to ROW would be slowSearching for postal codes over a large radius can be slow especially in densely coded regions From the data it could be inferred that 92% of searches are within the first 5 miles and 98% of searches are within the first 10 miles This meant that searching over an incremental radius until a zip code is found would yield results fasterGeoSet search operations are implemented with a start search radius of 5 miles alternating between UCM and ROW GeoSets doubling until found with a boundary limit of 1500 kmsIt was also noticed that only one half of the event user lat/longs coming in the batch of events were unique A local cache over Redis was implemented to store the location coordinates and the postal codes to avoid a network call to Redis if the same coordinates had been processed earlierIP address ranges are stored as a Redis SortedSet (internally implemented as skip-lists) without the need for expansion as was done in the original solution with the score represented by the range start IP address The zip code retrieval is a zrevrangebyscore query which has a time complexity of O(log(N) + M) with N being the number of elements in the sorted set and M the number of elements being returned In our case N = 35 million and M = 1 since just the start of the IP range that the user IP address belongs to is requiredAfter the optimizations the overall time consumed by the Spark job to unzip files containing 100000 events and load them into the warehouse after determining the postal codes is now at an average of 12 minutesRedis is still single threaded and can be a bottleneck as multiple spark executors try to access the same Redis instance especially during large backfills Replication can help speed up reads and we are currently building a service that will help load balance several small replicas with the same geo indexes across spark executors in the spark clusterWhen cooking a meal doing yard work or fixing a kitchen faucet it is essential to have the right tools for the right job Over the past few years explosion of data generated in terms of volume and variety is leading to increasingly complex data integration challenges that cannot be solved just by one tool or technology Spark and Redis solved our problem of efficiently adding geographical context in the form of postal codes to our customer events That being said it cannot replace SQL which is our de facto language to solve many of our data warehousing and analytical problems rather complements it
gnwYicetVRb9EMxv24tjFK,For several years I have helped organizations ramp up their analytics efforts Believe it or not many organizations try to run their business without the benefits of proper historical analyses Instead they often do reporting out of transactional systems and share data using spreadsheetsThis may work well in the beginning but invariably limitations arise At this point a recruiter will typically reach out to me and ask (with the language everybody uses) Bob we want you to build us a data warehouse That is all well and good but a data warehouse is only one component What needs to be built is a data platform and a warehouse is only part of the data platform ecosystem The ins and outs of a data platform is a topic for another day For now suffice to say that I have had the genuine pleasure to work on five greenfield data warehouse projects in my career where I got to control everything from beginning to endAn early realization in my career in business analytics is that I was essentially a drug dealer Data is my product and everybody is an addict While that may sound unprofessional the reason I use this analogy is due to the behavior of executives when they do not get their data I know I do not have the personality traits necessary to be an executive of a large organization While I am pretty gruff myself it is expected that C-Suite executives are not So when a process breaks and reports do not get delivered or a data project runs behind schedule it is funny to me to watch these polished sophisticated individuals exhibit telltale withdrawal symptoms Cmon Bob where is my data? I need the dataI am not unsympathetic I know my product and its power which is why for several years now I have been obsessed with the idea of a real-time data warehouse It has only been within the past few years that the technology has been available to make such a thing possible without specialized equipmentAnybody who follows Big Data tech closely knows about Lambda Architecture It is a term coined by Nathan Marz the creator of Apache Storm a real-time processing framework It is a relatively old concept Lambda Architecture was developed in the olden days of a few years ago when we were still struggling with exactly-once semantics when delivering real-time data That problem has since been solvedAnother development that has made Lambda Architecture obsolete is the open sourcing of Greenplum That was an incredibly exciting development for me Now all of the elements necessary for a real-time data warehouse as I understood it are in placeThe reason Greenplum getting open-sourced is a big deal is because it put MPP databases on a low shelf where smaller organizations could reach it Prior to this if one wanted to store historical data in real time it required either the massive resources of a mega-corporation or the use of a NoSQL database which came with its own challenges • Many NoSQL databases have their own query language which means it was something extra the analyst needed to learn • The data had to be modeled in a fashion inconsistent with Kimball modeling techniques This can be a good or bad thing depending • Real time required the use of a transactional database Most NoSQL databases are not optimized for an analytic workload • The one NoSQL database out there that would fit the use case is Druid but it is not the easiest database in the world to install and manageGreenplum is a fork of PostgreSQL So far my experience with the software has been that anything I wanted to do in PostgreSQL I could do in Greenplum but since it is an MPP database I could do it faster and with more dataSo now that we have the technology the question for me is how do we translate the batch processing paradigm into a streaming one that still uses basic data warehouse loading techniques? Why I am trying to meld the old with the new? Because I do not find new ways of modeling data to be superior to the old methodsAs yet I have not been able to convince a client to use new technology to store their historical data As such there are several questions I have surrounding just how to store real-time data that I have yet to find answers to for various reasons not the least of which is that there are only 168 hours in the week and I only have two handsAt the last meeting of the Kansas City Chapter of the Association of Computing Machinery I had the opportunity to discuss some of these challenges They asked that I outline a few aspects I was dealing with as a starting point to find a speaker that was familiar with these issues so here we goKimball methodology is designed to store data in such a manner that it optimizes the function between data size and query speed Some duplication of data exists but it is fine as long as the queries come back fast This method of storing data has worked great for years and despite the fact that we can now store a lot of data cheaply I do not see a legitimate reason to abandon itMost methods of modeling big data that I see have to do with putting together specific datasets to answer predefined questions That is not what a data warehouse is about A data warehouse stores data in such a manner that questions can be answered ad hoc without an a priori understanding of exactly what is being sought at the time the warehouse was designedThe modern method to do that is a data lake The data lake is where all the data is dumped and determining what to do with it comes later While this sounds like a great idea reports from the field indicate theory and reality are not quite coming together I think the best description I have seen so far is the quip Every data lake turns into a Superfund siteStoring data in a data lake requires an immense amount of discipline that most organizations do not have It requires a process (full disclosure: process is my Valentine) good data governance clean accurate documentation and schema management all on a level of cult-like devotion All of this takes time Time is money so naturally people want to take shortcuts which then turns the data lake into an unusable toxic dumpsiteA Kimball modeled data warehouse on the other hand helps to enforce some of the standards necessary to keep data clean and useable Data governance is still necessary but other requirements such as maintaining referential integrity exist It is a relational model so analysts can use their SQL skills for data discovery even if the documentation is not quite clear It takes time to do these tasks but it is worthwhile in the long run As I am constantly admonishing executives a buck saved up front putting together bad process will cost hundreds of thousands (if not millions) down the line in maintenance costsFor these reasons I prefer modeling historical data using tried and true methods over more modern NoSQL methodsThere is also the matter of loading the tables Normally there is an initial process where data is staged In real time however this does not make sense as the preference is to be continuously loading data The issue is you have to take a record and do several things with it before it finally lands where it belongsMy lack of knowledge in this area may have more to do with the fact that I have spent more time building ETL than I have building applications In the old days (circa 2000) loading the contents of a webform into a table was a convoluted affair Now I understand that we have these things called Object Relational Mappings That may provide a solution here but I am not certainLate arriving data is a particular challenge with real-time data loading Dimensions should be loaded before facts If a fact comes in before the dimension it is attached to arrives this will cause a referential integrity problemThis problem is exacerbated by the extra challenge of having to process master data Processing master data requires a human in the loop While a human decides if an incoming dimension record is a true duplicate or only looks like a duplicate facts that depend on that dimension could be piling upI have some ideas about how to handle this but it would be interesting to see how someone in the real world handles itFinally we have the challenge of data cleansing Of the three issues data cleansing is the least thorny This is where things such as real-time processing frameworks could come into play Data cleansing for me is normally just a matter of doing simple things like converting binary values to the words yes or no These are things that could be easily done on the fly but my knowledge in this area is more theoretical than practicalIf we can find a speaker to come talk to us about how to do these things I will be sure post about it For now they remain open questions that I need to explore
VWuiVkGr6EVhtsZiiLX9mD,Recently I gave a presentation on processing IoT data with Apache Spark While preparing the deck for the presentation I created a slide that listed the available stream processing frameworks I argued with myself for a good five minutes about whether or not Kafka belonged on that listIn August Confluent the commercial firm backing Kafka released KSQL (Full disclosure: My firm Mass Street Analytics is a Confluent consulting partner) KSQL gives Kafka capabilities that mirror those found in Spark Structured Streaming This is the newest addition to Kafkas stream processing capability which already included Kafka Connect and Kafka Streams This makes Kafka and Apache Spark the latter of which is backed by Databricks competitors in the stream processing space So whats the big deal you ask? Its just two companies jockeying for market share right? Well it is not a problem in and of itself I just believe the development of KSQL has larger implications than an increase in capabilities in KafkaOne of the first things I learned when entering the world of open source is the Unix design principle of many small programs working together The entire stack of Big Data technologies works on this idea The small programs do one thing and pass the results of their work on to other programs When I (or anybody really) diagrams stream processing pipelines there are several distinct pieces of software in the pipe that each do something different and that contribute to the overall resiliency of the process Kafka is your data hub and then you have choices for processing the stream You can use Storm or Flink or most likely SparkHowever with KSQL you now have an option to remove a piece of software from the pipeline On the one hand this significantly reduces complexity On the other hand this appears to be a departure from the Unix design philosophy of simplicity in function This makes me wonder if commercial pressures might drive open source software in a direction that it might not have evolved to naturallyOpen source is amazing This is an opinion that I reached only in the last few years I remember open source from the late 90s to early 2000s You had to have a PhD in CS just to install anything Now installing open source software is almost as easy as installing commercial software A lot of open source software is robust enough to drop right into an enterprise environment The implementation will be bare-bones but if your DevOps team is high speed that will not be an issueFollowing the pattern established by Red Hat so many years ago once a piece of software becomes commercially viable you build a company around that software Companies backing open source software is a good thing Since open source software does tend to be bare-bones we need commercial entities to build in all the nice stuff that we have grown addicted to over the years like GUIs and monitoring toolsOpen source software has to be the greatest expression of the phrase necessity is the mother of invention Somebody had a problem and wrote a piece of software to deal with it This process often takes place without profit motive Since the software is not being built to respond to the needs of customers open source software is often very pure In DoD procurement parlance it is not gold-plated and by that I mean bloated with a ton of features driven by the individual and independent request of a segment of a user base Applications are lightweight and efficient There is a beauty to open source software that after decades of dealing with Windows I almost marvel at Aside from attending an airshow working with open source software is about as close to a religious experience as I getThe reality of commercial software development is you have to respond to the needs of users who are often paying large sums of cash to use your product I have heard of developers abandoning their open source projects because of the sheer number of Jira tickets that were created Obviously that is not an option when you are trying to make a buck off your workThe tech world has not been this exciting since the late 90s The breakneck speed of innovation is absolutely breathtaking I often tell my non-tech friends that we are taking stuff straight out of academia and putting it to work in corporations without the intermediate step of monetization My fear is that all of this great innovation and software is going to get bogged down by backing companies having to answer to Wall Street instead of users who are often engineers themselvesPerhaps I am playing the role of Chicken Little and my fears are unfounded I hope so I guess only time will tell
UicCj7WQqJg2idq5K68oWQ,There are a lot of things wed like to do with Big Data such as querying aggregating transforming But one of the most important aspects of Big Data that affects all subsequent operations is storage The different ways in which we store data allow us to optimize for the more critical functions while accepting a decrease in performance in other areasWhen discussing storage of Big Data topics such as orientation (Row vs Column) object-store (in-memory HDFS S3…) data format (CSV JSON Parquet…) inevitably come up In this post Ill be focusing on how partitioning and bucketing your data can improve performance as well as decrease costBefore diving in it is vital to know what kind of data you are working with For example you may need to know the size of your data set the cardinality of key/important columns and/or the distribution of values in said columns The better you understand your data the better youll be able to optimize itPartitioning data is simply dividing our data into different sections or pieces Filters or columns for which the cardinality (number of unique values) is constant or limited are excellent choices for partitions We often want to query on various filters such as date id or type Therefore if we partition our data on date and type our queries will skip reading unnecessary partitions and read only from sections which are guaranteed to contain the necessary informationFor example at Chartboost we collect billions of auctions each day Ill be working with a small subset of the data along with AWS Athena to illustrate how partitioning can be usefulBelow is a CREATE TABLE DDL for my example auctions tableAs you can see this table is already partitioned by date Therefore if I query for a specific date range: Athena will only scan data under partitions that matching those dates This isnt quite good enough however so lets try to improve the table Often times we need to query or aggregate on a specific app placement or os And since I know my data I can see that I have 25 unique apps two placement types (Interstitial Rewarded) and two OS (Android iOS) These columns have well defined cardinality and therefore are excellent candidates for partitioningNote that country column would have also made a good candidate for partitioning since the values for countries are also limited And while it is tempting to partition on os as well (there are very good use cases) I chose not to because in my example data set I have many different OS versions and the cardinality will only increase in the futureNow with the following table: queries such as: will be much more performantBucketing also divided your data but in a different way By defining a constant number of buckets you force your data into a set number of files within each partition Think of it as grouping objects by attributes In this case we have rows with certain column values and wed like to group those column values into different buckets That way when we filter for these attributes we can go and look in the right bucket Bucketing works well when bucketing on columns with high cardinality and uniform distributionFor example in the above table both id and timestamp make great candidates for bucketing as both have very high cardinality and generally uniform dataThe bid column seems perfect to bucket on but this would be a mistake because it the example data set we have a large number of no-bids which corresponds to 00 in bid value Therefore even though we have high cardinality the overwhelmingly skewed distribution in bid values would put most of our rows into a single bucket which would decrease performancePartitioning and bucketing can be very powerful tools to increase performance of your Big Data operations But to properly use these tools you need to know your data However data can be really complex and difficult to understand in which case trial and error can help you get a better idea of your data distribution or point you in the right direction I found AWS Athena CTAS queries extremely useful in this regard as I was able to quickly create tables experimenting with different partitions and bucketing schemes
E3SQLjSkcQ2J6b9iBoKaPh,Im Elena and I have been working in HelloFresh for the past two years specifically in the Data Platform TribeBefore jumping into a description of my day-to-day work Id like to tell you readers a little about myself and how I moved from Cagliari to Berlin It was the summer of 2018 I had just graduated from my Master Degree in Electronic Engineering and I was sending out applications for my first job: I wanted it to be as challenging as possible and based in a city with a lot to offer which is what anyone wants from a jobI honestly didnt hope to find it with my first attempt but Fortune favours the bold and I started the interviewing process with HelloFresh: my interviewer made me feel at ease and he was the best I had the chance to speak with until then; he even asked one of his Italian female colleagues to speak with me so that she could answer all my questions about Berlin and dissipate my doubts Thinking in retrospect that was incredibly helpful and thoughtfulAfter overcoming the remaining technical steps of the process I signed and started right away a new period of my life in the vibrant city of Berlin working as a Junior Data EngineerThe first few months were a bit rough to be honest as I was plunged into a fast-paced environment which required an extra dose of flexibility and hard work to cope with the changes but there was a silver lining: since one of the data engineers decided to pursue a different opportunity I started working more and more on one of Data Platform main projects which is called Materialized ViewsIts a self-service tool used by all the analysts in HelloFresh to build their own views which will be refreshed automatically and it is very easy to use since the analysts will only need to pass a SQL file and a configuration file My first objective was to build validations on the configuration files passed by the analysts so that the view was actually buildable Once this was set up the next step was automating the most common jobs on Jenkins such as running backfills so that our users would be more self-sufficient and agile in their day-to-day work To this day I usually help the users when its necessary to troubleshoot problems or share the knowledge on how to work with Materialized ViewsSince day one I have never stopped learning and not only because for me everything was new: one of the core pillars of working in HelloFresh its that Learning never stops and every week there are various talks and presentations about the solutions and technologies applied within the company I have talked as well to the whole company during one of our Fridays Lightning Talks! Lightning Talks are very short presentations (in my case it was a 5-minutes presentation) and allow different speakers to present their topic to the entire company which is usually something they worked on recently or something they are passionate about This format aims to be a dynamic and informal way to share the knowledge and discover colleagues from different parts of the businessI talked about Materialized Views as my team and I had introduced some exciting improvements for this project that made it an almost complete self-service tool for our analysts! Among these improvements there were: automated deployments Jenkins jobs to create and backfill incremental views and overall a reduced need for support from Data EngineersFor 2020 the Data Platform Teams goal is to provide a self-service platform for all HelloFresh to allow the companys growth by making data-driven decisionsThe plan to achieve it is complex but exciting: we will maintain and build new self-service tools but we will also focus on knowledge sharing so that other analytics and engineering teams can maintain their own data pipelines; as already mentioned we are also moving to new technologies to increase reliability and provide a state-of-the-art data platformAll of the above means that during the past years I have been constantly improving not only my technical skills with software and data engineering problems and building my hands-on experience on new technologies but also increasing my leadership and teaching abilitiesThe hard work of the past two years has paid off as of July I was promoted and am now officially a Data Engineer!Even though working at HelloFresh can be very demanding sometimes I feel grateful to be part of the HelloFresh family as I am supported by my awesome colleagues and I know that we are building something to be proud of
oFZGAmAzGRaPFAULJoeEq2,Data yes data A new language spoken by every industry in the modern eraSince the data helps you in many ways be it a decision making or measuring your KPIs or many other ways It is important to know how we consume the data? how we transform the data? and also how we look at the data? (since a single data point may have multiple dimensions)Today well showcase our data platform at Saltside and discuss about how were building it Also well talk about the traditional Data-Pipeline and various technologies to consider before you jump in into building oneData undergoes various stages during its journey in data-pipeline where data gets stored and undergoes various transformationsYoull have the source data coming in and data gets stored and transformed in your pipeline and finally goes out for helping your business and stakeholdersFollowing are the different technologies which are good to know in building a data-pipeline And what specific tools/frameworks to choose is depending on the use case you are trying to solveNow coming to the interesting part that is How did we start the data analytics journey at small/mid sized Organisation like Saltside? From the early stage of our data-platform establishment wed hundreds of thousands of events coming in every day So we took the traditional way of pipeline development to process themOur data-pipeline at Saltside looks likeObviously its a very high level design so lets break it into pieces to understand it betterKafka message queuing systemWeve various data sources lets call them as upstream data generator These upstream data generator would push the events to Kafka broker using Kafka publisher So lets look into Kafka architecture firstThe above Kafka architecture is out of the scope for this post but in short Kafka is a message queuing system So lets talk about our Kafka implementation a bit We have multiple topics in Kafka segregating the homogeneous data at one place and divided by partition based on desired business logic with replication factor for disaster recovery just in case if a node failsSome of the basic configuration of our Kafka cluster Note that some of the configs may change for a specific topic • node Kafka cluster across all the marketsApache Storm processing engineNow weve data stored in Kafka next step is to consume this data and transform it according to the needs and get it into a required shape For that matter weve our processing engine as Apache STORM which is a real-time distributed processing frameworkAgain its a very high level design We consume data from Kafka using KafkaSpout in our storm topology And we do have multiple topologies for consuming data from various Kafka topics • nodes storm cluster with30+ topologies processing 100s of thousands of msgs from KafkaRedshift data storage systemThe final bit once we slice and dice the data in Apache Storm Next step is to store it in data-warehouse But we dont store them directly to Redshift instead storm will upload the data to S3(simple storage service from AWS) and then do a Redshift S3 bulk load Why we keep a copy in S3? just to a backup copy and also create a data-lakeRedshift with 6TiB storage for analytics And rest of the data will be stored in S3 data lakeAfter the data gets stored in Redshift we run a periodic ETL (Extract-Transform-Load) jobs triggered from Apache Airflow (A tool from Apache for job scheduling) These jobs are written in SQL and triggered from simple python script within AirflowOnce were done with all the processing transformation slicing and dicing finally a cube is ready for reporting So for that matter weve Tableau BI tool for consuming the aggregated data from Redshift and visualise it
H2DQHdHDwnNgY6gSCKTpEC,The General Data Protection Regulation (GDPR) will become effective on May 25 this year As a result the industry is applying both small and large changes to the way they process their data To help the local community learn more about GDPR and its implications we co-hosted the second Data Engineering Meetup in Oslo last week with more than 120 attendeesThe first talk given by Graham Moore from Sesam discussed how to bridge the gap between analysis and implementation Graham explained how to think about the process of fulfilling GDPR restrictions and what is the real goal He explained the most important articles of the regulation including right to access data portability rectification consent and last but not least the right to be forgotten It was pointed out that GDPR is a continuous process and is never done because the nature of consent and contracts change same as systems that hold them Moreover Graham presented a potential high-level architecture of a GDPR solution as well as the end-to-end process with possible fields for automation based on one of the products his company works onNext speaker Narasimha Raghavan Veeraragavan of Schibsted presented data lifecycle management systems The talk started with a recap of GDPR roles: data subject data controller data processor and supervisory authority as well as previous rights of data subjects including right to access and right to erasure Narasimha continued with the technical challenges he faces at work on a daily basis with regards to access and erasure This mainly included resolving data dependencies and deletion logic Schibsteds approaches for solving those problems were presented including their topic-based system called Privacy BrokerThe third speaker was Andy Petrella from Kensu who discussed data science governance He introduced the audience to the governance of data and emphasized how many new things we can still discover on top of data activities Then he continued with the importance of the data pipeline in the process of decision making He pointed out how important it is to know the original source of your data and what anxieties we have to tackle In the end Andy mentioned that both accountability and transparency can be more easily achieved with an automated process registry which is the main product of his companyThe penultimate speaker Torgeir Hovden from Signatu focused his presentation on the consent and data ecosystem His talk started with a short history of data processing related milestones in the software industry and continued with emphasizing the main point of GDPR: Its forbidden to process the data unless you have permission Torgeir also presented a very interesting demo of Google Analytics consent on data collection embedded in the websiteLast but not least the final speaker was Jens Christian Gjesti from the law firm Kvale who discussed what responsibility we have for the software and systems we use Jens started by clarifying some common misconceptions about GDPR Then he continued with what is required to be GDPR compliant and presented the law interpretations and details Moreover he presented what GDPR holds us responsible for: accountability lawful processing its technical meaning and how to prove this One of the main points made by the speaker was that using 3rd party system (eg Facebook page) does not free you from responsibility for your sensitive dataA lot of great content was discussed at this event and we hope others felt better informed about GDPR and its technical implications
4TFYV8URZotAyyKLSfPMi8,This week Tapad was proud to collaborate with Schibsted Finn and Confluent to kick off the first event for the Data Engineering Oslo Meetup group With more than 60 people in attendance it was a very successful event that focused on educating the audience on Apache KafkaThe first speaker Kai Wähner of Confluent presented his talk: Introduction to Apache Kafka as an Event-Driven Open Source Streaming Platform In addition to covering the basics of Apache Kafka and what it can do he addressed how you can leverage Kafka Connect for integration and the Kafka Streams API for building lightweight stream processing microservices in autonomous teams He also gave some good examples from his work at ConfluentHis talk was followed by Fredrik Vraalsen of Schibsted who talked about Kafka and Kafka Streams in the Global Schibsted Data Platform He presented how Schibsted has set up a new global streaming data platform using Kafka and Kafka Streams replacing a homegrown solution based on Kinesis and micro batches in Amazon S3Overall it was a great opportunity to learn from some very talented engineers and network with the larger Oslo engineering community There will be more events to follow in the New Year so stay tuned by following the Data Engineering group here and our Twitter @TapadEng If youre interested in learning more about Tapad and our own work with Apache Kafka leave us a comment or reach out to us at engoslo@tapadcom
YdxDVfMLzcqVZv96LxMCKs,"Redshift is quickly taking its place as the worlds most popular solution for dumping obscene amounts of data into storage Its nice to see good services flourish while clunky Hadoop-based stacks of yesterdecade suffer a long painful death Regardless of whether youre in data science data engineering or analysis its only a matter of time before all of us work with the worlds most popular data warehouseWhile Redshifts rise to power has been deserved the unanimous popularity of any service can cause problems… namely the knowledge gaps that come with defaulting to any de facto industry solution Most of us are locked into Redshift by default by merely being AWS customers While we save time on researching and comparing solutions this might come at the cost of answering some vital questions such as why Redshift? or how do I even get started with this thing? If youre looking for some easily-to-digest mediocre answers youve come to the right placeIf you happen to be new to Redshift or data warehouses in general youre in for a treat If relational databases were Honda Civics Redshift would be a Formula 1 race car While Formula 1 machines might outperform your Honda by various metrics the amount of upkeep and maintenance that goes into F1 racers would make it unsuitable for casual usage Using Redshift effectively requires much more awareness of underlying database technologies than one would need to build a system which prioritizes ACID transactionsWhen compared to traditional databases data warehouses makes a ton of tradeoffs to optimize for the analysis of large amounts of data Maintaining a Redshift cluster is a lot of work Thus its essential to understand the pros and cons of Redshift before making such a big architectural decision (this notion has already been articulated by people more intelligent than I am)Every Redshift cluster is comprised of multiple machines each of which only stores a fraction of our data Each of these machines working in parallel to save and retrieve data which adds a ton of complexity to how we should work with dataNodes which perform computations are called compute nodes These nodes are managed by a leader node which is responsible for managing data distribution and query execution amongst the other nodesEach compute node is actually split into multiple partitions themselves called slices Depending on the size of nodes in your cluster each compute node might support anywhere between 2 32 slices Each slice is an individual partition containing a fraction of our dataset Redshift performs best when slices have a close-to-equal distribution of data When data is disproportionally distributed across slices this phenomenon is called skew (well touch on how to optimize against skew in a bit)Redshift can accommodate a variable number of machines in a cluster thus making Redshift horizontally scalable which is a key advantage Redshift can scale outward almost infinitely which makes Redshift great for use cases where we need to query huge amounts of data in the realm of petabytes and beyondRedshift is great for data analysis but we shouldnt use Redshift to power production-ready applications To understand why one consideration would undoubtedly be the associated tradeoffs of columnar storage Redshift is a column-oriented database management system which means that our data is partitioned by column as opposed to rowIn a traditional database rows are assigned an index to identify rows uniquely: retrieving a row by index will return the entirety of that row A relational database with an index will perform significantly faster than a relational database without one because its faster to query rows sorted logically Imagine I asked you to try to find items in a messy garage If I asked to find the first 100 items I placed in here it would be much easier to accomplish this had I first arranged everything in the order in which I added them to the garageColumnar databases trade the benefits of traditional indexing to solve a problem which becomes more significant with scale: the time of reading records from disk If your tables have millions of rows and tons of columns the mere act of retrieving entire rows creates a bottleneck Partitioning data by column means that each time we retrieve a value from a partition were now only retrieving a single value per partition: this significantly reduces the load we put on the hard disk and results in overall faster speed across massive amounts of dataWhen dealing with production-level information like user information or transactions the data needed to keep an application running is relatively small Applications should prioritize staying functional and minimizing loss of data while high volumes of database interactions take place Data warehouses solve a different problem which is allowing for analysis across tons of data in a performant manner typically dealing with a small number of transactions at any given timeData is added to Redshift by first moving into a file stored in an S3 bucket as a static file (CSVs JSON etc) Once in S3 data can then be loaded into Redshift This workflow of pipeline > S3 > Redshift is changed a bit by the introduction of Redshift SpectrumUnlike regular Redshift Redshift Spectrum can directly query data stored across S3 buckets This can make life a bit easier for people who dont necessarily want to write scripts for every piece of data that comes their way: its a good way to sift through absurd amounts of historical data for ad-hoc requests We wont be concerning ourselves too much with Redshift Spectrum for nowRedshift charges by uptime with the smallest single-node cluster costing $025 per hour Running a single-node cluster would be pointless so its pretty safe to assume any production Redshift cluster will cost at least 1 dollar per hour Thats a steep price for hobbyists just looking to learn the ropes In practice production Redshift clusters will easily break costs of hundreds of thousands of dollars per yearIts clear from pricing alone that Redshift isnt intended for hobbyists thus it can actually be quite difficult to get up-to-speed with Redshift management unless a company happens to be paying the bill associated with you figuring out what youre doingRedshifts traditional pricing structure can be contrasted with competitors like Google BigQuery BigQuery charges by size of each individual query which allows companies of any size to start using a data warehouse before they can justify the costs of a dedicated cluster Due to the fundamental technical differences between these solutions Redshift will execute queries significantly faster than BigQuery and the associated expenses reflect thisTo get more intimately acquainted well quickly look at what the creation of a Redshift cluster looks likeLike most things in AWS getting started with Redshift kicks off with Amazons excruciating IAM permissions Once we have a user with the proper permissions we can move on to creating our cluster using Redshifts Quick Launch Lastly well need to be sure we set the proper security groups to prevent our cluster from getting pillaged by the outside worldLets focus on what goes into creating a cluster: The default settings make creating a Redshift cluster easy Aside from naming conventions and user-related options all we need to do is select which type of node we want and the number of total nodesRedshift has two unique flavors for loading data: LOAD or COPY Understanding the differences between these methods is a great way to wrap our head around how data warehouses workCOPY is very much the preferred method for loading data: its a lightning-fast way of loading massive amounts of data into Redshift (which is our primary use case) Unlike INSERT COPY is designed to load data en masse by utilizing parallel loading When we move data into our cluster via COPY each node in our cluster works in tandem to load from multiple sources at once (such as multiple files in an S3 bucket) Splitting incoming data amongst multiple files will result in faster load times with COPYCOPY accepts data from 4 types of sources: S3 EMR Dynamo or even directly from a machine via SSH 3/4 of those options are AWS products and I wouldnt be surprised if this were done intentionally to incentivize customers to sticking to AWS architecture in order to utilize the superior optimization of COPYPROTIP: Redshift allows you to perform the reserve of a COPY by running an UNLOAD query Using UNLOAD will load the results of a query into an S3 bucket like so: Then we have INSERT: a word that should look familiar from SQL INSERT will always be slower than COPY and data compression is rendered inefficient when loading data in this way The most common use case for INSERT is when COPY is not an optionRedshift has a couple of housekeeping operations intended to run after adding or modifying massive amounts of data in Redshift: VACUUM and ANALYZE Using VACUUM purges data marked for deletion thus recovering space and allowing the sort order of records to be updated VACUUM was previously an operation which needed to be run manually by a Redshift admin Luckily for us Redshift has since been updated to run VACUUM in the background automatically after manipulation Regardless we should be aware of what VACUUM does and when it is runningTo find records most efficiently during a query Redshift looks at the statistical metadata of a given table to help determine where a record might be hiding ANALYZE updates this metadata for a given table The best time to run ANALYZE would be after a sizable percentage of a table has been modifiedAn important topic we should be aware of is table distribution styles Distribution styles are set on tables upon creation and determine how rows will be distributed amongst slices in the cluster Distribution style can be set to AUTO EVEN KEY or ALL: Unlike traditional databases Redshift doesnt support indexes Instead Redshift uses sort keys and dist keys to aid in the organization and retrieval of rowsBy setting a sort key on a table were telling Redshift that the values in this column are a useful indicator for finding data within a table If you think back to algorithms 101 its easy to imagine how dealing with sorted data would be beneficial to a search query similar to finding values in a sorted array versus an unsorted array As a result how you set you sort keys will have a significant effect on how quickly your queries will be able to executeHow you define your sort keys should differ depending on how you plan on querying your data Setting nonsensical sort keys on your data will actually slow your queries down as Redshift attempts to make sense of finding data via some worthless indicator Try to set sort keys based on columns which can 1) be sorted in such a way that is coherent and 2) are useful to the way in which youre expecting to query your data Sort keys can only be set during table creation so its important to try and get this right off the bat: Its extremely important to always cognizant of how Redshift is expecting to look for data whether youre creating your tables or querying existing tables which already have their sort keys determined""In the above example we sorted a table named automobile_sales by a column named model_number (a model number would presumably be an ID which represents the type of car sold) Since we're working data which is distributed across multiple machines there's a very real possibility that rows with the same model number will be saved to different nodes in our cluster If we were able to explicitly bunch together rows with the same model number on the same cluster nodes it would be much easier to find records which had the model number for a Ford Taurus because we'd be telling Redshift that all records for Ford Tauruses live on node A (or whatever)Distribution keys tell Redshift to store all rows which share the value in a column on the same machine to improve query performance Without a distribution key present Redshift would be forced to hit every machine in our cluster and perform and aggregation to see if Ford Tauruses exist on each machine To prevent this from happening we can set an additional distribution key on our sort key: What if 90% of all automobiles sold happen to be the same model? That would suddenly make model_number a weak candidate for a sort key/distribution key since all these rows would be bottlenecked to a single machine This disproportionate distribution of data across slices referred to as skew To visualize how skew affects query time negatively check out the performance of a query on a badly skewed Redshift cluster: We can see clearly that Node 0 is bottlenecking our entire query by taking the majority of a querys load while the other nodes site idle This is why its important to consider how evenly distributed our data will be when were deciding on distribution keysIn our automobile_sales example we could instead rely on something more evenly distributed like the date of transactions: Setting useful sort keys and distribution keys is very much dependent on your data set and how you plan to interact with said data There are no one-size-fits-all answers in Redshift Performance tuning is a continually moving target where only you can determine works best for your data I recommend investigating query execution time and experimenting with different sort key/distribution key combos using Redshifts performance toolsRedshift shares the same concepts of unique keys primary keys and foreign keys from relational databases but these keys exist strictly for informational purposes Redshift will not actually enforce the uniqueness of unique keys which are intended to be enforced by whichever layer exists in your ETL pipeline before Redshift""Interestingly the presence of these keys does help Redshift infer things about your data during queries which can help in the execution of queries such as those which involve a SELECT DISTINCT clause It's worth noting that setting primary or unique keys on columns which don't actually contain unique values can result in queries returning incorrect resultsTo continue optimizing the performance of your Redshift queries AWS actually provides a useful query in their documentation to inspect your tables: This query returns beneficial high-level information about your tables such as which keys exist and how your data distribution is skewed For the sole purpose of identifying skew this query might be even more helpful: We could spend all week getting into Redshift performance tuning but I think weve covered enough ground for an intro-to-Redshift post (to say the least)Originally published at https://hackersandslackerscom on June 20 2019"
5WFMZjMAKzWaZEWyLzPDdL,"Now that weve gotten the fundamentals of creating databases and tables out of the way we can start getting into the meat and potatoes of SQL interactions: selecting updating and deleting dataWell start with the basic structure of these queries and then break into the powerful operations with enough detail to make you dangerousAs mentioned previously SQL operations have a rather strict order of operations which clauses have to respect in order to make a valid query Well begin by dissecting a common SELECT statement: This is perhaps the most common structure of SELECT queries First we list the names of the columns wed like to select separated by commas To receive all columns we can simply say SELECT *These columns need to come from somewhere so we specify the table were referring to next This either takes a form of FROM table_name (non-PostgreSQL) or FROM schema_nametable_name (PostgreSQL) In theory a semicolon here would result in a valid query but we usually want to select rows that meet certain criteria""This is where the WHERE clause comes in: only rows which return true for our WHERE conditional will be returned In the above example we're validating that a string matches exactly ValueSomething that often comes in handy is selecting distinct values in a column In other words if a value exists in the same column in 100 rows running DISTINCT query will only show us that value once This is a good way of seeing the unique content of a column without yet diving into the distribution of said value""When selecting data the combination of OFFSET and LIMIT are critical at times If we're selecting from a database with hundreds of thousands of rows we would be wasting an obscene amount of system resources to fetch all rows at once; instead we can have our application or API paginate the resultsLIMIT is followed by an integer which in essence says return no more than X resultsOFFSET is also followed by an integer which denotes a numerical starting point for returned results aka: return all results which occur after the Xth result: The above returns the first 50 results If we wanted to build paginated results on the application side we could construct our query like this: Such an application could increment page_number by 1 each time the user clicks on to the next page which would then appropriately modify our query to return the next page of resultsAnother use for OFFSET could be to pick up where a failed script left off If we were to write an entire database to a CSV and experience a failure We could pick up where the script left off by setting OFFSET equal to the number of rows in the CSV to avoid running the entire script all over again""Last to consider for now is sorting our results by using the ORDER BY clauseOf course we can select rows with WHERE logic that goes much deeper than an exact match One of the most versatile of these operations is LIKE""LIKE is perhaps the most powerful way to select columns with string values With LIKE we can leverage regular expressions to build highly complex logic""Passing a string to LIKE with percentage signs on both sides is essentially a contains statement % is equivalent to a wildcard thus placing % on either side of our string will return true whether the person's first name middle name or last name is WadeThe opposite of LIKE is of course NOT LIKE which runs the same conditional but returns the opposite true/false value of LIKE: DateTime columns are extremely useful for selecting data Unlike plain strings we can easily extract numerical values for month day and year from a DateTime by using MONTH(column_name) DAY(column_name) and YEAR(column_name) respectively For example using MONTH() on a column that contains a DateTime of 2019-01-26 05:42:34 would return 1 aka January Because the values come back as integers it is then trivial to find results within a date range: NULL is a special datatype which essentially denotes the absence of something therefore no conditional will never equal NULL Instead we find rows where a value IS NULL: This should not come as a surprise to anybody familiar with validating datatypesMany things could result in a failed insert For one the number of values must match the number of columns we specify; if we dont weve either provided too few or too many valuesSecond vales must respect a columns data type If we try to insert an integer into a DateTime column well receive an errorFinally we must consider the keys and constraints of the table If keys exist that specify certain columns must not be empty or must be unique those keys must too be respectedAs a shorthand trick if were inserting values into all of a tables columns we can skip the part where we explicitly list the column names: Heres a quick example of an insert query with real data: Updating rows is where things get interesting""Thats as simple as it gets: the value of a column in a row that matches our conditional Note that SET always comes before WHEREYou will find that its common practice to update rows based on data which already exists in said rows: in other words sanitizing or modifying data A great string operator is CONCAT() CONCAT(string_1 string_2) will join all the strings passed to a single string""Below is a real-world example of using CONCAT() in conjunction with NOT LIKE to determine which post excerpts don't end in punctuationREPLACE() works in SQL as it does in nearly every programming language We pass REPLACE() three values: We can do plenty of clever things with REPLACE() This is an example that changes the featured image of blog posts to contain the retina image suffix: I across a fun exercise the other day when dealing with a nightmare situation involving changing CDNs It touches on everything weve reviewed thus far and provided an excellent example of what can be achieved in SQL aloneThe challenge in moving hundreds of images for hundreds of posts came in the form of a file structure Ghost likes to save images in a dated folder structure like 2019/02/imagejpg Our previous CDN did not abide by this at all so had a dump of all images in a single folder Not idealThankfully we can leverage the metadata of our posts to discern this file structure Because images are added to posts when posts are created we can use the created_at column from our posts table to figure out the right dated folder: Lets break down the contents in our CONCAT: The result for every image will now look like this: Lets wrap up for today with our last type of query deleting rows: Originally published at hackersandslackerscom on February 22 2019"
2qdxKMVB79AiQnANf4eVhr,Here at HMH we use many types of databases relational databases like Postgres and Oracle NoSql databases like DynamoDB and graph databases What do you do when you need a single view for analytics purposes over different types of databases? Given the popularity of SQL among analysts we decided to create a relational export of our graph database This article discusses approaches to this problemThe above graph models school classes and the students that are in those classes we also introduce a third node type for group which represents a class that is divided into smaller groups of students for tuition reasons If we were to map these to a relational world each node type would become a table and edges could be mapped to either a foreign key column in one of the two tables it joins or it could be mapped to a join table See diagram below for what this would look like: If your graph is running on Neo4j graph database you could use a Cypher Query Language (cql) query such as the following to extract a relational view of your graph dataThe above query works on all nodes in the graph which are of type Section For each node it finds it returns a row with 7 columns those columns are sectionRefId period name hasGroups leadTeacherCount teamTeacherCount and studentCount For the first 3 columns we tell cypher to simply select the value from properties on the node for the latter 4 columns we get cypher to work a little harder We use the cypher EXISTS function to figure out if it has outgoing edges of type groups to nodes of type Group If any edges exist cypher returns hasGroups as true We can also get cypher to count all outgoing edges by using the size function  this allows us to return some key metrics associated with the class  the number of teachers and studentsSo if you are running a graph on Neo4j you can prepare an extract like the above for each of your tables and run this on perhaps a daily basis to get a daily relational snapshot of your graph But what if you are not using Neo4j or you have a very large graph dataset but you dont have access to a single large machine that such a graph on Neo4j would require? Or what if you only need to run this process once a day and you dont want to keep a server running for 24 hours In that case you might be less interested in a graph database server like Neo4j and more interested in a Graph processing engine like GraphFrames on Apache Spark Graph processing engines are very different from graph databases  they do not support OLTP transaction processing However a key benefit of running graph algorithms using GraphFrames on Spark is being able to parallelise graph algorithms over commodity hardware This can really make a difference if your dataset is very largeIf we take the above sample problem we can solve it using GraphFrames -see full code example here: https://githubcom/kevinmcgarry/spark-graph-public-demo
LyvNPs43fEetuEm583WYnc,Kubernetes has emerged as go to container orchestration platform for data engineering teams Kubernetes has a massive community support and momentum behind it In 2018 a widespread adaptation of Kubernetes for big data processing is inevitable
SwrGWMMGRZjtLF3KWhPvmt,"In this article I will use the Schiphol Flight API StreamSets Data Collector Apache Kafka ElastichSearch and Kibana to build a real-time data pipeline of arriving Flights at Schiphol (Amsterdam international airport) Ill show you the high-level architecture and corresponding configurations that enable us to create this data pipeline The end result is going to be a Kibana dashboard fetching real-time data from ElasticSearchTo follow this tutorial you should have some basic understanding of Docker and Data Engineering concepts such as transformations and streamingThe high-level architecture consists of two data pipelines; one pipeline streams data from Public Flight API transforms the data and publishes that data to a Kafka topic called flight_info: The second pipeline consumes data from this topic and writes the data to ElasticSearch Finally Kibana is connected to ElasticSearch and fetching new data every 5 seconds: In the remainder of this article I will briefly go over each component of the pipeline and their corresponding configurationsWe can easily spin up all the services that were going to use with this simple docker-compose fileThe Schiphol Public Flight API is a REST API that offers a wealth of information on current and scheduled flights to and from Schiphol airport Its free of use and access is being granted to it after signing up on the websiteAfter signing up you can access detailed information about flights such as the destination Flight status Airplane type scheduled time and more For our particular use case were interested in incoming flights and other useful information closely related to thisStreamSets Data Collector (SDC) is an Open Source low-latency ingest tool that lets you design complex any-to-any data pipelines in an easy to use UI It is designed to work with continuous arriving data (streaming) and batch data as well SDC is compatible with a wide variety of systems and tools such as Spark relational databases the cloud (AWS Azure GCP) Hadoop and moreIf you executed the previous docker-compose command then you can access SDC through this link The default login credentials are admin with password admin and the next step is to create a new pipeline called Flight data to KafkaAfter clicking Save you are being left with an empty pipeline Select HTTP Client as origin and start configuring the HTTP Client origin with the following configurations: This configuration allows us to do a GET request to the Public Flight API which on his turn will respond with a large JSON response containing 20 flights The Public Flight API works with pagination which basically means that its designed to keep the responses down to a reasonable size In the link header in the response the URI for the next and last pages are returnedIn all requests we need to provide both the APP ID and APP KEY This has to be done in the Header from version v4 and onwards We set the Mode to Streaming and we set the HTTP Method to GET All the HTTP Client parameters such as theResource URL app_id and app_key are provided by the Public Flight API and can easily be found hereEach JSON response is one big JSON object containing several different flights However its much easier to work with one JSON object for each flight So we use a Field Pivoter to flatten the Big JSON object into one message for each flights: After flattening the JSON object we can easily select the fields that were interested in with a Field Remover: So we are now left with the fields that were interested in (see: left side of below picture) However were still burdened with a nested field (Route-destinations-0) in order to overcome this discomfort we will apply a Field Flattener This field flattener flattens our JSON object in a more desirable format (the right side of the picture illustrates the end result)""Lastly we will also apply a Field Renamer to our JSON object This Field Renamer simply converts the field /routedestinations0' into /departedFromOur data can now be written to Kafka; the final step in our Flight data to Kafka pipelineKafkaWhat is Kafka? In simple terms: Kafka is an open source software which provides a framework for storing reading and analysing streaming data in a distributed environment Kafkas popularity can particularly be explained by its high throughput (millions/sec) scalability and reliability If youre unfamiliar with Kafka take a look at their website and documentationIn SDC we have to specify Kafka Producer as a destination Theres a chance that Kafka Producer isnt listed in your destinations tab in that case you have to install it via the Package Manager which you can access by clicking the gift icon on the top of the screenThis configuration (eg previous picture) creates a Kafka topic called flight_info In the Broker URI you have to specify <Docker Host IP:9092>  Thats it we have finished our first pipeline and it should look like this: The second pipeline consists of only two stages The origin is a Kafka Consumer that reads data from the topic flight_info and the destination of this pipeline is ElasticSearchWhat is ElasticSearch? ElasticSearch is a distributed open source search and analytics engine for all types of data including textual numerical geospatial structured and unstructured Known for its simple REST APIs distributed nature speed and scalability ElasticSearch is the central component of the Elastic Stack a set of open source tools for data ingestion enrichment storage analysis and visualisationTo keep things simple for this article we will just write the flight data to an ElasticSearch index (eg a kind of table) called flights set the mapping to flights (although we didnt upload any metadata to ElasticSearch) and set the Document ID field to ${record:value(/flightName)}Finally we check Enable Upsert so records will automatically be updated or inserted into ElasticSearch based on the Document IDNow we can run both pipelines Flight data to Kafka and Kafka to ElasticSearch: Kibana is an open source data visualisation plugin for ElasticSearch It provides visualisation capabilities on top of the content indexed on an Elasticsearch clusterKibana can be accessed through this link After clicking the link we need to set up an index pattern Search for the index flights and create the index pattern we should now be able to visualise and see our data in KibanaAs you can see creating a relatively complex pipeline like this has been simplified with several orders of magnitude thanks to StreamSets Im looking forward to using this powerful tool in my current project and I might as well write an article about StreamSets Transformer in the near future The docker-compose file of this article can be found on here Please feel free to provide me with any feedback or commentsFinally feel free to contact me if youre interested in attending a Data Engineering Bootcamp or if youre just curious about how I experienced these two intense months:) I can definitely recommend this for a perfect kickstart of your career"
QRNN2z2HZhsqdmFNTsFEXg,Spark 300 was officially released yesterday (18/Jun/2020) and it is a major change (no pun intended) in the most popular data processing engine of the world (Flink you are great as well but Spark is a bit more famous)First a reminder what actually is Spark? It is an open-source fault-tolerant and high-speed in-memory data processing engine It supports batch and streaming processing plus some cool analyticsBesides Databricks services and your Spark running on-premises setup this will affect lots of workloads Cloudera distributions companies that built products on top of Spark and every public cloud provider (they all have a Spark managed service) will react to it and Im eagerly waiting to see what they will build! (Databricks already made Spark 30 available!)Going back a few years Spark 2 brought many fundamental new things to the table (the catalyst optimizer is the most remarkable one in my opinion) that really pushed Spark to the enterprise world The integration with different input sources the Hive metastore connection Structured Streaming and much more turned Spark into what it is today: the most used engine for building data pipelines and workloads on top of data lakesIf Excel rules the world SQL rules the underworld of data It is easy to learn it is intuitive and you can do a lot with it (and see some nice SQL abuses on this Reddit post) However we always had to learn some tips and tricks for writing SQL in different systems because they had minor distinctionsOf course we are not completely there as we are not full ANSI/SQL with Spark 30 but you can check the status of that endeavor hereDoes that mean that if I upgrade my Spark version my old Spark-SQL code will stop working? Well upgrading major versions can always be a problem but the ANSI SQL part can be toggled as a configuration to reserve some keywords by just adding: Observability on your jobs is not a nice-to-have but a MUST when running production workloads Even though we could get lots of metrics from Spark jobs I think almost everyone built some custom-made gateways to forward their metrics to somewhere else to monitor and alarm about your jobsMonitoring and alerting nowadays is almost synonymous with Prometheus This amazing open-source project by CNCF is excellent with lots of contributors fast-moving and easy to set up The majority of Spark 2x projects weve worked we would build a Prometheus exporter to Graphite and scrape it so we could get metrics information and enable alerting for the executors(I) Exporting data into GraphiteWe would deploy the exporter in a container and run in parallel to the cluster with proper auto-scaling A simple docker-compose file for local running and its configuration file can be seen below (the configuration files is the propper tagging of metrics so we can query them with Prometheus)(II) When running your spark application you must have a proper metricsproperties file on your $SPARK_HOME/conf/ folder that points to the Graphite Exporter from (I) as the sinkAs you can see not that complex but cumbersome Especially when you start to have hundreds of new Spark apps and some people may forget this  and you may just realize that when the app was not running for a few hours Of course you can add these steps to your CD pipeline but you can imagine how much work goes into that automation
k62H7ECMpFg3GdybFaWosA,Apache Spark is one of the most popular platforms for distributed data processing and analysis Although it is associated with a server farm Hadoop and cloud technologies you can successfully launch it on your machine In this entry you will learn several ways to configure the Apache Spark development environmentThe base system in this case is Ubuntu Desktop 2004 LTSThe first way is to run Spark in the terminal Lets start by downloading Apache Spark You can download it here After downloading we have to unpack the package with tarApache Spark is written in Scala which means that we need a Java Virtual Machine (JVM) For Spark 30 it will be Java 11If you need a library (eg you want to download data from MySQL do something about it and save it elsewhere) you can attach the jars manually (  jars) or download them from the maven repository (  packages)In spark-shell we write in Scala if you prefer Python your choice will be PySparkThere is no Python in the system so well do a trick We will install pip3 and the Python will be installed as a dependency 🙂But it turns out that its not enough Pyspark doesnt find the variable pythonWe need to indicate the Python version using an environmental variableNow the pyspark starts in the terminalMost people using Python more than a terminal prefer notebooks The most popular is Jupyter Notebook Lets install it Well use a pip3 and then add the /local/bin folder to the path…Jupyter Notebook will automatically launch together with the pysparkIf you need to add a library use the environment variable belowHowever if you prefer to use Scala there is an option with spylon kernel The installation is as follows: Then set the environment variable SPARK_HOMENow we have access to the spylon-kernel in the jupiter notebookYoull need the Scala pluginWe are creating a new Scala -> sbt projectWe choose Scala 212 and JDK Ive chosen Amazon Corretto 11Lets add the necessary packages to the buildsbt fileAfter loading the changes by sbt we can start writing applications in the Spark Create an object in src/main/scala path and start coding 😎
CTouuNi3daUUcSLbiMLqxm,In the past month we had so many in-depth articles and this newsletter highlights some of themHave you heard of terms such as DOS Attacks XSS and SQL/NoSQL Injection Attacks? Of course you have! When building your application and API you will want them to be as secure as possibleIf you doubt whether you should use Redux in your next application or not then check this articleAs you may know learning and understanding of Recursion is really challenging Good luck if you are trying to learn it by using Fibonacci and Exponential functions Those values actually dont have anything to do with recursion Instead this article suggests you should use arraysWhat is the most commonly used language on GitHub? If your answer is JavaScript youre ✅ If you didnt blurt JS out then check this article to know the key features of ECMAScript 2018 and the proposals expected to be included in the following specifications ECMAScript 2019 and ECMAScript 2020You may be completely new to JavaScript or you may have only used it sporadically over the years One thing is clear though  a lot has changed and there are some features YOU should be using In this article Chris Noring describes the features you should be using on a daily basis if you are serious about JavaScriptNew to Vuejs? Always wanted to learn more about Vuejs or just curious? This amazing book about Vuejs is definitely worth reading! Lets dive headfirst into the Vuejs source code (v2515)This was a terrible tragedy but sometimes these things just happen and theres nothing anyone can do to stop them said fullstack developer Bob Dynald on Reddit This echoed sentiments expressed by tens of millions of developers who have experienced a lot of package management disasters This community is 200 times more likely to experience unexpected package updates than those of other established communitiesThe post was inspired by the No Way To Prevent This Says Only Nation Where This Regularly Happens articles released by The OnionDont like Test Driven Development? Then you might like this article! Mike Cronin thinks a great idea is hiding behind a terrible implementation Like a shady Influencer subtly editing out their natural cellulite its giving jr devs unrealistic goalsIn this post Arseny Zinchenko provides a brief overview of the main Kubernetes components and its architectureThroughout the lifecycle of your Kubernetes cluster you may need to access a cluster worker node This access could be for maintenance configuration inspection log collection or other troubleshooting operations More than that it would be nice if you could enable this access whenever its needed and disable when you finish your taskWhile its possible to configure SSH access to every Kubernetes node this solution increases the attack surface and adds an additional overhead of maintaining SSH infrastructure (access logs bastions SSH keys etcIn this article Alexei Ledenev suggests to take a Kubernetes approach and deploy a pod (as DeamonSet) that will enable shell access to selected Kubernetes nodes on-demandThere is an immense variety of tools services and workflows available when it comes to provisioning and configuring resources on the AWS Cloud Platform In reality though most choices are often based on team experience vendor relationship and the specific business caseThis post explores such one set of easily-integrated tools for provisioning and configuring AWS resources These tools are Red Hat Ansible AWS CloudFormation and AWS CodeBuild along with some other techArgo from Applatix is an open-source project that provides container-native workflows for Kubernetes implementing each step in a workflow as a container If you want to know more about Argo check this articleHave you worked a lot with Go? Having difficulty learning? Everyone makes mistakes and Teiva has made quite a comprehensive list of them Read this excellent article about the Top 10 Most Common Mistakes Ive Seen in Go ProjectsHave you heard of a LinkedList? It is a collection of nodes each containing data and pointers to the previous node Instead of arrays the data is not ordered by a nodes physical placement in memory It instead ordered by which other nodes it is connected to This article shows you how to create a LinkedList in Swift 5 and Xcode PlaygroundsIntroducing Laravel Vapor by Taylor Otwell the new serverless platform for Laravel applicationsJake and Surma take a deep dive into the world of variable scoping in JavaScriptA nice little game to see which class of developer you areAnswer the questions here to determine which of the Heroic Developer Classes best describes youOn Wednesday October 30th 2019 our annual event ITNEXT SUMMIT 2019 will take place at Pakhuis de Zwijger in Amsterdam This years event has three parallel tracks for JavaScript DevOps and Data Engineering The event is aimed at IT-professionals who want to deepen their knowledge and meet like-minded fellowsWe are also offering diversity tickets Do you belong to an underrepresented group in IT? Claim one of the free diversity tickets Check https://diversitytickets
WVznyr3KybSW852Q4d2iFm,Hey there! Up for a recap of the best articles from the last month? Take some time to check these great articles Before you do that however we would like to let you know we finally selected all the speakers for our yearly SUMMIT that takes place on the 30th of October Weve got a surprise for you if you are interested you can read more about it at the bottomMicroservices with Spring Boot and Spring CloudITNEXTio has been founded by LINKIT a knowledge-driven IT sourcing partner from The Netherlands LINKIT is also organising monthly meetups under the label of ITNEXTGo to wwwitnextsummitcom to check the full program of ITNEXT Summit 2019As ITNEXT we provide you with four opportunities to win a ticket for ITNEXT Summit 2019For each method you complete you will get an additional entry in the raffle:1 Create an account on meetupcom and follow the ITNEXT meetup group for interesting meetups within the themes: JavaScript DevOps and Engineering2 Go to ITNEXT platform on mediumcom and clap 10 times for the post about Full program announced and ticket sale ITNEXT Summit 20193 Follow ITNEXT on twitter and retweet the post about Full program announced and ticket sale ITNEXT Summit 20194 Follow ITNEXT on Facebook and like & share the post about Full program announced and ticket sale ITNEXT Summit 2019PS: If you dont win a ticket ITNEXT is still giving the ITNEXT community 25% discount on all tickets until ITNEXT Summit 2019 is sold out! Use the following link to claim your discount ticket: https://wwweventbrite
aX5JJ3B6sJ2qMPesnWMxQJ,Hope you had a wonderful daySince JS powers so much of the web the author wants to provide a bunch of articles and examples of ES6+ features that she uses regularly for other developers to referenceThe series: The author shows you on this story how to stop fearing the DOM use it to its full potential and actually start loving it The DOM API is an incredibly powerful and versatile albeit verbose API Keep in mind that it is meant to provide low-level building blocks for developers to build abstractions upon so in that sense it needs to be verbose to provide an unambiguous and clear APIU worthy article that talks about the following tips: Enjoy reading them and we hope it can really help you improve your VueThe confusion I found about the redux state is that it is often used to store data records and the apps state at the same time in the same place If we want to add a record into a redux it means we will update an apps state as well Read more hereMulti-tenant a common problem in application development one for which there are many established solutions These solutions are dependant on the language/toolkit/framework Broadly we can think of the solutions as being of two different flavors namely Static and DynamicIn this article the author will share a dynamic approach to solving this problem that is based on the Angular frameworkIn this article we are going to cover almost every single topic there is in Bash programming It will not deal with how different UNIX command works Bash (AKA Bourne Again Shell) is a type of interpreter that processes shell commands A shell interpreter takes commands in plain text format and calls Operating System services to do somethingRedux is a tool for managing data-state and UI-state in Javascript applications In the tutorial written by Erşah you can see how to organise the structure of the projects that consist of ReduxOne of the hardest challenges with new Kubernetes deployments is wading through all of the best practices and opinions to get your deployment pipeline setup correctly the first time Helm deployments have a lot of gotchas that youll need to navigate through to setup correctly You can read all about them right hereIn this article were going to demonstrate how Apache Spark can be utilised for writing powerful ETL jobs in Python If youre already familiar with Python and working with data from day to day then PySpark is going to help you to create more scalable processing and analysis of (big) dataDid you also move to Kotlin as it became more widely embraced by Google and the Android developer community? If you are coming from a Java background then you may find that constructors in Kotlin can be a little daunting at first Hopefully this article helps you out with that learning curveIn this article the author demonstrates how Apache Spark can be utilised for writing powerful ETL jobs in Python If youre already familiar with Python and working with data from day to day then PySpark is going to help you to create more scalable processing and analysis of (big) dataLets talk about applying continuous deployment and integration (CI/CD) to put some services online  the GitOps way In a nutshell we like to commit new features to Git and have these features pushed to users immediately via micro-servicesIf you use Python you likely encountered pip on your path Pip is the most used package manager for Python modules Remember the time when pip was considered a liability? Pip had this label of being a second rebellious captain on a ship Today with containers  such as Docker  the worries are goneAdding SQL Server Pagination to an Outsystems REST API method by João DuroUpgrading OutSystems Platform to version 11 by Kees Kleybeuker: Etienne Lemay is sharing his experiences building Missive with best practices and the pros and cons of building desktop and mobile apps with a single codebase → ⏯️← (Audio) By Wes Bos and Scott Tolinski  Full Stack JavaScript Web DevelopersLets talk about 3factor app these 2 podcasts talk about the first factor GraphQL and the 2nd factor Become the best software developer you can be by Reliable Eventing Stay tuned to this newsletter for the 3rd factor in November! → 1st ⏯ | 2️nd ⏯ ← (Audio) By Allen Underwood Michael Outlaw Joe Zack Allen Underwood Michael Outlaw and Joe ZackYou can find 101 coding problems and few tips to crack your next programming interviews right hereDo you want to know whats next in #JS #DevOps and #DataEngineering? Join us at #ITNEXTSUMMIT2019 this month on the 30th of October 2019! A multi-track interactive conference for visionary developers
Vp64kNgnJwK7WSJpBVdKSC,Are you getting started with NestJS? Then you should go through this tutorial and learn how to build web services with TypeScript & NestJS This simple API server with NestJS will handle a basic application scenario: creating storing and retrieving a list of products for a general storeReacts new hooks make building UIs easier than ever but some things like dynamic forms can still be a bit tricky to understand This is all were designing today And since were using the new hooks we wont be using any classes like we used to You can find this thorough article hereTo understand how a browser renders a webpage this article dives into the DOM and CSSOM The browser blocks some rendering of a webpage until certain resources are loaded first while other resources are loaded asynchronouslyAnd dont miss the following articles by Lorenzo Spyna about push notification: Get to know why NET Core is sexy and you should know it! The author has the goal of convincing you that we can easily and quickly develop in C#and NET Core as we can do in JavaScript and NodeJSWant to find out how? Read it all hereIn this article you can find tips and tricks for both beginners and forgetful professionals! Simply put before you lies a metric ton of handy Ruby Array methods And to keep things shorter the author writes return values in comments so arr # -> stuff means that the return value for arr is stuffOne of the most-read articles on Medium about git rebase encourages to not do so This article advises the only purpose of rebasing is to have a good looking git log historyIve come to the conclusion that its about vanity Rebasing is a purely aesthetic operationI want to list a few good reasons why in my opinion rebasing is a good practice that brings many benefits to your workflowIn this article you will learn how to build a simple calculator app with Dart and Flutter The excellent features available in the Dart language and Flutter SDK provide the modern developer with a great toolkit for quickly trying new ideas and building high-performance applicationsThere are different ways to build a microservice architecture If you use the traditional PHP framework it is very difficult to achieve But using Swoft is very easy in comparisonWith its high performance and stability find out in this article why you should use Swoft instead of traditional PHPIf you want to know what exactly Swoft is check out this article In short Swoft is a PHP microservices coroutine framework based on the Swoole extensionThrough three years of accumulation and direction exploration Swoft has made Swoft the Spring Cloud in the PHP world which is the best choice for PHPs high-performance framework and microservices managementEver wondered how you can develop Angular applications on your local machine with minimal setup? How about production? This tutorial will cut the corners! It will show you how to Dockerize an Angular app build with Angular CLI and finally using Docker and Docker Compose for both development and productionGitOps is a way to do Continuous Delivery introduced by Weaveworks It works by using Git as a single source of truth for declarative infrastructure and applicationsThis post will demo Flux a component at the heart of GitOps accepted in the CNCFs sandbox a couple of weeks agoAs an OutSystems developer you have no doubt you encountered issues with overflowing text in divs or table rows with double size because of the text wrapping You can handle this easily with CSS How? In this article João will demonstrate this using 2 containers wrapping textOutSystems Mobile Development Best Practices exist for a reason William will explain in this article why it is essential to follow these best practicesIn the latest releases of OutSystems the NET-stack was updated while the Java-stack was not Time to consider whether to stay with Java or migrate to NET In this article Kees Kleybeuker walks you through the migration step by stepWanna become the best software developer you can be? Looking to build pragmatic teams ? → ⏯️← (Audio) By Allen Underwood Michael Outlaw Joe ZackYou should read the new edition of the Pragmatic Programmer with Dave Thomas Andy Hunt! You can find it here: Tune in here → ⏯️← (Audio) By CodeNewbieYou can draw your own figure here: http://developersdevelopeOn Wednesday October 30th 2019 our annual event ITNEXT SUMMIT 2019 will take place at Pakhuis de Zwijger in Amsterdam This years event has three parallel tracks for JavaScript DevOps and Data Engineering The event is aimed at IT-professionals who want to deepen their knowledge and meet like-minded fellowsWe are also offering diversity tickets Do you belong to an underrepresented group in IT? Claim one of the free diversity tickets Check: https://diversitytickets
ms5xUSgL68U9PJgUGVtVT8,ITNEXT summit 2019 our annual conference is taking place on the 30th of October! We are busy organizing this 4th edition and we would like to include our authors and audienceIf you have a great project that you are excited to share learned a valuable lesson or came across a useful tool worth telling others please submit your talk Your submission will be reviewed by 3 Program Chairs: You can also help us by sharing this Call for Speakers with your networkHappy coding!The ITNEXTDont want to receive emails like this anymore? Go to your Medium settings and disable Letters
o6NZCiHHfTqd8iMVuHetBM,On Wednesday October 30th 2019 our annual event ITNEXT Summit 2019 will take place at Pakhuis de Zwijger in Amsterdam This years event has three parallel tracks for JavaScript DevOps and Data Engineering The event is aimed at IT-professionals who want to deepen their knowledge and meet like-minded fellowsAre you curious to know more about best practices that are core to production excellence? Are you choosing data for AI projects wisely? Do you want to know how to revolutionize your software process by kicking off with AWS Lambda? Are you changing JS frameworks at the same pace as fashion is changing and want to make well-considered decisions on what to spend your time on? If one or more of these questions trigger your developers-mind then youd better not miss ITNEXT Summit! In this event speakers will showcase the nextgen tech and the necessary skills to build scalable applications in an era where time-to-market is one of the most critical factorsITNEXT Summit creates a safe and inclusive environment for everyone and this year DIVERSITY is a priority Therefore we have invited Tara Ojo and Holden Karau two amazing engineers to be the first and second keynote speakers Liz Fong-Jones former SRE at Google and now SRE Dev Advocate at Honeycomb will give the other fantastic (and funny) keynote talk And Kris Nova one of the most brilliant Kubernetes engineers in the world will do the closing keynote All these fantastic engineers have a strong presence in the community as well as being part of the underrepresented groups in techThe other brilliant speakers are from different companies such as Google Amazon Web Services Buffer Atlassian etc Go to wwwitnextsummitcom to check the full program and purchase your ticket! We are also offering diversity tickets Do you belong to an underrepresented group in IT? Claim one of the free diversity tickets Check https://diversitytickets
m3rmdqcCa72kZkfqaYwie3,The 2019 edition of ITNEXT Summit took place on Oct 30 2019 at Amsterdam NLThe event had around 450+ passionate attendees that came to learn from great experts and get inspiredThere were 3 tracks: Each track was led and curated by an MC: From looking at the agenda one can say that the Devops was packed with Serverless sessions and the Data Engineer track had an emphasis on Machine Learning This reflects what is going on in the European industry but not only Europe! Machine learning at scale is starting to take off globally and it seems like many companies and open source software trying to create a solution for the challengesData Engineering track had 5 sessions: Katrin Strasser kicked off the track and started with highlighting the unconscious bias that we all suffer from That is a direct reflection of stereotypes culture education surrounding and many moreAI is strictly bounded to the data we feed into itWhere is the bias? Data can be bias! Most of the data for classifying humans are produced by … HUMANS! This creates a vicious cycle where bias is being emphasized by AIWord embedding from the Natural Language Processing (NLP) domain illustrates how stereotype bias comes from the corpus: Katrin shows how the human classification algorithm is mostly based on features that we dont have control overKatrin continued with face recognition and the error rate of 034% with 300K people per day huge numbersKnow your use cases and be awareIn his session Juantomas Garcia walked us through the process of using Reinforcement Learning to win games his example was focused on Abbey Of The Crime  a Spanish game Combining beer and tech projects can result in bad engineering: Step and notes from the project: 7 The project is a work in progress! Not done yet You can join the fun on GithubAt this stage of the day I was in desperate need of coffeeWe continue with the moving parts of building machine learning pipeline and how we can distribute the efforts across teams as well as how can we use Kubeflow to visualize all the deployments and working pieces: How we can integrate with the cloud: It was a great session that walked us through Kubeflow ML pipelines and how to make everyone work togetherThe closing session for the Data Engineer track was: Stephan is one of the co-creators of Flink and he walked us through what are stream processing and what the future brings: The talk walked us through 3 ways of stream processing concepts: Stephan finished off with an example of how to leverage all of these techniques into one system in the demo example he showed a high-level architecture of Ridesharing backend: An overall great session where we got a glimpse of where Flink is going and what the industry needs areThis was only a glimpse of ITNEXT Summit 2019 Overall the conference had wonderful sessions up to date content and full of openness and welcoming community atmosphere
YRmrCL6eBrnYXLrH6ByQMC,Its a tradition to dine share expectations and knowledge with all the speakers in advance of the ITNEXT Summit but especially to enjoy a cosy evening as a warm-up for the eventOn Wednesday October 30th 2019 our annual event ITNEXT Summit took place at Pakhuis de Zwijger in Amsterdam This years Summit was a multi-track interactive conference for visionary developers who want to know whats next in JavaScript DevOps and Data EngineeringSharing high-quality knowledge is our core value and the reason we exist Furthermore tech is built for humans and by humans Different mindsets as well as points of view represent a better learning and sharing environment: the core ideas behind the ITNEXT initiative Therefore diversity was one of our priorities for the 4th edition of the ITNEXT SummitWe started the day with three plenary keynote talks given by Liz Fong-Jones (Honeycomb) Holden Karau (Apple) and Tara Ojo (FutureLearn) Kris Nova (Sysdig) was the plenary closing keynote of the dayWe live in an era where building software shouldnt be the same routine anymore Our generation doesnt know a nine-to-five mentality; instead we aim to make an impact with every action Eagerness to explore challenging opportunities is part of our cultureJavaScriptTara Ojo was the chair of the JavaScript track She selected five amazing talksData EngineeringThe Data Engineering chair was Holden KarauOpen SpacesThis year we introduced the Open Spaces (unconference) in our program This unconference part was a participant-oriented meeting where the attendees decide on the agenda and discussion topicsOn behalf of LINKIT we are proud of having you at the ITNEXT Summit We would like to thank this years sponsors: Anchormen Bartosz Confluent Datadog DiVetro Honeypot Microsoft and Relay42
jUaj6r3gkGheUDCVrnTSbL,Web scraping mostly involves text-intensive tasks such as product review scraping gathering real-estate listings or even tracking online reputation and presence When one application scrapes only String data types for qualitative analysis it may not need type safety However in case the end goal of the web scraping is to do quantitative analysis with prices or weather forecasts using a type-safe language might be quite handyIn this article we aim to give a small and interesting example of price scraping for crypto-currencies by using Scala and storing those into a PostgreSQL database To scrape the prices we selected to use CoinMarketCap homepage It is a crypto-currency knowledge website which gives information also on market capitalizations (relative market sizes) circulating supply and trading volumes Even though it is fascinating to see all those information together to keep it simple we will be only scraping the pricesThis article might be considered as a tutorial and it requires a basic level of knowledge of docker-compose and ScalaWhile web scraping in Scala we will be using an HTML parsing library called scala-scraper with JSoup Following that we will be inserting the scraped prices to the PostgreSQL database by using a functional JDBC tool called doobieAlthough we mentioned some fancy library and tool names the real magic happens in case classes For each call to the CoinMarketCap homepage we aim to retrieve the long crypto-currency table with type safety To do that we created CoinCreate and CoinInsert case classes and companion objectsWe will start explaining first the case classes together with their companion objects as we aimed to model the data while creating those Then we will explain the simple functions for retrieving the updated price table from the homepage Lastly we will explain how we inserted the table records into the PostgreSQL database running locally We can power the database with this simple docker-compose file In the docker-compose file we initialized a PostgreSQL database with a name dev username admin and a password as adminThe steps that are explained in this tutorial are displayed in the pipeline aboveAlthough there might be different approaches to model the data we can start by creating two case classes as CoinCreate and CoinInsert Those will help us to keep the data types safe while scraping the price table and inserting into a databaseCoinCreate aims to safely type a pair of crypto-currency code and its current price Thus it has two parameters code(referring to the currency code) and price(current price in USD) However while thinking about its companion object we need to consider the shape of the price records in each row For instance if we consider only to use coin names and prices in our case class in an array of records their indices will be 1 and 3 This is quite similar to column indices for tablesBy observing the price table (above you can find a screenshot from the homepage) we decide to use a companion object to have an apply method for functionally transforming an input of String List to CoinCreate Although this transformation is not that straightforward we can use helper functions to get only the coin code (getCoinCode) and transform the dollar price string into a double (numberStringToDouble)CoinInsert aims to safely type a pair of crypto-currency code its current price and a log timestamp for insertion time logging We can use this case class while inserting a vector of CoinCreate into PostgreSQL As its parameters are so similar to CoinCreate we can create a simple companion object to transform a CoinCreate to CoinInsert This objects apply method can naturally add the current timestamp to a CoinCreate to obtain a CoinInsertHence the only difference between a CoinCreate case class and CoinInsert case class will be the current Timestamp  notated as a logTimestamp parameterScraping with scala-scraper and JSoup is quite easy First we need to GET request to the homepage by creating a new JSoup browser A new JSoup browser enables to fetch HTML from the web Since we need only HTML parsing JSoup was enough in this case for Javascript using pages other browser options could be usedBy using the GET request we need to find the main table and store it as a Vector of Strings Luckily when we specify that we are looking for a table element scala-scrapers table method does all the job for usLastly we need to slice the Vector starting from the second index till the last lines as the first line contains column names (headers) The resulting sliced Vector would still have the table rows with their HTML elements as String So we can benefit from functional programming to map all the table rows (Vector elements) while extracting text in the elements then transform to CoinCreate (the comfort of having a tailor-made apply function)doobie is an amazing functional JDBC layer for Scala It provides a functional way to write any JDBC program In this tutorial we will keep it quite simple by writing only a connection Transactor to connect to the local PostgreSQL database and an insertion function to make the transactions with type-safetyTo connect to the database we need to use a Transactor stating the type of the driver (in our case it is a PostgreSQL driver) URL for connection user name password and an Execution Context (EC) The transactor needs an implicit val to define the EC For non-blocking operations doobies Transactor uses contextShift For testing our code doobie documentation recommends using synchronous ECFor writing a row by row insertion function we can use SQL interpolation The function has an input of CoinInsert and an output of Update0 (representing a parameterized statement where the arguments are known)Lastly we can GET request the homepage by using the getCoinUpdatedTable function This will return a Vector of StringsConsequently we can use this Vector (coinTable) to transform CoinCreate to CoinInsert case class and execute the insert statement we prepared in the previous stepThanks to doobie with only a few lines we were able to scrape the crypto-currency prices from CoinMarketCap and insert into a local PostgreSQL database Although the code does its job for now the source code can be extended with exception handling and monitoring The whole main class can be found below And you can find the whole project in this Github repositoryWith this short article we aimed to introduce some Scala concepts to web scraping To learn more on data engineering one can choose to attend a Bootcamp with hands-on training do more projects read and share more
Z2TPrDfVQen25arF4WZDPx,Streaming industrial IoT data can be complicated to process because it often involves a large amount of data received in continuous streams that never end Theres never one single machine to monitor Always a number of them Multiple buildings one or more factories a fleet of ships all of an operators subsea oil pipelines That kind of scaleIoT data may need to be buffered for multiple weeks if your sensors and equipment are at sea They may need to be transferred to the cloud when the ship is in a harbour or the data may be brought to shore on removable disks IoT data streams dont have to be transmitted in real time to provide value Though sometimes youll want to perform at least parts of the processing on site For example optimization of fuel consumption for tankers and tug boats or detection of anomalies for subsea oil pumpsIts perfectly possible to run trained machine learning algorithms on quite primitive equipment like relatives of Raspberry PI or a small rugged PC Maybe directly on your IoT gateway that in some cases can be as powerful as any server blade You also wont need a lot of machines to be able to deploy a reasonably redundant Kubernetes cluster even if your architectural choices become much more limited than they would have been with a cloud providerAs a Data Scientist youll often find that just observing the values that stream by doesnt give you enough context to perform complex calculations You may need a cache of recent data or a place to store partial precalculations aggregates and potentially the output of machine learning models typically for use in dashboards and interactive parameterized reports Ill call this data cache warm storageWarm storage is often used used by Data Science applications and algorithms that call machine learning models in order to quickly retrieve sufficient sensor values and state changes from streaming data Warm Storage data may typically be used to detect anomalies generate warnings or calculate Key Performance Indicators that add insight and value to streaming data for customersThose KPIs are often aggregated to minute ten minute hour or day averages and stored back into warm storage with a longer retention period to facilitate reuse by further calculations The stored partial aggregates may also be used for presentation purposes typically by dashboards or in some cases reports with selectable parameters Thus optimal retention periods may vary quite a bit between tenants and use casesA good Warm Storage needs flexible low latency querying and filtering for multiple tags simultaneously It must be able to relate to hierarchical tag IDs representing an asset hierarchy for flexible matching and filtering It needs storage support for KPIs and aggregates calculated from within algorithms that may optionally call machine learning models It needs query support for seamlessly mixing calculated data sensor data and state changesAn often overlooked topic is that Warm Storage should be opt out by default not opt in Every insert into Warm Storage will cost you a little bit of performance storage space and memory space for indexes before it is removed as obsolete All data that is scanned through as part of index scans during queries will also cost you a little bit of io memory and cpu every time you run a query Tenants that have no intention to use Warm Storage should not have their data stored there Neither should tags that will never be retrieved from Warm Storage The first rule of warm storage is: Dont store data you wont need Excess data affects the performance of retrieving the data you want costs more money and brings you closer to a need for scaling outA good Warm Storage should have a Python wrapper to make it easier to use from Data Science projects If youre using Warm Storage in a resource restricted environment the API should behave the same as in a cloud environmentIll get into the nitty gritty details of all this in the video presentation I made while preparing for the Knowit Developer Summit 2020 Its about real world experience with building and deploying industrial IoT warm storage solutions during the last couple of years Ill talk about some of the existing storage components on the market strategies for how to utilize them effectively some of the unexpected problems we encountered on our way and what we learnt from it Ill describe the warm storage solution were currently using in production including the lazy aggregator and a rule engine with fully configurable aggregating input selectors powerful expression parsing machine learning model integration plus customizable outputs and alerts Check out the video if any of this sounds interesting and please tell me what you think about it in the response section
fx74Gzo6pArptCxgEYDPoG,Hello! Im Lars Data Engineer at Unacast In this post Ill be diving into my experiences working with Google BigQuery for the past year Ill point out some of the ways working with a modern tool is different from working with the more traditional Business Intelligence tools I used to deal with as a consultant highlight a few advantages and warn about certain pitfalls that weve come to discover And as a former consultant I will of course give the conclusion away right at the beginning: Working with BigQuery is great! But if you dont pay attention to what youre doing it will make you regret itUnacast is as you may already know a data company building a platform for location data to better understand the real world We collect data from smartphones in the US with the help of our partners process and enrich the data and sell the resulting data products to clients in the advertising and research industry As a Data Engineer my job is to make sure that the collection processing storage and reporting of the data goes smoothly And as a startup with an eye to the future we are of course doing it all in the cloud using Google Cloud Platform and Google BigQuery as our primary database and query engineBigQuery is a Software-as-a-Service query engine specifically designed to handle large volumes of data It is serverless fully managed and easy to use: All you need to do is to load your structured data into it and Google will handle the rest You query the data using SQL and the system provide you with results within very reasonable time frames scaling automatically with the size of your data and complexity of your queries It does this by cleverly distributing the data and processing over the many machines in Googles huge data centers Unlike in traditional relational databases the data isnt indexed Instead BigQuery relies on very fast full scanning of the data that is referenced in queries And this bring us to where things start getting complicated: The pricing model In addition to a small fee for data storage you pay per query for the amount of data scannedA recurring problem I faced as consultant making interactive reports for (impatient) business users were queries that took too long to complete To improve the execution times and avoid losing the interest of our users there were a couple of things we could try the first of which was to re-write queries and/or optimize the database for the most frequent ones If that didnt work we would try materializing aggregates to use in the slowest queries The last resort was to upgrade the database itself if the budget would allow it Naturally we would try everything before even bringing that idea up to the client I remember coming across BigQuery in one of these moments but I was skeptical to its promises of performance and ease of use Another reason why we never tried it then was how hard it was to estimate how much it would cost to put it to use not knowing how much querying the users of the reporting solution would be doing in practiceNow that Ive been using BigQuery for a year I can vouch for most of the claims of how well BigQuery works Using it is very simple and it has surprised us on several occasions by successfully dealing with nasty queries we didnt think wed get away with Its good for ad-hoc data exploration scheduled bulk transforms that can be formulated in SQL and as the engine for interactive reports Changes can easily be made to our business logic as we for the most part can query the raw data directly We also store our data de-normalised and dont have to get caught up in data modelling Beyond not being able to sort large data sets we simply arent hitting any walls with BigQueryThe main advantage we get from using BigQuery is the increased speed at which we can prototype We can get the results we want quickly perhaps by taking shortcuts here and there since BigQuery scales under the hood to whatever we throw at it And if is often tempting to deploy things to production when the results are there It then becomes very convenient that BigQuery scales so well as we can feel confident that what works today will keep working tomorrow with the ever-increasing volumes of data But thats the thing sometimes it might have been better if it didnt With so little friction on a day-to-day basis it is easy to forget all about the pricing model which is one of those pitfalls I mentioned earlierMy father is an editor and offered me the following analogy to me when I (as the good son I am) tested my material for this post on him The transition from conventional relational databases to BigQuery is like the transition from analog to digital film makingFilm is expensive Back when they relied on it directors would always have to plan their shots carefully to not waste film and blow their budgets When the editors took over in the editing room they would benefit from that planning and could piece shots together accordingly Then digital video came along and things changed Directors no longer had to worry as much about the cost of filming and started experimenting more on the set resulting in much less thought-out material in large quantities The editors had to spend much more time and energy piecing the film together than before With BigQuery something similar is happening We the directors in the analogy dont have to be as conservative as we used to and are led to sending more work to the query engine or the editing room Its nice for us as we feel liberated from the shackles of the past but also very nice for our liberator Google who made us run a lot more complex queries and is gladly charging us for each of themTo avoid falling into a pattern of overspending we have to be conscious about what we are doing We can query directly on the raw data but should we? Our report works but should we deploy it to production and become dependant on it? Flexibility is handy but the value of planning and finding efficient solutions isnt going away We shouldnt rely on the query engine to do all the dirty work for us just because we canI can conclude that after working with BigQuery for one year it feels like the game has changed One bottleneck has been eliminated and we get quicker results now but the underlying challenges of data management are still there Its easy to let the principles of best practice go when you dont immediately feel the consequences  Everything still works but the monthly bill suddenly grows out of control I dont miss working with traditional databases but the things I learned back then are still as important todayFinally a quick example: I wanted to run a stress test to underpin the claim that BigQuery always works so I ran a query that counts the number of distinct IDs in all of our raw data from the past 30 days The query scanned one column of 94 billion rows 32 TB of data and cost 16 USD A pretty nasty query by our standards that took 13 minutes to complete I repeated the query a couple of times (you know just in case) If I were to run this experiment once an hour every day at work the costs would exceed my salary! Clearly with the great freedom of BigQuery comes its share of responsibility
G6CmKdfDBCNPpWnWPF2Y99,In this publication we are going to make 2 VMs one with Windows (Server 2016 just the 180-day evaluation version) and one with Linux (CentOS 7)The reason why Im doing this is theres a lot of people who dont know how to install an Operating System This guide will help them understand how to do itI know theres a lot of guides out there But here I can keep track and make updates on the guides if I find something needed to be changed once we move on to the next chaptersFor Windows you could search the correct ISO from the Microsoft Eval Center (link) Here youll find a lot of other tools to eval too But out interest now is the Windows Server ISOsOnce we click the Continue button here well need to complete some info and then Microsoft will leave us to download the ISO imageFor Linux we go to the CentOS Download Center (link) Since the current version of CentOS is 8 well need to choose to download the Older Versions And then select the CentOS 7 mirrorsOnce you downloaded the installers then you could start creating the VMsOur starting config in both situations will be a 2 Core CPU 8 GB of RAM with a single 50 GB HDD (60 GB for the Windows VM Setup) This will help us install the OS without problems and we could make a starting image to redeploy in other cases for testingNote: Im going to show you how to set the VM on a VMware tool Youll need to transfer those settings to your preferred HypervisorChoose the Typical setup (we will custom the virtual hardware after everything is created)Since we are creating 2 VMs we will choose the Install OS later optionAnd the version of Linux we will installJust a simple name for the VM nothing complexThe recommended size for the Linux HDD is 20 GB but we will change that to 50 GB Windows Server default is 60 GB and doesnt need to be changedIn the end we see some settings are in default values (CPU RAM Network) We are going to change some of them in the next stepsIn this preview we see both of the VMs created The next step is to customize some of the default settingsHere we remove the Soundcard and the Printer from the default settings (for what were going to do we dont need them) and then we increase the amount of RAM to 8 GB and the CPU Core count to 2 (1 Processor / 2 Cores)The final result of the settings page needs to look like thisWe didnt modify any setting about Network adapter or amount of HDDs of the VM Since this will be base machines for all of our Windows and Linux testings we are going to install the OS and export the machine to have a starting image of the VMThe next steps are going to be Installing Linux and WindowsInstalling Linux isnt difficult But if youre here you probably still need some help And thats why this post existsThe first step is choosing the correct ISO for the installation (obviously the one we downloaded from the CentOS recommended mirror list) For that we need to edit the settings select the CD/DVD device and then change the config to choose the ISO we downloaded from the webOnce selected you start the VM and then choose to install the OSOnce the installer loads everything the first thing is to choose your languageTo make things simple we are going to choose English as the OS languageThe settings page is pretty simple Here what were going to do is set the networking info date & time settings and then choose the software for the installationThe VMware DHCP server assigns the IP based on the VM settings (currently our network adapter is working in NAT configuration)Once we clicked the Done button on the top left we proceed to change the Date & Time settingsIn this screen we enable the Network Time so the machine will always be time-synchronized (its good to set the NTP Server… and I didnt do it)On the Software Selection page we will choose the Minimal Install and enable some add-onsWith these settings for the default the System will work without GUI (just console) No worries to you we are going to install the GUI after the install finished (but we arent going to enabled it by default it will be disabled and if needed you will need to run a command to start the desktop environment)The final step before proceeding to the install of the OS is choosing a partition configuration We will leave the default Automatic option for this timeOnce finished we click on Done and then on the Begin Installation buttonOn this screen we see 2 last settings to define: Root Password and User CreationFor the user we choose the option to make the user an Administrator and set the password for the accountWhen the install finish we reboot the VMOnce we choose the Kernel and login we could test the connectivityEverything until now is OK The next step will be to run an update commandNote: for this command to run the user needs to be root or have sudo rights Thanks to choosing to make the user we created part of the Administrator group he has the rights to run sudoThe command will list the packages needed to updated o even install (this will probably be a dependency or something like that)You type Yes and the install will runEverything installed fine and without problems even we got a Kernel update at the same time So we are going to reboot the system againThis guide will not end here since as I promised earlier we are going to install the GUI to run on-demandInstalling the GUI is simple but takes some timeOnce you type Yes the GUI components and his dependencies will be installedFor default this installed GUI will not run on startup If you want to run it just simply type this command: sudo systemctl isolate graphicalIf everything is OK you will see the following screen on the VMFirst were going to install the NTP serviceOnce everything installs if we see the ntpconf file we will see some servers of the CentOS NTP Server Pool already Here you can add more servers You can see a list of some servers in your region in the NTP Pool Project SiteSince Im from South America I will add some of the regional servers to the ntpconf fileTo edit the file we run the next commandsudo vim /etc/ntpAnd then to add the lines to the file we press the INS(ERT) key in your keyboard We mark the original lines as comments with an # symbolOnce we finish the edit we press the Esc key and to save the changes we write the command :wq (Write & Quit)After the changes we need to restart the NTP DaemonFor that we are going to run the next commandsOnce the finish the NTP config were going to export this VM to create a base image for future labsFor this task you need to have the VM selected and then on the menu choose to Export to OVFThis will show a Save Dialog window Once you choose the path and click the Save button the process will startNote: remember to unmount/deselect the ISO image we used to make the installation since the Export will try to create a copy of that image as a dependency needed to run the VMWhen everything finishes you will see the files on the path you chooseAll right this is enough for Linux now its time to install the Windows Server VMOn this part we are going to use the Windows Server 2016 ISO we downloaded from the Microsoft Eval Center to make a VMLike the previous section we choose the VM and edit their settings to associate the CD/DVD virtual device to the ISO image of the OS installerOnce you start the VM the installer will prompt you to press a key Once pressed the installer will startHere we choose the language formats and keyboard distribution and then we press the Next buttonFor this installation we will choose the Standard Evaluation with Desktop Experience The difference between Standard and Datacenter will be negligible for our usageFor this installation we need to choose to Install Windows OnlyLike with the CentOS installation here we left the OS to automatically partition the virtual driveOnce clicked next the install process will beginThe final step will be to set the Administrator password for the VMOnce confirmed the pass we will see the lock screen of our Windows Server 2016 VMWhen you log in youll see the Administrator Dashboard with some info We are going to skip this part and go directly to some settings on the new Control PanelLike with the Linux VM we configure the system to automatically set time and timezone The next step is to update the Computer nameFor this change you need to restart the PC but we will choose to restart laterThe Linux VM installs automatically the Open VM Tools (Compatibles with VMware Products) we dont need to make this extra install stepBut for Windows you need to install the VMware Tools on the guest OS to make him work seamlessly with the host OSInstall the VMware Tools is pretty simple With the VM running right-click on the tab and check on the menu for the option to install the componentsHere we double click the DVD drive and the installer will autostartOnce finished we reboot the system We will see how the VM opens using all the screen space available inside the toolThe next step for this Windows installation is to run the Windows UpdateOnce the update finish and you restart and check that everything is ok we shut down the system and like with the Linux VM we will export the VM to have a base image for future projectsLike with Linux shut down the VM click on the name in the list and then on the main menu click Export to OVFOnce the Save Dialog appears choose the final path where the files will be created and click the Save buttonWhen the export ends you will see the files on the location you chooseIve created a lot of VMs to test some tools The most recent ones would be VMs to test: And you can use these VMs to test everything you want tooI didnt show today the most useful trick (the Snapshots) but once you learn about it you will love it… but remember most of the tricks we will see works for our test labs only and I dont encourage you to use them in production environments No no and noThis is everything for today so see you next week
KsEoBWSfVfRSmXbLTPobmZ,Ranking functions are used to rank each row of data based on their relative position to the sections assigned or the whole data table SQL supports 4 different types of ranking functions as shown below and all ranking function output are positive integers starting from 1: Rank: the most commonly used ranking function It assigns a ranking number to each row of data based on the column and order that user specifies If multiple values in the column are the same they would receive the same rank output and the next value in the order will skip the next few values to fill the true rank in the column For example if two values are the same and they are rank 3 in the group then the next value in the order will be assigned 5Dense_Rank: the idea for dense_rank is the same as rank before The only difference is that next value in the order will not skip even if previous few are the same For example if two values are the same and they are rank 3 the next value in the order will be assigned 4Ntile: User needs to specify the N which is the number of groups that they want to divide the data group The data will be evenly split and the output number is assigned based on the value range that it falls under<Rank Function> here refers to the 4 different rank functions as shown above Bracket after Over defines the way you want to rank it Order by clause in bracket is required and it defines which column you want to rank the data on You can put ASC or DESC after the <column name> for ascending or descending order(default is descending) The Partition by clause is optional and it defines the sub-category that you want to rank the data on For example you want to rank the salary of employee based on their respective department so you should partition by department Order by and Partition by can be used in other SQL windows function as wellThe only syntax difference between the 4 ranking function is that Ntile needs to specify number of groups in the bracket after <Rank Function> but other 3 need to leave it blank Eg Ntile(4) Over(Partition by department Order by salary) creates the 4 different groups in each department based on the salary and assign each row an integer number from 1 4Lets see a real data example Table company has salary information for 10 employees across 3 different departments I applied all 4 ranking functions in the select statement on salary to show the difference across 4 functionsFor Rank and Dense_Rank function they will give same ranking number if multiple values are the same The only difference between them is that rank function will skip the space for same values while dense_rank will be just +1 and will not skip the space For example we can see that there are 4 employees with 7000 salary and rank 3 and so the next salary person will jump to ranking 7 while dense_rank will go to 4 Rank function output represents the true salary position in the firm regarding row as an entity while dense_rank tells the rank on salary value regarding each unique value as an entityRow_number function will out consider the same value situation and just give you the true representation on the position of this row in the whole table Ntile function here I specify the N as 4 so we can see each employee salary is under which quantileFor each rank column we just need to add partition by department in the over bracket
EaWpL5WcMVRSfgnDGZkoqX,Mars is there waiting to be reachedLast time in LAG functionality on SQL Data we imagined traveling to Mars and visiting the base of (one of many mountains) Olympus Mons where a food stand was selling astronaut ice cream In the example from the post I used imaginary data to showcase the LAG window function with the sales SQL data tableThis post is going to use the LAG as well as the LEAD window functions to solve an algorithmic problem I came across on LeetCode I will introduce LEAD functionality as well as solve the problem: display the records in the sales table which have three or more consecutive sales values equal to or greater than a given number Lets beginRecall the data we used last time had five records of net_sales; each net sale for a month that the food stand was open The table called sales looked like this: For the purpose of this blog I am going to add more data  specifically ten more records This represents ten additional months where the net_sales of the food stand were recorded To do that I use INSERT: A quick view of the table (with SELECT) shows fifteen records each representing a month with values for net_sales: Perfect Now we have more records to queryThe LEAD window function is similar to the LAG function Recall that with the LAG function we can access a row at a specific offset that comes before the current row Lets review an example: Suppose I want to look at the net sales from the previous month while at the same time viewing the current month If the current row is the 4th month and the net_sales value is 7319 then with an offset of 1 I expect to see 5618 in the next column for the 4th rowSimilarly with the LEAD window function we can access a row at a specific offset that comes after the current row Similar to the previous example: Suppose I want to look at the net sales from the following month while at the same time viewing the current month If the current row is the 4th month and the net_sales value is 7319 then with an offset of 1 I expect to see 9282 in the next column for the 4th rowApplying this to the first five rows of sales: Now we are equipped with some handy tricks to solve the problem where the owner of the food stand wants to know some information about the seasonality of her sales Specifically she wants to know if there are three or more consecutive months that have sale values equal to or greater than 7500There are a handful of methods to solve this problem but here is a solution using LAG and LEAD window functions: Wow  thats a complex query Lets break it downFirst notice the subquery: Second notice the WHERE clause: Putting it all together we are querying the table created by the the subquery s and looking for three or more consecutive rows that have three values greater than 7500Result: So it seems that months 5 through (and including) 8 have peak salesThats it for this fake data folks! Another addition to your SQL toolboxFor your reference I have a series of SQL tutorials that cover basic and intermediate queries
ihyZiDe5ssfXkN3m6iwREu,Mars is there waiting to be reachedTraveling and vacationing on Mars  its the stuff of science fiction Mars a neighboring plant to Earth has deep valleys and the tall mountains One mountain is Olympus Mons  with a staggering height of 72000 ft (two and a half times Mount Everests height above sea level) It is a shield volcano much like the the large volcanoes making up the Hawaiian IslandsAlthough Mars environment is hostile to humans (one characteristic is the amount of radiation received on the surface could pose health risks) Im going to create a scenario that allows us to imagine vacationing on this red planet Suppose we spent 4 years on a spacecraft traveling to the planet Among seeing many sites on the planet we are visiting Olympus Mons this week At the mountain base a food stand is selling astronaut ice cream and other freeze dried foodsTodays SQL post uses fake data for the purpose of showcasing the LAG window function Lets beginSince this is a hypothetical scenario I am going to create the monthly sales imaginary data for the food standHere I create the table named sales: And then I insert data into the table: I finally view my work with a SELECT statement: So there are 5 records of net_sales each for a month that the food stand is openSuppose I wanted to look at the net sales from the previous month while at the same time viewing the current month This is possible with the LAG window function This allows us to access a row at a specific offset that comes before the current row In terms of our example if the current row is the 4th month and thenet_sales value is 7319 then with an offset of 1 I expect to see 5618 in the next column for that row Similarly for the other records we can expect the followingCoding this up: Two comments about this: Looking at the result of the query: Thats nice But lets take this one step further Suppose we want to compare two month as the percent change of the sales vales This can be done as well: This query is a little more complex and calculates a value from the table Lets break it downResult: Reading and interpreting this table the 2nd month of sales dropped ~53% in comparison with the previous month The net sales in the 3rd month increased by 33% and so onThats it for this fake data folks! Another topic for your SQL toolboxFor your reference I have a series of SQL tutorials that cover basic and intermediate queries
WNqJBZTj6RLdaNeUTrdF5a,"I was recently faced with the problem of finding an apartment in Berlin Following my previous experience in this same effort I decided to automate the task and write a software to send me an alert of the best deals In this article I explain how I built the foundations of this platform""The platform I've written is a Go application deployed to Google Cloud using Terraform Also it has Continuous Deployment from a private GitHub repositoryAfter a quick research I came to the following list of platforms to monitor: A few hours later I have a Go binary that does everything I need to run the application locally It uses a web scraping framework called Colly to browse all the platforms listings extract basic attributes and export to CSV files in the local filesystemSince I didnt want to maintain the application running locally my first choice would be to get a cheap instance at Google Cloud Once I had this rented virtual machine I could write a startup script to compile the app from GitHub and set up a crontab to scrape the platforms on a daily basisSince in the past I was involved in multiple projects involving some sort of scraping application I believed it was worth the effort I could easily reuse this setup in the future""My hypothesis was that I didn't need a virtual machine running 24/7; thus it should not cost the same as a full month price In fact my application was able to download all the properties I was interested in under 3 minutes so I expected something significantly lowerMy exploration through the latest Google Cloud services resulted in finding Cloud Run a service that run(s) stateless containers on a fully managed environment or in your own GKE cluster Still classified as a beta product by Google Cloud it is built on top of Knative and Kubernetes The key proposal is its pricing model: it charges in chunks of milliseconds rather than hours of runtimeWith a few tweaks my Go application was wrapped in a Docker container to be runnable by Cloud Run Once it gets a HTTP POST request it collects attributes from all the advertised properties and publishes as CSV files to a Google Storage bucket For my use case I created two possible ways to hit this endpoint: an Internet-accessible address so I can trigger it whenever I want and through Cloud Scheduler which is configured to hit it once a dayThe application is fairly simple: it consists of an HTTP server with a single endpoint On every hit it scrapes all the platforms and saves results in CSVs inside a Storage bucketOther application files can be found in this Gist All the feedback is appreciated as this is one of my first Go projectsNow with permissions already given use Terraform to set up the rest of the infrastructureThe initial deployment may take about five minutes since Terraform waits for Cloud Run to build and start before configuring Cloud SchedulerSince Cloud Run is still in beta  with API endpoints in alpha stage I was not able to declare all the infrastructure in Terraform files As a temporary workaround Ive written a couple of auxiliary bash scripts that trigger the Cloud API through its CLI command Fortunately all this happens in background when a developer triggers terraform applyEvery day without any human interaction Cloud Scheduler creates a new folder with a number of CSV files with the most recently available apartments in my city""Not all the services in use are available in the official calculator Either way I've made a rough estimation for my personal use considering an unrealistic number of one deployment each dayFor comparison an f1-micro instance  with 06GB of RAM  running over a full month on Google Cloud is included in the free tier; a g1-small instance with 17GB would cost US$ 1380 per month Also it is reasonable to consider the cost could decrease or increase depending on how accurate were my initial assumptions and further optimizations"
k5sVUPeyM6co326QyGgcTP,After 25 years in the Data Warehouse business I still see a very similar pattern of activity emerge You might describe it as a linear approach to building data warehouses as it typically involves creating a data storage mechanism then loading some source data then doing some data quality work and finally creating some reports or dashboardsIn the mid to late 2000s as Hadoop eco-systems and Big Data emerged I hoped that along with the new technology we might improve our processes for creating data lakes I think we actually regressed as we tried to load as much data as we could possibly get our hands on into those early Hadoop clusters Quite often what we loaded (or ingested) was never used but since storage was reducing in cost we did not seem to care muchMore recently the focus on AI/machine learning has given us another opportunity to re-think our way of building these data systems (I choose to use this term to try and remain neutral from the data warehouse vs data lake debate)Once again I believe we have failedWe have focussed on technology innovation over management innovationThe approach that has (arguably) been the closest to management innovation in the past 20 years has been the advent of the Agile movement The combination of a growing tech contingent in the list of valuable companies combined with the decline in some of the paragons of the industrial age and the perceived increase in VUCA (volatility uncertainty complexity and ambiguity) has resulted in many traditional companies seeking to adopt various forms of agile and leanNote: I choose to use the word Agile to describe a variety of approaches which have common roots These include Agile Beyond Budgeting Lean Lean Startup Kanban and Scrum I deliberately exclude Systems Thinking since it has roots in a different field of study I also choose not to argue the details of content hereIf we assume that the purpose of a Data Warehouse Data Lake Data Pipeline BI Dashboard ML Model (or similar) is to provide data/information/insights/intelligence to change the outcome of a business process then it would seem logical that the start of any data project would be to consider either the relevant business process or the desired outcome (please see my earlier post: https://martinchesbroughnet/big-data-is-all-about-process-c962db25d6eb)The starting point for agile and lean is an appreciation of customer value (lean explicitly and agile implicitly) This should make agile and lean approaches well suited to data projects if we can adapt the underlying techniques in an appropriate mannerAn interesting side effect of data projects (at least those that I have been involved with) is that they usually make flow explicit Not necessarily the flow of value but the flow of data within the data system is an important discussion As lean practitioners will know the concept of flow is core to lean thinking so how do we ensure that value (more than data) flows freely towards the customer Don Reinertsen demonstrated that the principles of flow applied equally well to product development as to manufacturing (see https://wwwamazoncomau/Principles-Product-Development-Flow-Generation/dp/1935401009)Indeed there are and far be it for me to try and represent myself as an expert since there are so many options availableId say there are 2 main directions to go if you want to follow the experts adviceThe first applies to those that adapt Scrum XP Kanban (and other) approaches to Data Warehousing In general they will provide you with with variations of the following advice: What these approaches do is take agile at face value and adapt the data engineering process in order to give the appearance of agile Whilst this approach is useful (since it applies key agile concepts like continuous improvement learning adaptive planning etc) it often fails to provide anything in addition to the core management innovation that is already provided through agileThe second option is usually to adopt a more complicated approach that may be a hybrid of agile lean and other techniques SAFe and DAD spring to mind as they both have specific success stories that relate to their use in a data warehouse environment The use of a hybrid approach presents many options These approaches are often criticised for their complexity and lack of flexibility For most data engineering teams they may find the approach overwhelming and will tend to get lost in detail and lose sight of the bigger pictureWhilst the current options are useful they do not represent any significant innovation over what is done in other software development areasThe ideas that Id like to share are grouped around a set of topics that assume one thing not explicitly stated above In the second section above I referred to the purpose of a data engineering system as changing the outcome of a business process I used that terminology to separate the data output (ie reports dashboards ML predictions) from the action (ie to shut down a plant to launch a new product to make changes to a government service to change a prescription dose) which then creates the outcome At this point Id like to suggest we go further … that the purpose of any significant data engineering effort is transformationTransformation in the sense that data insights should inform action that create significant change in the operations of the enterpriseHence we are innovating our data engineering in order to drive transformation Which becomes important when we consider the following suggestions: In a series of posts to follow this one I will elaborate more on the concepts above and show you how to turn them into something useful
LcxGxvnadyaC5rEH7V3nQU,ETL without the overhead Modern ETL management tools are production systems in their own right How can we do some basic ETL from production Aurora RDS to Redshift with minimal managementProduction and reporting have very different needs and trying to perform both on the same database can be quite cumbersome Production optimization and denormalization leads to horrific queries and performance concerns For this reason it is common practice to extract data from production systems transform it into a schema which is easier to work with for reporting purposes (such as a star schema) and load the data into a reporting and analytics database This process is a form of Extract Transform and Load (ETL) and is a very common pattern since it can be very powerful The transformation handles production quirks and special cases and reporting queries become simpler and more intuitive for business analysts ETL also enables the joining of various other data sources and tracking of historical metrics not necessary for use in productionAs we are avid AWS users we wanted to be able to extract data from our production database in the AWS managed Aurora RDS transform this data and load it into AWS Redshift which is optimized for reporting and analytics With all of the powerful offerings of AWS surely we can do this in a flexible serverless and scalable manner For this task we examined typical ETL pipeline management systems such as Apache Airflow and Luigi but the overhead was just too high These systems provide great capabilities but also require management and maintenance We investigated AWS Glue which is a powerful managed ETL tool utilizing Spark and it too was very powerful but the scale of extracting data from our production database just didnt warrant usage of Spark We chose to use AWS Lambda and build a system to run any arbitrary SQL queries we define chaining them together with SNS messages and storing intermediate results in S3 using Auroras select into S3 capabilities This system gives us all of the features we need with minimal maintenance or overheadThe basic architecture utilizes three lambdas: Did you notice those three steps are not extract transform and load? Lets take a look at a basic example with some code to get a better idea of whats happening hereThe whole chain starts when CloudWatch Events triggers our data dump into S3 For this we use the select into S3 capabilities of aurora (docs) For this well use a query thats in the format: This query saves the result of our select query where we can do some minor transformation such as joins and flattening as well as selecting only records we care about The data is stored with a prefix which includes the date and destination table name (so step 2 knows where to copy the data) Something like: dumps/2018/04/12/dwt which will create potentially multiple objects in S3 like dumps/2018/04/12/dwtpart_00000 each of which triggers the copy command It is assumed in the next part that dwtis the name of the data warehouse table to copy this data into We can then craft all of our queries and add them as sql files with the lambda and we just need some generic python to connect to our database and run all the queries it findsThis simple Python36 Lambda gets a database connection string from SSM parameter store connects to the database and runs the contents of any sql files in the current directoryTo handle the likely possibility of multiple files per query and the fact that these data could already exist in Redshift we need to merge the records since Redshift does not support upserts There are a couple of options discussed here and we chose to merge by replacing existing rows So this lambda triggered by an S3 put operation builds the copy and merge commands and executes them on the redshift cluster for each file It then publishes a message to SNS using their MessageAttributes to specify which table was updated and trigger step 3For this to work youll need to make sure you have a primary key configured for any tables which are used as this is used for the deduplication We again use SSM parameter store for secrets that the Lambda needs so theyre not exposed in the console For each S3 trigger we use the filename part of the prefix as the table name We get the primary key for that table and then fill in the blanks on our copy and merge by primary key SQL After executing the statement we publish a message to SNS that the table has been updated which will trigger step 3This lambda is triggered by the SNS notification stating that a table was updated and it looks for sql files matching the table name So to perform any downstream ETL or update any other fields you just define all the queries you want to run in a file dwtsql  which will be run each time the dwt table is updated This lambda also uses a regular expression to parse any INSERT INTO statements and notifies that these tables have also been updated So if your dwtsql updates a downstream table like user_counts_daily then you could have another file user_counts_dailysql which will be run when that table is updated This allows for chaining of queries within redshift to keep tables up to date We also do primary key validation on table update since Redshift does not actually enforce primary key constraintsThe first thing we do is check the primary key constraint and raise any errors of duplicate items This way we can configure alerting on Lambda failure to identify query issues We then just look for any sql files which match the table name and execute them then send table updates based on any downstream tables we may have updated which will again trigger this functionTo deploy these lambdas you need to handle the appropriate dependencies For the Dump function we use sqlalchemy and pymysql which can be pip installed in the amazonlinux docker container and copied into your deployment The connections to Redshift use psycopg2 which is a bit more complicated as its tied to system libraries For this you can use this precompiled version which works in Lambdas just copy the 36 version into your lambda deployment and ensure the folder is named psycopg2Herein we have presented a simple framework for basic ETL from Aurora RDS to Redshift All of the lambdas are barely 300 lines of Python and allow the execution of arbitrary queries with very little overhead or management Compared to other ETL offerings this pipeline has many limitations but for basic ETL which can be done within SQL queries on (relatively) small data this system works fine Over time our needs will probably expand beyond this solution but for now it can do everything we need If you too are looking to get started with some simple ETL maybe something similar could be just the solution youre looking for
aPnvy9W7BhQyGdeQG4YVRK,The misconception that Apache Spark is all youll need for your data pipeline is common The reality is that youre going to need components from three different general types of technologies in order to create a data pipeline These three general types of Big Data technologies are: Fixing and remedying this misconception is crucial to success with Big Data projects or ones own learning about Big Data Spark is just one part of a larger Big Data ecosystem thats necessary to create data pipelinesThere are generally 2 core problems that you have to solve in a batch data pipeline The first is compute and the second is the storage of data Spark is a good solution for handling batch compute But the more difficult solution is to find the right storage  or more correctly finding the different and optimized storage technologies for that use caseCompute is how your data gets processed These compute frameworks are responsible for running the algorithms and the majority of your code For Big Data frameworks theyre responsible for all resource allocation running the code in a distributed fashion and persisting the resultsStorage is how your data gets persisted permanently For simple storage requirements people will just dump their files into a directory As it becomes slightly more difficult we start to use partitioning This will put files in directories with specific names A common partitioning method is to use the date of the data as part of the directory nameFor more optimized storage requirements we start using NoSQL databases The need for NoSQL databases is especially prevalent when you have a real-time system Most companies will store data in both a simple storage technology and one or more NoSQL database Storing data multiple times handles the different use cases or read/write patterns that are necessary One application may need to read everything and another application may only need specific dataMessaging is how knowledge or events get passed in real-time You start to use messaging when there is a need for real-time systems These messaging frameworks are used to ingest and disseminate a large amount of data This ingestion and dissemination is crucial to real-time systems because it solves the first mile and last mile problemsThis was originally published on jesse-andersoncom
MzbCQnvyiNfjD7F2QCssrt,These questions arent meant to send you running in fear  instead I want you to approach security pragmatically As data engineers were often managing the most valuable company resource For this reason it only makes sense we learn and apply security engineering to our workHow can we do so? Here are a few tips: For your particular role or company there may be even more low-hanging security fruit  and scheduling a regular security sprint into your planning is a great way to stay on top of these issues and improve security over time When faced with those questions again you and your team can respond with ease of mind  knowing your data engineering workflows are secure
7dLgzyuHtPBQZdisnitBeT,One subtle and novel paradigms in computer systems architecture is the concept of message passing This simple albeit useful construct allowed for magnitude gains in parallel processing by allowing processes (applications/kernel tasks/etc) within a single computer OS to take part in a conversation The beauty of this was that processes could now participate in conversations in either synchronous or asynchronous fashion and distribute work amongst many applications without the necessary overhead of locking and synchronizationThis novel approach to solving parallel processing tasks within a single system was further expanded to distributed systems processing with the advent of the message queue as-a-service The basic message queue allowed for one or many channels (or topics) to be created in a distributed FIFO (first-in first-out) style queue that could be run as a service on top of a network addressable location (eg ip-address:port) Now many systems across many servers could communicate in a distributed work share style decomposition of tasksIt isnt hard to conceptualize how the pipeline architecture grew out of the concept of distributed systems communicating over network addressable message queues It kind of makes sense on a macro level Essentially we had all of the components of the modern pipeline architecture sitting on the assembly line just waiting to be assembled But first we had to solve a tiny little issue which was failure in the face of partially processed messagesAs you may imagine if we have a distributed queue and we take a message from that queue then we could assume that the queue would purge said message and life would go on However in the face of failure  no matter where you point the blame  there would be data loss associated with a message that has been purged from the queue where work was lost in process without a means of recovering Now this is where things evolvedGiven that we wanted to ensure our applications would complete all work they took from the queue it made sense to store a log of the messages within a channel (or topic) and allow our systems to keep track of what they consumed and essentially where their processing left off This simple idea of acknowledging where an application was in the queue led to the concept of offset tracking and checkpointing for a consumer group within a queue Apache Kafka was the first project that treated a message queue as a reliable and more importantly replay-able queue that could easily be divided and shared amongst multiple applications within a shared consumer group Now there was a reliable highly available and highly scalable system that could be used for more than just message passing and essentially created the foundation of the streaming pipeline architecture
5EoMSS6WuCcf96FYX5LeiZ,This goal of this series of posts is to help you gain the perspective of an experienced data engineer in real-world scenarios After reading all posts in the series you should have an intuition for the thought process of a data engineer working in Google Cloud
dJGkKZDhA85bDwQBFehaXm,On any Data Engineering project someone will need to go through the process of deciding which parts of the stack your team will be responsible for The parts which you are not willing to own or are not capable of maintaining must be entrusted to a third partyUnfortunately the reality of Data Engineering is that it is rare for the existing tools to do exactly what you need So there will virtually always be a significant amount of code or at least configuration for you and your team to write and maintain Enterprises will find that unlike traditional software where theres a choice between Build vs Buy in data engineering youll need to Build no matter what you choose and the decision is really layer of the stack you want to focus your efforts onRolling your own service? Youll be dealing with the OS and application code Using a managed service? Youll be writing code to orchestrate API callsEven if youve selected a vendor and the vendors product does everything you want even if that vendor sends professional services to set it up for you youll ultimately need to own the artifacts defining all the specific things that product needs to do for you and your clients How you manage those artifacts and exactly how you compose them together to serve your business is something a vendor is unlikely to be able to decide for youUnderstanding that there will always be work to do lets start with the example of a data storage service Ill leave it unspecified whether its SQL NOSQL or block storageA simple way to begin to approach this problem is to clearly define whats important to you Which features are must-have and which are just nice-to-have? What metrics are you held accountable for? Just these simple questions can often rule out several optionsNow that youve seen some example questions that would be asked during requirements gathering Ill give specific scenarios where Ive seen teams forced to operate their own service rather than using a product off the shelf in order of frequency that Id guess they appear in the wildA similar decision must be made by data engineers on the software side to use an open-source framework or build your own toolsHere is a list of reasons why you might choose to build: 1On the flip side here are reasons to avoid rolling your own: 1
7JhPW65iy5wwoZMjaeuFoB,How do you see past marketing fluff and make sound decisions on which data products to run your business on? This post outlines considerations made during the selection process along with balanced product evaluations What youll see is that Data Engineering requires consideration of the user population in order to judge the the merits of any given product Youll also learn that as a data engineer rather than a series of decisions that you need to make theres something more like a general acceptance of where a product fits into the landscape Often the choice is really made for you and you just need to deal with it If youre lucky you can call out cases where unwise selections have been madeIll start with an example scenario evaluating BigQuery Dataproc and DataflowBigQuery is a massively scalable database that requires no configuration It exposes an API to submit jobs but there is no hard requirement to write any code as everything can be done with SQL via the Cloud Console web interface The authorization model around Datasets and Tables feels somewhat simplistic because permissions can only be set at the Dataset level The query engine is incredibly powerful and a number of powerful Machine Learning features have been recently added to the product Because its so easy to use and theres a JDBC driver for it its tempting to use BigQuery for things its not really optimized for like single-row lookups on small tables for user-facing web applications There is a BI-focused execution engine in the works that may help with thisDataproc is an orchestration system for deployment and execution of Hadoop applications It provisions compute nodes in your own Google Cloud project A major difference is that a Dataproc cluster is reusable and multi-purpose The Dataproc service exposes a JSON HTTP API that allows jobs to be submitted via CLI web console or client SDK to run on the cluster The cluster is a fully working YARN cluster and its possible to submit jobs directly to YARN without using the Dataproc API which causes the job to be submitted via the Dataproc agent running on the master node Because its a normal Hadoop cluster there is a lot of configuration and customization possible and potentially necessary to get jobs to run most efficientlyDataflow is massively scalable and simple to operate It requres use of either an inscrutable Java API or a slighly easier to use Python API that comes with a performance penalty Both APIs are provided by the Apache Beam open source project The way that the Java SDK uses generics (type arguments) is unconventional and will appear unfamiliar even to relatively seasoned Java users Scala users will be disappointed that the ability of a Beam project to compile successfully has very little bearing on its ability to execute as intended Dataflow provisions compute nodes in your own Google Cloud project which yields excellent scalability but a downside is that node startup takes several minutes before any pipeline code can begin processing Its not easy to modify code on an existing pipeline and it seems like the best way to deploy a new version of a pipeline is to delete the old pipeline and start a new one The way that deployment works on Dataflow is a complicated but the complexity is hidden from users by the Runner class that submits the pipeline for execution In theory its supposed to be easy to switch between runners but in practice there are a lot of runner specific argments that need to be provided For example the Dataflow Runner needs to know whether to use public ips what subnet to run in which bucket to stage artifacts to and how many workers to use in addition to several other related additional arguments which are totally irrelevant to the Local Runner The deployment process is so complex that there is no way to do it manually  theres no deploy button on the web UI and although Dataflow exposes an HTTP API it makes no sense to try to use it directly because no sane person would try to build their own protobuf message defining a job The Dataflow Runner has to scan your code to convert it into a massive proto that references the classes you use and the arguments that will be passed to them at runtime it has to identify all your dependencies and upload them to the staging bucket to be used at runtime and finally it has to submit the protobuf message defining your job to the Dataflow API to initiate the pipeline Dataflow is powerful but this is achieved by using a significant amount of magic You will need to decide for yourself whether this is a good thing or notBecause of its ease of use BigQuery is the only one that can be given to Business Users out of the box It may be possible to use Dataproc as a backend for Hive Spark SQL or Presto which is somehow exposed to Business Users but there would be a lot of custom development to get that working For a user population that is starting with SQL and doesnt have a software engineering background Dataflow is a complete non-starterFor large data engineering teams there is some kind of existing codebase which will dictate the product which must be used For complex branching logic use of open source libraries and integration with HTTP APIs and a mix of datastores and storage formats its not possible to use SQL alone This is where Dataproc and the Hadoop ecosystem come into play A team may have inherited a large number of legacy MapReduce jobs Hive queries or if theyre lucky Spark jobs They may be need to use Presto in order to use metastore federation Dataproc is one of the most versatile products on Google Cloud which means its often the target of large amounts of customizationFor smaller teams with strong development skills that dont want to spend time dealing with cluster configuration Dataflow begins to look like an attractive option This is especially true if the team plans to use Cloud PubSub and if the team has relatively few pipelines to implement and those pipelines are simple but need significant scale Dataflow / Beam are much easier to use the pipelines are simple but complex pipelines are definitely possible to run
RNkbqfiAnZcUYgC5skuU9v,When the business needs to allow users to execute code remotely on a compute cluster in order to get their jobs done its near impossible to be 100% certain that code submitted by an authorized user will not be able to exploit a privilege escalation vulnerabilityIf its your responsibility to operate clusters that are shared by many users this is something you will need to think about as you build your strategy for defense in depthWith Dataproc you can use your existing Linux hardening process to build custom images to be used as the base upon which Dataproc installs Hadoop services This may reduce your attack surface slightly and also give you the opportunity to put some intrusion detection in place Known vulnerabilities should be eventually mitigated by a combination of patching and configuration but 0day exploits are nearly impossible to prevent by definitionIt is your duty as a data engineer to do everything you can to make it unprofitable to use a 0day on your systems You want to be able to detect if a cluster member has been exploited and make it possible for incident response to identify the source of the exploit and the attack vector You want to regularly replace clusters/VMs so that a successful attack that initially goes undetected is difficult to persist in the environmentIf preventing direct access to the clusters is an option your organization might require all code to go through code review and be deploy through CI/CD The idea here is that teammates will prevent malicious code from being being checked in It addition to this the information security team may regularly scan the contents of the package repository for either known vulnerable dependencies or actively malicious codeTo the extent possible you may also want to prevent you yourself as well as your teammates from access the cluster by implementing some kind of breakglass system where you need to explicitly request interactive administrative access on a temporary basis How much access would an attacker gain from the compromise of one of your teams laptop or desktop? Minimize this with 2 factor authentication and breakglassYou will also want to have a strategy for preventing data exfiltration in case all of the above defenses are bypassed With VPC Service Controls your organization can limit what buckets and datasets are accessible Normal firewall rules may also be used to prevent exfiltration to compromised hosts on your internal network
FoGGiGkwuoPqR4pdoYcNN2,Data Engineering teams exist to serve the needs of one or more business units whose operations produce consume or publish data Dataset security is a discussion that data engineers will have with the business unit Like many things in data engineering theres not really a choice to be made so much as a foregone conclusion as to what the business requirements are The data engineer is there to elicit information from the business explain how the requirements can be implemented and then implement the necessary security In order to do this successfully its helpful to understand what security options are available and which Google Cloud products exist to provide themEncryption is enabled by default on most Google Cloud products Cloud Storage Compute Engine Persistent Disk and BigQuery are the most prominent examples While this default level of encryption may be enough to comply with basic security policies there is often good reason to have additional layers of protectionEncryption at rest protects you from someone in the datacenter pulling a disk that has your data on it and being able to read it While this is necessary in a Google datacenter its a pretty unlikely attack vectorEncryption in transit protects data from eavesdropping while in transit across an untrusted or compromised network segment This is also very necessary but on a corporate intranet or private Cloud VPC network the most likely man in the middle is your companys own security organizationEncryption at rest and in transit is table stakes but doesnt protect against some of the most common threats that you might face Neither of them do anything to protect against a malicious authorized user Also because they are handled transparently they both place trust on the receiving end to implement them correctlyIf youre in a situation where trust is not a luxury that you can afford you may need to consider some kind of client-side encryption This goes beyond encryption at rest and in transit by by transmitting ciphertext rather than plaintext If the receiving system doesnt have access to your encryption key you dont need to rely on trust to be sure that it cant read your data Client-side encryption is often used with KMS Envelope Encryption which can be implemented with Cloud KMS Envelope encryption potentially adds an additional layer of authorization required to read a given piece of data A user needs to have both read access to the data and access to read or use the decryption keyWith Envelope Encryption assuming that keys are not copied or cached somewhere you can deny access to a large amount of data instantly by destroying or denying access to the key required to decrypt the data and you dont need to destroy or deny access to the data itselfSometimes security is not concerned as much with the encryption method of a dataset so much as the contents of the dataset In these cases a product like Cloud DLP can be used to scan a dataset for sensitive information or make sensitive information inaccessible in the dataset by replacing it with a token or encrypted stringAs a data engineer you may also need to decide how to manage authorization of the datasets you are responsible for
jsgSygPT8BnagP9R58MpaS,For being successful as a Data Engineer working with open source software is an indispensable skill This post covers how open source fits into a data engineering workflow and contrasts Data Engineering and Software Engineering contextsThe typical data engineering project is built on a collection of well-known frameworks like Apache Spark Hive Tez Beam YARN and similar If youre working at the platform level it might be necessary to have the source loaded into your IDE for easy browsingData pipelines are very business specific so its rare to find a complete open source example that does exactly what you need for your own data pipeline Data pipelines also tend to have a lot of company-specific integrations that arent helpful to share externally and this reduces the availability of production-ready code that can be found in searches Its common to have to comb through dozens of pages of search results full of examples for slightly different use cases older versions of the chosen framework or some other confounding factor and draw tiny snippets and marginally relevant concepts together to synthesize a working solutionSo on a data engineering task you might feel that youre both reading more code (due to the large volume of irrelevant junk) and writing more code (because a complete example doesnt exist) relative to a software engineering task of equivalent complexity In software engineering tasks it seems a bit easier to either find a well documented framework library or working example
FwZ64i3Qwf8cGgpt5uwZom,Conversations with InfoSec arent really a negotiation because they generally have the power to veto anything that doesnt comply Think of it as a discussion where your shared goal is find the intersection of corporate security policy and the needs of the businessInfoSecs needs are pretty simple They want to make sure that data is encrypted in transit and at rest and that access is limited to authorized principals onlyYou can make the interaction more efficient by presenting the information they need in an easily consumable format without extraneous detail This means simple diagrams and concise architecture descriptions KISS principle applies here no need to get fancyIf theyre giving you information just digest it accept it and move onIf youre getting their approval for something as part of a product launch expect that it may take a while for them to work through their ticket queue and be a little bit patient with themIf you are doing something unconventional and need their engineering advice dont expect them to build anything for you Expect to propose a solution and get a sense for whether theyll allow it then implement it yourself After implementation its also not common that they will review your implementation in much detail Its ultimately up to you to do what was agreed upon If you deviate from the plan its on youAs a data engineer security is your responsibility and you are on the front lines Infosec may alert you if their scans detect a misconfiguration that exposes data but dont rely on them If you do your job well you shouldnt need to hear from them much after getting your architecture approved
DAfbMawv6qVdoQL2MtCHcn,"""Apache Beam is a unified programming model to create Batch and Stream data processing pipelines Simplifying a bit it's a Java SDK that we can use to develop analytics pipelines such as for counting users or events extracting trending topics or analyzing user sessionsIn this post we are going to see how to launch a demo of Apache Beam in minutes thanks to a Docker image with Apache Flink and Beam pre-packaged and ready to use As a bonus the repo used to create this demo is also available on GitHub to get started creating Beam pipelinesTo run this demo we need Docker and Docker Compose For building Beam pipelines we also need Java and Maven""Let's get started and deploy a Flink cluster with Docker ComposeFirst get the latest repo  in fact all we need is the docker-composeThe launch script also uploads a custom JAR with Beam pre-packaged and ready to use For more technical details on the cluster refer to the repo""Let's now run our first Beam pipelineOpen Flink web UI exposed on port 48080""Here's what happened under the hood The JAR is built from this starter repo By default it runs the class WordCount with input file /tmp/kingleartxt and output file /tmp/outputtxt The input file is also pre-loaded in the Docker image so all Flink task managers can read itWe can check the result of WordCount by connecting to a task manager and looking at the content of the output file Note that the output file name starts with /tmp/output""Finally let's look at how to build our own JAR fileThe repo used for this WordCount demo is based on Beam documentation for the Flink runner with minor changes to overcome some imprecision This repo is a good starter to build any Beam pipelineJust clone the starter repo and build it: The last command will create a JAR file within the target/ directory We can upload the JAR via the Flink web UI and run it as we saw aboveFrom here it should be easy to tweak the file WordCountjava and create other Beam pipelines""As a concrete example I've used Beam to analyze trending topics on Twitter during the Oscars"
bfmv44eLk42y72EA4jrdB3,When we think about Ansible the first thing that comes to mind is automation for IT applications environments through a simple language that describes Infrastructure as a Code aka playbooks We would try to understand how this tool could also be very relevant anytime for a data engineerA team of professionals are involved in a Big Data projects and they should all collaborate to help the company extract value from raw data These professionals are: As we can see above we can regroup these 4 roles in 2 super roles: data anlyst/scientist aka the muscle ( as their reason of living is to show their muscles to client/manager) & Bi developer/data engineer aka the brain( as thier reason of living is to do a good work ):P However even if the 2 sub-roles of the brain seems very similar at functionnal level its not really true from a technical point of viewIf a BI developer also has to design datawarehouse this will often mean a choice between different complete entreprise solution ( PentahoQlikview…) unlike a data engineer that will make the same choice but for among OSS as shown below: What does it mean ? It means that he has to configure all these components each other to ensure a good performance all over the timeNota Bene: The previous assertion about brain/muscle was obviously a joke I deeply respect the work of my data scientists colleagues ;)As I said previously there are multiple components in Big Data ecosystem that should be installed/configured to work perfectly  So we might think that ansible should be used just for the installation & configurationYou are right It should be the case in the best of all possible worlds but unfortunately this world is not ours In this world things tends to be messy if you dont do anything If you arent convinced of this fact i suggest you to do some researchs about what scientists called entropyBecause this messy could break our infrastructure we also deploy monitoring systems as a part of our datalake Of course this reduce the probability of messy in our system but it would never be equals to 0  so we will still have troubleshootings and thats where Ansible comes to save us All the components of our data lake are distributed systems thus the datalake itself should be considered as a complex distributed system Obviously you could do maintenance operations through an ssh connection to all machines You could argue that 100 machines is just for production purposes and this platform should be more robust & controlled Thats right but think about a little bit if you develop playbooks during your dev phase as workaround for troubleshooting  this same playbook could be used in production( more nodes than in dev ) without heavy changesA simple exampleYou have a Spark cluster deployed in your dev /prod environment ( configuration as closest as possible between envs) This Spark cluster was used to query an huge dataset but the dataset grows and the request seems to take more & more timeTo achieve better perfomance you decided to write better quality Spark code but also tune your Spark configuration ( dev cluster) You noticed that by using less worker memory it reduces significantly query mean time In your dev env it had been very easily with SSH because it just consist of 3 nodes As in production env you have more nodes thi simple task would become a long boring taskNowimagine that you have already done it using ansible for relevant parameters of your Spark cluster  so you could easily do/undo various parameters to achieve the best performanceData engineers should always think about automate boring parts of thier work in order to focus on releveant challengesBy the way what are your experience of using Ansible for data mining ? Let me know in comments :)
fahn6JHdhnBazrxfMdqn8b,Lately I started working with Elasticsearch with the intention to load and query some Twitter data In this post I want to share with you how I defined the mapping of this particular data and what challenges I faced I consider Twitter a very popular source of data so having the process of defining the mapping summarised here would be potentially useful to anyone who wants to use this dataIf you happened to work with Elasticsearch you already know that the first step of using it is to create an index where the documents live This index also holds information about what type each of the documents fields is which is defined in mapping Elasticsearch docs define mapping as: Mapping is the process of defining how a document and the fields it contains are stored and indexedHowever Elasticsearch gives the flexibility thanks to dynamic mapping to automatically add and define a fields type just by indexing a document Given this I first tried to index a document without defining the mapping and look into what Elasticsearch came up with for the field typesEach tweet in Twitter data is a JSON document with several fields referring to the actual tweet and the Twitter user as well One can find more about Twitters data format in the Twitter docsand the result was (Note that the majority of the fields were removed for displaying purposes; find the whole mapping in this gist): The first interesting thing is that Elasticsearch is creating two versions of the string fields a text one and a keyword one The former is being analyzed by the chosen analyzer while being indexed while the latter is being stored as it is and it is really useful when it comes to aggregations and grouping This saves us a lot of time of having to define the keyword type for every single field as it used to be in previous Elasticsearch versionsAlthough Elasticsearch did a good job identifying the type of the fields there were some cases that specific definition had to be given Dynamic mapping is good when we want to index data quickly; however the mapping of the existing fields cannot be updated Thus we should carefully choose the type of specific fields before indexing to avoid the process of re-indexing every time we want to update our mappingGoing back to the mapping I noticed that the created_at field (along with the retweeted_statuscreated_at usercreated_at and retweeted_statususercreated_at fields) are indexed as string fields while it would be really useful to store them with date type in case we want to perform a date range queryAn example of a tweets date is: According to the pattern syntax the tweet format is as follows: Given this I defined the mapping for the above fields as: Other fields that are not automatically indexed as should be are the geo fields (they are indexed as array of float) Twitter provides two fields that hold the same information in different format the coordinatescoordinates and the geocoordinates nested fields
VyJcGMEb5hJ2KkdRSsiFJQ,You might have heard about Panama Papers If not they are 115 million leaked documents that detail financial and attorney-client information for more than 200000 offshore entities Some of those entities were shell corporations that were used for illegal purposes to keep personal financial information of wealthy individuals including public officials privateThe documents were leaked to German newspaper Der Spiegel which then shared them with more than 400 colleagues in 80 countries to be able to analyze and digest them more efficientlyGraph databases were used to analyze the data and find relationships between those people and entities The people exploring the data were reporters and journalists with little to none technical education and since theyve managed to reveal a lot and connect many dots  its obvious graph databases are very intuitive and easy to use (If you want to explore Panama Papers on your own you can try graph databases for yourselfWe at CROZ first got introduced to graph databases a few years ago (you can read more how we used graph databases here) and weve been working with them since then in one aspect or another -either as a standalone product used for data exploration or as a part of a bigger system for storing data as an intermediaryThe neat thing about graph databases is that they are graphs and there are a lot of graph algorithms that can be used to find a solution to your problem That is why they started getting more traction lately and why I decided to look deeper into the matterGraph databases have been in development for the past 20 years or so (with their roots set in 1960s) but they havent caught on until now (except for a few specific use cases which will I will mention later) To understand the concept of graph databases it is important to understand what a graph is and what is a databaseWhat is a database?  In laymans terms a database is an organized collection of data or a place to store your data for somewhat easy access Data is stored in tables and tables can reference each other When you want to combine data from different tables you must use joins which are costly When you want to see a connection from point A to point B in a relational database it could take time for you to write the query and time for the engine to execute itThis is where graph databases get their moment to shine It is also essential to know your data which can be achieved by exploring it Data exploration is done much more straightforward in graph databases than in relational databases because it is a simple matter of following connections once you have a starting point while in relational databases it is necessary to write queries to examine your data Now that we defined a database we should also establishgraphingWhat is a graph?  Without going into mathematical definitions and formulas a graph is a representation of objects (nodes entities) and their relationships (edges) Nodes represent entities (eg people and businesses and they can be loosely thought of as an equivalent of a row in a relational database Edges are the lines that connect nodes to other nodes Edges can be directed (that means that an edge describes a one-way relationship) or undirected (describes a two-way relationship) In graph databases there are also properties properties are additional information about nodes and edges (key-value pairs)Graph databases are a combination of the two mentioned concepts They are intended to hold data without constricting it to a pre-defined model Graph databases excel at managing highly connected data and complex queries which gives them an edge over traditional relational databasesSomething that could have been achieved with a very complex combination of joins in relational database can be done a lot simpler using graph databases (eg finding all the partners of a person running a business that supplies fresh fruit to your kitchen would require some work in relational databases while in a graph database it is a simple manner of displaying all nodes connected to the person running a delivery business)With everything we covered up until now you are probably wondering why graph databases have not been more popular Well they still havent reached their full potential and are mostly used in some particular use cases Typical use cases include fraud detection (anti-money laundering e-commerce fraud insurance fraud) recommendation engine network monitoring identity access management and many othersThere are currently only a few graph database vendors ( Neo4j JanusGraph Amazon Neptune ArangoDB) but probably the one that stands out the most is Neo4j It has a massive community and the language used is Cypher which is considered easy to learn The size of its community currently gives them an edge over their competitionIf you want to learn more about graph databases or use them in one of your projects feel free to contact our data engineering team Originally published at https://croznet
a6xLqTVt7ooUu4fp4bVkDT,Techopedia defines a task manager as: … a utility that provides a view of active processes or tasks as well as related information and may also allow users to enter commands that will manipulate those tasks in various waysCongrats you built a data set Most likely you build a cron job that calls a bash Python or R script to dump data into a relational table You stitched together some internal data with data from an API and now there is a new table with great insights A few people in marketing or analytics hear about the new insights which help drive a strategic decision and your boss is happy Pop the champagneA senior engineer or close advisor tells you to implement a task manager like Airflow or Amazons Simple Workflow Service After looking at the complex documentation you shrug them off Those tools are too complicated and its unnecessary Youll be fineYoure going about your day when you get a call from analytics The data isnt there No problem open up the server and diagnose the problem Seems like the API changed Okay reconfigure the job a little and set it back in motion Back to workAgain you get a call the data isnt there Okay lets diagnose again Seems like one of the columns you joined against in an external table has changed up a bit Diagnose fix move on AgainThe job fails a third time and you decide to be smart and implement a test that shoots an email if theres no data How clever you areMarketing is wondering if that data can be linked to another data set Yup it can This means youre job needs to wait on another job to happen before it can run Okay you set up the email system to check the other job and set up youre job to run after Congrats youre now responsible for two data setsThe job fails again While testing it over and over again you abused the API a bit too much and it cuts you off The company cant get access to the data until the API resumes tomorrow FantasticJust getting notified isnt enough; you implement a logger Now the next time the job fails youll be able to check to see why Should cut back on debugging timeNow analytics is calling you when either job fails Its not like you have other work to do The data is now necessary for certain functions of the company and its reliability and availability are critical for the business You try to push back but its too late the company is more successful because of your work; cant go back nowMaybe there is another engineer who can take this off your hands Maybe an external service can manage this Engineering shrugs theyre overworked while simultaneously playing Call of DutyOver the weekend the job stopped; no data came in on Saturday or Sunday You have to run the job for two days and make sure you didnt duplicate data Great way to start a Monday Maybe you should have listened to your adviser who told you to build a task managerThe job fails over a weekend This time the data set grew too much and a single server couldnt handle it Time to do something drastic as this is cutting in to your sleepTurns out that others have faced your same problems before: Depending on other tasks (precedence) recording run time information (logging) emailing or texting systems (notifications) repopulating tables (back filling) data scale (parallelism) and a dashboard to look at the tasks (system visibility) are regular problems for data engineers unfortunately you had to learn this lesson the hard wayCaserta has had great success implementing Airflow as a data ingestion task as it provides the foundation for system management and data quality I encountered these problems while at Rocket Fuel and implementing a task manager solved this issues effectively and efficiently
5bQ7Go4TuEJsWuL65Q6CT8,New and aspiring data engineers looking to level up their skills outside of work hours usually have two options: Ive done a fair share of side projects From the basic apps I completed during coding boot camp to my graduate school capstone project that I bring up in interviews every chance I get However throughout my first years as a data engineer I almost exclusively relied on tutorials to develop new skillsThe week my teams lead engineer went on vacation our production environment seemed to go on vacation with himWe develop and maintain data pipelines using dockerized airflow deployed on AWS EC2 That week a few of our pipelines started malfunctioning This was during a crucial sales month for our company I had to troubleshoot the issues push up changes and restart the pipeline I was comfortable with all the tools we used at an individual level: Airflow for scheduling our ETL processes Docker for containerization AWS RDS for data warehousing and AWS EC2 The problem was I did not really understand how these tools were deployed or how they all came together to form an enterprise-level data ingestion system In short I spent the week struggling to keep our pipeline running with tools I THOUGHT I was comfortable withAfter this tough week I took some time to reevaluate myself as an engineer and find out why I was so unprepared I realized that all along the brilliance of the lead engineer was actually a crutch that was weakening my personal development Having the safety blanket of a more experienced engineer can make you relax and not push yourself beyond your comfort zoneI quickly realized that In order to remove this crutch and strengthen my development I needed to: In this article I will go over the strengths and weaknesses of tutorial-based learning and learning through side projects I will also point out the specific reasons my heavy reliance on tutorials failed me when it was most important and how building my own data ingestion system closed those gaps and greatly improved my value as a data engineerDespite the wakeup call I received after binging on tutorials for years I still use them to this day and believe they are a critical resource for engineers to develop new skillsWhen learning a subject from scratch you may have the urge to search stack overflow and piece together a working prototype from several posts This approach will give you headaches Even if you are able to get something working you will not be able to deeply understand how it all works A quality tutorial will give you the building blocks you need to start hacking around on your own For example: A tutorial taught by an experienced instructor will get you familiar with common terms and language which will help you communicate with other users and ask quality questionsAt this stage the controlled learning environment a tutorial provides is perfectly healthy It builds confidence and allows you to develop more interest in the subjectIf you already have the experience but its been months or even years since implementing what you learned you may need a quick refresher to get up and running again The value you get from using a tutorial at this stage is still worthwhile You will be formally reintroduced to the subject and able to cover the material quickly as most of the work is just refreshing stale knowledgeHave you been hearing a lot of hype about some tool that is supposed to change everything and leave the one youre currently using in the dust? If you can find a credible expert who has jumped on early and created a tutorial that is a great opportunity to take a few hours out and quickly dive in You can treat it as a self-guided tour skipping around sections and engaging in code alongs as you see fitThe story I shared earlier highlighted how taking tutorials on several different subjects may help you learn them individually but in the real world you will need to combine several technologies in order to create a useful system There may be a tutorial that focuses on the use of two tightly associated tools (ie Docker and Kubernetes) but you are unlikely to find a course that combines every technology you will use for a future project Especially in the world of data engineering you need to be able to combine and swap several tools depending on the project youre working onLearning through tutorials is like navigating with Google Maps Yeah you got to your destination but do you even remember what exit you took? I bet if your phone died and you had to actually read the signs yourself you would remember Thats because like following a tutorial following google maps requires much less concentration to reach your goal Tutorials have a lesson plan which means the whole experience is curated for you by the instructor to be completed in a specific way to reach a specific outcome This hand-holding means you dont have to focus as much on what youre doing because everything is laid out for youWe all start tutorials with lots of enthusiasm However as time goes on that enthusiasm fades Why? Because most of what you are doing is just watching someone else or reading along and following examples Youre putting your effort into someone elses vision for a project you have no real stake in This lack of connection to the work is a major reason you will not master a subject through tutorials alone You need more skin in the game and a bigger purpose than getting to the next section of a lessonIm sure youve heard it before software/data engineering is mostly problem-solving (Aka Googling) Engineers work with so many evolving technologies that you will probably never do the same thing in exactly the same way more than once You need to adapt and be flexible in how you build The way it was done today is not how it will be done tomorrow Tutorials do not give you the opportunity to struggle through problems and search for answers on your own This ability to struggle and persevere is a skill in itself and like any other skill it can be developed through repetition Tutorials do not provide suitable opportunities to grow this skillIn order to create a compelling data engineering project it should go beyond just building a pipeline or data warehouse You should strive to create a fully functioning data platform that will solve a real problem and provide insightThe data engineering ecosystem is vast You will have to piece together several technologies to create a data platform If you know SQL and Python but have never worked with a cloud platform (ie Amazon Web Services Google Cloud Platform) workflow scheduling tools (ie Apache Airflow) or containerization (ie Docker) you should first get yourself familiar with those tools individually Getting all of these pieces to work together at a production level is already a big challenge and will be the main focus of your projectIf youre going to put this much effort into a project take some extra time to think carefully about what tools you choose to complete it It can be easy to get excited over using the newest and coolest tools But ask yourself will these technologies last? Do you see yourself having a strong use case for them now or in the future? After deeper research you may decide that a cutting edge tool has a bright future and you want to be an early adopter I applaud this forward-thinking approach However if your research does not leave you feeling confident that it will boost your value as a data engineer you should pass on it and stick with a more promising optionSide projects are hard and can take months to complete (Some projects are never really completed) If your team is planning on experimenting with a machine learning algorithm or there is a one-off task that requires you to use cron jobs just take a tutorial and call it a day Save the energy for only the most important and lasting skills you think youll needThe first thing you should do when considering a side project is to assess your time and energy because a side project will take up a considerable amount of it Sure the side in side project implicates that there are no deadlines or required work hours but if you start up a project and let it sit for more than a week it becomes easier and easier to never open that folder again As your familiarity with the tools and details of the project itself begin to fade you may start saying to yourself Its going to take so long to get familiar with the code again or I forgot my AWS password Then youll justify delaying the project until you are able to block out a larger chunk of time to refamiliarise yourself Ideally you should dedicate one hour a day or a few hours a week in order to keep up good momentumRelying solely on tutorials will stunt your career growth and leave you unprepared to take on increased responsibility as a data engineer Every aspiring or developing engineer should have an active side project that replicates the systems they currently work on or want to work on in the futureIn order to build a solid data engineering side project you will have to implement several technologies: If you are unfamiliar with a few of the tools dont let that stop the entire project Start the project and take breaks to do tutorials when neededWhen my reliance on tutorials finally caught up to me I decided to start my own project I chose to implement many tools I use at work but also tools I had no experience with and wanted to learnIn future posts I will discuss the process I went through to design and build my data engineering project and the residual effects it had on my career growth
9GY7fJ3caEkq3uDCifuiTv,Ive been working on a project that uses Googles Dataflow product  basically a better Spark data processing pipeline  that is now Apache Beam Ive managed to get some good stuff done but as I added more to my pipelines I wanted to get away from relentlessly extending the typical example code you see My pipelines had started to look like little utility scripts that grew and grew and grew plus you know the only test was run it and see if what comes out looks goodSome key things Ive done is build custom DoFnand PTransfomclasses These are the building blocks that take an element and transform to another element (or elements) or a collection of elements into another collection of elements via multiple steps Both DoFn and Ptransformcan be tested using some JUnit help  however note many of the examples and documentation regarding DoFn testing are out of date since they recommend using a DoFnTester class a technique which is now deprecated Instead testing of both should be done with a normal JUnit test method that is annotated with @Category(NeedsRunnerclass) and then run it its own mini TestPipeline instance Youll have to do some extra work to put test data into the pipeline run it and use a special PAssert method to look at the output but it works  and apparently this is the only true way to be sure your code is working I personally can confirm this (see below)A typical DoFn test for a class MyCustomDoFn that converts elements of MyInputData to a string representing their value might look like: Basically you just put data into the pipeline with the Createof transform processes it with a ParDo wrapping your custom DoFn and then look at the output with PAssert Note @Rule annotation is required although Im not sure why See more here: https://beamapacheAnother issue that has burned up a lot of my time is trying to use complex data types with Beam So many of the examples are all about processing string and numbers that are instances of simple classes like String or Integer However much of my code is dealing with rows of data from a data source like BigQuery which needs to be wrapped in a simple POJO or Value Object As I coded up more pipeline processing I found a lot of commonality and wanted to write DoFn and PTransform classes that could deal with a class and sub-classes For instance collating data about website visits might include or not include the URL or have other dataHowever Ive found that if you have transform that processes a class MyData and a sub-class MySpecialData when your elements get serialized on the wire elements of type MySpecialData are serialized as MyData I thought a way around this might be to use generics and have my transforms be parameterized with <T extends MyData> but that didnt work eitherAs a workaround for the above I tried using a class with a Map in it to contain a bucket of data fields that might vary eg a representation of fields of a record from the databaseBut oh-noes You cant put a plain old Object into your class and expect it to be serialized  the coders for data dont know what to do with an Object You could write a custom coder but I really didnt find any information on how to make that workIn the end in an attempt to workaround the Map with Object values problem I decided to create my own class Field and then have a subclass for each of the datatypes I wanted to put into the map So I ended up with something like: Now youd think this would work  my Map is no longer storing Objects but nopeGaaaah! I thought initially this was because Field is abstract So I tried changing it to a concrete base class that just returns null as the value The consequence of that was all my elements ended up with every field having a null valueNow reading through the documents about the Avro coder which tells Beam how to serialize my classes I saw stuff about custom schemas to tell Beam how to deal with specific fields  underneath it appears to use a JSON or JSON-like encoding The problem is I could never find any good examples of how to use a schema The only thing I figured out was that by adding the @Stringable annotation to a class I could tell Avro to encode it using its toString() method and decode it with a constructor taking a single String parameter I really didnt want to do that  sure encoding a Map as string while preserving type information and decoding it isnt that hard heck for my simple numbers and strings case a snippet of JSON could do just fineWhile reading the schema docs I saw there is a thing called a Union type it can represent so I went hunting in the AvroCoder source code that is part of the Beam project Now tests arent always helpful but sometimes they are  finally I found this snippet of test code demonstrating use of the @Union annotationSo I went back to my class and added one single line above the abstract base class Field like so: and BINGO all my tests started passing My map values were faithfully encoded to one of the two types and decoded correctly on the other side I even put a breakpoint in the PAssert code to ensure it was really working and yes it wasNow Im sure someone who knows Avro and Beam well will think these things are basic but to a beginning in this code base I have to admit I burned a lot of time trying to get it all working I could have saved a couple of expensive refactoring exercises if Id know it was this simple to make Avro do what I want without resorting to the hacky solution of stringify all the things which is what I see many Beam examples do
NKhHpF4JZEzWDvPpi4mfby,I will like to say i am completely a newbie in the Data Engineering scene and my experience so far isnt up to a year Some information that i will be passing of here might not work for everyone and you might need to dig in more to find out what will best work for youThis Tutorial will include using the following tools: SAP Hana Express sandbox Hadoop Hive and Sap Hana Studio and Oracle VM virtualBoxThe second step of this tutorial will go through installing Hortonworks in the virtualbox installing the neccessary tools in HortonworksThe third step of this tutorial will include moving the data from SAP into Hadoop/Hive and Archive it using the ORC formatStep 1: Installing Oracle VirtualBoxGo to https://wwwvirtualboxorg/wiki/Downloads and download the virtualbox for your OS If you are using the windows OS like i am click on the exe file and installInstalling SAP HANA express editionGo to https://wwwsapcom/developer/topics/sap-hana-expressYou will have to fill in your details and it will redirect you to a page where you install the download manager for your Operating System The downloaded files will include an ova file named hxexsaovaThis is the file we are going to import into the virtualbox You can click on the OVA file and the virtualbox will open up with a pop up alerting you to import the ova fileIf you are ready to install the file click on import and it will import the system into your virtualboxStart the vm sandbox and a new system will open up on your computer screen You will be prompted for a username and passwordThe SAP Hana Express sandbox is an opensuse Linux operating system If you are familiar with Ubuntu or any linux operating system then the learning curve wont be that difficult and even if you arent Still follow this tutorial as i plan to go step by stepInstalling CygwinIf you are using a windows operating system for this tutorial you will need to install cygwin to remote login into the SAP Hana Express Server I found a tutorial already that goes through this alreadyhttp://wwwmcclean-cooperIn the instruction when you get to step 9 make sure openssh is installed Our next step will be to configure SAP Hana Studio to connect to the systemSAP Hana Studio In the next step we will be using SAP Hana Studio to log into our SAP Hana Database For those who are new to these I will explain what the Hana Studio is and what it is used for If you are already experienced with this you can move on to the next stepAccording to http://saphanatutorialYou can follow this tutorial to install SAP Hana Studio I have a valid license so i downloaded HANA Studio from SAP Service Market Place The next step will be to log in to the Hana StudioThe credentials needed to log into the System: Once logged in we will create a schema and import data that will be used for this exerciseTo create a schema:Right click on the system that you just added in Hana Studio and click on Open SQL ConsoleTo import data: I have an excel file which i am going to use for this exercise The size of the file is over 1GB if you do not have any data to use you can download data from the internet and save them in an exercise file I will include websites where you can get data for this exerciseIt will take a long time for the data to import if its a large file Once the import is done You can run any sql queries to make sure its the right data For me After moving the file from the excel into the table in SAP the size of the data decreased from 4GB to 0Installing HDB CLIENT FOR SAP:The SAP HANA database clients (for connecting applications) is used for connecting application Right in the beginning of this tutorial we downloaded some files The files downloaded includes a SAP Hana Client that will be moved into SAP Hana ExpressDownload filezilla on your Computer It will be used to move the SAP HANA CLIENT into Hana ExpressClick on OK and you will see the file structure of your virtual boxLog in to the root accountWith that we are done installing SAP Hana Express and its ready to communicate with HortonworksThe next step of this tutorial will include installing hortonworks in the virtualbox configuring hive installing the necessary files needed for SAP Hana Express to communicate with HadoopStep 2: Follow this tutorial on hortonworks website to install hortonworks https://hortonworksI am using HDP 24 because of memory issues with my Windows OS HDP 26 uses more memory and takes a long time before the installation is done so using HDP 24 is okayIf you are using HDP24: Once you are done remote log in to the system like we did for SAP HanaFor my virtual-box network i am using host only adapters so my ipaddress is 19216856202 To remote login into hortonworks run ssh root@ipaddress The bolded should be the IP Address of your Hortonworks sandboxWe need to log into ambari but before we do that the password has to be reset Run the following command ambari-admin-password-reset and put in your new passwordGo back into SAP Hana Studio Remember the installation path that was created while installing the hdb client? Navigate into the directory cd /usr/sap/hdbclient make sure there is a file called ngdbcjar in the hdbclient directoryOpen filezilla and connect to your SAP sandbox Go into the hdbclient directory and download the ngdbc file on your local desktopOpen filezilla and connect it to the Hortonworks sandboxReturn into your hortonworks sandbox and go into the home directory then run the following command if you followed my instructions on moving the ngdbcjar file into hortonworks then the file will be in the home directory The jar file must be placed in sqoop library directoryA little bit of background of what sqoop is and what it does According to sqoop website  Apache Sqoop(TM) is a tool designed for efficiently transferring bulk data between Apache Hadoop and structured datastores such as relational databases Apache sqoop will be used to handle the movement of data from the SAP database into HadoopI like sqoop a lot easy to use and less hassle It doesnt take a lot of time to configure I havent tried using FLUME before and that may have to do with me always working on structured data Sqoop is used to ingest structured data from RDBMS or Enterprise data warehouses to HDFS while flume is used to move unstructured dataHopefully one of these days I will get a requirement that will require me to work with flume but for this tutorial We will be making use of SqoopBack to the topic at hand the ngdbc file has been placed in the right directory our next task will be running a sqoop command to import the filesGot exception running Sqoop: javalangRuntime Exception: Could not load db driver class: comsapdbjdbcDriver javalangRuntimeException: Could not load db driver class: comsapdbjdbcStep 3: This step includes moving the data from sap into hadoop and archiving it using the ORC formatTo import using sqoop there are different parameters that will be neededsqoop import  username SYSTEM  password ******   connect jdbc:sap://***********:39015/?currentschema=******  table RECIPES  driver comsapdbjdbcChange the password to your password and the currentschema from PRACTICE to your schema name table to your table nameIf you notice i am importing the table directly into hive I used hive because it allows for data to be queried through HQL(Hive Query Language) which provides an SQL type of feel that is familiar to anyone that has ever worked on any Relational Database Management SystemTo view the imported Data You can run select * from defaulttblNameNext we are going to archive this data Use the command belowCREATE TABLE RECIPES_ORC AS ORC AS SELECT * FROM RECIPES
3U5fxSkZDGKfG7BXMo9ZX4,The promise of Customer Data Platforms (CDPs) is to unify all the customer data into one platform They promise to enable interesting use cases from typical reporting to advanced user activation scenarios such as driving personalized UI experiences and giving out offers based on customers behaviorMost of the CDP products out there started as something else and morphed into what they are today There are good reasons for this Customer-related data is always important not only to understand the behavior of the customers but also to act upon it Also customer-related data tends to be big raw and full of problems and inconsistencies This data should be consumed by people without technical skills such as a growth manager These are some of the reasons that made the existence of CDPs viable in the past two to three yearsToday its possible to literally capture the entire journey of your customers from what products they searched to what they clicked or ignored to an eventual purchase Also its possible to enrich this journey with data coming from every customer touchpoint the enterprise has The touchpoints could be anything from customer support and management systems to marketing platforms and invoice management productsThis rich dataset can enable a lot more use cases beyond the traditional reporting for enterprises with limitless possibilities This is currently driving the development of a number of products that together form a complete data stack or infrastructure that enterprises can utilize to capture analyze and productize this data setThe cornerstone of this data stack is the data warehouse This technology has evolved so much in the past few years that it can now support almost infinite scale the separation between storage and processing advanced analytics functionalities and even machine learning Hence Cloud Data Warehouses (CDWs) are on a rage being one of the main enablers of data-driven enterprisesIts no surprise that one of the most valuable startups in Silicon Valley currently is Snowflake a CDW provider who just raised $500M at a $12B valuationAs CDW technology evolved technology today exists for building a custom scalable and efficient CDPThere are plenty of reasons but we would like to focus on the followingNo matter how mature an off-the-shelf vendor solution might be it will always be a generic one Customers are unique to each business even in the same market How to define a customer? What sources of data the business has? What data models are used? How should the customers be identified? These are just some of the issues that a CDP has to address Theres no better way to answer these questions than to have access to all the raw data a company has collected in a data warehouse or Data LakeAs businesses are evolving more people need to interact with customer data Everyone needs to probe the data This includes marketing managers who use BI solutions such as Looker as well as data scientists who use Jupiter notebooks and advanced SQL No one knows whos going to be using the data in the future but its almost certain that the list will just grow longer Solutions like a data stack built around CDW or Data Lake guarantee that the company has a flexible infrastructure that allows data access to everyoneWe are entering an era where more and more control will be put around data from different actors including the government Companies will have to comply and be ultra-protective of the data they manage Anything that is generated by the customers will be considered sensitive information Adding more and more products around the data stack where data has to be replicated across different tools makes privacy and data governance a tricky jobOne of the best ways to address this issue is to rely on well-defined and industry-accepted data architecture Here the data is stored in well-defined repositories such as a Data Lake and a data warehouse and strict policies around data access and security are implementedThus implementing your Customer Data Platform over your data warehouse will also contribute to the data privacy and governance of your companyWe always focus on new value creation but reducing costs is another important impact technology has Putting together an infrastructure to process this volume of data costs money Add to it the SaaS margins we are looking at hundreds of thousands of dollars of investment to buy and deploy third-party SaaS solutions Instead by building a Customer Data Platform on an existing data warehouse a company can reuse already-procured cloud infrastructureThe data infrastructure a company has is completely useless without data Hence the layer of the data stack that captures collects and delivers data is extremely important This layer is responsible for the following: The importance of this layer running on top of the data warehouse is paramount Thats what we are building at RudderStackInterested talk to us
hZHcXzdJ6QnBPNSFHAbUVx,I wrote this piece during my time working as an intern on SnapTravels data engineering team: Data has a large presence at SnapTravel and is consumed by stakeholders both internally and externally For example internally our growth team uses data to make clever marketing decisions to help the business grow across various channels and our product/design teams use experiments to decide on features Externally data is provided to partners for reporting purposesDue to the critical use-cases for the data and its role in fueling our growth we had a need for data infrastructure stability and accountability for incidents We needed to build a sustainable service level-framework that satisfied the following requirements: Before delving into our implementation lets review some terminology related to tracking service levels: Service Level Agreement (SLA)  A contract between two parties in our case an agreement between the data engineering team and other teams within the company who rely on usService Level Indicator (SLI)  A metric that tracks the performance of the indicator In our case the indicator is the number of times that the pipeline ran successfully within the specified time windowSome definitions are here https://awsamazonIn order to implement our solution we used the following tools that were part of our existing data engineering setup: Airflow- A Python-based workflow orchestration framework that is based on Direct Acyclic Graphs (DAGs) All our pipelines are managed through Airflow and we have a DAG corresponding to each pipeline You can read more about how we use it hereGoogle Sheets- We use sheets to manage service level requirements with business usersHealthchecksio- A lightweight cron-job monitoring service that waits for heartbeat pingsZapier- A user-friendly automation tool well use for automating JIRA ticket creationJIRA- Issue tracking software we use to manage our projects at SnapTravelBI Tool  Data visualization platform that can connect to databases We use Periscope at SnapTravelAlthough these are the tools used in our implementation any of them can easily be swapped out with other tools depending on your own setupAt SnapTravel we measure our service levels based on a single indicator the on time percentage It is calculated by: successful DAG runs/total DAG runs where a successful DAG run is defined as follows: The main idea will be to augment our existing DAGs to track our SLIs This way there will be minimal changes to our existing infrastructure and the SLA tasks will only run once the regular pipeline has succeededWe broke down implementation into three phases: DataSanityOperator allows us to perform an end-to-end check after the pipeline runs This works by making a copy of the source file in S3 and then comparing the destination table against the source once the pipeline has successfully run For example for advertising data you may want to check that the total number of clicks is the same in the source data and destination tableAs shown in the code above the push step happens very early in the pipeline and the data sanity assertion is run at the end Notice that we do not check for business logic such as absolute or variance thresholds as these values may change and thus are monitored through alerting on PeriscopePipelineSuccessOperator is responsible for pinging healthchecksio to signal that there has been a successful run for a pipeline On execution it checks the Configuration Google Sheet to get a URL and pings itThe sheet should have the following columns: ReportOntimeOperator records whether or not the pipeline completed by the deadline specified We worked with our business stakeholders on setting deadlines on each of our pipelines and stored them in our Configuration Google Sheet This only succeeds if the pipeline completed within the agreed time It runs parallel to the PipelineSuccessOperator and therefore will only run if the rest of the pipeline has succeededOnce our new Airflow operators were created we simply had to append them to the end of the pipelines we wanted to trackIn addition to the additional Airflow functionality we also used Healthchecksio as a third-party monitor on our data pipelines As described earlier the PipelineSuccessOperator pings this service upon a successful pipeline runThe flowchart above outlines what happens when the Healthchecksio service detects a pipeline is down Here are some more details about each stepThis setup allows us to efficiently communicate the status of pipeline failures to end-usersOnce we created the framework it was time to measure our service levels and report them to users We used the number of times the ReportOntimeOperator succeeded as the service level indicator This can easily be pulled from the task_instance table in the Airflow databaseWe connected our Airflow database to Periscope our BI platform and produced a dashboard showing the service level indicators per pipeline
ga9AJVqgCpiUHP3xkziwK3,Google Cloud Platform (GCP) offers a wide range of tools and services for robust and scalable Data Engineering tasks Data problems  such as  getting data from source location(s) or storage repository to sink/destination would ideally be solved leveraging Cloud Dataflow Google Cloud Composer (Airflow) or both In this article I have used Cloud DataflowDataflow is GCPs fully managed service for executing Apache Beam pipelines Depending on the complexity of your project you could create a solution by either using Dataflow Templates (made available in GCP) for straight-forward problems or code your solution all the way  in Java or PythonThis article explains how to load csv files in Google Cloud Storage (GCS) into Google BigQuery with the use of Cloud Dataflow I have used Python as the coding language but you can reproduce the same steps using Java as wellEven though expert knowledge of the entire architecture and products of GCP is not needed a decent knowledge of how the Cloud works especially on GCP is necessaryRunning Python Dataflow/Airflow and other packages together may require some environment configuration package installation and ensuring compatibility of package versions In order to avoid installation setup troubles it is best that we work in a virtual environment • Activate the Cloud Shell on the top-right hand corner of the Cloud ConsoleWhen the Cloud Shell launches you will see a welcome message stating your current project If the selected project is not the one you want to work with use the command in the welcome screen to change it to the project we just created • Assign a PROJECT variable (you can replace stocks-project-2 with your project ID) • Print the PROJECT variable to be sure that it works • On the top-right corner of the Cloud Shell click on the Launch Editor button to see the Cloud Shell and Code Editor in another window • When the Editor opens create a new directory where we would store our pipeline programs and navigate into it Use these commands: 9 Once in the project directory create a python file: 10 Use the Cloud Shell Editor (see further below) to Examine the python code for the pipeline Feel free to copy and paste this code or make personalized adjustmentsBefore we continue we need to create a Service Account  which is like a user profile upon which we have granted necessary privileges just so it could access certain GCP services •  Grant the Service Account the following roles as shown Please make sure you assign the least required privileges/roles and in accordance with your organizations security policy •  Download the key as a json file •  Upload the key (json) file into stocks-project folder by right-clicking on the project folder in the Editor and clicking on Upload Files Once uploaded you will see the json file in the list files in the left pane of the Editor Take note of the filename because we are going to reference it in out python pipeline application Below it the snapshot of the python file  please read the comment blocks to understand the code: 14 Download the stocks dataset from this kaggle link to your computer Extract the zipped content •  Setup the data source: Since the input data would be coming from a Cloud Storage Bucket then we would need to create one For convenience of reference I have named it exactly as the value of my Project ID •  From the downloaded stocks dataset upload the entire content in the folder fh_20190420/full_history into the created GCS bucket above •  Setup the data destination: We are using BigQuery to store the data so we need to create a BigQuery Dataset name stocks_data We would specify the storage table in the pipeline (python file) •  Setup the virtual environment: In order not to worry about installing specific and latest versions of the Operating System or required software needed to run the pipeline we would be using a virtual environment Run the following commands to install it: 19 In the Editors Terminal window navigate to (or ensure you are in) the project directory Then run these commands to create a new virtual environment activate it and install Apache Beam for GCP: 20 Run the python application: The below command creates a dataflow job which runs using the DataflowRunner The pipeline requires a staging location and temp location which are storage buckets (will be automatically created if they do not already exist) which store processing and temporary files respectively while the pipeline is still running The input variable is the cloud storage bucket/file location of the csv file(s)Did you notice the    input part? I mean this line  input gs://stocks-project-2/full_history/ZY*csv \\ I have used wildcard ZY*csv to load ONLY all the files whose filename starts with ZY followed by any other set of characters or none You could as well load a single csv file or all as shown: Side Note: I would advise that whenever you are creating any GCP Resource such as Storage Buckets BigQuery Datasets Compute or Kubernetes resources endeavor to have them collocating in the same geo-location and at best same region for the sake of latency and accessibility  except of course if its a strategic design requirement  •  To see the execution results return to the Cloud Console Home and navigate to the Cloud Dataflow page The last run will be seen at the very top of the page •  A full green check box shows that the job was run successfully Click on the name beside it to show the job completion details •  Final result in BigQuery TableThis simple data pipeline would do the basic data engineering/ETL tasks However you could improve this project by doing one or more of these: If you enjoyed reading this post or if it was useful in any way please drop some claps and comments I would like to have your feedback and encouragement to write more on Data Engineering Also share this post on LinkedIn and twitterFeel free to engage and follow me on twitter on this and future posts
QaK4y58pY2FtR8cgFARgWN,When it comes to Big Data infrastructure on Google Cloud Platform the most popular choices Data architects need to consider today are Google BigQuery  A serverless highly scalable and cost-effective cloud data warehouse Apache Beam based Cloud Dataflow and Dataproc  a fully managed cloud service for running Apache Spark and Apache Hadoop clusters in a simpler more cost-efficient wayThis variety also presents challenges to architects and engineers looking at moving to Google Cloud Platform in selecting the best technology stack based on their requirements and to process large volumes of data in a cost effective yet reliable mannerIn the following sections we look at research we had undertaken to provide interactive business intelligence reports and visualizations for thousands of end users Furthermore as these users can concurrently generate a variety of such interactive reports we need to design a system that can analyze billions of data points in real timeFor technology evaluation purposes we narrowed down to following requirements   • Raw data set of 175TB size : This dataset is quite diverse with scores of tables and columns consisting of metrics and dimensions derived from multiple sources2 Catering to 30000 unique users3The problem statement due to the size of the base dataset and requirement for a high real time querying paradigm requires a solution in the Big Data domainThe solution took into consideration following 3 main characteristics of desired system:1 Analyzing and classifying expected user queries and their frequency2 Developing various pre-aggregations and projections to reduce data churn while serving various classes of user queries3 Developing state of the art Query Rewrite Algorithm to serve the user queries using a combination of aggregated datasets This will allow the Query Engine to serve maximum user queries with minimum number of aggregationsFor benchmarking performance and the resulting cost implications following technology stack options using managed big data technologies available in Google Cloud Platform were considered: 1 Cloud DataProc + Google Cloud StorageFor Distributed processing  Apache Spark on Cloud DataProcFor Distributed Storage  Apache Parquet File format stored in Google Cloud Storage • Cloud DataProc + Google BigQuery using Storage APIFor Distributed processing  Apache Spark on Cloud DataProcFor Distributed Storage  BigQuery Native Storage (Capacitor File Format over Colossus Storage) accessible through BigQuery Storage API • Native Google BigQuery for both Storage and processing  On Demand QueriesUsing BigQuery Native Storage (Capacitor File Format over Colossus Storage) and execution on BigQuery Native MPP (Dremel Query Engine)All the queries were run in on demand fashion Project will be billed on the total amount of data processed by user queries • Native Google BigQuery with fixed price modelUsing BigQuery Native Storage (Capacitor File Format over Colossus Storage) and execution on BigQuery Native MPP (Dremel Query Engine)Slots reservations were made and slots assignments were done to dedicated GCP projects All the queries and their processing will be done on the fixed number of BigQuery Slots assigned to the projectAfter analyzing the dataset and expected query patterns a data schema was modeled Dataset was segregated into various tables based on various facets Several layers of aggregation tables were planned to speed up the user queriesAll the user data was partitioned in time series fashion and loaded into respective fact tables Furthermore various aggregation tables were created on top of these tables All the metrics in these aggregation tables were grouped by frequently queried dimensionsIn the next layer on top of this base dataset various aggregation tables were added where the metrics data was rolled up on a per day basisAll the probable user queries were divided into 5 categories  1 Queries running over raw data and lifting over 3 months of data2 Queries running over aggregated data and lifting over 3 months of data3 Queries running over 7 days of aggregated data4 Queries running over 15 days of aggregated data5 Queries running over 1 month of raw dataThe total data processed by individual query depends upon time window being queried and granularity of the tables being hitLongevity Tests  BigQuery Native REST APIOnce it was established that BigQuery Native outperformed other tech stack options in all aspects We also ran extensive longevity tests to evaluate response time consistency of data queries on BigQuery Native REST APIIt is evident from the above graph that over long periods of running the queries the query response time remains consistent and the system performance and responsiveness doesnt degrade over timeTo evaluate the ETL performance and infer various metrics with respect to execution of ETL jobs we ran several types of jobs at varied concurrencyIn BigQuery similar to interactive queries the ETL jobs running in batch mode were very performant and finished within expected time windows This should allow all the ETL jobs to load hourly data into user facing tables and complete in a timely fashionRunning the ETL jobs in batch mode has another benefit All jobs running in batch mode do not count against the maximum number of allowed concurrent BigQuery jobs per projectHere we capture the comparison undertaken to evaluate the cost viability of the identified technology stacksActual Data Size used in exploration:Two Months billable dataset size in BigQuery: 5973 TBTwo Months billable dataset size of Parquet stored in Google Cloud Storage: 35 TB Parquet file format follows columnar storage resulting in great compression reducing the overall storage costsIn BigQuery  even though on disk data is stored in Capacitor a columnar file format storage pricing is based on the amount of data stored in your tables when it is uncompressed Hence Data Storage size in BigQuery is ~17x higher than that in Spark on GCS in parquet format • For both small and large datasets user queries performance on BigQuery Native platform was significantly better than that on Spark DataProc cluster2 Query cost for both On Demand queries with BigQuery and Spark based queries on Cloud DataProc is substantially high3 Using BigQuery with Flat-rate priced model resulted in sufficient cost reduction with minimal performance degradationAbout the AuthorsPrateek Srivastava is Technical Lead at Sigmoid with expertise in Bigdata Streaming Cloud and Service Oriented architectureFor more about Sigmoids data Engineering Services visit -https://wwwsigmoid
XcWDPwJRXZD9DTWVn933Hd,Financial institutions globally deal with massive data volumes that calls for large scale data warehousing and effective processing of real-time transactionsLet us begin with the exploration of a use case: A Real-time transaction monitoring service for an online financial firm that deals with products such as Pay Later and Personal LoanAnd the data lake should have a single record for each transaction and it should be the latest stateA three steps process can be: 2 Merge all the new files and the historical data to come up with the new dataset at a regular interval may be once in every 3 hrs and the same can be consumed in the downstream through any of the querying systems like Presto AWS Athena Google BigQuery etc • Create a Presto or Athena table to make this data available for queryingAdding this additional where condition adds extra latency to each of the queries and it would soon become an extra overhead when the data reaches petabytes scaleDelta Lake by Databricks addresses the above issues when used along with Apache Spark for not just Structured Streaming but also for use with DataFrame (batch-based application)Enterprises have been spending millions of dollars getting data into data lakes with Apache Spark The aspiration is to do Machine Learning on all that data  Recommendation Engines Forecasting Fraud/Risk Detection IoT & Predictive Maintenance Genomics DNA Sequencing and more But majority of the projects fail to see fruition due to unreliable data and data that is not ready for ML~60% of big data projects are failing each year  GartnerThese include data reliability challenges with data lakes such as: Thats where Delta Lake comes inThe table below indicates how solutions with Data Lakes & Delta lakes compare with each other on different parameters and highlights the advantages that Delta Lakes have to offerSidhartha Ray is Technical Lead at Sigmoid with expertise in Big Data  Apache Spark Structured Streaming Kafka AWS Cloud and Service-less architecture He is passionate about designing and developing large scale cloud-based data pipelines and learning & evaluating new technologies
9XzjgsSkAjvnhmpuCdnSbk,A few months back I found myself in a situation Im sure someone at your company has been in before We had some goal data in Google Sheets that we wanted to bring into Redshift to join up to our sales data and trend against This is small data like maybe a few dozen rows Small as it is it was important and we needed to join it with the rest of our dataIt would have been easy to populate a table with those goals directly in Redshift but then our CFO would have had to manually update it in SQL which would have been a much harder task So I spent a few hours coding up a Google Sheets integration for Stitch that makes loading to Redshift trivial and Ill explain how it works belowFeel free to skip ahead to the repoI knew that there were a few key things that this script needed to do: The easiest way for a general user to setup and use the script is to add a dropdown to the menu Google Sheets makes this easy with the addMenu functionI added a menu that has a two options that execute functions: The onOpen function makes sure the menu dropdown loads every time the spreadsheet loads and it contains the options for the other functionsThe onInstall function will have prompts to set up the spreadsheet and current sheet for syncing The Stitch API requires an API token and the Client ID of the Stitch account You will also need to define the primary key(s) for the sheet so Stitch can determine what makes each row unique Users can just click cancel on the API token and CID fields if they are already added for additional sheetsYoull notice in the below code that I use the normalizeHeaders function which is defined later on but basically makes sure that the sheet keys are formatted for the API and RedshiftIn Google Scripts you can define specific ranges of cells based on coordinates of the sheet Below you can see how we can get all of the data for a sheet into a JavaScript object I took the below code from a section of Google Scripts documentation about sheets-to-JSON that seems to no longer be available I edited it a bit to account for spaces in headers and to make sure the headers are alphabeticalNext I need to have the script talk to the Stitch API This involves three main parts: We pretty much already have points 2 and 3 covered by capturing the details in our first section Below youll see what the ideal JSON package looks like to Stitch while the headers include the authentication For more information on this check out the Stitch API docsBut before we can push all of the data we need to account for the 10k record limit per request that the Stitch API enforces While most spreadsheets Ive worked with are not larger than that limit you never really know The push function includes a simple conditional statement that takes care of the initial breaking apart of large docs vs small docs while getting the relevant information staged for the other functionsIm only going to focus on the smalldoc section because most of the code is the same and it doesnt need to be repeatedThere is one function that handles the push and one that handles the formatting of the record for the push both below It makes one call out to transform the JavaScript object to Transit format which is like JSON enriched with data type encoding The Stitch API can accept Transit and will use the encoding to have data show up in your data destination with encoding in tactAt the top of the script there is a large minified library which is the JavaScript Transit library We use a simple function to transform the data to TransitSo there is one part I havent explained the sequence_id and why it is so important Basically you could theoretically push two versions of the same record within milliseconds of each other Stitch which is a distributed system in AWS has to make sure that your latest update is the one of record Ive laid out a flow chart of why this matters below and will probably write a longer post about why this is very important to get right in the near futureThe easiest way to handle this is to assign a Unix timestamp as the sequence id which is shown in the insertKeys function above Future syncs will have a higher number which Stitch will account forThe full script which accounts for sheets larger than 10k records can be found here and please let me know if youd like any clarification on anything above Though not officially supported by Stitch Im happy to help out Please open an issue on Github if you have any problems using this* Additional records can be nested within top level records Learn more about de-nesting in our docs
ZVvudacB9qoVYadhpK79ji,Data visualization have come a fairly long way from its early onset where Excel was the only tool that could derive conclusions by performing mathematical operations of a set of compatible data Although excel was good enough to help a small-scale business drive entry-level data-driven conclusion its efficiency and use cases substantially decreased when the amount of raw data increased Modern enterprises need better-suited business solutions to make the best use of their data and make a good market decision In such a scenario custom application development services can save the day for an enterprise and help them benefit from a business application which is specifically made to enhance their business type and processData visualization is a modern age enterprise application which helps segregate and filter data based on its use case By employing data visualization businesses can easily understand the hidden patterns and insights from a bulk of raw data and use it to derive better business decisions and explore newer market opportunities and trendsData visualization essentially makes a graphical representation of data which better understandable even for people from non-tech background Decision makers can use this technique to derive better conclusions regarding a prior campaign or sales figures and make a timely decision which can help them improve their efficiency and ROI By taking up effective data visualization services market leaders can analyze the ongoing market trends and patterns and invest on it to get increased market revenuesIt is specifically very important for a data analyst who has to understand and evaluate current market trends and suitability and come up with a plan of action to make better profitable deals for his employerVisual data is far more conceivable and easier to understand than compared to the bulk business and process data which is generated every second within a working enterprise With businesses today going to major extents to harness and make use of their potential employing data visualization comes across as the most appropriate tool for analysts and key decision-makers to incur actionable insights from huge chunk of business data It is easier for decision-makers to derive conclusion when data is represented in the form of graphs or visual chartsIt is a fast and easily accessible business technique which can be used to understand customer behavior identify patterns forecast sales and make the best decision according to currently available metrics
H8QwiysYpmzf7nv6s2QQxi,Data Engineer Jitse-Jan van Waterschoot on unlocking the data warehouseIt is often the case with start-ups that the transitioning framework is built first around an enhanced functionality of the product to engage with the customers and at a later stage around a more consistent structure of the system as a whole The story was no different in the early days of MarketInvoice Features had to be added rapidly to accommodate the great customer adoption and there were no manuals illustrating a viable architecture or even predictable working timelines for the platform of a fast-growing companyOver time with the increasing participation of new customers it was evident that the existing structure needed to be more scalable so that reaching the prospected growth was sustainable Our recent partnership with Barclays for example means that the expectations and demands on the platform are set to grow at full speed According to our internal projections by the end of 2019 the traffic on the platform could easily be incremented by 10X While this is a remarkable opportunity it would not have been possible if we hadnt been prepared to create new paradigms to promptly categorise the data by cleaning it and organising it for our business intelligence (BI) and machine learning modelsThe goal was to increase the automation process so that we can give our customers faster responses reducing the waiting time that occurs while making an application and receiving the funds Decreasing the number of manual steps in the application process also facilitates the work for our risk underwriters Tracking additional data appears to be an astute evaluation since it will ensure the creation of new consistent decision-making models intended at automating some of the tasks that the underwriters are currently spending the majority of their time onIn my previous job I was working as a data engineer in the tech field for a hardware manufacturer They were building out their data platform and asked me to help design their data-flows via different pipelines Their data source was relatively simple as they had five different types of machines only generating logging data containing details from testing and producing The main difficulty I faced while working on this specific project was the amount of data and the speed to calculate the business KPIs During my assignment I received data from 2500 machines every 2 hours containing roughly 50 MB per file thus in total 15 TB per day Calculating the statistics for this data took over 24 hours This was reduced to 10 minutes through platform optimisations (and scaling up the computing power) The data arrived from various locations around the world in a shared folder where it was fed into an aggregation script A parsing job reads this data and aggregates it into a data warehouse to be subsequently used by the BI teams While I was dealing with large volumes of information the complexity of the task was simple because the data was homogeneousWhen I joined MarketInvoice it was far from a clean and structured platform Two cloud providers were being used for different segments of the company The platform for our customers was built on Microsoft Azure Platform as a Service (PAAS) and contained different database servers with a large number of tables Part of the data was pushed to Amazon AWS to make it accessible in the BI tool Looker Additionally data from third-party sources such as Google Analytics and Salesforce were pushed into AWS with the support of third-party synchronisation tools Machine learning was performed by combining the data from AWS and Azure and was hosted on Azure The data in AWS resided in one colossal database with tables from diverse sources plus aggregated tables based on the raw dataWhile that approach seemed effective and the important data was effortlessly available in the necessary locations it didnt show an actual prospect of being scalable There were challenges while performing tasks requiring heavy computing or while adding new datasets from external sources Virtual machines were constantly running to extract data but were idle the majority of the time Data was synced at fixed intervals making it impossible for the platform to do live predictionsbased on the most recent dataMy challenge when I started at MarketInvoice was to find an optimal solution for the scalability of the data platform Fundamentally to scale the system we needed a novel data architectureThe data platform can be characterised by the following requirements: • Gather additional data in a centralised place to analyse leads traffic etcAfter the requirements of the new data platform were well-defined it was necessary to investigate the effectiveness of as many approaches as possible to really make sure that only the most suitable solution was taken into considerationWe had several conversations with various third-parties like Microsoft and AWS to discuss their tools and data warehouse common practises more specifically the ones dealing with financial data In the meanwhile as a data team we aimed at building different proof of concepts on Azure and AWS (and the potential mixture of the two) to test data ingestion data migration and implementation of machine learning modelsIn reality what we were truly trying to achieve was the creation of a so-called platform agnosticsolution which could run on any cloud provider with minimal changes ie running a Python extraction script on AWS is easy using AWS Lambda but we can simply copy that same script and run it on Azure In the future prospect of possibly moving to another cloud provider the entire migration process should be uncomplicated and just a matter of placing the correct scripts in the right place to get the same data pipelines workingIn the new data platform we use both AWS S3 and Azure Blob file storage as a data lake to save all the raw data from the diverse sources By using this redundancy we can smoothly switch between the platforms and an additional backup is readily available in case one of the two providers develops issues We store the history of the tables on both platforms and we can gather historical statistics Data is kept in sync between the platform using a data-driven approach; when there is new data in the database it will trigger a function to push that data into file storage on Azure which will in turn be copied to AWS using Azure Functions By using triggers based on the data we do not need to have machines running 24/7 but only pay for the execution time of the triggered functions Data from the external sources is retrieved using AWS Lambda consisting of minor running jobs that will pull data from the APIs and push the data into the data lake These jobs do not necessitate running machines as they are simply scheduled to retrieve data at a set intervalFinally the platform is set up to listen to events in the data lake so when new data is added to a certain folder it can trigger an ETL (Extract Transform Load) job to put the new data in the right projections for the data warehouse Similarly the data warehouse is using AWS S3 which means the projections are saved in a readable format users of the platform can inspect the data easily and querying it in a BI tool is straightforward We deemed it unnecessary to push the data into a database since that will limit the usability of the data and significantly increases the cost of the platform Were using the data analytics tool Looker which in fact integrates well with S3 using the AWS Glue Data Catalog to index the data in the data warehouse Furthermore the machine learning tools and parallel computing platform of AWS work even better when using data saved on S3 instead of a databaseAs the platform scales up this approach will give us the ability to easily handle large volumes of data without affecting the platform performance or generating huge bills at the end of the monthWhile the new data platform has contributed towards making us less dependent on any PAAS provider our user platform is still the Goliath of the story At the time of writing this the Tech team is busy modularising the platform and changing the entire underlying structureIdeally the data that was originally written to a database from the platform will follow a different route Instead of writing data directly to the database it will send the data event to a message bus making it possible to have different clients listening to that bus and triggering a job to respond to that event In the data-driven approach we respond to a row added in the table (Figure 4) Whereas for our event-driven approach we do not want to check the table but write the event data straight to the data lake and thereby avoiding the necessity of checking databases (Figure 5) While it does not influence the architecture of the data platform it will help to respond faster to events on the user platform and therefore increase the reliability of the data platform ensuring that the last data is always present instead of waiting for the daily extraction job to get data from the databases in the data lakesWe made a cost-effective solution by using the best of both platforms without depending on any specific vendor Following the new approach we created modular jobs to get data from the raw sources into the data warehouse using both Azure Functions and AWS Lambda Data is stored in files on Azure Blob Storage and AWS S3 making it the perfect source for machine learning and business intelligenceIf youd like to chat about tech at MarketInvoice get in touch with me at jwaterschoot@marketinvoicecomOriginally published at blogmarketinvoicecom on October 2 2018
XPcnuY3djLxRnMZ4RfCEgE,The Open Data Science Conference brings together a variety of data professionals each year in Boston This weeks episode consists of a pair of brief interviews conducted on-site at the conference First up youll hear from Andy Eschbacher of Carto He dscribes some of the complexities inherent to working with geospatial data how they are handling it and some of the interesting use cases that they enable for their customers Next is Todd Blaschka COO of TigerGraph He explains how graph databases differ from relational engines where graph algorithms are useful and how TigerGraph is built to alow for fast and scalable operation
FvHF2riSeiNevqS57RTvQR,One of the longest running and most popular open source database projects is PostgreSQL Because of its extensibility and a community focus on stability it has stayed relevant as the ecosystem of development environments and data requirements have changed and evolved over its lifetime It is difficult to capture any single facet of this database in a single conversation let alone the entire surface area but in this episode Jonathan Katz does an admirable job of it He explains how Postgres started and how it has grown over the years highlights the fundamental features that make it such a popular choice for application developers and the ongoing efforts to add the complex features needed by the demanding workloads of todays data layer To cap it off he reviews some of the exciting features that the community is working on building into future releases
fcdcYTxyTzCaqJqS7qNf6q,Most businesses end up with data in a myriad of places with varying levels of structure This makes it difficult to gain insights from across departments projects or people Presto is a distributed SQL engine that allows you to tie all of your information together without having to first aggregate it all into a data warehouse Kamil Bajda-Pawlikowski co-founded Starburst Data to provide support and tooling for Presto as well as contributing advanced features back to the project In this episode he describes how Presto is architected how you can use it for your analytics and the work that he is doing at Starburst Data
CBigS75zDfZ4ASAtxry87A,As your data needs scale across an organization the need for a carefully considered approach to collection storage organization and access becomes increasingly critical In this episode Todd Walter shares his considerable experience in data curation to clarify the many aspects that are necessary for a successful platform for your business Using the metaphor of a museum curator carefully managing the precious resources on display and in the vaults he discusses the various layers of an enterprise data strategy This includes modeling the lifecycle of your information as a pipeline from the raw messy loosely structured records in your data lake through a series of transformations and ultimately to your data warehouse He also explains which layers are useful for the different members of the business and which pitfalls to look out for along the path to a mature and flexible data platform
3U2ZhBvP6fcehwgdnusJBj,The Hadoop platform is purpose built for processing large slow moving data in long-running batch jobs As the ecosystem around it has grown so has the need for fast data analytics on fast moving data To fill this need the Kudu project was created with a column oriented table format that was tuned for high volumes of writes and rapid query execution across those tables For a perfect pairing they made it easy to connect to the Impala SQL engine In this episode Brock Noland and Jordan Birdsell from PhData explain how Kudu is architected how it compares to other storage systems in the Hadoop orbit and how to start integrating it into you analytics pipeline
C4YYvo9wBmHxxc7gMS3dS2,A data lake can be a highly valuable resource as long as it is well built and well managed Unfortunately that can be a complex and time-consuming effort requiring specialized knowledge and diverting resources from your primary business In this episode Yoni Iny CTO of Upsolver discusses the various components that are necessary for a successful data lake project how the Upsolver platform is architected and how modern data lakes can benefit your organization
eEwDPrqWKyhSHvqYBrER9h,Managing an analytics project can be difficult due to the number of systems involved and the need to ensure that new information can be delivered quickly and reliably That challenge can be met by adopting practices and principles from lean manufacturing and agile software development and the cross-functional collaboration feedback loops and focus on automation in the DevOps movement In this episode Christopher Bergh discusses ways that you can start adding reliability and speed to your workflow to deliver results with confidence and consistency
MLHfrT9R2FZxk87wowGyj3,Distributed systems are complex to build and operate and there are certain primitives that are common to a majority of them Rather then re-implement the same capabilities every time many projects build on top of Apache Zookeeper In this episode Patrick Hunt explains how the Apache Zookeeper project was started how it functions and how it is used as a building block for other distributed systems He also explains the operational considerations for running your own cluster how it compares to more recent entrants such as Consul and EtcD and what is in store for the future
ZLAHH3XA4EnCHDmEpVdqgF,When your data lives in multiple locations belonging to at least as many applications it is exceedingly difficult to ask complex questions of it The default way to manage this situation is by crafting pipelines that will extract the data from source systems and load it into a data lake or data warehouse In order to make this situation more manageable and allow everyone in the business to gain value from the data the folks at Dremio built a self service data platform In this episode Tomer Shiran CEO and co-founder of Dremio explains how it fits into the modern data landscape how it works under the hood and how you can start using it today to make your life easier
4xsJTTEVsFEnoZoSVvy26D,The past year has been an active one for the timeseries market New products have been launched more businesses have moved to streaming analytics and the team at Timescale has been keeping busy In this episode the TimescaleDB CEO Ajay Kulkarni and CTO Michael Freedman stop by to talk about their 10 release how the use cases for timeseries data have proliferated and how they are continuing to simplify the task of processing your time oriented events
CnttYBMQeQNpgKBosCQhGu,With the increased ease of gaining access to servers in data centers across the world has come the need for supporting globally distributed data storage With the first wave of cloud era databases the ability to replicate information geographically came at the expense of transactions and familiar query languages To address these shortcomings the engineers at Cockroach Labs have built a globally distributed SQL database with full ACID semantics in Cockroach DB In this episode Peter Mattis the co-founder and VP of Engineering at Cockroach Labs describes the architecture that underlies the database the challenges they have faced along the way and the ways that you can use it in your own environments today
cD3dgVmTPCo3tXADeH5Xha,With the attention being paid to the systems that power large volumes of high velocity data it is easy to forget about the value of data collection at human scales Ona is a company that is building technologies to support mobile data collection analysis of the aggregated information and user-friendly presentations In this episode CTO Peter Lubell-Doughtie describes the architecture of the platform the types of environments and use cases where it is being employed and the value of small data
bfNVSC33NQdFQVkCmkH5XV,Data is an increasingly sought after raw material for business in the modern economy One of the factors driving this trend is the increase in applications for machine learning and AI which require large quantities of information to work from As the demand for data becomes more widespread the market for providing it will begin transform the ways that information is collected and shared among and between organizations With his experience as a chair for the OReilly AI conference and an investor for data driven businesses Roger Chen is well versed in the challenges and solutions being facing us In this episode he shares his perspective on the ways that businesses can work together to create shared data resources that will allow them to reduce the redundancy of their foundational data and improve their overall effectiveness in collecting useful training sets for their particular products
bi5sudu3DpaBFffxumrqoD,The way that you store your data can have a huge impact on the ways that it can be practically used For a substantial number of use cases the optimal format for storing and querying that information is as a graph however databases architected around that use case have historically been difficult to use at scale or for serving fast distributed queries In this episode Manish Jain explains how DGraph is overcoming those limitations how the project got started and how you can start using it today He also discusses the various cases where a graph storage layer is beneficial and when you would be better off using something else In addition he talks about the challenges of building a distributed consistent database and the tradeoffs that were made to make DGraph a reality
ZRRhBGdWfmWRVYm43NRAQC,Business intelligence is a necessity for any organization that wants to be able to make informed decisions based on the data that they collect Unfortunately it is common for different portions of the business to build their reports with different assumptions leading to conflicting views and poor choices Looker is a modern tool for building and sharing reports that makes it easy to get everyone on the same page In this episode Daniel Mintz explains how the product is architected the features that make it easy for any business user to access and explore their reports and how you can use it for your organization today
brPQX47YZipSwUeenHdnAE,One of the sources of data that often gets overlooked is the systems that we use to run our businesses This data is not used to directly provide value to customers or understand the functioning of the business but it is still a critical component of a successful system Sam Stokes is an engineer at Honeycomb where he helps to build a platform that is able to capture all of the events and context that occur in our production environments and use them to answer all of your questions about what is happening in your system right now In this episode he discusses the challenges inherent in capturing and analyzing event data the tools that his team is using to make it possible and how this type of knowledge can be used to improve your critical infrastructure
4mUx3XMUUofcj4d7HZSDAD,As software lifecycles move faster the database needs to be able to keep up Practices such as version controlled migration scripts and iterative schema evolution provide the necessary mechanisms to ensure that your data layer is as agile as your application Pramod Sadalage saw the need for these capabilities during the early days of the introduction of modern development practices and co-authored a book to codify a large number of patterns to aid practitioners and in this episode he reflects on the current state of affairs and how things have changed over the past 12 years
Pcaen5m4qDbugpRXNw6eE6,Search is a common requirement for applications of all varieties Elasticsearch was built to make it easy to include search functionality in projects built in any language From that foundation the rest of the Elastic Stack has been built expanding to many more use cases in the proces In this episode Philipp Krenn describes the various pieces of the stack how they fit together and how you can use them in your infrastructure to store search and analyze your data
Ljo769TyY99xpvpcurK6WB,The data that is used in financial markets is time oriented and multidimensional which makes it difficult to manage in either relational or timeseries databases To make this information more manageable the team at Alapaca built a new data store specifically for retrieving and analyzing data generated by trading markets In this episode Hitoshi Harada the CTO of Alapaca and Christopher Ryan their lead software engineer explain their motivation for building MarketStore how it operates and how it has helped to simplify their development workflows
KUgqP4EPWE2AFyBvF5t27q,One of the critical components for modern data infrastructure is a scalable and reliable messaging system Publish-subscribe systems have been popular for many years and recently stream oriented systems such as Kafka have been rising in prominence This week Rajan Dhabalia and Matteo Merli discuss the work they have done on Pulsar which supports both options in addition to being globally scalable and fast They explain how Pulsar is architected how to scale it and how it fits into your existing infrastructure
Pc74ARxyrugse2drUfqDjd,One of the most complex aspects of managing data for analytical workloads is moving it from a transactional database into the data warehouse What if you didnt have to do that at all? MemSQL is a distributed database built to support concurrent use by transactional application oriented and analytical high volume workloads on the same hardware In this episode the CEO of MemSQL describes how the company and database got started how it is architected for scale and speed and how it is being used in production This was a deep dive on how to build a successful company around a powerful platform and how that platform simplifies operations for enterprise grade data management
SC3yUEQyU3DgRJsaw32Wwo,Using a multi-model database in your applications can greatly reduce the amount of infrastructure and complexity required ArangoDB is a storage engine that supports documents dey/value and graph data formats as well as being fast and scalable In this episode Jan Steeman and Jan Stücke explain where Arango fits in the crowded database market how it works under the hood and how you can start working with it today
9rCU8aSpbEYREexiAq8Hb7,Web and mobile analytics are an important part of any business and difficult to get right The most frustrating part is when you realize that you havent been tracking a key interaction having to write custom logic to add that event and then waiting to collect data Heap is a platform that automatically tracks every event so that you can retroactively decide which actions are important to your business and easily build reports with or without SQL In this episode Dan Robinson CTO of Heap describes how they have architected their data infrastructure how they build their tracking agents and the data virtualization layer that enables users to define their own labels
iuFVKPsFu5k8iBDr87nByG,With the growth of the Hadoop ecosystem came a proliferation of implementations for the Hive table format Unfortunately with no formal specification each project works slightly different which increases the difficulty of integration across systems The Hive format is also built with the assumptions of a local filesystem which results in painful edge cases when leveraging cloud object storage for a data lake In this episode Ryan Blue explains how his work on the Iceberg table format specification and reference implementation has allowed Netflix to improve the performance and simplify operations for their S3 data lake This is a highly detailed and technical exploration of how a well-engineered metadata layer can improve the speed accuracy and utility of large scale multi-tenant cloud-native data platforms
EYToJuGtuBarxnuNWxgB52,The rate of change in the data engineering industry is alternately exciting and exhausting Joe Crobak found his way into the work of data management by accident as so many of us do After being engrossed with researching the details of distributed systems and big data management for his work he began sharing his findings with friends This led to his creation of the Hadoop Weekly newsletter which he recently rebranded as the Data Engineering Weekly newsletter In this episode he discusses his experiences working as a data engineer in industry and at the USDS his motivations and methods for creating a newsleteter and the insights that he has gleaned from it
UTvW48SF8gcnYwYecE7ME8,The theory behind how a tool is supposed to work and the realities of putting it into practice are often at odds with each other Learning the pitfalls and best practices from someone who has gained that knowledge the hard way can save you from wasted time and frustration In this episode James Meickle discusses his recent experience building a new installation of Airflow He points out the strengths design flaws and areas of improvement for the framework He also describes the design patterns and workflows that his team has built to allow them to use Airflow as the basis of their data science platform
h77xz6GSDrf8NbRLNixLcB,The information about how data is acquired and processed is often as important as the data itself For this reason metadata management systems are built to track the journey of your business data to aid in analysis presentation and compliance These systems are frequently cumbersome and difficult to maintain so Octopai was founded to alleviate that burden In this episode Amnon Drori CEO and co-founder of Octopai discusses the business problems he witnessed that led him to starting the company how their systems are able to provide valuable tools and insights and the direction that their product will be taking in the future
FepKRNtTgWLsDNKKWLfnxA,Processing high velocity time-series data in real-time is a complex challenge The team at PipelineDB has built a continuous query engine that simplifies the task of computing aggregates across incoming streams of events In this episode Derek Nelson and Usman Masood explain how it is architected strategies for designing your data flows how to scale it up and out and edge cases to be aware of
BThGQELf6oxfaHT7SHxWR4,Modern applications and data platforms aspire to process events and data in real time at scale and with low latency Apache Flink is a true stream processing engine with an impressive set of capabilities for stateful computation at scale In this episode Fabian Hueske one of the original authors explains how Flink is architected how it is being used to power some of the worlds largest businesses where it sits in the lanscape of stream processing tools and how you can start using it today
ULxWC5XCqVnUhGE2PCbAsP,Business Intelligence software is often cumbersome and requires specialized knowledge of the tools and data to be able to ask and answer questions about the state of the organization Metabase is a tool built with the goal of making the act of discovering information and asking questions of an organizations data easy and self-service for non-technical users In this episode the CEO of Metabase Sameer Al-Sakran discusses how and why the project got started the ways that it can be used to build and share useful reports some of the useful features planned for future releases and how to get it set up to start using it in your environment
inAZqopwJa6JwbYJzdxaMT,Data integration and routing is a constantly evolving problem and one that is fraught with edge cases and complicated requirements The Apache NiFi project models this problem as a collection of data flows that are created through a self-service graphical interface This framework provides a flexible platform for building a wide variety of integrations that can be managed and scaled easily to fit your particular needs In this episode project members Kevin Doran and Andy Lopresto discuss the ways that NiFi can be used how to start using it in your environment and plans for future development They also explained how it fits in the broad landscape of data tools the interesting and challenging aspects of the project and how to build new extensions
hcyxGrcxzDrXrNnTmD4n6f,As more companies and organizations are working to gain a real-time view of their business they are increasingly turning to stream processing technologies to fullfill that need However the storage requirements for continuous unbounded streams of data are markedly different than that of batch oriented workloads To address this shortcoming the team at Dell EMC has created the open source Pravega project In this episode Tom Kaitchuk explains how Pravega simplifies storage and processing of data streams how it integrates with processing engines such as Flink and the unique capabilities that it provides in the area of exactly once processing and transactions And if you listen at approximately the half-way mark you can hear as the hosts mind is blown by the possibilities of treating everything including schema information as a stream
f4ttxcByaFYPxetrLm4xxY,Apache Spark is a popular and widely used tool for a variety of data oriented projects With the large array of capabilities and the complexity of the underlying system it can be difficult to understand how to get started using it Jean George Perrin has been so impressed by the versatility of Spark that he is writing a book for data engineers to hit the ground running In this episode he helps to make sense of what Spark is how it works and the various ways that you can use it He also discusses what you need to know to get it deployed and keep it running in a production environment and how it fits into the overall data ecosystem
mDUV9GgjcaThPNXtSmfdWh,Every business with a website needs some way to keep track of how much traffic they are getting where it is coming from and which actions are being taken The default in most cases is Google Analytics but this can be limiting when you wish to perform detailed analysis of the captured data To address this problem Alex Dean co-founded Snowplow Analytics to build an open source platform that gives you total control of your website traffic data In this episode he explains how the project and company got started how the platform is architected and how you can start using it today to get a clearer view of how your customers are interacting with your web and mobile applications
NZjjGkMQe2K4ybLFkfRZwM,Building an ETL pipeline is a common need across businesses and industries Its easy to get one started but difficult to manage as new requirements are added and greater scalability becomes necessary Rather than duplicating the efforts of other engineers it might be best to use a hosted service to handle the plumbing so that you can focus on the parts that actually matter for your business In this episode CTO and co-founder of Alooma Yair Weinberger explains how the platform addresses the common needs of data collection manipulation and storage while allowing for flexible processing He describes the motivation for starting the company how their infrastructure is architected and the challenges of supporting multi-tenancy and a wide variety of integrations
3NNmDPDJwZkWRw2Pm597EJ,There are countless sources of data that are publicly available for use Unfortunately combining those sources and making them useful in aggregate is a time consuming and challenging process The team at Enigma builds a knowledge graph for use in your own data projects In this episode Chris Groskopf explains the platform they have built to consume large varieties and volumes of public data for constructing a graph for serving to their customers He discusses the challenges they are facing to scale the platform and engineering processes as well as the workflow that they have established to enable testing of their ETL jobs This is a great episode to listen to for ideas on how to organize a data engineering organization
oDWXNN8Z5vjpfdWDaJ9c2T,Every business needs a pipeline for their critical data even if it is just pasting into a spreadsheet As the organization grows and gains more customers the requirements for that pipeline will change In this episode Christian Heinzmann Head of Data Warehousing at Grubhub discusses the various requirements for data pipelines and how the overall system architecture evolves as more data is being processed He also covers the changes in how the output of the pipelines are used how that impacts the expectations for accuracy and availability and some useful advice on build vs buy for the components of a data platform
WMLRJkeLV79Hj58sTcak7C,As communications between machines become more commonplace the need to store the generated data in a time-oriented manner increases The market for timeseries data stores has many contenders but they are not all built to solve the same problems or to scale in the same manner In this episode the founders of TimescaleDB Ajay Kulkarni and Mike Freedman discuss how Timescale was started the problems that it solves and how it works under the covers They also explain how you can start using it in your infrastructure and their plans for the future
V3nM9J2miE29N44PAEXbEn,Cloud computing and ubiquitous virtualization have changed the ways that our applications are built and deployed This new environment requires a new way of tracking and addressing the security of our systems ThreatStack is a platform that collects all of the data that your servers generate and monitors for unexpected anomalies in behavior that would indicate a breach and notifies you in near-realtime In this episode ThreatStacks director of operations Pete Cheslock and senior infrastructure security engineer Patrick Cable discuss the data infrastructure that supports their platform how they capture and process the data from client systems and how that information can be used to keep your systems safe from attackers
LZwDiSwvuANfj56ya65hiF,When working with large volumes of data that you need to access in parallel across multiple instances you need a distributed filesystem that will scale with your workload Even better is when that same system provides multiple paradigms for interacting with the underlying storage Ceph is a highly available highly scalable and performant system that has support for object storage block storage and native filesystem access In this episode Sage Weil the creator and lead maintainer of the project discusses how it got started how it works and how you can start using it on your infrastructure today He also explains where it fits in the current landscape of distributed storage and the plans for future improvements
9A7ggvy5UCYdS5XVJGtJcB,Elasticsearch is a powerful tool for storing and analyzing data but when using it for logs and other time oriented information it can become problematic to keep all of your history Chaos Search was started to make it easy for you to keep all of your data and make it usable in S3 so that you can have the best of both worlds In this episode the CTO Thomas Hazel and VP of Product Pete Cheslock describe how they have built a platform to let you keep all of your history save money and reduce your operational overhead They also explain some of the types of data that you can use with Chaos Search how to load it into S3 and when you might want to choose it over Amazon Athena for our serverless data analysis
QLz8MFNAxSzrYGiP8Xsm3n,There are myriad reasons why data should be protected and just as many ways to enforce it in tranist or at rest Unfortunately there is still a weak point where attackers can gain access to your unencrypted information In this episode Ellison Anny Williams CEO of Enveil describes how her company uses homomorphic encryption to ensure that your analytical queries can be executed without ever having to decrypt your data
eMZm7yv3j4X4eknsCFAy2C,Continuous integration and continuous deployment (CI/CD) is a practice that enables an organization to rapidly iterate on software changes while maintaining stability performance and securityAlthough continuous integration and deployment (CI/CD) are not new concepts new tools such as Docker Kubernetes and Jenkins have allowed CI/CD to be more easily implemented in recent years However there are interesting challenges when applying CI/CD to data engineeringIn modern software engineering workflows there are usually development staging and production environments In software engineering development and staging databases usually mimic a fraction of the production database This assumes that the differences in data would not matter In software engineering this is usually the caseIn data engineering we are unable to create a staging environment for some 3rd party data sources Yet our development and staging environments should try to mimic our production environment to ensure consistency in environments But replicating data environments can be difficult depending on the size of your data and existing infrastructureTests are designed to give quick feedback in CI/CD ensuring as many iterations as possible so that bugs can be caught and fixed quickly However testing data pipelines might require testing queries that take more time than an average integration testExisting data pipelines might have been in place before CI/CD was even considered This makes things complicated because a lot of code would need to be redesigned to consider different environments Other than technical legacy it might also come in the form of organisational legacy where people are not used to having multiple environments Educating different stakeholders and engineering teams to get used to a CI/CD workflow would not be easy Besides it would also require significant changes in processesInstead of hypothetical examples lets take a look at a simplified data architecture which follows the ELT principle For this example we will be using Google Cloud Platform which is similar to other cloud providersLooking at this architecture we notice some challenges if we were to fully adopt the modern development workflow For one itd be impossible to create a BigQuery instance for each data engineer on their development environment since this is likely a local setup And if you were to create one BigQuery instance on the cloud for each engineer it will likely be too costlySay you wanted to test your pipelineIf you had thought about these issues youd be surprised that some companies had never thought of it Some teams might not be experienced or have an engineering mindset So they started building things from the get-go without considering how best to control technical debt Once enough things are built and the process is settled itll be difficult to make any significant changesSome of these issues convince some companies to simply go with one environment  production But there can be a compromise and Id argue that multiple environments encourage higher code quality and data integritySince we are unable to strictly adopt a software development workflow we will have to make some compromisesInstead of having a dedicated BigQuery instance to each data engineer for the development environment we will have a shared one For this to work all tables have to be easily torn down and rebuilt  following a functional approach to data engineering The BigQuery API integration with Airflow also helps us with thatWith every merge request a series of automated tests will be run BigQuery gives us some cool API calls to check the validity and cost of our queries which helps us with testing Though not a strict unit test it gives us quick feedback from automated testing so we could catch errors before they are pushed to productionA shared BigQuery instance in development and test environments will inevitably result in unexpected errors especially when the state of the shared BigQuery instance is dependent on the last executed Airflow DAG But since we have followed a functional approach to data engineering resetting the state of the data warehouse is quick and easyApplying CI/CD as best as we can gives us a nice workflow to test our pipelines and logic before pushing them to productionThough this is similar to our current setup at 90 Seconds every company has a different data architecture and constraints Id expect our setup to evolve as data needs change and better tools are being developedIn a lot of ways data engineers are software engineers though with different constraints Concepts such as functional programming containerization and TDD might not be applicable all the time but its a good place to start Lets start with first principles and maybe just maybe therell be a new set of conventions created by the data engineers built upon the shoulders of software engineering
K5fkKfHC6gDSj6uj2vSv8x,This blog gives an overview of how we were able to build a data platform framework for UrbanClap that would capture data in near real-time process it and put in a data warehouse/data lakeYou all must have seen the movie Charlie and The Chocolate Factory The movie is an all-time classic starring Johnny Depp The moment we think about the movie the first thought that comes to our mind is mouth-watering exquisite variety of chocolates being made on high-speed conveyor belts from mere nothing and kids just running around filling their mouths and pockets with chocolates and just enjoying them The kids have an unlimited supply of delicious chocolates and they are absolutely delighted What more could they have asked for? Maybe Im being too dramatic but its a perfect analogy depicting how impactful a data pipeline is for all the data-hungry analysts scientists and businessTo explain the above analogy in a more practical way a data platform framework is built for data that eliminates many manual steps from the data transition process and enables a smooth scalable automated flow of data from one station to the next It starts by defining what where and how data is collected It automates the processes involved in extracting transforming combining validating and loading data for further analysis and visualization It combats possible bottlenecks and latency and has the plug and play feature making it versatile and flexible to be used for multiple sources and sinksWhen the data engineering team was being set up at UrbanClap there was an urgent requirement for a data platform to drive all analytical needs both realtime and hourly After we got convinced about all the possible use cases we were able to design the whole architecture of the data pipelineLets dive into all of them one by oneIts been more than a decade since big data came into the picture and people actually understood the power of data and how data can help a company make a better smarter and adaptable product So with time the analytics has also become very insightful and mature and the majority of analytical use cases have driven more towards streaming (real-time data) from traditional batch (historical data)UrbanClap deals with both consumers and providers across various platforms like android ios and web at a huge scale and all our transactional and non-transactional data gets ingested in various topics of our multi-node Kafka cluster with appropriate replications partitions We went with confluent Kafka so that we can leverage the schema registry for schema evolution and connectors like Debezium for our CDC use casesOn top of that we wrote nodeJS and scala multi-threaded producers to push these events to Kafka in Avro file formatAlso we decided to go with Avro file format because : Traditional ELTs/ETLs with push and ingest models are long gone With ever-evolving business requirements we wanted a change of thinking towards the serve and pull model across all domains Instead of taking ownership of data end to end we wanted to design a platform that would give the ownership to different teams/domains They could pull data from a source quickly and in the way they want with the help of a skeleton framework we provide to themApache Spark the flagship large scale data processing framework originally developed at UC Berkeleys AMPLab The chosen framework of all tech giants like Netflix Airbnb Spotify etc out there And as Uncle Ben said With great power comes great responsibility Spark in our data pipeline does all the heavy lifting It is the engine of the pipeline doing all the cleaning transformations on the data based on the actions given to itTo fulfill business requirements we wanted a streaming framework to cater to queries on real-time data and a batch framework to process historical data Thus we went ahead with using Spark Structured Streaming We chose the newer Structured Streaming API instead of the legacy Spark Streaming API because: We use AWS EMR as the cloud service provider to run our spark jobsNow that we had figured out what we wanted to do with the data and how to process it the next important thing was to figure out where to put the raw/processed data that would eventually power-up the dashboards for analytics We wanted a data lake where we could persist raw data to be used by Data Scientists and a data warehouse from where anyone could generate views for reports and analysisIf you would have watched the popular TV show SUITS there are two protagonists Harvey and Mike They have different personalities traits but in the end they are the best at what they do Even though theyre different their strengths and weaknesses complement each otherData Lakes are still comparatively new/fresh in the data engineering ecosystem still taking shape evolving finding purpose but at the same time they are stable secure and scalable They have a bright future in the big data ecosystem whereas Data Warehouses have been there for a very long time they have a purpose to drive all business needs powerup the dashboards provide faster insights They are always at the forefront when it comes to decision making but they are comparatively more complicated and slower to adapt to evolving dataS3 takes in all the high throughput raw/transformed data coming from the streaming pipeline and also derived data from some of the batch jobs It is a very crucial piece in our data pipeline as it is the input source for the view spark jobs It contains data from all the possible sources well partitioned and in parquet format which makes querying super-fastSnowflake is the analytical and reporting tool that we use to generate views on the filtered transformed data It is the one source of truth for all data analysts sales product and marketing guys as most of the dashboards get powered up by Snowflake Its SQL like interface and Parallel Processing Architecture makes it easy to use and at the same time powerful and fast for all business needs The tables in Snowflake are updated half-hourly hourly and daily periodsIn this post we saw a zoomed-out overview of the data platform framework we have built so far We briefly discussed some of the paradigms and tools used by UrbanClap to meet business use cases In the coming posts well go in-depth around various tools and components we use
WqrMSkVk4F8UdUzj2VxwDe,Prologue: Whether you are into fintech e-commerce hospitality or hyperlocal  the 21st century technology playground is now becoming increasingly focussed on two major initiatives : Over the last 10 years it has become quite obvious that after a certain scale no tangible progress can be made on these fronts unless we analyze and learn from historical data The solution can be heuristics machine learning models or some periodical performance reports but the core funda here is that all things good or bad will be byproduct of the data you collect Thus rises the need for common standardised infrastructure where all stakeholders like analysts engineers data scientists product managers business teams etc can go to collect curated data that will be used to power their decisions and the component that makes it possible is called the data warehouseIn this article we will try and discuss why you need a data warehouse  what value it brings and how to build the first version of your data warehouseAs you can see for stage 1 any analysis would have required analysts to work on multiple databases but since the number of DBs is less it would have been manageable It might entirely be possible to run the analytical queries at non peak business hours directly on the SQL databaseFor data spanning across DBs a bit of excel magic over SQL output might do the trickWhen it comes to stage 2 then things start becoming tricky suddenly you have 6 different DBs to deal with (or data lies in isolated Silos) There might be duplicate information (example same attributes for an order) stored across application DBs Also combining information from the output of SQL queries on 6 different tables might be become a management mayhem Still you try to put certain processes in place delegate responsibilities with respect to ownership of data to make it work for whileBut as soon you hit stage 3 all hell breaks loose combining information from 15 different sources is a nightmare Older data has been moved to archival databases Some of them have timestamps in your local timezone some have it UTC Some of them are Mysql some Postgres  some Mongo Also your business is growing thus the analytical queries you run on the database are now scanning greater volume of data  this might lead to higher cpu usage and higher job run times Your scheduled jobs are triggered via crons from hidden machines and what not In simple words data analysis becomes almost impossibleAt this point (or before) you need to invest into creating a data platform starting from the warehouse The first version can just solve your immediate concerns and you can improve over it incrementally_____________ DB CPU spikes will still an issue as the queries to be run will still be delegated to individual databases_____________ Accessing older data will require the same query to be run over two different databases although both the current DB and archival DB has similar semantical meaningSeparate Compute from storage: _____________ Data standardisation is still an issue Timestamps and attribute duplication in multiple DBs being usual suspectsStandardise Data: _____________ Construction of standard tables still remains an issue because of flaky scheduling and dependency management among jobsStandardise Scheduling: Data Dictionary (Investment): In the above section we discussed how and why to approach these core issues: Once you have chosen the appropriate tech stack and implemented it then it becomes a matter of mere processes and procedures to keep it running smoothly From this point onwards you need to invest in making the query interface more user (analyst) friendly and secondly you also need to invest seriously in maintaining a dedicated support team for the data warehouseThe support team will make sure the utmost data quality and quick recovery incase of failures because any system will remain only as good as the people maintaining itNow that you have all the data available safely and reliably available in a curated manner at a centralised location thus your analystsPMs and data scientists can go about their business and start leveraging this data and focus on improving the UX and reducing inefficiencies in operations
ao9wp8cyb22hEUSq5z7nsK,In this article well build a data pipeline using the AWS ecosystem It is a basic solution created to familiarize the reader with AWS cloud technologies Well build a web tracker (a periodically scheduled web scraper) to track cryptocurrency prices from wwwcoinmarketcapWell be using : Lambda  To run python scriptsS3  To store raw dataDynamoDB  To store processed dataSetup : Our system consists of the following components : Coinmarketcap scraper Returns scraped data for a coinmarketcap pageAlso take note of this in-depth Lambda tutorial Head over to this link if you face any issues that I havent coveredStep 1 : Configure AWS S3 bucketFrom AWS console go to Services > Storage > S3Click Create bucket and name your bucketStep 2 : Lambda function that scrapes data and dumps into S3 bucketCreate function > Author from scratchName your lambda function and choose your programming language I named mine cryptoPriceScraper with Python version 38 My lambda function calls the scraper module a simple scraper written using beautifulsoup The lambda function serializes the JSON directly to this S3 bucket using boto3 (AWS SDK) library If you want to run the app from your local aws_access_key_id and aws_secret_access_key need to be passed while creating the boto clientWell be uploading the code as a zip file since our program uses third party libraries like beautifulsoup which cannot be imported directly from the Lambda code editor Before uploading the code zip file install python packages in the local code folderUpload the zip file Change Handler name on the lambda function page to your py fileStep 3 : Grant Lambda function access to S3 bucketCreate an IAM role Use case -> Lambda Policy -> S3AllAccessStep 4 : Test Lambda functionCreate a new test case and pass an empty dictionary as input since our function does not take any input parameters If the test runs successfully you should now see a json file inside your S3 bucket Sample outputWe want our scraper to be run on periodic intervals so well create a trigger that sets off our lambda functionAdd Trigger -> Cloudwatch Events Set scheduler expression -> rate(5 minutes) If all goes well a new json file should be added in the S3 bucket every 5 minutesLogs generated by Lambda are saved in Cloudwatch They can be seen by going to the Monitoring tab of lambda functionIf you see this warning add the required policy to your lambda functions execution roleOnce the policy is added youll be able to monitor your logs on cloudwatchStep 1 : Create a new Lambda function that picks any new file added to S3 bucket and inserts data into DynamoDBThis time well choose a blueprint to create our lambda functionStep 2 : Create DynamoDB tableGo to AWS console > services > DynamoDBDynamoDB is a no-sql database hence it does not specify a schema on-write That implies you do not need to define all the column names the table might expect Create a new table and just add the primary Key Sort Key is optionalStep 3 : Attach a AmazonDynamoDB full access policy to the role of this lambda function so that we can access the DynamoDB tableStep 4 : Write lambda functionThis time I wrote the code directly from the editor since we didnt need any third party librariesThis parses the S3 file into a JSON object and inserts it into DynamoDBBefore inserting you can choose to make any transformations you needStep 5 : Test the pipelineOur entire pipeline is now ready to be executed Enable the trigger on the first lambda function to start pulling data from the website It should place the json files in the S3 bucketThese files are then picked by the second lambda function that load them into the databaseIf everything worked correctly records should now be visible in the DynamoDB tableOur data pipeline now works fine and is being populated with dataIf you want to access this data from your local youll need to setup AWS CLIPass your secret_id and secret_keyPass the region where your db instance is located Passing another instance will not workThe exact instance of your DB instance can be seen under the overview tab of your tableYoure now good to query your Dynamodb table from your AWS consoleThats all for this article ! Do leave a feedback
SPBiVgTWojZLjrYPPE94sh,Airflow is fast becoming the de-facto orchestration tool for many data engineering organizations I for one am new to Airflow and wanted to setup an Airflow server and play around with it to understand what the buzz is all about But little did I know how complicated the installation would be After reading through multiple medium posts and stack-overflow responses I was finally able to setup a working Airflow server Given that none of the above mentioned resources completely captured what I really I had to do to get the server up and running I wanted to write this post to make it easy for folks who are in my boatWithout further ado let me jump right into it The use case I was pursuing was to setup an Airflow server on an AWS EC2 instance running Ubuntu 1804 OS and use the Airflow server to trigger Databricks JobsLogin to your AWS Account and navigate to Services>EC2 Launch and Instance and select Ubuntu AMI In the next screen you will be prompted to select an instance size You want to go with a fairly big instance as the t2micro instance doesnt cut it I chose t2medium and that seems to work just fine for small workloads If you are installing a production version you may want to go even bigger like m5axlarge Next ensure Auto-assign Public IP is set to Enable Select the defaults for the next few screens When it asks for setting up security group add the rule to open port 8080 to public as thats the port through which you can connect to airflow serverBy default airflow uses sqlite to store metadata but if you want a fairly robust installation you want to use postgres database to store all the metadataNext you need to ssh into the server to do the installation and setup of postgresql databaseFrom here on please make sure you are logged in as airflow user Going back to ubuntu user will mess up the installationOnce the postgres database is installed its time to create database and users to access the databaseNext we need to set a few config settings to ensure the airflow server can connect to the postgres database Find the pg_hbaconf file and edit itYou should see the airflow UINext we will work on connecting the airflow server to the Postgresql database as its metadata databaseOpen the airflowNext make the following changes You can see the difference between different executors at this linkFinally you dont want all the example dags to clutter your UI so set the load_examples to falsehello_worlddatabricks_trigger_jobdatabricks_create_jobWe are almost there The final thing we need to do is to ensure airflow starts up when your ec2 instance startsFinally to make sure your Databricks DAG is able to connect to your Databricks installation you need to setup your Databricks connection To accomplish this go to Admin>Connections and set it up using a Databricks access token The connection should look like belowIn this post we saw how to setup a development Airflow server and setup DAGs to trigger Databricks jobs To setup up a production Airflow server you may need to look into setting up multiple worker nodes using celery Hope this post was helpful If it helped you or if you got stuck in at any point please post a comment and I will try to improve this post Thanks for reading
k7sBQbnrz4no4GcB66n9bG,The most important pillar of data computing and processing is data structure which describes the schema by listing out columns  declaring types and constraintsOnce claiming and applying relevant data structure a significant improvement takes a place during the processing phase As data structure allows the processing engine to know what the data looks likeThrough my previous article I described the benefits of dataframes and the schema structure which is the central to the concept and to working with dataframesThe aim of this article is to describe the way we can deal with structured data schema inference in Spark • Inferred Schema : Inferring schema from data sources that already have a schema is generally straightforward We use the appropriate DataFrameReader method and Spark will read the metadata in the data source and create a schema based on itSpark can infer schema in multiple ways and support many popular data sources such as:   jdbc (…): Can infer schema from table metadata  json (path: String): Can infer schema from data itself  parquet (path: String): Can infer schema from parquet metadata  csv (path: String): Can infer schema from column names and data
Ft5fFtq5Rde2TSLQ3qqBKF,Building data warehouse has never been a small project multiple roles are involved done by many people in a few years For me its totally understandable since when we talk about data warehousing we talk about integrating data from various data sources and standardize the shape of data which we usually refer as : data modeling not to mention about cleansing the data from unwanted activity like web scrapping or stress test in OLTP systemIn the more conservative approach like waterfall method project activities are quite linear starting from requirement gathering design and architecture development UAT and finally production deployment which means business users need to wait patiently until the project is finished to see the result its quite often next gen data warehouse become old gen due to multi year implementation When data warehouse is finally delivered business has been changed :) With stiff business competition nowadays i believe business users cant wait a few years to measure business performance and derive insight from dataLets talk about agile methodology first According to atlassian agile is an iterative approach to project management and software development that helps teams deliver value to their customers faster and with fewer headaches Instead of betting everything on a big bang launch an agile team delivers work in small but consumable increments Requirements plans and results are evaluated continuously so teams have a natural mechanism for responding to change quickly Wow! seems promising business users will get the data faster with this agile methodologyFirst we cant go agile with this big chunk requirement (remember data warehouse has never been a small project) we have to split this big chunk into smaller chunk or you dont even need to go through complete requirement gathering in project kick off! just focus on small P0 requirements that business really needs and save other requirements in backlog first save them later for next iterationBut wait something is missing here how can we build robust data model without seeing everything thoroughly and comprehensively? our data model will be always changed when we work in new requirement then? For example for the first batch requirement we need to deliver X measurement which should be able sliced and diced by A-B-C dimension to fulfill this we build J table Then 2nd batch requirement is coming business users want to add D-E-F dimension around X measurement BOOM! data granularity has been changed then developer should change J table including ETL to populate J table Not to mention backfilling again historical data which actually this activity has been done in first batch requirement! Hmm seems rework on same ETL program is inevitable here Hmm actually its bit different with building software a mobile app for example where we can split requirement and create story by application features plan and even we can decompose each story by many things like application capability target device etc we can add more capability in application without breaking previous released featureWarren Sifre BI Solution Architect at Allegient has shared his views on the implementation of agile in the data warehouse For agile to work there are a few concessions that need to be made prior to beginning: Well it seems we have solved business users problem but it cause another problem for developers since if developers agree with above items means rework for same ETL job is inevitable We should find the balance between incremental delivery and rework Not to mention the size of data warehouse team usually big (remember that data warehouse never a small project) communication wont be easy here Mentioned in agile manifesto Individuals and interactions over processes and tools and Customer collaboration over contract negotiation intense communication and interaction cant be avoided in agile methodology and its quite difficult to do that with large team (remember again that data warehouse never a small project) Here what i thought: So does agile works on data warehouse? if it works its not because of agile mindset alone likewise if the project is fail its not agile fault alone Project methodology is important but its not the only success factor agile mindset is not answer of all problems you have Implementing any agile framework (scrum kanban) without good teamwork transparent communication fast feedback from stakeholder and fast response developer is useless nothing is good without STRONG COMMITMENTIn my opinion each project methodology has it owns pros and cons its not about A better than B but take decision based on your necessity and circumstance Pick what suits you compare to spend your time to find out which one better For example if youre working on mature industry which not so much changing over time and you have time in your hand its not sin to pick waterfall instead of agile! and vice versa
ZkMwY5rMPsNvpBHmv2Qnkb,With the price of compute engine is getting cheaper massive parallel processing advertised everywhere and big data term becomes more more common nowadays i heard many people say this is the end of data warehouseAccuracy is very important for business intelligence and data analysis in order to make impactful decision and action thats why using high quality data as input is a must Where we can find that high quality data? yep in data warehouse (or if you dont like data warehouse term you can just simply change the term with data vault data lake data fine whatever) So when we talk bout data warehousing we talk about these activity: For me data warehousing is about governance not related with specific technology or tool So back to the question is data warehouse dead? for me absolutely NO even more important in this big data era which information you gathered is huge and variedFor some organization RDBMS is the best solution but maybe not the case for other organization maybe other organization need DBMS (without R) with MPP capability or some organization doesnt want to adopt DBMS at all and fully adopt MPP by decoupling storage and compute engine and manage everything by themself some organization picks serverless architecture which actually MPP but infra arrangement is managed by vendor Every organization has freedom to pick their tech stack depend on situation they face technology maybe different but the ultimate goal to govern the data will never changeHow about well known method in building data warehouse like dimensional modeling building star or snow flake schema always denormalized ETL/ELT Slowly Changing Dimension etc? Are they dead in non-RDBMS environment? can be yes or no method can be different but goal is still same Lets take example on SCD which the goal is to keep historical data changes in data warehouse and matching historical data with fact record easily by implementing surrogate key start and end date of the record and sometimes record version columns on dimension side its suitable RDBMS which has DML feature also usually built-in feature in ETL tool like PDI how about SCD in Spark Beam or in BigQuery? is SCD dead in those framework/technology? its hard to answer that in BigQuery my team try to implement what is similar with SCD but without start and date columns which SCD type 2 signature we use different method but the end goal is same! to keep historical changes in data warehouseBack to question is data warehouse dead? the answer is depend on data warehouse definition If data warehouse is RDBMS system to support data analysis activity and BI then the answer can be YES or NO depend on organization situation and necessity If data warehouse is method to govern the data then the answer is NO
8v8SWbNgoh4G53Lr42AtYz,"Prerequisite: The full source code and file for the example could be found at: In now days microservice-based architectures is one of the most popular in industry and they are often found in enterprise scale applications lately The main goal of microservices is to keep the application small (micro) and have its own knowledge to serve the specified domain to be maintainable and have a readable code So microservice-based architecture known as Domain Driven Design (DDD) to keep application to handle its domain The applications will separated into small pieces along with its data it would be placed into different database to keep it small and neat To achieve the main goal there will be 1 of the biggest things to be sacrifice that is the data management The data will be scattered away and its kinda hard to reassemble the data for analytical purpose and any activity related to OLAP data One possible way to keep data synchronized across multiple services and stored properly with the expected structure for OLAP is to make use of an approach called change data capture or CDC for shortEssentially CDC allows listening to any modifications which are occurring at one end of a data flow (ie the data source) and communicate them as change events to other interested parties or storing them into a data sink Instead of doing this in a point-to-point fashion its advisable to decouple this flow of events between data sources and data sinks Such a scenario can be implemented based on Debezium and Apache Kafka This demo will show how to join two CDC event streams created by Debezium into a single topic and sink the aggregated change events into MongoDB using the Kafka Connect MongoDB sink connector (https://githubcom/hpgrahsl/kafka-connect-mongodb)These example consist of a microservice that handling customer data including addresses and orders To keep the example simple and straight forward we like to use one microservice with single database but you can do it with a microservice with many databases at once or multiple microservices For now we only interested to capture any data change if the customer have address information changes So we would only like to store the data to MongoDB if theres a data change on users and addresses to related usersBefore we start there will be some component to be explained: - makes sure all data changes captured- produces change events with a very low delay (eg ms range for MySQL or Postgres) while avoiding increased CPU usage of frequent polling- can capture deletes- can capture old record state and further metadata such as transaction id and causing query (depending on the databases capabilities and configuration)We will use docker to run through the PoC for debezium data aggregation and resource management on production environment First of all we will try to start the required stack to be started using docker-compose Docker compose is a tool provides a way to orchestrate multiple containers that work together So we could manage the docker resource such as docker network to make available for each resource to communicate each other without any prior knowledge about themOnce all services have been started register an instance of the Debezium MySQL connector by submitting the following JSON document: The config above is trying describe how we set up the connector for the specified database using the given credentials For our purposes were only interested in changes to the customers and addresses tables hence the tablewhitelist property given to just select these two tables Another noteworthy thing is the unwrap transform that is applied By default Debeziums CDC events would contain the old and new state of changed rows and some additional metadata on the source of the change By applying the iodebeziumtransformsUnwrapFromEnvelope SMT (single message transformation) on transformsunwraptype key only the new state / update will be propagated into the corresponding Kafka topicsWe can take a look at them once the connector has been deployed and finished its initial snapshot of the two captured tables by using the kafka-console-consumer tools that already provided by originally from Kafka We would like to watch to topic at the same time because we wanted to listen to two tables at the same times As the docker application already run we could use command docker-compose exec to try execute the tools inside the kafka container by running these command: You should see the following output (formatted and omitted the schema information for readability) for the topic with customer changes For the addresses changes output you should see something similar with an id and the payload Here are the example below: You should see the similar message including schema with the key and payload from the other terminal contains CDC from address table for the other data Before moving forward to the Kafka Stream application now take a look wheres the data came from Open a new terminal to going inside the MySQL dockerInside the docker now you can take a look for the data already being prepared for this example The connector will try to ingest all the data from the beginning of the time So its up to us whether we wanted to pick and aggregate the data from the beginning or the latest one For this example we would try to aggregate all the data from the beginning of the time""The other possible value is latest so the Kafka Stream application will not try to process the data from the beginning of the time it would only try to process the latest one This config only could apply once because once the offset already being marked you couldn't change it to the earliest (if you change your mind to aggregate the data from the beginning of the time) the possibility are trying to reset it and reprocess it with earliest parameter The application will respect AUTO_OFFSET_RESET_CONFIG parameter if the offset is not being setup on the system Once it already setup it would only read based on the offset We will try to catch up about offset later onWe would like to use Kafka Stream API to process and aggregate the data changes Kafka Streams is a client library for building applications and microservices where the input and output data stored in Kafka clusters It combines the simplicity of writing and deploying standard Java or Scala applications on the client side with the benefits of Kafkas server-side cluster technologyThe KStreams application is going to process data from the two Kafka topics These topics receive CDC events based on the customers and addresses relations found in MySQL each of which has its corresponding Jackson-annotated POJO (Customer and Address) enriching by a field holding the CDC event type (ie UPSERT/DELETE) Since the Kafka topic records are in Debezium JSON format with unwrapped envelopes a special SerDe has been written in order to be able to read/write these records using their POJO or Debezium event representation respectively While the serializer simply converts the POJOs into JSON using Jackson the deserializer is a hybrid one its possible to deserialize from either Debezium CDC events with POJOs With that functionality the KStreams possible to create and maintain DDD aggregates on-the-fly can be built as follows: All the customer records came from the customer topic into a KTable which will automatically maintain the latest state per customer according to the record key such as the customers PK that is the id columnFor the address records the processing is a bit more involved and needs several steps First all the address records streamed into a KStreamSecond a pseudo grouping of these address records done based on their keys (the original primary key in the relation) During this step the relationships towards the corresponding customer records still maintained This allows keeping track which address record belongs to which customer record even in the light of address record deletions To achieve this an additional LatestAddress POJO introduced which allows to store the latest known PK - FK relation in addition to the Address record itselfThird the intermediate KTable is again converted into a KStream The LatestAddress records transformed to have the customer id (FK relationship) as their new key in order to group them per customer During the grouping step customer specific addresses updated which can result in an address record being added or deleted For this purpose another POJO called Addresses introduced which holds a map of address records that gets updated accordingly The result is a KTable holding the most recent Addresses per customer id Based on the code above one thing to be highlight is Materialized It is the class available in Kafka Stream to keep the state store in the Kafka topic alongside with the changelog We will cover this topic later while discussing the file rotation to keep the file small and maintainableFinally we need to bring customers and addresses together by joining the customers KTable with the addresses KTable and thereby building the DDD aggregates which are represented by the CustomerAddressAggregate POJO At the end the KTable changes streamed to a KStream which in turn gets saved into a kafka topic This allows making use of the resulting DDD aggregates in clear ways""Records in the customers KTable might receive a CDC the delete event If so this can be detected by checking the event type field of the customer POJO and eg return 'null' instead of a DDD aggregate This convention can be helpful whenever consuming parties also need to act to deletions accordinglyThe next important part you should know is offset in Kafka Kafka remembers your application by storing consumer offsets in a special topic Offsets are numbers assigned to messages by the Kafka broker(s) indicating the order in which they arrived at the broker(s) By remembering your applications last committed offset your application is only going to process newly arrived messages The configuration setting offsetsretentionminutes controls how long Kafka will remember offsets in the special topic The default value is 10080 minutes (7 days)If your application stopped (hasnt connected to the Kafka cluster) for a while you could end up in a situation where you start reprocessing data on application restart because the broker(s) have deleted the offsets in the meantime The actual startup behavior depends on your autooffsetreset configuration that can be set to earliest latest or none To avoid this problem it is recommended to increase offsetsretentionminutes to an appropriately large valueAfter we have some overview to the code lets try to run compile and package the program For this example I will pick maven as the package manager to using pom file So to package the application we could run this command below: You should see the success message below: After that we would like try to run the application inside the docker First we would like try to build it from the docker file so we could try to run it by using this command below: The Kafka Stream application will aggregate the events and will try to publish the message into another Kafka Topic called: final_ddd_aggregates Once the aggregation pipeline is running we can take a look at the aggregated events using the console consumer: We originally set out to build these DDD aggregates in order to transfer data and synchronize changes between a data source (MySQL tables in this case) and a convenient data sink By definition DDD aggregates are typically complex data structures and therefore it makes perfect sense to write them to data stores which offer flexible ways and means to query and/or index them Talking about NoSQL databases a document store seems the most natural choice with MongoDB being the leading database for such use casesThanks to Kafka Connect and numerous ready (to used) connectors it is almost effortless to get this done Using MongoDB sink connector from the open-source community it is easy to have the DDD aggregates written into MongoDB All it needs is a proper configuration which can be posted to the REST API of Kafka Connect in order to run the connectorSo lets start MongoDb and another Kafka Connect instance for hosting the sink connector: In case the DDD aggregates should get written unmodified into MongoDB a configuration may look as simple as follows: The data will be stored in customers_with_addresses collection that already defined in mongodbcollection key As with the source connector file deploy the connector using curl: This connector will consume messages from the final_ddd_aggregates You can take a look by firing up a Mongo shell and querying the collections contents: The result would be look like this: Due to the combination of the data in a single document some parts arent needed or redundant To get rid of any unwanted data (eg _eventType customer_id of each address sub-document) it would also be possible to adapt the configuration in order to blacklist said fieldsFinally you update some customer or address data in the MySQL source database: After execute all MySQL above you should see that the corresponding aggregate document in MongoDB has been updated accordingly with execute the command below on MongoDB console: To enrich your knowledge about the config parameter and handling operational issue lets try some other thing Try to take down all of your application before we begin to explore some other config To shut down all the docker container you could use docker-compose down from the main directory After waiting for some times and its successfully shut down now on the main class StreamingAggregatesDDD we would like to try to change some line of code We will try to change the line: to become: That value above will attach to the parameter config named autooffsetreset""After you change it please follow through the command above until you reach this following steps Please make sure while you build the application the docker miss the cache while packing the application or you can do it manually to make sure it will miss the cache Inside the poc-ddd-aggregate folder you could find a file named Dockerfile actually its one of the file contains set of instructions how to build the aggregator application After that open the file using your favorite text editor and find the line RUN echo 20 and please change it to echo another one maybe it's a great idea to change it to a greater sequence number like 21 so it would become RUN echo 21After that try to watch the published message to final_ddd_aggregates topic using the very next command to view the message came in by using console consumer or you can see on the terminal runs your aggregator application You will not see any incoming message you see previously that you could see that the Kafka Stream application will try to ingest the data from the beginning Because the latest option would be read the data that have ingested time greater than current time when application started and would ignore any earlier data than that Also it related to offsetsretentionminutes config that how long Kafka Stream application will remember the current latest committed offset (please see the explanation above if you just jump into this part) If we already passed the specified the Kafka Stream application will respect to the value on autooffsetreset config Kafka stream will always renew the time while committed the offset each time a data processedIn information technology file rotation is an automated process used in system administration in which files are compressed moved (archived) renamed or deleted once they are too old or too big (there can be other metrics that can apply here) New incoming file is directed into a new fresh file (at the same location) with some added index or something like counterThe main purpose of file rotation is to restrict the volume of the file size and to avoid overflowing the record store and keeping the files small enough on the system for the efficiency purposeIn Kafka Stream application it implements the file rotation for now we would take a look at the change log rotation on the aggregator application while saving the aggregated record to the changelog file First we need to take down all the running applications by running command docker-compose down First of all lets see the snippet code: First the config SEGMENT_BYTES_CONFG it will limit the size of the change log file for every part We set it into 3000 it means the file size will not exceed more than 3000 bytes or 3kb If the file exceeds more than 3kb it will do some rotation to the file we will try to discuss it later how it will rotate the file The next config parameter is CLEANUP_POLICY_CONFIG how Kafka Stream application will handle the log rotation if it exceeds the retention time whether it will be deleted or just compressed (compacted) the available options are: CLEANUP_POLICY_DELETE or CLEANUP_POLICY_COMPACT The final parameter we would to explore is RETENTION_MS_CONFIG it describe how long the system will retain the file(s) if it is not elected as the main fileFrom all the config above we could say we would like to rotate the file if its size greater than 3kb and the file not elected as the main file it would be scheduled to deleted after 1 minutes""Maybe you could have a different output but with a similar format We could see the latest index is 0 and it's the latest committed index (not offset) and it would be elected as the main file (will be not schedule to deleted)Try to list down again all the available file it will be outputing: The file size would be increasing to 1596 from 2499 As you could see from the on the Kafka Stream terminal or Consumer Console on final_ddd_aggregates topic the Kafka Stream Application still aggregating all the data with address id: 17 18 19 While 18 and 19 are the new records that we just made Here are the output below on my terminal (formatted): Now try to restart the Kafka Stream Application (you can stop it with ctrl+c and rerun it again with docker-compose up --build aggregator) After that try to insert Address data again: It would be still aggregating all the previous data (because the data will be store to the file and will be re-read on starting the application) the output on the terminal console will be like this: Now try to wait until the Kafka schedule to delete file index 0 and 7 The output on Kafka terminal would be: After that try to go back to the Kafka console and trying to list down all the available files the output will be: The output could be like this (if the file already deleted): It would delete the index 0 and 7 also create a new index in my case it would be index 8 maybe you could have a different index If we try to restart the aggregator application and inserting some data: As the old file already deleted it would be only aggregating the latest data: If you try to execute the command below on mongo console it would only display the address with id 21Hopefully you could understand how the Debezium and Kafka Stream works together along with its basic config parameter and how we could handle file rotation"
gGSnQjSprdUDbMDAkJSEGp,At MédecinDirect we send every month reports to our partenaires about the usage of our services among other data Someone from our Data Science team used to do this by hand every month However with the growth of the company and the number of partners it became longer to generate all the reports In the end it was taking three days of work every month On top of that all partners didnt want the exact same report so we had to add or remove specific charts for some of them Another problem was for example that if we modified a specific representation of some graph we had to correct it on each report With this and many other points we could see that the risk of human error was very highWe decided to develop our own web platform to visualize dashboards for two reasons: Some of our partners told us that while the web version was good they would like to keep the PDF version of the dataSo is there a library in NodeJS to use this api? Yes and its name is PuppeteerFor the specificities of the partenaires we have created a database that stores the charts architecture for each partenaireWe are using python3 to simplify the treatment of the data with pandas and flask for the API part For the front we are using ReactjsTo generate the PDF we use a JS script executed in a NodeJS environment that gets the list of our partenaires connects to our react server for each partenaire using puppeteer and generates a PDF of the page This step is automated and launched during the night once a monthAmaury SANCHEZ : Junior Data Scientist chez Médecindirect
Xx4f79ExhA7fV6GqC8ejGp,"Apache Airflow is an open-source workflow orchestration tool There are many posts available that explain the core concepts of Airflow (I recommend this one) This post assumes you have some familiarity with these concepts and focuses on how we develop test and deploy Airflow and Airflow DAGs at Devoted Health Devoted is a Medicare Advantage startup aimed at making healthcare easier more affordable and believes every member should be treated like we would treat a member of our own familyThis part of the post discusses Kubernetes Helm Terraform and Docker but since they are all their own complicated things it does not go into detail about any of themWe have a very modern technology stack at Devoted so of course we run Airflow on Kubernetes This means we use Docker containers to deploy all of our Airflow infrastructure We have a single Docker image that is used for the Airflow Web Server Scheduler and most of our Tasks It has a core set of Python dependencies installed and whenever an update is made it is built and deployed to Amazon ECR via a CI jobWe deploy Airflow itself using a Helm chart (based on this one in charts/stable) that describes all of the Kubernetes resources (Deployments Services Persistent Volumes etc) we want for Airflow We couple this with Terraform which allows us to deploy a new instance of Airflow with a simple command like: This command creates a RDS database an EFS volume for DAG storage a Kubernetes namespace and Airflow Scheduler and Web Server podsNotice that I didnt mention any Airflow Workers We recently migrated to Airflows Kubernetes Executor which has no permanent Workers and no Redis/Celery requirement for distributing work Instead every Airflow Task is run in its own pod This means we can allocate resources for each Task rather than just having our workers sized for our most resource intensive jobs Additionally we can have a different Docker image for each Task If a Data Scientist writes a complicated Machine Learning job that has many dependencies this allows us to keep those separate from our core Airflow image Weve been running this setup for a few months now and it has been great for us so farAt Devoted we have many different people working on Airflow DAGs including a team of 8 Data Scientists (theyre awesome and theyre hiring!) This has led to some unique challenges since often different people are working on different parts of the same DAGWeve solved this by developing an internal tool that allows each developer to spin up their own Airflow instance on Kubernetes (these are smaller than our Staging/Production environments) along with their own clone of our data warehouse (its a Snowflake thing you should use Snowflake it is also awesome) This tool is called devflow because I am not creative when it comes to naming things except my cats ( Mac & Cheese) It wraps Helm kubectl and Terraform into a few simple commands so developers can run things like devflow start to start up their dev environment and devflow sync to deploy their local changes to their instanceBesides helping avoid collisions in DAG development this setup allows developers to use the same technology and environments in Dev that we use in Staging/Prod creating far less it works on my machine scenariosIn addition to devflow the Data Engineering team at Devoted has built another internal tool to streamline DAG development called DAG Builder This library provides a simple interface for creating a new data pipeline in Airflow Developers write a DDL query for an end table a transformation in SQL or Python and use a YAML file to describe the DAGThe example above generates a DAG that populates two tables one dependent on the other and automatically includes alerting monitoring support for integration testing and more This approach has allowed us to standardize our DAGs which makes adding new features/enhancements to all DAGs (like the data validation tests below) much easier and improves developer efficiency as Data Scientists can easily understand and work on pipelines they didnt originally writeWe use three different types of tests to verify that DAGs are working as expected""All DAGs must pass a suite of unit tests in our CI pipeline before being deployed These are tests that can be run independently of other resources and include a smoke test that validates every DAG can be imported into the Airflow DagBag (I will never not laugh when I type that) as well as tests for any python code used in our DAGs We use pytest to run these and we feel they're table stakes for testingThis set of tests interact with other resources which is obviously very important for a workflow tool like Airflow that connects to a bunch of platforms Thanks to the efforts of one of our Data Engineers Julie Rice we run end-to-end tests for most DAGs in another CI pipeline This helps validate that our SQL doesnt have errors and things like complicated CASE statements (who doesnt love these?) produce the expected results in our data warehouse This was a challenging thing to implement but we believe the investment will pay off in increasing Developer confidence as they make changesThe third form of testing we use is the only one that doesnt happen before deployments Instead data validation is done within each DAG at run-time We have a set of standard tasks that allow Airflow Developers to specify things like this column should be unique this one should never be NULL or this should have a record count greater than X This is the final protection we have against allowing our internal users to access reports with incorrect dataWe use a single AWS EFS volume to persistently store Airflow DAGs (and plugins) for each environment It is shared amongst all of the pods in the Airflow namespace (Web Server Scheduler and Tasks) so we only need to push new/updated DAGs to one place for all of our resources This is done via a simple CI job that runs once DAGs have passed our test suite described above No old-school release cycle here we deploy whenever a new change is ready which happens many times per dayIve gone over how we develop test and deploy Airflow but saved my favorite for last Monitoring AKA how we the team in charge of keeping Airflow running sleep soundly at night knowing it is in fact runningOur first line of defense against OpsGenie Alerts is a feature of Kubernetes called Liveness Checks This allows you to signal to Kubernetes that your container is in a bad state and should be restarted As all good technologists know sometimes turning it off and on again is all it takes to fix something For the Web Server we simply use Airflows /health endpoint to verify it is up and running For the Scheduler we have a custom script that says the Scheduler needs to be restarted if there are more than 0 queued tasks 0 running tasks and 0 tasks completely recentlyLiveness are nice for saving someone from a simple fix but theyre not really monitoring For that the core Airflow project is heavily instrumented with statsd metrics We send all of these to DataDog and use their dashboards to tell us about Airflows CPU and memory usage Additionally we have several DataDog monitors setup there that alert the team if key DAGs havent reported success in the expected time period Airflow has a SLA feature that does something similar but this allows us to decouple monitoring from the serviceOriginally published at https://adamboscarinome on October 21 2019"
82wbSQUuG28aXtWkKEMtLn,Here I have covered all the Spark SQL APIs by which you can read and write data from and to HDFS and local filesSample data is available here
iqM8orjvCoJNavh9kCfgyK,Welcome back from the holiday everybody! As usual if you want to work with me as an engineer or sales person click hereLeap seconds and NTP syncs are a form of dirty read: Two salutary reminders that machine learning and things technology have run ahead of security: Voice commands can be disguised from human listeners but clearly audible to devices This creates a fun threat model where TV broadcasts (as above) or any other audio source can perform voice commands without people realising
HSsv25JAxAxfrHy7PayEn5,The BigQuery team rolled out support for geography type a while ago and they have never stopped improving performances and GIS (Geographic Information System) functions This allows users to run complex geo-spatial analytics directly in BigQuery harnessing all its power simplicity and reliabilityHold on your keyboard (or your screen if you are reading this on a mobile device)Now you can cluster tables using a geography columnThis is game changing for users working heavily with geodata By clustering your table on a geography column BigQuery can reduce the amount of data that needs to read to serve the query This makes queries cheaper and run faster when filtering on clustering columnLets see the benefits of clustering table using geography column with an example We will use one of the great public datasets curated by Felipe Hoffa We will use the weather_gsod specifically the two tables all and all_geoclusteredLets say that we want the all-time minimum and maximum temperature within the Greater London area for each station Our query will look like this: Results: This query reads 902GB of data Now lets see how the same query perform on the clustered table: The result is obviously the same but this time BigQuery reads just 98So switching to the geo clustered table made this query almost 100 times cheaperDo you want to try other locations and test the differences yourself? You can use Huq Industries GisMap tool to quickly draw and export polygons in various formatsAuthors Github and Twitter
gQ9DnARaQFogDtsQwyL5Dm,BigQuery supports the * wildcard to reference multiple tables or files You can leverage this feature to load extract and query data across multiple sources destinations and tables Lets see what you can do with wildcards with some examplesThe first thing is definitely loading the data into BigQuery If you deal with a very large amount of data you will have most likely tens of thousands of files coming from a data pipelines that you want to load into BigQuery Using wildcards you can easily load data from different files into a single tableAlso this is not limited to only one prefix but you can specify multiple ones for example: The command above will load all the files matching all the prefixes into the specified tableWildcards can be used in the other direction too Namely they can be used to export data from BigQuery to GCS This is very useful especially because BigQuery limits exports to a single file only to tables smaller than 1GBThe previous command will result in multiple files exported into the my_data bucket within the prefix extract/prefix/ and all file names will be: The other very useful use of wildcards is evident in queries In fact you can reference multiple tables in a single query by using * to match all the table into the dataset with the same prefix For example consider you have a collection of tables like: The following query will return the count per day per country of events of type submit in our datasetYou can also filter out matched tables using _table_suffix in the where clause For example if you are only interested in Germany France and Japan just run the following: What I personally like the most of using wildcards is that it enables me to design better simpler and more generic analytics queries as well as ETL jobs The aim of this post was to help you improve your code quality and your productivityIf you believe you learnt something new or if you liked the post please ClapAuthors Github and Twitter
CYNeuyw423WR8y6pLGVbB6,Google Cloud Professional Data Engineer Certification is a technical attestation that proves the ability to know and use Google Cloud Platforms data big data and machine learning componentsAfter taking my certification in February 2020 I want to share some suggestions and tips that can be useful to achieve this goalIve prepared the exam from scratch with a background knowledge of data management and big data ecosystems (RDBMS data warehousing business intelligence big data Hadoop stack columnar databases …) but no expertise in machine learningFor me the most challenging aspect was to find the right resources with the good balance between the level of detail vs study effortThe following list of resources is based only on my personal experience and fits very well if youve already worked in the data and big data ecosystem The order of the items reflects the chronological order of my journeyOther resources that Ive heard about but I didnt use: Its very useful to take practice exams to identify gaps in your preparation At the end of each answer you can find useful links to the most important pages of the GCP official documentationThis is a (not exhaustive) list of topics that I found multiple times during my study: Data Catalog and Data Fusion should be in scope for the latest version of the exam but I didnt find any question about these new topicsDISCLAIMER: opinions expressed are solely my own and do not express the views or opinions of my current (or past) employers
6ki3oGYv5vjfFXoGeWVXKN,We are currently migrating our data warehouse to a new cluster which is GDPR compliant Historically there hasnt been a migration strategy in place so we dont have a source of truth for how our schemas are structuredHere is a little query that generates the DDL (Data Definition Language) statements for a whole schemaBefore you run the query create this view
nyqV5kz7hzCTAkPdqSnrJA,Ive been a data engineer for almost 5 years Nonetheless I wanted to write this article because we see a lot of articles about data trends or data science trends but not so many focused on my role: the data engineerHowever being a software engineer implies following up the software development trendsand being specialized in data shows the need of keeping up with data architecture paradigms and landscape Hence in the first and the second section of this article Ill focus on this type of trendsThe Data Engineer is creating value by delivering data analytics to different systems or by making the data scientists/analysts more productive thanks to industrialization and technical knowledge over the data platforms Thus the third and the last section of this article will refer to data use cases implications in data engineering
F3fPXhL6bcKtQtoxcKz22G,Explain like Im 5 (ELI5) comes from the popular concept that if you really know something well you can explain it simply so a five year old can understand Ill try to do an ELI5 about Slowly Changing Dimension (SCD) to prove I know it well Also this post has the added benefit of preparing study material for five year olds who want to become data engineersSCD is a concept in data warehousing that aims to solve how to store information about a dimension as it changes over time The rate of change is considered slow although I dont know if theres a percise function for calculating its speedMy nephew just turned five He loves dinosaurs so Ill pretend Im explaining SCDs to him His name is Rhys (pronounced Reese)So Rhys a dimension is just a fancy word for a person place or thing It can be a thing that you can touch or a thing you cant touch You can touch your stuffed animals theyre things but a party is also a thing but you cant touch it If you arent sure if something is a thing try to put an a in front of it: a stuffed animal a party a car a houseSo a stuffed dinosaur is a dimension because its a thing But we have lots of stuffed dinosaurs so lets say we wanted to list them We may give them numbers (surrogate keys) like 0123 and 4 but these dont mean anything about the animal itself Its just so we can quickly identify them Also if we ever want to give the two dinosaurs the same name their numbers would distinguish them1So we list our animals! We record their number and some stuff about them (attributes) Lets record their name and their description: But sometimes we may want to change our list We may get new animals as presents! Or we may rename one We would want our list to be up to date right? Adding a new animal is easy we can just add it to the bottom: Easy! But what if we want to rename one of our dinosaurs? That happens doesnt it? Well we have a few options of how to do this: Type 1  Lets call this way Type 1 In this way we make a whole new list with the updated info So if we want to rename Brachie to Bronchy we would pull out a new piece of paper and write: Awesome! Only now if someone asked us about Brachie we might not remember which one that used to be or when we changed his name We dont have any of his old data If we lost a dinosaur and made a new list we may forget that dinosaur ever existedType 2Maybe we want to cross out the Brachie line lightly so we can still read his old name and also write down the date that we crossed it out That way we can see his old name but we know its not real anymore Instead of crossing it out lets put an end date to show where we would have crossed it out then we can add his new information under it (with a new fake number) Lets also record when we got them as a start date! Like this: I know it looks like you have 5 dinosaurs when you really dont but thats okay You just have to look at all the rows where there is no date changed dateType 3We have all the information we want recorded but I guess it looks like you have five dinosaurs when you dont and if people think you have more than you really do they might stop buying you gifts I get thatI personally favor Type 2 best Do you? I get that but I really think your Auntie Katie will buy you stuffed animals no matter how many are on your listOne of many sources potentially on the web for SCD: http://wwworaclecom/webfolder/technetwork/tutorials/obe/db/10g/r2/owb/owb10gr2_gs/owb/lesson3/slowlychangingdimensionsOriginally published at wwwalisa-intech on July 18 2018
av72RgAmUc3Vqi2mm3WN8Y,This post is for my friend Mitch who despite his persistent belief that he annoys me with his questions has reminded me why its important that we stay hungry for theoretical knowledge even as we get caught up in our day-to-day workNormal forms are properties one can apply to a database to normalize it Does that not help? I felt like it helped but then I re-read it and I realized it was meaningless
nqGTHQJASC3ZiHPye2DhWT,Message queues play an important role in software architecture Thanks to them we can easily decouple data acquisition from data processing and thus help to improve availability reliability and scalability of a software system In this article I will show you how to implement asynchronous processing of HTTP requests with RabbitMQ and Spring FrameworkFirst lets create a project and add all required dependenciesFor that we use SpringInitializr web application which facilitates the creation of our Maven project by generating project files with chosen dependencies In our case these are: And also with additional dependency which speeds up development process: Now lets configure RabbitMQ and our client applicationWe initialize topic exchange named devpeer and queue named signupsWe bind the queue to the routing key pattern signup# This way every message sent to the exchange with a routing key starting with signup will be routed to message queue signupsIn this example fanout exchange would be absolutely fine and would even have better performance (no routing mechanism) but I would usually go with topic exchange by default because it gives more flexibility in routing messages So if you would like to have multiple queues in an exchange and have precise control over which message will go to which queue then choose topic exchange On the other hand if performance is an important factor then consider fanout exchange (or at least direct exchange)If you would like to learn more about types of exchanges then I recommend reading the following article: We also set RabbitMQ server URI (we can also set different parts separately and put it in applicationproperties) and configure RabbitMQ message serialization and deserialization to support JSON formatIn Spring creating a HTTP server is as easy as creating a single class The class needs to be annotated with @RestController and its methods can handle requests to specified paths with specified HTTP methodsWe create a POST method handler mapped to path /api/v1/signup which expects JSON body with user signup information containing full name and phone numberA request will respond with 400 Bad Request HTTP error code whenever a full name field or phone number is missing from the request bodyIf both fields are present then data will be sent to a message queue in a specified exchange using a routing key Finally HTTP code 202 Accepted will be sent in a responseMessage is consumed by a method annotated with @RabbitListener in a class annotated with @Component By default it listens for messages in a single dedicated thread but the number of consumers can be increased using the concurrency parameterThe consumer method will process message and print results to standard outputIn our example both HTTP server and RabbitMQ consumer are running within the same application but in real-life situations it is usually much better to have them in separate processes in order to be able to scale them independently
8Up2kqtEjEivMRMVcbHvr4,As a Data Engineer we are constantly working on developing data pipelines using various tools  frameworks etc which can deliver data fast and efficiently Data pipelines involve heavy lift and shift operations with the data from one source to another  for the same purpose I use Apache NiFi which is an open source JAVA based software product from Apache Foundation which automate the flow of data between software systemsA NiFi Processor is the basic building block for creating an Apache NiFi data flow Processors provide an interface through which NiFi provides access to a flow-file its attributes and its content NiFi also provides capability to create your custom processor which can provides a way to perform different operations or to transform flow-file content according to objectiveCatalog File : https://repomavenapacheorg/maven2/archetype-catalogAbove step will load all the remote archetypes of maven repository when we will create a new maven project with NiFi archetype Select Maven Catalog from the drop down and filter string as NiFi and select nifi-processor-bundle-archetype •  Provide groupId  ArtifactId in the next step  You can provide your standard groupId and artifact name I have added for sample onlyThen go to Myprocessor project and run mvn : clean install •  A nar file will be created in the Myprocessor basename directory with the extention of nar  •  Copy the nar file into the lib directory of Apache Nifi and restart the application By above steps you will be able to see the processor in the Nifi UI  You can add custom code for any specific source based on your scenario or you can modify the existing processor code as well If you face any issues please dont forget to mention in comments and if this post helped you please share a clap for me 
6jK7bzNQaf7hvuPBxbRaDG,As a data engineer you always have confusion on Data Warehouse Lake and Pond What are they and the most important why there are three or many of them? Do we really need all of them ? can I make use of one and get the advantages of all? The answer is yes !! if you have a small teamBefore we feed data to the machine learning algorithm we need to take into consideration how valuable clean and reliable our data is Data can be clean but it is not valuable it can be valuable but not reliable and when it is reliable it is not clean This is a short tale of every client I have worked withEvery company wants to be ahead in technology than their competitor in the same way we like to flaunt our cars in front of our neighbors However just buying the best car in the market does not make you the fastest driver in the town right! likewise getting the best tech stack for your company does not mean you will get a better insight into your data if you have poor data architectureThe right match of the best tech stack data architecture and agile working can surely make you crack the puzzle of insight in your dataIf you would have asked the same question to me before the cloud revolution I would have said getting their faster with minimum cost is near to impossibleIn the on-prem big data era compute and storage were running like a linear equation if compute needs to be increased we have to add another machine with some storage and vice versa Then came the cloud technology which did a brilliant job to separate these two layers where both compute and storage can scale independentlyNow data can be on cloud storage and rest there as long as needed and no dollar will be spent on computing this created great opportunity to handle big data ingest data with numerous capabilities structured data semi-structured data and unstructured data ingestion techniques also changed as we are going serverless with the advancement in batch and streaming data ingestionmore on ingestion in the next blog now lets talk about Data lake pond and warehouseWith the cloud revolution creating and maintaining data lake is quite easy spoiler alert !! blog coming up on AWS data lake formation A data lake is a place where we can ingest and store all data from our organization now you might ask why would someone in their complete sense do that dont we have source system from which we can fetch data whenever we want for our teamThis increasing demand for changing data in all dimensions can be easily handled by a data lake A data lake can ingest data with changing schema a concept called Schema on reading Data lake knows how to handle when columns are dropped and added without breaking workflows It can ingest all types of data whether it is batch incremental or streaming of structured semi-structured and unstructured datain general Data lake consists of three tiers which can be called ponds as wellTier 1: RawHere data is kept as it is data from all sources with different date format file format timezone and reference is ingested Schema and structure are not taken into consideration at this point in time Our main goal here is to ingest as much data as possible by creating a plug and play pipelinesTier 2: In Tier 2 data is cleaned and converted into a more structured unified format with reliable file formats like parquet/orc more cleansing is done on data with respect to data quality and data is synced with master and reference data to have a single source of truth for complete organizationTier 3: This is the most important part of Data lake in this stage ABTs (analytical business tables) are created according to business needs this data can be consumed by various business departments and teams with proper governance and access which is another quality of data lake providing granular access to users and groupsStrict schema dataset/table can be created for Data Warehouse to provide smooth operation another advantage is agile changes if Data Warehouse needs another column for its BI operation we just need to add that column from relevant dataset since that extra column is already ingested in our data lake isnt that fascinating! I still remember days when our team used to struggle ingesting more data for our Data WarehouseData Lake also supports data catalog which makes it exclusive to have a single search platform for all your data with data lineage and stats Support of data catalog allows other systems to connect with data lake easily which means more flexibility to Data Analysts Data Scientists and Data nerdsData pond as described above reside in tiers and can differ from company to company raw tier can be a data pond that consists of all raw data In tier 3 there can be many ponds for example there can an analytics pond which only has analytics data another data pond consists of anonymized data which can provide static and dynamic anonymized data Data pond can also be created for specific business teams which provide them with required ABTsEach Tier and Pond can have their specific data retention policy and can be archived accordingly This reduces the storage cost and increases the compliance as per data type (sensitive/general purpose)Data Warehouse is a hub for all data present in the organization required for business analytics to create reports and dashboards It contains all historical data that is again compliant with regional data protection rules and optimized to run queries on big data Nowadays most of the Data warehouses are equipped with MPP (massively parallel processing) and some of them are serverless which is a great way to save money and energyData Warehouse is capable of maintaining history even if the source system does not maintain it due to their insufficient capabilities DWH is for business personnel hence it contains most of the business logic which is provided by enterprise data architects in close collaboration with stakeholdersRestructuring of data to deliver excellent query performance without impacting operations was the main task of DWH data engineer which used to take plenty of knowledge and time with the old enemy bad documentation it can increase exponentiallyModern Data Warehouses are smart and can optimize according to queries running on them this is the solution for the problem stated above
jSAA7a3nyBEvfbeU7FBhom,In the world of distributed computing we are bound to write efficient programs to reduce the latency which can be achieved by following certain best practises Lets see how data partitioning reduces latency and improves parallelismWe can see the difference in the time taken by tasks like Reading 1 MB sequentially from memory whereas reading the same from the disk/SSD and while sending bytes across networkNow that we have a better intution about the latency lets relate Whats the link between latency and partitioning and how they are connectedSimply put the data within an RDD is split into many partitions and sent across the worker nodesNote that the data partition never span across the nodesBy default when a spark job starts the number of partitions is equal to the total number of cores on all executor nodesSo for example if all of the machines in your cluster have 4 cores and you have 6 worker nodes then that means the default number of partitions you can start with may be 24Spark reads data from HDFS and number of partitions will be determined by numbers of input split for the file in HDFS If there are N input splits then there will be N partitions if we are reading a textfile the the input splits will be set by TextInputFormatMy Input file size is about 1145 GB which is 114500MB divided by 32 (fslocalblocksize default value is 32 MB)) gives around 3579 partitions But if the lines in your file is too long (longer than the block size) amount of partitions would be smallerUses Javas ObjecthashCode method to determine the partition as partition = keyhashCode() % numPartitionsWe will go with an example of some tuples (data) and convert it to RDD (hashpartrdd) and then partition itWe can mention the type of partition we want on the data by partitionBy() method The number of partitions we want can be done by passing it inside the HashPartitioner() methodNote that partitionBy() is a transformation so it always returns a new RDD Therefore it is important to persist otherwise partitioning is repeatedly applied which involves shuffling each time the RDD is usedHash partitioning tries to spread around the data as evenly as possible over all of the partitions based on the keys Here we can see the data entering the partions as belowLets assume this in a distributed system then the data on partition 0 1 2 3 are sent to worker nodes 1 2 3 4If we want to group the elements by key across the worker nodes then we will be calling the groupByKey methodNotice that above groupByKey method actually groups the data over the network across the worker nodes (shuffling) This paves way to more time taken to compute the resulting grouped RDDTo eliminate the above shuffling across the network we can use Range paritition where the keys of our Pair RDDs can be segregated within certain ranges Lets consider the previous example dataRangePartitioner will sort the records based on the key and then it will divide the records into a number of partitions based on the given valueimport org  apache  spark We can notice that the range partitioned data is more skewed while the hash partitioned data is more evenly distributed across the nodes yet the compute time is less for range partitioner because it eliminates the data shuffle between nodesSparks Java and Python APIs benefit from partitioning in the sameway as the Scala API However in Python you cannot pass a HashPartitioner object to partitionBy; instead you just pass the number of partitions desired (eg rddpartitionBy(100))we have defined a class named CustomPartitoner lets partition our data using this classThe custom partitioner has partitioned as belowThus we can customize the condition on which the partition of data happensLets see how sortByKey invokes partitioningNotice that map and flatMap function isnt listed above but mapValues and flatMapValues are This is because the map and flatMap functions have the ability to change the keys of the RDD hence has the power to change the partitioned RDDNOTE : Before using a map or Flatmap over a large transformed RDD its good to cache that RDD Partitioning will not be helpful in all applications for example if a given RDD is scanned only once there is no point in partitioning it in advance It is useful only when a dataset is reused multiple times in key-oriented operations such as joins
D3zWd2n93wiRw2LUDnobDR,Spark is a fast and general-purpose cluster computing system for real-time processing It was developed at the AMPLab at UC Berkeley in 2009 and later donated and open-sourced by Apache It has a thriving open-source community and is the most active Apache project at the momentUnlike Hadoop Spark uses RAM for processing data and this makes it 100x faster than that of HadoopSpark provides for lots of instructions that are a higher level of abstraction than what MapReduce provided Currently spark gives support for Scala Python R and Java API to program and as well can be integrated with Yarn Mesos Kafka Cassandra Hbase MongoDB Amazon S3Computation in Spark doesnt start unless an action is invoked This is called as Lazy Evaluation and this makes spark faster and resourceful Spark adds them to DAG (Directed Acyclic Graph) of computation and only when the driver requests some data does this DAG actually gets executedSpark uses master/slave architecture one master node and one or many slave worker nodesHere Driver is the central coordinator that runs on master node or name node and executors are on the worker nodes or data nodes that are distributedSpark has its own standalone cluster manager to run the spark applications it also supports other cluster managers like YARN Mesos etcSpark uses RDD (Resilient Distributed Dataset) which is a fault-tolerant collection of elements that can be operated in parallel They are immutable in natureSpark translates the RDD transformations into DAG and starts the execution When an action is called the DAG is submitted to the DAG schedulerDAG scheduler divides operators into Stages and each Stages are comprised of units of work called as TasksStages are passed on to the Task Scheduler The task scheduler launches the tasks via cluster managerThe executors then executes the tasks on the worker nodesAfter Spark 20 the entry point of spark is Spark Session Spark enables its users to create as many sessions as possible for the SparkPrior Spark 20 Spark Context was the entry point of any spark application and used to access all spark features and needed a sparkConf which had all the cluster configurations and parameters to create a Spark Context objectSpark session can be created using the builder patternThe spark session builder will try to get a spark session if there is one already created (in case of spark shell or databricks ) or create a new one and assigns the newly created SparkSession as the global defaultInside the spark session we can get to create the Spark context and create our RDD objectsSpark gives a straight forward API to create a new session which shares the same spark context sparknewSession() creates a new spark session objectBefore implementing Spark session for Hive or Sql has integrations with spark seperate SparkContext was needed to be created for each of them To avoid this Spark Session has been created with well defined APIs for most commonly used componentsOnce we apply transformations to the RDDs we create an RDD lineageWhen we apply transformations on an existing RDD it creates a new child RDD and this Child RDD carries a pointer to the Parent RDD along with the metadata about what type of relationship it has with the parent RDDThese dependencies are logged as a graph which is called as RDD lineage or RDD dependency graphYou can learn about a RDD lineage graph using RDDtoDebugString method which gives an output as belowWe can see the RDDs created at each transformation for this wordcount exampleA lineage will keep track of what all transformations has to be applied on that RDD including the location from where it has to read the data It is also called as Logical execution planThis RDD lineage is used to recompute the data if there are any faults as it contains the pattern of the computation thus the resilience and the fault tolerance of SparkDAG Scheduler is the scheduling layer of Apache Spark that implements stage-oriented scheduling It transforms a logical execution plan to a physical execution plan (using stages)We will cover the how the Physical plan is created in this blog other two will be discussed in the upcoming blog seriesDAGScheduler uses event queue architecture to process incoming events which is implemented by the DAGSchedulerEventProcessLoop class The purpose of the DAGSchedulerEventProcessLoop is to have a separate thread to process events asynchronously and serially ie one by one and let DAGScheduler do its work on the main threadWhenever an action is called over an RDD it is submitted as an event of type DAGSchedulerEvent by the Spark Context to DAGScheduler It is submitted as a JobSubmitted caseThe first thing done by DAGScheduler is to create a ResultStage which will provide the result of the spark job which is submittedNow to execute the submitted job we need to find out on which operation our RDD is based on So backtracking beginsIn backtracking we find the current operation and the type of RDD it createsDAG scheduler creates a shuffle boundary when it encounters Shuffle dependency or Wide transformation and creates a new stage This new stages output will be the input to our ResultStageIf we find another shuffle operation happening then again a new shuffleMapStage will be created and will be placed before the current stage (also a ShuffleMapStage) and the newly created shuffleMapStage will provide an input to the current shuffleMapStageHence all the intermediate stages will be ShuffleMapStages and the last one will always be a ResultStageThe DAG scheduler pipelines operators together Narrow transformations on RDD produces RDDs without Shuffle dependency that are collected together as a single stage by the DAG schedulerDAG also determines the execution order of stages Execution order is accomplished while building DAG Spark can understand what part of your pipeline can run in parallel It optimises minimal stages to run the Job or actionThe final result of a DAG scheduler is a set of stages and it hands over the stage to Task Scheduler for its execution which will do the rest of the computationStages consists of set of tasks which are given as input to the Task scheduler When a Spark application starts TaskSchedulerImpl with a SchedulerBackend and DAGScheduler are created and startedTaskSchedulerImpl is the default task scheduler in Spark that generates tasks It uses the SchedulerBackend which schedules tasks on a cluster manager It also uses the SchedulableBuilder to build and manage the pool submitTasks registers a new TaskSetManager (for the given TaskSet ) and requests the SchedulerBackendend to handle resource allocation offers (from the scheduling system) using reviveOffers methodsubmitTasks requests the SchedulableBuilder to submit the task from TaskSetManager to the schedulable poolAny task either finishes succesfully or fails TaskSetManager gets notified and also has the power to abort a TaskSet if the number of failures of task is greater than that of sparktaskMaxfailures Tasks are then scheduled to the acquired Executors according to resources and locality constraints They live in Worker nodes or slave to execute the tasksThe ExecutorBackend uses launchTask to send tasks to Executor to get executed where the TaskRunner manages to do the execution of a single TaskThe ShuffleMap Task involves with the shuffling of data and the steps involved with itCurrently there are three shuffle writers as mentioned below • Shuffle Reader : Fetches data from the buckets • Shuffle Block Manager : Manages the mapping of data between the buckets and the data blocks written in diskWhen executed a ShuffleMapStage saves map output files using BlockManager from Mapper(MapTask) to buckets via Shuffle Writer that can later be fetched by Shuffle Reader and given to Reducer(ReduceTask)ShuffleMapStage uses outputLocsand _numAvailableOutputs internal registries to track how many shuffle map outputs are availableAll intermediate stages will be ShuffleMapStages that are computed will end up with a ResultStage finally that will be computed by TaskRunner and the computed result is sent back to the Driver to display to User
Jw9NnNWQVY3MsHMHCasR3H,Its important to understand how where and when the data is generated and whats its significance to your organisation at different instance of time from its origin so whats data timeline and how to architect data timeline and hows its different from data pipelineBuilding data timeline is more intelligent and holistic process would require very thorough understanding of the data who is going to consume at what time horizon from time of origin of the data The data has usability level on the timeline and as it move and accelerate  matures over time horizon to right it benefits data operators  data analyst and data scientist in your organisation As its moves to end of lifecycle  it might be just less useful  least accessed and has different needs to store / archive and different analytics use-cases attached to itAs we mature the data over its time horizon its important to classify  tag  secure and harness your data for further use The intelligent data timeline have less duplication data  better interfaces to engage analytical tool and processing enginesThe back-bone of data timeline architecture is a solid designed data infrastructure which suits and necessity of data storage at time particular interval of time and analyse it by various operators and functionsThe data timeline architecture diagram is high-level holistic view of data flow and defining above mentioned attributes  The low level design who will involve designing data pipelines branched from data timeline and provide different business perspective of data to user This can be viewed as very rich intelligent design process laid out to support your organisation data analytics and decision making needs for any kind of dataWe will discuss each timeline horizon and what are reference data architectures which suits the use cases related to those time horizon in Part-II
3h4ppZCrFJAWQVDvmZpe4g,Use case: Dropped a table using phoenix command line or recovering table using HFILES • Once the table is dropped from Phoenix it is vanished from Hbase and the data is gone with it But you will find the dropped tables HFILE in hbase archival directorySample HFILE looks like: 3 Currently I have only one regionInside the region 6666cc0195ea49e00c9a32e64252477f we can see the columnFamily family directory • Our aim is to load this HFILE into table Considering we have huge amount of data and multiple Hbase regions we need to create table with pre-splits We can avoid creating pre-splitted tables but it will take days to load the files into the new table as HFILE loader will scan each of the rowKey and assign it to a region • Based on you rowKey design you can figure out the region split keys One way is to look into the <dataFile> on each region to get the rowKeysvoldybot_ can be my first split key Similarly for the other region data files I got summerbot_ companyName_etc • Create a table with predefined splits: 7 Verify the newly created table with pre-splits on hbase UI • Now loading will be faster as we have the regions available Region splits dont have to be perfect based on the load Hbase will perform minor compaction and it will split regions if necessaryLoading HFILES using incremental loadThis will load the 6666cc0195ea49e00c9a32e64252477f region data into hbase We can repeat this for all regions Even manually copying the archive directory HFILEs into appropriate Hbase region is an optionWe manually copied 7b51be36f2ee4e71bf038742b8a25d3d (rowKey:voldybot_)into HBASEs 5199df549d7dd6f2d6dea2657c7a0c8b (region with start key voldybot_)region • Perform disable enable and verify the data
HKfwarYGUBxbyiydyxfAvE,In this new Data world companies are hiring new profiles to turn their customers data into insights It can be a customer analyst a statistician or a data scientist Their role is to better understand the customers and to divide them into meaningful groups They come up with innovative segmentations usually based on purchasing behaviour RFM churn or lifecycle stagesIn this article we will discuss how to activate this data on the two main paid marketing platforms: Facebook and AdwordsYou have a model ready to use in R Python or SQL? The key is to successfully deploy your modelThe more identifiers you have for one customer the better your targeting is: Do not neglect your mobile applications their identifiers are the most powerful when it comes to advertising Start collecting the mobile ids: AAID for Android and IDFA for Apple and add them to your user profilesWe all know that your most engaged customers who represent a small percentage of your user database are generating most of your revenueA custom audience is a list containing encrypted identifiers that you can upload to Facebook and Adwords Their platforms are matching your list with their own users and you can use these audiences as a target for your adsThe next step is to push your data: segmentation and identifiers to Facebook and Adwords custom audiences There are multiple ways to do so and the fastest is to upload CSV files directly to their platformsUsing Facebook and Adwords SDK you can push automatically your user segments to custom audiences See the code below: Make sure your audiences are up to dateTo keep an audience up to date you have to regularly remove the users that do not belong to the segment anymore ideally everyday The code below: Now you have all the ingredients to run successful paid ads and focus on your strategy
Krk8ug6C6y8TeFcGSpjHr6,To process data in AWS Glue ETL DataFrame or DynamicFrame is required A DataFrame is similar to a table and supports functional-style (map/reduce/filter/etc) along with SQL operations The AWS Glue DynamicFrame is similar to DataFrame except that each record is self-describing so no schema is required initially It computes a schema on-the-fly when required and explicitly encodes schema inconsistencies using a choice (or union) typeDynamicFrame can be created using the below options : This post elaborates on the steps needed to access cross account AWS Glue catalog to create the DynamicFrames using create_dynamic_frame_from_catalog optionAccount A  AWS Glue ETL execution accountAccount B  Data stored in S3 and cataloged in AWS GlueIn the above code datasource0 is the DynamicFrame created by reading the data from marvel_superheroes table under marvel database from another AWS account mentioned in catalog_id parameterTo conclude DynamicFrames in AWS Glue ETL can be created by reading the data from cross-account Glue catalog with the correctly defined IAM permissions and policies
9meHj2xttq6fFRSzdToN84,AWS Glue is a fully managed ETL service to load large amounts of datasets from various sources for analytics and data processing with Apache Spark ETL jobsIn this post I will discuss the use of AWS Glue Job Bookmarks feature in the following architectureAWS Glue Job Bookmarks help Glue maintain state information of the ETL job and helps process new data when rerunning on a scheduled interval preventing the reprocess of old dataIn a nutshell Job bookmarks are used by AWS Glue jobs to process incremental data since the last job run avoiding duplicate processingIn the above architecture Kinesis Data Firehose streams events data to S3 bucket referred as raw data store based on buffer size or buffer interval configuration settings Supposing the condition of buffer interval set at 900 seconds is satisfied first it triggers data delivery to S3 every 15mins writing the data to the S3 destination prefix In Firehose the S3 destination Prefix is configurable and is optionalThe AWS Glue ETL job is triggered using Glue ETL trigger As AWS Glue job bookmark is enabled it processes the incremental data since the last successful run For S3 input sources AWS Glue job bookmarks check the last modified time of the objects to verify which objects need to be reprocessed If input source data has been modified since last job run the files are reprocessed when the job is run againYou can enable Job bookmark feature either while creating the job or later under Advanced propertiesDuring the job execution it skips partition if it is empty or the creation timestamp of object is older than the timestamp of the last successful job run as captured by job bookmarkGlue ETL job output a manifest file containing a list of processed files per path The manifest file is stored in the temporary location specified with the job The default location is s3://aws-glue-temporary-<account_id>-<region>/admin/partitionlisting/<job_name>/<job_run_id>/datasource<n>input-filesSome of the backfilling scenarios can be supported using the Rewind Job Bookmark to any previous job run resulting in the subsequent job run reprocessing data only from the bookmarked job runNow for Job bookmark feature to work as expected with S3 input source make sure of the following:: To conclude based on the use-case AWS Glue Job bookmark feature can be very useful as it helps process incremental data from S3 and relation database systems and also supports data backfilling scenarios betterReference : https://docsawsamazoncom/glue/latest/dg/monitor-continuations
WprS2do7jna9T3SZKybGPA,I had a use case to read data (few columns) from parquet file stored in S3 and write to DynamoDB table every time a file was uploaded Thinking to use AWS Lambda I was looking at options of how to read parquet files within lambda until I stumbled upon AWS Data WranglerAn open-source Python package that extends the power of Pandas library to AWS connecting DataFrames and AWS data related services (Amazon Redshift AWS Glue Amazon Athena Amazon EMR etc)Built on top of other open-source projects like Pandas Apache Arrow Boto3 s3fs SQLAlchemy Psycopg2 and PyMySQL it offers abstracted functions to execute usual ETL tasks like load/unload data from Data Lakes Data Warehouses and DatabasesDestination : Record stored in Dynamodb table It consist of ticker_symbol sector and last_updated (in epoch time) data from parquet file  with extra attribute location specifying the S3 file containing the particular record and source with default value of system • Created Lambda layer for AWS Data Wrangler • Created the function code with few highlights • Added S3 Event type: ObjectCreated as trigger So every time a new file is uploaded to S3 the trigger gets fired invoking the lambda function to read the parquet file and write the data to dynamodb table
UVY85c26PT223vQsmedMhm,In this blog post I will list down the steps required to setup the AWS Glue job to scan the dynamodb table in another account In my setup I scan the dynamodb table in Account A (us-west-2) perform glue transformations in Account B (us-east-1) and write it to S3 in Account BAccount A  Dynamodb tableAccount B  AWS Glue Job S3 Bucket • Create an IAM role in Account B (us-east-1) with Glue trusted entity attach AWSGlueServiceRole service policy and attach another policy allowing sts and s3 action Make note of the role arnarn:aws:iam::xxxxxxxxxxxx:role/ddb-to-s3-glue-role • Create IAM role in Account A (us-east-1) where Dynamodb table resides to allow Scan actionMake sure to modify the Trust relationship allowing the role created in step 1 to allow assume role and the access conditions for the role • Create AWS Glue job in Account B (us-east-1) Choose the IAM role created in step 1 as the role to be assumed by the job • Create Glue script to scan the tableReference -https://martinapugliesegithub
ZkAGq6FKbFb8br6pAKyqWY,AWS recently announced Amazon RDS Snapshot Export to S3 feature wherein you can now export Amazon Relational Database Service (Amazon RDS) or Amazon Aurora snapshots to Amazon S3 as Apache Parquet an efficient open columnar storage format for analyticsI had a use-case to refresh Athena tables daily with full data set in Account B(us-east-1) from Aurora MySQL database running under Private subnet in Account A (us-west-2) The two solutions I could think of was : I used this new feature with cross-region replication enabled for S3 bucket to replicate the data to S3 bucket in Account BIn this post I will go through the steps to have the data into staging bucket of Account B and few issues I faced during this setup : 1 Setup Cross-Region replication between the Source S3 bucket in Account A(us-west-2) and Destination Bucket in Account B(us-east-1)I created a new S3 Bucket and navigated to Replication under Management tab For both the source and destination S3 buckets versioning needs to be enabled If you want to replicate objects encrypted with AWS KMS make sure to enable the check box under Replication criteriaYou will be asked to add both the required S3 bucket policy and KMS policy in to the destination account along with enabling Receive Objects in destination bucketYou would observe Layers (1) attached with the lambda function in the diagram above So what are AWS Lambda Layers? A layer is a ZIP archive that contains libraries a custom runtime or other dependenciesThe layer was added in lambda function to avoid below error : The error is because lambda used older version of boto3 and botocore To avoid this I had to add layer for boto3 11114 and botocore-11414I wrote a simple lambda function to get the latest system snapshot and invoke start_export_task method to initiate export to s3As I added TABLE_LIST to be environment variable it took some time to figure out how to provide list of table names In RDS console it is quite simple You can provide multiple table separated by space but the same was not working in lambda functionAfter spending lot of time looking over and over again into the IAM Role I figured out that the issue was in Resource where I had mentioned DB Cluster ARN Changing it to * resolved the issue Also make sure to have proper permissions for KMS (KMS ARN works)Once the objects (parquet files) were available in the staging bucket of Account B lambda function was triggered to initiate glue job to perform the required transformation write the data (as new parquet files) into final S3 bucket and update the location for Athena tables
HhxphweB3v4iBDhyYpixeg,For data automation and analyzing the business activities people use revenue intelligence and CRM Using data pipeline we can ingest and analyzes millions of data activities(such as emails mails or calls) on daily basis For analysis and exploration tasks most of the data stores were either in key value or relational databases with both offline and online which requires oversized clusters to store data This approach is more expensive and not scalable in long run and the performance was poor when reading the data Now offline and online storage was fully separated and using optimal technology for bothThe concept of data lake is hardly new and having multiple batch jobs analyzing the big data sets to build and offline storage Amazon S3 ad Apache parquet have proven great ft for both the requirements S3 storage is highly cost effective and offers great read throughput and parquet files allows up to query data efficientlyUsing amazon kinesis and API gate way we can able to stream the data Amazon kinesis will buffer the data or holds the data From amazon kinesis we inject data into lambda Lambda will process single data to firehouseHaving eCommerce platform it can be android (or) browser and we are injecting the data from amazon kinesis and API gatewayThe activity ingestion pipeline process the data through lambda then to firehouse and stores the data into S3 The raw data is stored in S3 in a JSON format We are taking mandatory fields required for payload in a JSON in a record Once we perform the valid record we can push them into data lakeS3 will always have one copy as of raw data as default Data Destination for data access into Data LakeThough as per functional use case data destination will get add in architecture aws has multiple option for storing of structure  semi-structure and unstructure data With managed services provided by cloud players like AWS  GCP its easy to set datapipline with less effort and pay as data scale At low maintenance cost it becomes highly efficient from small to big organization to setup data pipeline and scale as volume increaseBut raw data is no use until it is easily accessible and let data team play with Big Data to get insights from it In next article we will discuss how to use of raw data and how we to practice modern data warehousing over cloud
eAaskwMnA6cQBSadDBa3Et,Running Elasticseach at scale often requires performing performance comparisons for many reasons: Rally developed at Elastic becomes a swiss-army-knife for running suite of benchmarks for your Elasticsearch clustersA micro-benchmarking framework for Elasticsearch developed by Elastic Rally is extensible to run your own custom workloads along with a wide variety of built-in workloads Rally can setup and tear down new elasticsearch clusters for benchmarking It can maintain the data across the benchmarks to perform comparisons between different runsRally follows the racing terminologies: Rally provides a set of tracks that have their own indices templates data operations and schedules The available tracks are: Rally comes with multiple ways of installation using pip virtualenv docker and offline installs using downloadable tarball and setup scripts Head back Rally installation docs to setup Rally On successful installation and configuration you will see: Download and install Elasticsearch version you plan to use for benchmarking Rally can setup Elasticsearch cluster for you try using install start or stop options with esrally For this exercise we are going to use an existing cluster and run races against it with  pipeline=benchmark-only flag with esrallyIn this blog post we focus on comparing the results of two races using esrally compare One of the race is considered as a baseline to compare performance againstTo start the contender race the new version of Elasticsearch has been setupIn general if youre running Elasticsearch clusters at scale  Rally can be used to run multiple benchmarks with built-in or configurable benchmarks Rally maintains benchmark data across runs to make benchmarking runs easily comparable and presentableNotes: Output of rally commands have been omitted for brevity at multiple places Headline image credits: https://unsplashcom/
3CjMXgqAGWimKnSHzAkGKP,This article was co-authored by Andrew Mori and Vahe KoestlineWe do a lot with data at VideoAmp Our data is not just for use by data scientists but also data engineers developers and product owners Providing access to our data for all who needed it was difficult since we had many disparate data sets with different methods of querying the data Prior to using Dremio we had to setup a Jupyter notebook which was capable of joining these datasets in Spark across our HDFS Postgres CSV and other data setsThis required a fair amount of programming experience and was out of reach for our product folks who have SQL experience Our product team and others were reliant on team members with Spark skills to extract and transform data so that it could be used in a meaningful way We needed a way to provide a holistic view of our data and allow for greater ability to explore it This would also break down silos and bottlenecks across our organizationEnter DremioDremio caught our attention as it had many connectors that provide users with metadata across Hive RDS and ElasticSearch as well as provide views into file systems such as HDFS S3 and NAS It also provided a simple UI that would allow and manage tables data definitions queries and other aspects of what our team members would do with the dataAfter deciding to explore Dremio the next steps were to setup a proof-of-concept using the open source version of Dremio Luckily installation was as easy as running yum install of a rpm provided by Dremio The initial setup was running Dremio in standalone mode with the coordinator and worker running on the same box Configuration required providing a few Hadoop configuration files such as hive-sitexml and core-sitexmlAfter running start dremio service we were able to view the UI and connect Dremio to Hive S3 Hdfs and Postgres RDS instances Next we tested running queries outside of exploring the structure of Hive databases When test-running queries on Hive tables we ran into authentication issues looking into the docs it was required to setup impersonation for the user dremio We were able to get queries to work on Hive with one caveat being that the account running Dremio queries on Hive had to be user dremio It would not be until we productionalized of Dremio were we able to solve this but for the POC this was fineAfter testing standalone mode we wanted to test scaling out Dremio workers on Yarn Setup was also fairly painless with some issues around passing the correct S3 access / secret keys to allow Dremio to use S3 for storing job results downloads uploads etcThe biggest issues with setting up the POC for Dremio were holes in the documentation For example when dealing with the Dremio impersonation issues the documentation provided only worked partially and not for every user There was no concrete information within the Hive settings that needed to be updated to allow for access Another issue that would need to be addressed was tuning the coordinator process as default settings were causing frequent timeouts or process crashes Despite these issues we were able to fully test the POC to validate and establish buy-in with our key stakeholdersNow that we had established a viable POC with Dremio the next step was to productionalize it This involved a number of tasks including addressing stability disaster recovery scalability tooling data management and configuration managementThe architecture we decided on is below: Configuration management and our automated build process involved using AWS Opsworks and Chef recipes to manage the install and configuration of the Dremio tool There were also additional configurations that needed to be added to Hive to allow for the Dremio tool to be able to impersonate users when querying Hive datasets This was a difficult challenge as it required some custom configurations that were not documentedFor stability and disaster recovery we employed an active-standby with our Dremio Controller The controller configurations which include dataset and datasource configurations were stored on AWS EFS so that we could share the mount between the two controllers We added some additional tooling to allow the standby to automatically fail over In addition we also backup configurations to AWS S3 regularly to ensure any corruption of configs can be undone with a simple restore of configurationsAs for the query results we store the results on S3 and expunge result set after 7 days to ensure we are managing our data and bucket reasonably We also enabled the collection of metrics through Grafana and monitoring through Sensu to provide us visibility and the ability to quickly recover from any future incidentsWe employed Yarn to manage the scheduling and execution of queries across a cluster of servers Another challenge was determining the number of worker threads and performance tuning them to allow for maximum performance Our first iteration crashed regularly and had many long running queries that crashed This was due to the default JVM settings These settings were tuned to be able to manage a production load and we observed that previously long running queries were returning with results incredibly fast The JVM tuning was done as we rolled out a beta of the Dremio tool to our teams Their uses of the Dremio tool allowed us to monitor and get the performance tuned exactly how we wantedIn addition to the UI provided by Dremio we knew that a lot of our engineers like to use Jupyter Notebooks when accessing and querying our data From a usability perspective it made sense to give that same flexibility in Jupyter Dremio provided an ODBC driver which we installed and templatized so that our teams can use them in their running Jupyter instances providing a second way of accessing the data With this last piece in place the productionalizing of Dremio was complete and we were ready to make this available to everyoneThe biggest impact of Dremio was that we now have a unified place and an easy way to view our data Our production setup of Dremio pulls in data from RDS Hive HDFS S3 ElasticSearch and Jupyter This allows our customer service and product engineers additional access and the ability to query data that was not previously available to them This also allowed our data teams to query across multiple datasets in a simpler method using the SQL editor in Dremio instead of ETL jobs in Spark As a result our product managers are now empowered to answer key business questions without needing to rely on data engineers or operations teams to write or provide Spark ETL jobs The ability to store results in S3 also enables a simple way to create new datasets that can them be used in future queriesWe noticed the adoption rate quickly picked up across different teams and we constantly received feedback of how it has helped our team members in day-to-day operationsNow that we have Dremio were excited to see the future enhancements that will come with this product Some easy wins would be to increase available datasets including adding Google Cloud Storage which some of our clients use We also have a desire to explore its uses within the operations and devops world with respect to improving observability through efficiently querying against system performance metrics and application logs We also have a desire to hook this tool into BI machine learning and visualization tools that will give us better clarity on our data
2spqLmqoWCUEhyCMhB3Jwz,Im a Junior Developer working on the Data Engineering team at Clir The Data Engineering team builds infrastructure to help sift through data and organize it into a form that allows our domain experts to perform analyticsThe world of tech is fast-paced and constantly evolving It can be hard keeping up with all the new frameworks and language-specific changes while balancing all the obstacles life throws your wayFor this reason at Clir we encourage our employees to pursue their professional interests outside the scope of work through a professional development program The program can take the form of workshops online courses night school or even just a good old fashioned textbookThe benefit of the program is that work hours are designated to be used towards any of these modes of learning One of our core values at Clir is innovation which we embody by always learning new things and constantly challenging the way were currently doing things Professional development is just one way that Clir fosters innovationAn added benefit of professional development is the spread of knowledge throughout the teams At a tech company developers are constantly learning from each other Whether it be through code reviews paired programming or design meetings theres a plethora of knowledge sharing between members As a result any additional insight gained from a team member provides value for the whole groupAs part of Clirs professional development program I attended a week-long deep learning workshop The workshop provided me with many motivation experiences from speaking to researchers using time based convolutional neural networks to detect tumors in the human body to attending lectures where they went to great lengths about the vast capabilities that neural networks have Being around many brilliant minds is a very humbling but yet motivating experienceApart from figuring out that deep learning is indeed a complicated and broad subject to grasp in a week of lectures the workshop I attended left me with a few takeaways from my professional development experience
WhNyEoncMKwZoFHgjBawtD,If you are trying to design the analytics platform or a streaming data pipeline then considering todays technological move it is highly probable that you have decide to start with Apache Spark Because Spark is empowered to support multi-lingual development including Scala as its native language Java and Python It also includes a powerful machine learning library that can be incorporated into your platform if you have future plans to turn your platform into real time machine learning platformA typical spark application requires a spark-session dataframe RDDs datasets sql queries and then storing the query results into some data ware house or data lakeWe are going to discuss the scenario when you will have your own S3 service but not the public cloud object storage services like AWS or GCP In this case resolving the ip address mapped to your s3-domain name is the crucial partThe application start by creating a spark session reading a file into dataframe and then performing a simple count query on success code 200 The log file here is webLogcsv as you can seeresultshow() is nothing but a dataset<row> consisting of two string literals Staus and count generated from log file We do not just calculate the results but want to store them into a placeholder like S3 or CassandraWe are familiar with AWS SDK  which you can import into scala build tools or maven packages for your projects and you have those APIs available But unlike S3 service by AWS it is not as straightforward to write to something else using S3 API calls Why? Because you need to have good address resolution mechanism set up on your machine and also using AWS S3 makes sure that you are connecting the domain name like bucket-names3amazonawscomSo how do handle this situation? Here is the quickest solution : You need to create a s3 client but this time with your own s3 url and authentication keyss3ClientBingo! You are done You should be able to see your data/string/files written into the buckets of your S3 The key takeaways from this article are : In case youre wondering if you can use PutObjectRequest instance with your s3 client the answer is positive Because S3 is an object storage service developed not just by Amazon but many other vendors and many companies do not want their clients to break their head on just APIs AWS being the most widely used service out there the AWS apis are going to work for your service exactly in the same way they work for AWS services
mEbaxCMzwiKbyNV56MJgeJ,With the potentials of data being recognised in all fields software developers are required to increase their knowledge set in the field of technologies working with data Scaling from message broker (Apache Kafka Apache Pulsar) streaming technologies (Kafka stream Alpakka Spark Flink) to large scale data queries (AWS Athena Presto)In my experience as a software developer working with data requires skills to produce or consume data correctly to or via message brokers enabling other or your application to react store and analyse data without alarming lag As Kafka is the most prominent message broker I am going to focus on KafkaTo get started with Kafka as a software developer you need some best practice guide for configuration which can be easy to start wrong and difficult to correct later You can take a look at the extensive list of configurations Kafka provides But with a vast amount of configuration options it is easy to get lost and not know what are the most important configurations to focus onHere I am going to focus on the fundamental configurations you would require as a software developer to work successfully with Kafka This blog post is meant to provide insights into basic Kafka topic producer and consumer configurations for Software Developers starting with KafkaTopic is an abstraction layer for data production It is a category name where messages are produced Here we will look at some of the most important topic-level configurationsIn case you are not using any Kafka management UI you can use command-line tools to create a topic with correct parameters> bin/kafka-topics> bin/kafka-topicssh  zookeeper localhost:2181  alter  topic topic-name  config retentionIn case you set your retention to less than 7 days and your application depends on the logic that the messages are deleted within the time frame take a look at the topic level configuration `segmentms` You can check the following blog postYou can write data to Kafka using Producer API Here are some of the most important configuration to focus onWhile consuming from Kafka using Consumer API you should think about the following configuration
GP8oSSghAko6c8WAqQ7yXL,Through this post i am Going to explore and Contrast the Most used Databases Cassandra and MongoDb By experimenting two types of structured and unstructured Datasets Basically on the early stages we used the Sql type of databases to Deal with Structured dataThen by the evolution in the Advanced technology the Data structure get more complex and it tense to the unstructured Type of Data FormatThen No-Sql Databases came into big pictureSo Lets Stop From Here about more Theories and Lets Get our Hands DirtyYou Could Download The MongoDb From Here………… • Cassandraif you need to know about the installation MongoDb is easy but You can find some tutorials in web for Cassandra or else for Cassandra you can use a Cassandra docker containerNow we are ready with the installationsLets QueryThis Experiment will be Conducted By inserting the structured and unstructured data to both Databases and measuring the query timeNOTE : Below Structured data experiments are did using this dataset( Annual enterprise survey: 2017 financial year (provisional)  CSV)Also i am very Sorry For The variable Names That i use 😉First Make The Mongo up and Running…According to the above image the time consumed to insert data is 0532 seconds • The following queries were used to retrieve data from the databaseAbove query retrieve documents which contain Industry_aggregation as Level 4 using find() method and get the count using the aggregation function count()The time taken for the query performance is 00182 secondsAbove query retrieve documents in which the Variable_category starts with word combination Fina using regular expressions and the count is taken using the aggregation functionThe time taken for query performance is 003 secondsThe aggregation function group() was used to count the number of documents having Industry code as 99999 in 2017 after grouping the documents by year field According to the query performed 155 documents were identifiedCOPY annual(annual_idYearIndustry_aggregation_NZSIOCIndustry_code_NZSIOCIndustry_name_NZSIOCUnitsVariable_codeVariable_nameVariable_categoryValueIndustry_code_ANZSIC06) FROM annualHere the query execution time was identified as 3314 sec to insert the CSV to cassandra • The following queries were used to retrieve data from the databaseAbove query retrieve counts which contain Industry_aggregation as Level 4 using session prepared method and get the count using the aggregation function count()The time taken for the query performance is 020541 secondsAbove query is to retrieve results in which the Variable_category starts with word combination Fina using regular expressions and the count is taken using the aggregation functionAbove function is to count the number of documents having Industry code as 99999 in 2017 we have sed grouping the documents by year fieldOn the Part 2 of this Blog i will analyze Some more Interesting Queries with the Unstructured Dataset which is (Donald Trump TWEETS Kaggle )datasetFinally i will conclude all of this
e2a3vNEeKwwjjsYxd6sCa5,There are various amount information about big data architectures and softwares scattered around the internet In addition to this there are also similar tools which are doing to similar jobs and are created by different vendors However one can quickly be lost while deciding which tool is appropriate for the needs is going to be used for whatThis article is going to be focus mostly on Hadoop and Spark Alongside other technologies are going to be mentioned briefly in order to understand the power in the data technologyHadoop originated from need for crawling investigating and collecting the information of them into a database As a result Hadoops HDFS has borned for the need in massive distributed storage and its MapReduce engine has borned for the need in processing the informationHadoop is designed to be used on commodity hardwares which do not require large memory and processing power With the great amount data and average hardware clusters it makes easier to use all processing capacity and storage and to run parallelly distributed processes against vast amounts of data Hadoop establishes the backbone on which other softwares can run Many applications collecting data in various formats can place data into the Hadoop clusterHadoop can be used in Amazon Web Services under the name of Amazon EMRHadoop MapReduce is Hadoops batch-processing engine which is used to process massive amounts of data (terabytes) parallelly on big clusters (thousands) It excels on text processing as its first job which is crawling was a text-based taskMapReduce has no interactive mode as Spark but add-ons such as Hive make working with MapReduce a little easier for adoptersSpark is also a distributed processing system like Hadoops MapReduce The difference is that MapReduce uses commodity hardware while Spark needs high end hardware consisting of lots of memory Spark leverages in-memory caching for fast performance Also it supports batch processing streaming analytics machine learning graph database and ad-hoc queriesThere can be a misconception about comparing Hadoop vs Spark however Hadoop and Spark are designed to work in a same team It is better to compare Hadoop MapReduce to Spark since they are both data processing engines In addition Spark has designed to be a module of HadoopThe main difference between them is that Spark uses memory and disk for processing while MapReduce is only disk based Since Spark leverages memory most critics state that Spark is way more faster (Up to 100 times than Hadoop MapReduce) Although Spark can perform batch processing its expertise focused more on streaming workloads interactive queries and machine learning Hence it is more preferable when in or near real time data processing is required for such as marketing campaigns machine learning Internet of Things sensors log monitoring security analytics and social media sitesSpark has a clean API for Java Python and Scala (its native language)AWS equivalent is Amazon Kinesis Data Analytics Below is the matching comparisonPresto is an open source distributed SQL query engine running on data sources in Hadoop Distributed File System (HDFS) and Amazon S3 It is used for ad-hoc analysis of data running between sub-seconds and minutes It supports the ANSI SQL standard including complex queries aggregations joins and window functionsFacebook uses Presto for interactive queries against several internal data stores including their 300PB data warehouse Over 1000 Facebook employees use Presto daily to run more than 30000 queries that in total scan over a petabyte each per dayHive is a Hadoop module which works with Hadoop MapReduce using a SQL-like (HQL) interface It enables analytics at a massive scale besides distributed and fault-tolerant data warehousing Main difference between Hive and Presto is like Hadoop MapReduce and Spark respectively Hive makes batch processing while Presto uses memory while a query is running However both are using HDFSHBase and Cassandra are both non-relational databases HBase uses HDFS for storage while Cassandra solves storage need by itself They seem to be same but actually they are complete strangers to each other Cassandra has a masterless architecture while HBase has a master-based one Being masterless Cassandra has the power of being always-available If any downtime is critical for you Cassandra should be your choice However Cassandra replicates and duplicates data causing data inconsistencies If data consistency is important Cassandra is a bad choice regarded HBase since HBase writes data only to one place and always knows where to find itCassandra is good at writes whereas HBase is good at intensive reads Cassandras weak spot is data consistency while HBases pain is data availability although both try to mitigate the adverse consequences of these problems Also both dont stand frequent data deletes and updatesMore information about Cassandra HBase and HDFS can found in the following linksAn interactive notebook that enables interactive data exploration You can play with it on this link Scala Python or Java usage on Spark can be experienced thereSpark Streaming is an extension of the core Spark API that enables scalable high-throughput fault-tolerant stream processing of live data streamsAWS equivalent is Amazon Kinesis Data Streams See the Image-1 and Image-2 aboveHere is the real and explanatory exampleAlright There are a lot of tools and power The real question is that Is the data big enough to use these power? followed by Is it worth to build all these things? Well the answer is: Hadoop is useful to companies when data sets become so large or so complex that their current solutions cannot effectively process the information in what the data users consider being a reasonable amount of time
8KQN9tcjbUdHKpso6bcVSp,Airflow still becomes one of the widely used job scheduler and monitoring Its capability to scale well and also its flexibility to run many types of job is the main reasons why it becomes that popular among the others similar open-source product In this article we will discuss how to configure Airflow to be able to run well in production environment After following the entire article we will be able to: In this article we have discussed how to deploy Airflow on docker by using puckel/docker-airflow image We will continue those steps and then apply some basic configurations on the deployed docker imageThe current running docker container (after following steps in the previous article) runs using the default SequentialExecutorThis executor will only run one task instance at a time can be used for debuggingThis kind of behavior is not the one we expect since we need the scheduler to be able to process multiple jobs at the same time So we need to change the executor type We will use LocalExecutor (assuming we only have one machine otherwise CeleryExecutor will be more preferred)LocalExecutor executes tasks locally in parallel It uses the multiprocessing Python library and queues to parallelize the execution of tasksBy default Airflow running on sqlite when using SequentialExecutor However sqlite doesnt support multiple connections So we need to deploy another database that will be used by Airflow to store its necessary metadata We use PostgreSQL database as one of the alternative and postgres image that can be pulled from Docker HubBefore running the container we have to specify the user that will use this database We store this configuration in the postgresenv file containing lines like belowWe then can run the container by specifying the postgresenv as the environment variable We also want to give an identifier to this container as it will be referenced by the Airflow container in the next step So we will pass a container name to the --name optionThe database container is ready Now we want to connect the Airflow container to the running database container In order to do that we have to specify some configurations for the Airflow container Airflow store all of its configuration in the airflowcfg file inside the container However as mentioned in the documentation of puckel/docker-airflow we can specify those configurations from the environment variable named in AIRFLOW__<section>__<key> format We store those environment variables in airflowenv fileAs we want to run using LocalExecutor we also need to specify the executor type by appending some other environment variables to the airflowenvOnce the connection is configured we then need to initialize the database by creating the table needed by Airflow Fortunately it is easy to do this step as Airflow has already included the setup function The only thing we have to run is the airflow initdb commandWe can check the result by connecting to the database container and see whether these tables has been created in the airflow databaseAfter successfully connecting the Airflow container to the PostgreSQL container and then initializing tables in the database we are finally able to run the Airflow containerIf everything run smoothly you can access the dashboard on port 8080 in your machine Try to run some DAG (for debugging you may want to load the example DAG by specifying LOAD_EX=y on the -e option when running the Airflow container) You will see that Airflow now can run some jobs in parallelSo running jobs in parallel is not an issue now However the problem arises because we still allow everybody to access the dashboard without any authentication Everyone is the admin is obviously not something that we want to have in our system We want to give role to everybody that use the dashboard so that they can use it accordinglyIn order to do that we will configure authentication mechanism for everyone that want to login to our Airflow dashboard Airflow already has authentication mechanism so our job is only to make the mechanism works in our container We need to add some lines in the airflowenv fileBy the way if you havent got any Fernet key you can make one using the below Python scriptThen we need to restart our running container using the current airflowenv file If everything is doing good we will see this login page when trying to open Airflow dashboardAirflow doesnt give option to specify any first user when deploying the container As a workaround we can jump in to the container and manually add the user from the Python console inside the containerSpecify the desired user and password in the script below and then run it in the opened Python consoleThis user will be the user that has super-admin role and can be used to create another user in specific role from inside the Airflow dashboard The look of the Airflow dashboard when we login using the super-admin user is like the image belowNow we have successfully applied LocalExecutor to our running Airflow docker container so that we can run job in parallel We also have configured an authentication mechanism and role-based credentials when using the Airflow dashboard It is supposed to be a good starting point to use Airflow in production environment
S2qxnJj3Tx8t2r5mgBXtMX,Airflow is a workflow management system that is widely used these days It is originally developed by Airbnb to manage their data flow in their fast growing data set It is now under incubation of Apache Software foundation as Airbnb decided to open source itAirflow make your work easier by being able to monitor and programmatically schedule and manage your workflow It also comes with some additional features that is very useful in production such as giving alert when your job is ended with an error It is also very handy because you can schedule your command by either calling a shell script python code or even running a docker containerHowever those benefits will be meaningless unless we can install it in the production environment The most tricky part in the production (especially in a very critical and highly secured data center) is that the servers is not connected to the internet One of the easiest solution is to download all the package required to install Airflow from your internet-connected device then upload and install them in the offline servers Unfortunately the required packages are so many and it is going to be a very cumbersome jobThe first part of the job is to install docker both in your offline server and in your internet-connected device You can use the official package from your corresponding OS repository For security reason I suggest you to use the official package especially if you use enterprise operating system such as RedHat Enterprise Linux so that you still can get the official support on your docker service as well However if it is not suitable for you you can use the community edition as well We will not cover the docker installation part in this topic as the official documentation is already includes a very good guidanceAfter successfully installing docker we need to get the docker image of Airflow There are lots of its versions in the internet so you can choose one that you prefer In this topic we will use puckel/docker-airflow image that can be pulled from Docker HubIn your internet-connected device pull the docker image by typing the command belowThe image will be downloaded to your device in a while Once it is completed we then want to move the downloaded image to the offline server We can export the image to a file using this commandThen we can copy the file to our offline server Make sure you double check the result of the copy since we dont want to get any missing data on the file You can use md5sum to check itYou can check whether the image loaded successfully by running docker images and then see if puckel/docker-airflow is in the image list Once your image is loaded successfully you can use the image to start your Airflow serviceIf it can run successfully this screen will appear if you hit the port 8080 on your offline serverThere are some other configurations to run the airflow docker as can be seen on the Github documentation of puckel/docker-airflow 
UXbULbud3jCSjLqPbiu4ii,Objective: Our objective is to load data incrementally or fully from a source table to a destination table using Azure Data Factory Pipeline First we will go through the concept and design and later the implementation steps using Azure Data FactoryPrerequisites: Following basic knowledge are required to understand the data flow properly 1 Basics of ETL Process : ETL stands for Extract Transformation and Load Extract is the process of reading data from a file system or database Transform is the process of validating modifying data base on various business rules Load is the process of writing the data into the destination or target or sink file system/database2 Azure Data Lake Gen 2 Azure SQL DB and Azure Data Factory Components understandingSuppose you are planning to move data from a source system to a target system So for the first time there is nothing in your target system You should build tables (if it is SQL DB) in the destination and build ETL Pipeline to move data Suppose you are moving data from an online transaction system which can create 1 million data regularly Now for the first time you will load all data which are available in the source You can say this is a Full Load of data from source to destination From the next data you can plan to load only yesterdays data to the destination Actually if you have all the data in you destination except the yesterdays data you should load the new/extra data which is available in your source system and this is a type of Incremental Load Basically incremental load helps to keep your destination system up to date with your source systemeg Suppose I have a customer table in source which have 100k records as per today and we have performed a full load and loaded 100k records in to the customer table of target As the source is a online transaction system I can expect few new records into my source system Suppose there are 100 new rows and now the total records in the source is (100k + 100) Now if we go for a full load again we have to load (100k + 100) into the target which is costly for me it will take more time So it will be good that I should identify those 100 new records in source system and only load them into the target system which is less expensive for meScenario: Here I have tried to explain a scenario using that we can do full or incremental load of multiple files of a source system For this purpose we should create a configuration table Based on that we will go forward The attributes of the configuration file should be following:Table_Name: Name of the table which we want to load from source & targetIncremental_Full_Load: If 0  go for Full Load if 1 Incremental LoadMax_Last_Updated_Date: Latest added_date/updated_date of records for a particular table in target system Active_Indicator: Binary value(0/1) identify to perform load for which tables If 1 select table names for ETL processDuring the first run the load activity is the full load of tables Then insert few records and perform the load activity once again which should be the incremental load The configuration table data will updated automatically after the first run(full load) so that in second run it will automatically go for incremental loadDevelopment Process using Azure Data Factory: 1 First create two Azure SQL DB resources: a Source DB and b: Target DB2 Create the Azure Data Factory resource3 Create the configuration Table4 Create two tables in the source DB eg customer table and transaction table5 Load data into the configuration table for customer table and transaction table Load also an extra row for promotion table After loading data it should be look like • Load few data into the customer table and transaction table7 Go to the Azure Data Factory > Manage > Linked services > New and create two new linked services one for source and another for target • Then create two parameterized data set (Azure Data Factory > Author > Datasets > New dataset) one for source and another for target so that we can pass the table names on runtime from configuration table and this will help to handle multiple table using a single pipeline • Go to the Azure Data Factory > Author > Pipeline > New Pipeline10
S9BU385irKiYoYTzGAgwQu,In mid 2019 Microsoft released two new exams : DP-200 Implementing an Azure Data Solution DP-201: Designing an Azure Data Solution After successfully clearing both these exams we can become a Microsoft certified Azure Data Engineer AssociateWe can get a detailed study guide for the list of the topics here These topics are regularly updated as this exam was launched recently Please check on official certification page for the updated course structure These exams focus on a variety of products across azure so there is major focus around breadth of most products instead of in-depth expertise of all productsThe learning path given on Microsoft website is very good and has mix of both hands on examples as well as conceptual knowledge This learning path is common for both DP-200 & DP-201 These modules have labs and are provided with free sandboxes We can also use Azure free credits to practice for certain Azure ProductsLink to learning path for various modules: For a more interactive experience or a quick refresher course we can also look into following learning path on pluralsight Azure monitoring is explained quite extensively in these coursesWe also need to go through microsoft documentation for following topics in order to gain much in depth knowledge for following areas: Azure Stream Analytics: Throughput Windowing Integration with Event hubs • Azure Cosmos DB: Consistency Levels HA Partitioning • Synapse Analytics: Integration with ADL/Blob using polybase Distrbution Strategy for tables • SQL DB: Security Data-Masking TDE solution to use depending on use casesTotal exam duration is 210 minutes including reading instructions and submitting a survey post the exam Number of question varies from person to person There were 47 questions for me to be answered in 180 minutes Out of 47 8 questions were based on a case study 700/1000 is required to clear the examI found the exam to be easy as I had good hands on experience working on Azure Data products previously I was able to finish the exam within 90 100 minutes and used some more time to revisit some of the tricky questions which I had marked for reviewIf you have further queries and questions please comment below Connect with me on LinkedInFor DP-201 refer hereNote: Please provide feedback using comments as this is my first Medium article
c2JULLhySvDqynfJJHBpSJ,In mid 2019 Microsoft released two new exams : DP-200 Implementing an Azure Data Solution DP-201: Designing an Azure Data Solution After successfully clearing both these exams we can become a Microsoft certified Azure Data Engineer AssociateWe can get a detailed study guide for the list of the topics here These topics are regularly updated as this exam was launched recently Please check on official certification page for the updated course structure These exams focus on a variety of products across azure so there is major focus around breadth of most products instead of in-depth expertise of all productsThe learning path given on Microsoft website is very good and has mix of both hands on examples as well as conceptual knowledge This learning path is common for both DP-200 & DP-201 These modules have labs and are provided with free sandboxes We can also use Azure free credits to practice for certain Azure ProductsLink to learning path for various modules: For a more interactive experience or a quick refresher course we can also look into following learning path on pluralsight Azure monitoring is explained quite extensively in these coursesWe also need to go through microsoft documentation for following topics in order to gain much in depth knowledge for following areas: 1 Cosmos DB: Geo Redundancy Distribution • Synapse Analytics SQL Database solutions: Data masking Data auditing & Threat detection etc from Monitoring • Synapse Analytics: Distrbution Strategy for tables • Azure Databricks: Real time integration workload distribution • Data Security & Compliance for blobs ADL SQL DB DWH Databricks • Real time architectures (Lambda & Kappa)Total exam duration is 180 minutes including reading instructions and submitting a survey post the exam Number of question varies from person to person There were 39 questions for me to be answered in 150 minutes 700/1000 is required to clear the examI found the exam to be slightly challenging than DP-200 as some of the questions had very close options or were a bit trickyIf you have further queries and questions please comment below Connect with me on LinkedInFor DP-200 refer here
ZKUjHhpNEEHNh5fJSk2Boi,I just passed the second of my GCP certifications After about 2 years of designing data architectures for solutions on GCP cloud at a large financial organisation as a consultant I thought it was necessary to formalise my experience The certifications also look good on my profile for would be clients and employersThis short blog is about the training materials I used and some tips for others interested in pursuing the same certifications It is also a reference for me because Google certifications expire after 2 years! • Linux Academy   https://applinuxacademyNotes: This is my first choice for those studying of these certifications The course content is presented without the Google marketing I noticed on Coursera It costs just under $50 for a monthly subscription that covers all courses available on site with access to the mobile version where one can download course content to watch offline on the go Subscriptions also include unlimited access to Google Labs My favourite feature on the site is the reference to foundational courses to prepare students for work and the examination The tests at the end of the courses simulate the real exam and passing them indicate the student is nearly prepared to write the exam • Coursera   https://wwwcourseraNotes: Coursera is the Google recommended training provider for preparing for their certification examinations But not for me The instructors were all Google employees who were out to market the companys services first before delivering the courses I dont think the content is sufficient to pass the exams The course delivery sounded mechanical in places like they were read from slides On the plus side Courseras courses include Qwiklabs hands-on-labs and tests after each module Coursera offers monthly subscription although students can audit courses for free ie take course without access to tests of labs a good way get started Another plus • A Cloud Guru   https://learnacloudNotes: A Cloud Guru are stronger on the AWS courses than Google although I found the Associate Cloud Engineer course useful after taking courses on Coursera The Professional Cloud Architect course on A Cloud Guru was total waste of my time! For those without GCP experience I suggest taking the Associate Cloud Engineer course on A Cloud Guru and if you can write the exam before attempting the PCA certificationIn the exam I thought the number was closer to 25% of the questions which comes to about 13 questions in all My advice: read and understand the case studies thoroughly sketch the as-is-state and the target state by yourself for each scenario applying the Google recommended tools services and approaches before writing the exam Understanding the business and technical requirements for each case studio helped me in small measureVery few question were theoretical most were scenario-based On the whole the expectation is to have all round understanding of GCP servicesHere is what I recall: Identity & Access Management: Organisations Folders Projects and resources inside projects Understand inheritance of the resource hierarchies with respect to IAM Identity Federation (GCDS) and specifically roles and access control for BigQueryCloud Spanner: This is a bonus; question always has keywords such as global consistency transactional database etcCloud Pub/Sub: I only recall this came out under the case study question but one should know when to use itCloud DataFlow: Almost all questions I recalled that included DataFlow also had Pub/Sub as a part of the answer So understand the Kappa architecture or stream processing ie Pub/Sub \uf0e0DataFlow \uf0e0 BigQuery/BigTableCloud Storage Transfer Service / Cloud Transfer Appliance: Understand the difference between these two services and when to use one over the other Understand the what rehydration means and when it is usedNetworking (VPC VPN Subnets & Connectivity): Luckily I had few questions on this; my weakest area! But there were a couple of question on setting VPN between on-premises and Google I remember one of was about the type of connectivity to support a 20 Gbps speed to GoogleReg & Compliance: There was one or two question about GDPR and data retention to meet regulations Understand that using Google services do not mean that a solution is compliant the design approach is what makes it compliantOthers… • tMake sure youre consistently scoring the 90% and above in the preparatory tests on any of the training sites eg on Linux Academy tests Review the explanations for the questions you miss and dont hesitate to challenge them if the answers provided are wrong I did this a few times on Linux Academy and it was corrected It is probably a sign that youre prepared for the exam • tMake sure you are consistently scoring 100% on the Google Cloud Practice exam • tI found the Qwiklabs Codelabs and Hands-on-Labs from training websites very useful to cement understanding of the concepts
XJy4ZB8FCn4nVqspsQtcQK,I know its late( modules are already out for quite a time) but better late than never 😅 Also this part contains explanation and an example about RedisGraph module only Others will be covered in next part in coming weekendMachine learning models artificial neural networks are definitely the new in thing not only for research but for major enterprise industries also Already enough data is available with various organizations operating in multiple domains especially the big ones Now the task is to find suitable mechanism to make the most out this resource available in abundant quantity To carry out of the box POCs (Proof of Concepts) on all the possible options available in market to maximize the results as per your needsBut in times of such great advancements in the trendy fields of ML AI and Deep Learning it would be quite unjustified to not discuss about advancements in databases which are founding stones for everything from research projects to industries in terms of storing data Write choice of a database can save you millions of dollar on enterprise level applications over the years Databases and their architectures are evolving to serve this modern industrial need of rising ML and AI tech One of the best examples that I have seen is Redis(a key-value in-memory store with persistence) which becomes more powerful with its incredible modules which provides it large number of functionalitiesA time-saver tip: You can skip to any section mentioned below that interests you and start reading They are written in modular manner 😅Lets carry on our discussion in following manner stated below So that we can learn all about Redis from its basics to different functionalities of below mentioned modulesIf you are here for learning how to use Redis for ML & AI use-cases directly jump onto 345 section specified above Only section 3 present in this article 😉Redis is an in-memory key-value NoSQL data structure store that can be used either as a database or for cache Its specialty is it supports same data-structures that are used in programming languages like strings hashes lists sets etc and some other special ones too few of them will be discussed later in this article It uses simple commands like SET DEL for setting and deleting keys RPUSH LPUSH LLEN LRANGE LPOP and RPOP for working with list data structure And many more commands to interact with the database store Structure to datastore is provided by these commands only which also encodes typing mechanisms to data Just follow along documentation itll be an excellent guide for you for getting familiar with commands and their functionalities Also you can opt for Redis Introductory course RU101The finest way to answer that question will be to mention all the important architectural features of Redis and then present suitable use-case scenarios for itFast and Addition of new Modules: It is very fast as written in C (plus bench-marked as fastest database) and supports addition of multiple modules that can be loaded as binary object file via foreign function interface mechanism Modules are dynamic libraries that extend functionality of Redis by implementing new commands with features that acts as if they are implemented in Redis core itself Here is a sample command to load any module onto your redis-serverCAP theorem Where exactly it falls ? If you are using only single Redis instance or a master/slave configuration then itll be quite wrong to evaluate to evaluate it with CAP theorem as these will not be called as distributed systems in true sense Here P will be zero as it has no native partition tolerances the way to scale it horizontally is by partitioning data across multiple Redis instances It can be done via client side partitioning where clients determine on which instance to write or Proxy Assisted partitioning where a proxy looking like a single Redis instance partitions the dataWe can discuss CAP theorem in context of Redis Sentinal which is a high availability solution with multiple master Redis instances and a list of replicating slaves It monitors masters and promote slaves via a consensus when master node dies But Sentinal is designed keeping performance and strong data model values in mind It is a general solution high availability and horizontal scalabilityTrue consistency is given up in favor of performance with replication being performed asynchronously Similarly strong availability(all writes be successful even when the nodes cannot reach each other across the network) is given up in favor of strong data model of Redis Lets say both masters should have same data and if a master joins later data must be merged onto it But Redis never actually merges data because it is difficult to correctly merge lists sets and some of the other advanced data structures Redis supports Instead it will always accept the most recently written state of the dataAlong with three parameters of CAP theorem Redis clusters are designed to keeping in mind to retain performance and strong data models Salability being the ideal gem for it to focus on It wont be good match for heavy industrial grade application demanding very high consistency and availability(HTAP or OLTP use-cases) but for applications demanding high performance or analytics (OLAP use-cases) as prime criteria it can do wonders for themI have seen big organization skip away from Redis and its module when it comes to utilizing as primary datastore because many organizations demand consistency and high availability But it is used majorly as caching layer in the existing data pipeline for providing high throughputSo when to use Redis ? It can be used for cache sessions of users It isnt a mission critical consistency issue and with persistence it gains advantage over Memcached systems Real time analytics like leader-boards order by user-votes and time deletion and filtering etc are some other prvelant use-cases Also it can broadcast messages to every server that listens to a channel using Publisher/Subscriber technologies Redis used Pub/Sub feature to trigger scripts create a list of most common tasks on system social network connections and many more boundless use-cases for it Redis implements queues for jobs where master posts the jobs and worker picks up these jobsKey Takeaway: Do consider all Cloud Databases available before considering Redis as your primary data-store Also Redis increases performance for many other use-casesNow exploring its potential with the modules it provides to extend its functionalities and carry on multiple new operations Each module deserves its special article but well explore modules related to AI and ML here Also keeping out RediSearch module of our discussion as it will be out of scope for now and requires a complete separate discussion on its featuresThis page contains list of all the modules supported by Redis Here I ll be listing discussing about major ones that are often used with Redis and list some examples use-cases related to that module In following sections Ill be explaining in detail modules for working with graphs & its queryable relationships ML models and neural networks Lets get started with introduction to these modules in this sectionRediSearch: It implements search engine like functionalities on top of Redis it is a full-text and secondary index engine Secondary indices are often implemented when you dont want to alter your database structure but still want some feature specific queries to be executed with high efficiency For example querying for customers with shoe size 7 or all customer in a given city from a NoSQL storing information customer information indexed with their unique-id It also have advanced features numeric filtering for text queries and exact phrase matching Its popular use-cases are auto-complete and other fuzzy suggestions even with a typo on prefix When index is big enough it can be sharded across multiple machines But instead of normal approach RediSearch uses index partitioning method which queries all the shards concurrently and returns the result after merging This module clearly requires more discussion hopefully a dedicated future article and some coding examples for this moduleRediSQL: RediSQL is a module that embed a SQLite database which makes it possible to have several smaller decentralized databases instead of a single giant one It is created to add more structure on the given in-memory database instead of redis commands we can use SQL queries for our datastore It utilizes velocity portability simplicity and capability to work in memory of SQLite DB It is developed by RedBeardLabs which for enterprise version provide 14 days free trial But I personally feel its 990€/year cost is quite expensive Hence you must analyze your use-case requirements carefully before selecting this oneWell the remaining popular modules add different data structure capabilities for Redis datastore for example: ReJSON provides JSON as native datatype and allows storing updating and fetching JSON values from Redis keys Also RedisTimeSeries module which provides times series as linked list of memory chunks where each chunk has a predefined size of samples each sample is a tuple of the time and the valueJust a Quick Fact to end discussion about redis Redis is incredibly fast If performance is your prime criteria for your datastore It has been bench-marked with 50 million operations per sec with 099 seconds of latency to be precise It is a incredibly high throughput for a database to have Also it can be used for session cache which is one of its prime usage But there are also other parameters that are needed to be kept in mind while selecting a datastore like replication mechanism conflict handling etc Do analyze it before using it for your organizationIntelligence can be defined as ability to see and analyze connections In AI we always attempt to make intelligent in some sense or the other which can do inexplicable tasks in fairly simple manner This connection knowledge can be extracted out and modeled in the form of a knowledge graph to leverage out AI related capabilities like intent detection sentiment analysis connectivity etc Other important use-cases are connected feature extraction graph accelerated ML and AI explainabilityNow it is clear that graphs do have a important role when it comes to applications is AI And a data modelling process carried out specifically to analyze graphical relations will definitely be more effective as compared to any NoSQL or relational modelling approachFor example: Fraud ring detection (basically a ring of authentic looking accounts defaulting together) traditional databases uses discrete data points for analysis of such rings but the important part of this data analysis problem is to detect relationships amongst these malicious nodes Other use cases like social network analysis and real-time recommendation engine systems shows better results for graph databases as compared to other database approachesLets start playing with these graph databases and learn in depth how RedisGraph works And why it will be a great choice (one of the most innovative thing I read about previous year 😉) in future Lets do comparative analysis with Neo4j the current industry standard Both in terms of small bench-mark test and other featuresAgain like Redis its module RedisGraph is also bench-marked as fastest graph database Behind the scenes it uses graph algorithms to carry out query operations in database with GraphBLAS library ie it uses linear algebra and matrix representation instead of adjacency list representation to perform faster operations Matrix representation are used to get an idea about exact relationship between each pair of nodes and it is space efficient because it uses sparse matrix representation which leverage the fact that most real life graphs are sparse Where as traditional graph databases use hexastore or adjacency list which makes them slower relatively as follow standard graph implementations instead of matrix algebra for finding relationshipsAlgorithms explained like above in matrix-multiplication terms can be thought of as more abstract representation of one hop relation query like finding friends-of-friends Such queries can abstracted out from different graph algorithms to suite any particular use-case Meaning all graph algorithms can be reiterated in terms of matrix algebra operations to find out different relationships in a super-fast manner with more optimized matrix multiplication algorithmsDo remember it currently in its initial stage only and product is not mature enough as compared to databases like Neo4j ArangoDB etc Also the highly awaited feature will be GPU support for matrix multiplication which will make it even more faster(Remember GPUs from deep learning) It is a very promising one of its kind database with huge scope of improvements even with such great results already in handHow to install and run? Here are the following instructions to install it on your linux distro (Also there is a bug while in installation CentOS7 A very similar issue was encountered by installing it on RHEL 7)Follow the instructions below: Lets do a speed test: For this make sure Neo4j is also installed on your system Follow the steps from following documentation link which involves JDK installation repository addition and sudo apt-get install neo4j=1:352 commandLets run some CREATE and MATCH cypher query commands for evaluation of performance for both these graphs by using moto gp example from RedisGraph documentationStarting with our motoGP queries example below are mentioned queries with their execution time for both the DBs and a visualization chart for performance comparisonIn detail Feature Comparison with Neo4j: Firstly there is no UI available for RedisGraph is the free opensource version where as an interactive GUI is there for neo4j that helps us visualize the returned query results also For both clustering is available for enterprise version only Also Neo4j uses causal clustering using raft protocol to maintain consistency and is not partition tolerant Neo4j fully maintains data integrity and ensure good transactional behavior It supports the ACID properties where as Redis follows Strong eventual consistency with CRDTs(conflict-free replicated data types)Now discussing about maturity of the product RedisGraph is very new product with still enhancements and bugs pending to be handled where as Neo4j is the most matured database in the industry With 10 times more adoption rate than all others graph DBs combined Also both of them use same query language cypher with minor syntax variation for execution as shown aboveRedisGraph has huge potential to be a big player in graph industry with Redis being the fastest growing database and with most used image in docker As per my views it is not ready for production environments considering the open issues are very basic in nature and will cause problems in deployment Ill definitely be spending more time on it for an in-depth analysis for this module present some of large scale benchmarks and will discuss that in separate articleStay tuned !! In next part discussion will be about neural-redis (It can execute neural networks while they are ongoing training) and Redis-ML (Implements ML models as redis data types which can be used for either prediction or evaluation)
ci9ZvTrJyqJAVptZBs3u3Z,If you are a data analyst or part of reporting team no doubt you have heard of data warehousing If your company also uses AWS as its cloud provider you are likely to use Redshift Redshift is a fully managed data warehousing service by AWSPrior to diving into Redshift I would like to reflect on a common journey many companies take Before going to the cloud many companies will have their reporting databases hosted on MS SQL Server Oracle MySQL and the sorts These are great database technologies suited for fast application access Microsoft takes SQL Server one step further to add the BI suite of tools to transform SQL Server into a warehouse ecosystem of tools Slowly over time as data grows and reporting needs increase these on-premises systems are migrated to Redshift if using AWS However the data warehousing methodology and the reporting mindsets do not evolve to adapt to using cloud technologies As part of this article Ill try to highlight some differences between the traditional warehouses (for lack of better term) and RedshiftRedshift is a distributed and highly scalable data warehouse solution It has some clear differentiators that make it one of the leading solutions in cloud Lets take these one by one and discussData is ever-growing and the need to report on it is becoming critical Redshift is a clustered solution; containing multiple nodes and the ability to grow and shrink the compute and storage There are 3 types of nodes available and storage ranges from 5TB to 2PB depending on selected nodes There are two methods to scale a cluster; classic which incurs downtime and elastic no downtime Both methods have their pros and cons but will not be discussed in this postRecently AWS has introduced scaling compute and storage independently allowing flexibility and cost control as data grows or compute needs change over timeRedshift is designed and built as a massively parallel processing database This means the performance is scaled efficiently based on the size of the data across multiple nodes Redshift is based on columnar storage which is highly beneficial for analytical queries As much as hardware has a role so does schema design Redshift offers multiple optimisations including compression and encoding at a column level Combining all these result in sub-minute query times even on terabytes of dataSecurity is ingrained into Redshift It offers everything from disk-level encryption to table and column permissions IAM policies allow controlling access to the infrastructure as well as multiple authentication methods that allow control at connection level (JDBC/ODBC) A RBAC model is the foundation of all authentication and authorisationAmazon Redshift is a highly available and durable managed service Providing 999% online uptime Redshift provides auto-recovery from hardware failures and even network availability A robust set of tooling to manage reliable service for end-users through the granular Workload Management (WLM) systemAmazon Redshift is a massively parallel processing data warehouse solution offering highly reliable consistent and secure service
bXgiX2TT4CDb8ZwWpLF2f9,This post is about my journey of getting GCP Data Certification in 3 weeks It was not difficult to get the certification in 3 weeks but it wasnt easy either All I needed to have was the dedication and will power to follow my daily study scheduleBeing a Consultant at Servian I get involved in multiple digital and data projects where my role varies from being a full stack software developer to a ETL Data Engineer But never worked with the Google Cloud PlatformServian being a consultancy domain company always encourages its employees to keep learning new things and get as many certifications as they can So I thought of getting my GCP Professional Data Engineering Certification For almost 4 months I just thought about taking it up but never really started studying for it I think its the phase of every passionate individual where they want to do something but cant find time for it Out of the blue just to avoid my procrastination I booked my GCP Data Engineer exam because I dont know about passion but fear does make you do thingsIt wasnt easy to study after working for 9 hours And I knew that success always comes at a priceSo I tested multiple daily schedules for studying after my workPlan A: First and most common one which didnt work for me was to immediately start studying after arriving home from work Even though I used to study with total concentration I wasnt receptive to new conceptsPlan B: To make myself productive I created plan B which helped me not only with GCP preparation but also with other piled up topics • 00 pm: return from work • 00 pm  630 pm: change and have dinner • 30 pm  730 pm: Sleep (dont forget to wake up :-P) • 00pm  11The above plan doesnt seems intimidating at first but it did torture me for a couple of daysEnough story now time for some real stuffI gathered ample number of tutorials but I was confused about where I needed to start from as there were a lot of topics to coverShort Answer: Depends Because for certain topics in depth knowledge is necessary and for certain other topics a high level overview will sufficeRead Article by Kayleigh Rix for more informationGCP Exam question topics: • Storage (20%) • Big Data Processing (35%) • Machine Learning (18%) • case studies (same as sample case studies 15%) and • others (Hadoop security stackdriver about 12%)Data Engineering on Google Cloud Platform Specialization on Coursera is the best first step and helps to prepare for about 70 75% of the total exam content It has a 7-day free trial and if you are consistent with your schedule you can complete it within the allotted time Its important to understand the labs because questions asked on GCP exams arent straight forwardWe will come back to the other 25 30% later in the postOnce you complete the Coursera course then its time to test your knowledgeAttempt sample case study Its fine if you are not able to get it in the first shot Its common and the same has happened with me Just try to at least understand the case study expectations and break down the task if possibleAlso attempt GC official sample exam questions to get used to the type of questions expected to be in a real GCP exam and keep note of the unknown topics and incorrectly answered questionsNow Coursera has an amazing course on Preparing for the Google Cloud Professional Data Engineer Exam It walks you through the sample solutions for sample case studies and also provides high-level knowledge on GCP products required to get GCP DE certifiedWe still lack those 25 30% of the concepts which majorly belong to IAM Machine learning and Hadoop Ecosystem (Spark Beam Pig Hive Kafka Flink Sqoop MapReduce HDFS vs GFS Oozie) There is an excellent course on Udemy for GCP Complete Guide which helps you to go through the selected topics and helps gain a good amount of knowledge on those topics of which you are unsureIf you cant afford Udemy then go through the Google Cloud Next video on youtube Its good to visit as many topics as you can but dont forget to go through the topics like Security Dataprep Hadoop Ecosystem BigQuery Dataflow Dataproc etcAs of now you should be familiar with all topics and finally its time to hypertune your knowledge parametersStart reading sample implementation for Spotify and also the best practices for GCP productsI hope you guys have prepared notes for last minute revision If not then dont worry just say thank you to who did the hard work for usoriginal post link (https://guptaavinash
N7uhJBAJjWomruLVQsq6M5,Working as a Data Engineer you are directly connected to your data science teams and some senior-level personnel who give you business use-cases You have to be involved in the complete process to gain awareness of why a particular action should be taken Hence I have been reading Ross&Kimball and Krishnan for Big Data conceptsThis blog post will cover some fundamentals of the engineering involved in Big DataBig Data analytics comprises of two steps: While building pipelines for a data stream we capture data in both structured and unstructured storage Keeping track of the data in unstructured storage is what makes your integration good and keeps the data warehouse(lake as well) alive and useful This approach is called as data virtualizationTraditional analytics cover large subsets of a data collection that confirm gross relationships in data the origin of those relationships and of what those relationships mean Data Mining uncovers outliers and data that doesnt fit in with the general flowNow outliers may be just outliers and the gross relationship view can ignore the subtle details You have to balance between these two to derive the highest value from your BI systemAfter capturing data in a more or less raw state it gets structured and re-structured repeatedlyYou get or build a hypothesis and using that you structure and re-structure the data The objective is to help(allow) everyone to explore and test that hypothesis So in a nutshell structuring → data mining and analytics to test hypotheses → feedback on structuring These three are followed iteratively to serve the BI team and draw KPIs on the wayRemember the objective: allow executives or analysts to derive actionable business intelligencePresenting the information with visualization and reports is not enough You got to provide the BI team a platform/tool that allows them to see your results in the context of their organizational data and missionThis is your objective now For example consider Tweets They often come with a hashtag that describes what the tweet is about Similarly if you attach the information to your structured dimensional data it ll become more useful for Big Data discoveriesProblem: Visualizations that are useful today maybe gone tomorrowBig Data varies like hell Your structured data in a data warehouse has dimensions and measures that let you and others draw some visualizations but even then the structures are transient and so the visualizations that are useful today maybe gone tomorrowKimball and Ross mention five practices for approaching this problemStructural Feedback Through Visualization: We often think of visualizations to serve our BI team whereas we can use the same visualizations to refine our structures(storage structures mentioned above) I have received the same statement from experts  Big Data visualizations will help you to refine your data lake and then they ll be thrown awayUsing Maps: Geospatial data is important and it gives your precision So either start using it if you have it or make sure that you have the capability of storing geospatial data soonDimensions: Dimensions are those quantities that let you correlate Big Data One dimension that doesnt vary as Big Data does is time actually its the calendar dimension See how important role time plays to understand your Big Data click here Just layout time on x-axis and measures(number of tweets etc) on the y-axisCategorization and Correlation: We all have data lakes that contain data which require multi-way categorization and correlation Rookies jump to apply Machine Learning(no offense) but we may have a workaround here Consider connections on your LinkedIn Facebook etc Such type of data doesnt fit neatly with measures and axes Now think about the large volume of such data you are dead! You can solve this using a network graph A network graph looks like this: This allows effective exploration among connections that otherwise would be senseless Make sure you check out supernodesThe first point Structural Feedback Through Visualization was for structural data This same technique applies and is required much more for the Big Data So you use your visualizations to disprove your hypothesis and then you re-structure the storage of data to generate visualizations Its a vicious but a required cycleIm still confused between Data Science and Statistics However I do know (and I am told by my boss) that Data Science is used to develop and provide actionable insightApplying data science algorithms or statistics requires your large data sets to be in a particular structure(not storage structure) throughout the pipeline(ETL) Data Scientists ensure that certain metrics and attributes are consistent throughout the dataset So we can say that Data Science is used in data conformanceThe life of a data engineer would be a lot easier if the predictive analysis would start workingPredictive analysis is used to identify trends in historical data and then point out to possible causes to these trendsIt involves three things: To perform the analysis of Big Data and other large data sets machine learning is required The objective is to define parameters for your analysis algorithm and then have them auto-adjusted by the results of the algorithmSome common ways to address the velocity and volume of Big Data: The problem statement is to be sure that we have a data loss Why? Because parallelizing gives us speed and can also give us redundancy in case of failure So we need to apply some scrutiny to validate whether we have data loss or notUsing a distributed network ie distributed data storage systems you have to take care of the three qualities: The CAP theorem suggests for selecting any two of the above three qualities In 1999 UC Berkeleys Eric Brewer proposed a conjecture that Consistency Availability and Partition Tolerance could not be simultaneously guaranteed in a distributed data system This leaves us with having to choose which of these qualities is important and necessary and which can be dropped or deemphasizedIn the context of Big Data availability and partition tolerance are of higher value than consistency Do you know about the fine-grained consistency that is essential in transaction processing? Yeah thats not needed in a high-velocity Big Data world The mission statement then becomes: Sooner or later the data readout to all requestors of a particular data element will contain the last updated value HDFS and other distributed file systems and databases (eg Cassandra) follow this conceptThe fundamental technique unique to Big Data is massively parallel processingThe business intelligence system is data-driven which is to say that it is metadata-driven Unlike most of the data we have discussed to this point we will be storing this in a database and fully normalizing it This is also referred to as BI Data Dictionary
JedNXcBLt74BNMhcFdenTm,Monitoring and Logging are both essential together They are like two sides of the same coin: health-checkInitially the dev community had monolithic environments so checking faults was an easy task Even today one can check the output(s) of an application with Kubernetes CLI but things get complicated when there are more than a few containers involved for an application/serviceThe need of an hour is to get the logs from the VMs deployed on cloud from Kubernetes and of course from the deployed applicationsA perfect illustration of deploying the above use-case is done here; well-explained following architecture of fluent-d and fluent -bitThere is a video I would recommend which teaches how to setup fluentd for Kubernetes although it doesnt cover up securityAlthough one thing that I have realized through multiple internships is that when health-checks are concerned it is always better to buy rather than to build LogDNA costs you a meagre amount and saves a lot of dev effort when you need logs for ES on KubernetesLogstash and its web interface tool Kibana with ES are termed as ESK and they are used for monitoring KS as well This tutorial explains why and how one should use(setup) them using falco and fluentdThe best way to do this in a generic sense is to use Heapster(inject it) This web-page gives a brief description to get started on Why Heapster? One needs to measure ES performance keep an eye on all the logs in a minimal effort Heapster with Kibana (for visualization) is the answerBy this article I wanted to convey that even though being a Data Engineer I am doing the infra setup for logging; main motive being I know the loose points of my infra and my data services(microservices) The aforementioned resources were referred by me while developing Event Sourcing mechanism at SocialCops We are yet to finalize the stack for health-checkups but we know our workaround since all sort of built and available tools are already tried
TNMviYAJAVJCyH2qys6aTq,While working with big data sets I have found designing ETL systems really easy when those data sets are for a customer However when working with data sets with B2B customers the data sets are so diverse that it is almost impossible for me to perceive all the queries(kinds) analysts would want Hence I started reading this book about building a data warehouse by Kimball and Ross It has provided me a fresh and new perspective having empathy for the data analysts and business analysts So here s my blog post sharing my learnings in a synthesized way I want to explain how and why data modeling is useful while designing ETL systemsSome prerequisites to know(just the google definition would do): The Gurus/Gods of Data Warehousing Kimball and Ross defined the following goals for a data warehouse: Presenting information consistently timeliness and adaptability all are achieved in ETL processes You design your ETL well and keep business logic in mind you have achieved major goals of a data warehouse This is challenging and not something that you can achieve by just mastering AirflowETL is not doing anything for me other than just pulling the data transforming it and loading it in my DBs where I can query it You would say that Transformation would have me achieve accessibility but there are cases which we will see later where T cant be used to achieve accessibility Even if you do want to achieve it you ll have to model your data and then apply transformations to have that desired data modeling Thus data modeling is required to achieve accessibility In other words T is not only transformations on the data that you want to present but it could also be data modeling for future use casesBefore we think or get into ETL and data modeling let us recall the primary objective of building a data warehouse: to make it straightforward and unambiguous  easy  for analysts to write analysis queries quickly and effectively You can assume that the end-user will be able to write SQL queries but you cannot and should not assume that they will be writing complex outer joins grouped subqueries using primary-foreign key relations recursive queries etcWhat we should expect from our Business and Data analysts: So the world moved from OLTP to Facts and Dimensions long ago but I still feel that we dont adhere to the philosophy of dimensional data modeling as much as we should do  in the same way that we dont use UML diagrams as much as we should doWe often design our data systems while keeping in mind the retention of data and we end up losing so much on the accessibility We should understand that the real use case of your app or product is transactional  order placed order received etc but the analysis that you are going to do for the sake of your business is rarely going to be transactional You got to aggregate or analyze to discover information about the functioning of the business processes And so you have to ensure that there is no overlap in b/w two events that you recorded in your transactional database otherwise the aggregation would be wrong irreversibly wrongIf your data captured is having primary keys you are all set But that is not always the case I have seen so many datasets that are really crucial for business logic but they have no primary key In such cases amateur as I was(am) I allowed the duplicates or I considered the hash of all the columns combined as the primary key Even then you would have duplicates but thatd be alright mostly because I inserted two exact same rows once None of these are the ideal solutions or solutions at all You have to have something using which you remove redundancy And so we use different dimensions of the data in order to capture unique rowsOne perfect example would be Indian Premier League tickets sales You record tickets sale and their price but you end up running analysis on maximum money earned wrt a team which weekdays are most profitable etc So these are the measures that you want to derive from your facts(in fact table) Enter the concept of filling measures in your dataIt is always preferable to have as much atomicity as possible But data is not kind: you can have sales in dollars per hour and sales in INR per quarter And I challenge you to convert both of them at the same rate and gain some insights And so we need Periodic and Accumulating snapshot tables in order to have data at some granularityPeriodic snapshot tables are collected at regular time intervals Consumption of some resource like water or electricity are the natural case for periodic snapshots However there are also periodic snapshots that are essential audits The summary in a bank account statement is that kind of periodic snapshot: starting balance ending balance interest (Not of course including the details that is the transactionsOne business question that may be difficult to answer in business intelligence is the passing of time between facts If youve ever seen the timers in a restaurant drive-through you know that companies can find this very important But if you take the atomic-grained fact of your ordering a fish sandwich and the fact of the bag being handed through the window of the car you cant assess performance That timer on the wall knows but you dont When a business performance indicator is the rate of completing a multi-step business process you may want to capture at the grain of the entirety of the process and record the start the completion and the steps in between This may be transaction-grained but it has a lot of measures in betweenI would like to recall our main objective: driving business intelligenceIn order to query the fact tables and get some business insights you have to consider the information provided by the dimension tables Thus it is important for you to know how you can leverage the dimensional modeling and query the factsWe search the table by searching the dimensions in which we are interestedKimball and Ross introduce dimension tables as descriptive context the who what where when how and why of a factWe have taken this schema for granted so much that we often mess up our design and our data warehouse gets off its track If we keep checking our dimensional modeling we are way better off than just de-normalized or normalized database designOne of the interesting aspects of dimensional modeling is that in some ways the fact table is most clearly defined not by its primary key or any unique identifier but the combination of dimensions it carries That is to say each fact is identified as the unique intersection of values in each of its dimensionsWhile there is often some sort of identifier that can serve as a primary key in a fact table standard data warehousing practice creates the primary key as a composite of all the dimensions it carries That is for any combination of the dimensions there is exactly one fact record with measures that can be aggregated and analyzed for example in conjunction with other records sharing the same values or ranges of those dimensionsBecause if you do this all of your querying and all of your reporting will come down to choosing values or ranges of dimensions to query and aggregate facts across them When you get to queries across facts duplicates among dimension combinations will turn into a disasterA checklist again taken from Kimball and Ross that should be used while data modeling: I hope this article brought a lot of obvious rules back in your conscious Please give me feedback on this article and if you have something neat to work on let me know: avisrivastava254084@gmail
CbSFpKC7wNujGx8x4TipZQ,After working on MongoDB for about six years I am sharing certain practices that have worked well for meIn relational database designs the schema is statically defined In document-based databases such as MongoDB the schema is dynamic and is based on the document structure MongoDB is a schemaless databaseThe structure of documents should be application-driven and the focus of this design should be on the access pattern Giving priority to access-patterns mostly results in inconsistency of data However linking within the designs creates a structure that avoids consistency issuesA thumb rule that I have encountered from multiple resources is that if you are designing your document store in a relational database manner you are probably not utilizing the document databaseMongoDB provides support for one-to-one and one-to-many relationshipsJOIN operations adversely affect the performance of MongoDB and so in order to avoid using them we can leverage BSON (Binary JSON) documents These documents let you focus on embedded designs by logically organizing information in one single document Having logically related data in one single document lets you avoid JOINs You should also avoid JOINs even at the application level This can be called a tradeoff between performance and consistency and we prioritized performanceOur objective is to model the different types of relations from the relational model We shall cover Linking Representation and Embedding RepresentationLinking is performed with the intent of using primary and foreign keys In the snippet below we can observe that authors is a list of IDs in the Book table This representation is an example of one to many relationshipsThere is an anomaly in the Book table belowAs with any design there is a trade-off between performance and consistency which should be potentially mitigated by the application logicIn the above table we see two tables: Authors and Book We assume that we would query Book more often than AuthorsLets organize our data in a different way from a perspective of Authors and embed a collection of BooksIn this example the assumption is that we often query our data by AuthorsObserve possible anomalies within both the names of the book and the languageAs a general guideline use linking in case of true 1:Many relationships and embedding in case of 1:Few relationshipsFor eg College: Students will be 1:Many and so we could use Linking but Tweet: Replies on a normal Twitter account would be 1:Few and so we could instead just embed the fewIn the snippet above we have modeled a many: many relationship in the Published_Book We have linked the two tables: Book and Authors using _idCardinality decides the relationship design in document databases In the earlier example Book: Author could come up as Few: Few We can model such relations using the Embedding technique but we have to make ourselves aware of the query direction: whether we will be querying books more often or authorsOne: One relationship can be modeled the same way as few: few with embedding the preferred option depending on the access pattern with a linking pattern used for bidirectional accessFor performance model in a way that entities accessed together are a part of the same document (this is the embedding example we explored earlier If documents are growing in size then use linking instead of embedding since the maximum size of each document should not be more than 16 MB
ZbbthhwZYd79ffDts2gHSy,A large scale distributed system that can support cyclic dataflowsBack in 2013 and even now every distributed programming model was tightly coupled with the engine that executed this model and so interoperability was an issue It is an issue even today but not that much  thanks to systems like Naiad and Spark Before Naiad we had systems like MapReduce for batch processing time stream for stream processing and Giraffe for graph/vertex processingNaiad is a distributed cyclic dataflow system that offers both batch and stream processing on top of the same engine It introduced a new computational model: a timely dataflow that enables Naiad to process cyclic dataflows This helps in building applications on the lines of graph analytics It has an efficient mechanism of incremental computations like that of the Page-Rank algorithm Naiad demonstrated high throughput and low latencyI believe understanding Naiad would strengthen the concepts of generalized distributed computing platforms and hence writing this blog postThe PageRank algorithm reads text files to parse URLs and then counts each one of them to rank URL pages Usually Apache Spark is used for this purpose but we needed a more refined way in which input data can be updatedIf the input data is updated in Spark we would need to rerun the entire thing However Spark has support for streaming but that does not yield high throughput as Naiad simply because the computational model is not supported for streaming data Note: I am not considering Structured Streaming of Spark that could perform this job I am considering the base research paper of vanilla Spark introduced to us in 13In Spark we can have incremental computations like that of the PageRank algorithm but we cannot have cycles in our graphs Spark uses Directed Acyclic Graphs(DAG) for computation
EncVm8QAskRGdvACeZtKXb,While working as a data engineer I often encounter the problem of data storage: whether data access should be kept faster or should I go for cold-storage or something in between? Usually I focus on the frequency of data-reads and make sure that I am using tools that reference the read-data However I dont want to be anecdotal in my observations and hence wanted to look upon some fundamentals So I started reading a research paper by Jim Gray Goetz Graefe published at Microsoft in 1998 I am writing this blog post as a pre-requisite for reading this paper to help you understand some basic terminology and to give you some contextClick here for the research paperWhile deciding the appropriate page sizes and main memory size for accessing data we should consider both economic and performance aspects And so we seek the break-even time duration This duration is often referred to as the reference interval It tells us the time for which a page should be kept in the main memoryIn 1997 at the time of publishing this paper the reference interval suggested was 5 minutes This means that at any given point of time t data referenced within from t-5 to t should be in the main memoryNow this 5-minute rule does not apply to all machines for randomly accessed pages For eg mainframes follow a three-minute rule instead This is because of the higher disk prices From the equation given above (Pages Per MB of RAM) / (Accesses Per Second Per Disk) is called the technology ratio (Price of Disk Drive) / (Price Of MB of RAM) is called the economic ratio When technology ratio decreases and economic ratio increases reference interval increases ie you get to keep data in the main memory for a longer period of timeIn the paper a rule named 10-byte rule is mentioned Let me give you a bit of context here Processors are expensive and so they store the results of instructions (frequent ones) in the memory The 10-byte rule states that you can keep 10 bytes of CPU instructions in the DRAMIf a sequential operation reads data and never references it then there is no need to cache the data in RAMIn section 12 of the paper Sequential Data Access this line mentions a kind of operations that dont reference the data What is the meaning of never referencing it? Usually when an operation reads the data it creates a reference to it basically it indexes the data Now if an operation is reading some data and not referencing it (usually done when data is not required again) then we dont need to cache it as it was a one-time readThe paper mentions one-pass and two-pass sorting algorithms It again uses the first equation to get the break-even point in order to decide which algorithm to use: 1-pass or 2-pass You might have experienced this yourself while fetching a small file in memory v/s reading a huge file in chunks The authors finally suggest the 1-minute sequential rule Notice the lines in Figure 3 in the paper Fundamentally tape technology is VERY expensive to access This encourages very large trape page sizes and very cold data on tape Cold data storage is a term used to store large amounts of inactive data Deciding which data would be inactive is another pain in the neck that engineers still solve case-to-caseAn index increases the speed of the searchB-tree is commonly used in data storage and file systems It keeps all the keys sorted for sequential traversing keeps the index-balanced can handle an arbitrary number of insertions and deletions uses hierarchy to minimize the number of disk reads Click here to read in detailNow you have to read these pages which consist of indexes This reading comes at a cost and your target is that the total cost of reading the index page and then fetching the data is significantly lower than reading the data as if it wasnt indexedThe paper also addresses the depreciation cost used in storage systems I wonder how storage services like S3 and GCS decide their pricing module considering the depreciation cost in mindI found the research paper quite good It uses case studies to identify problems and suggests solutions However it would be great if there were formulae derivations in the paper or some references given to them I hope that after reading the research paper you get to decide your main-memory cache and cold-storage size using some concrete formulaeI am also looking to read some similar papers published recently and want to draw some analogies  1997 to 2020 Please give me feedback on how can I improve my writing what else could be included for you to understand the paper better and faster
AwHUmzBr8YmhzfzFa4L97N,In this first part of my articles we are going to run Airflow DAG using docker Before that lets get a quick idea about the airflow and some of its termsFirst of all in this project we will try to use Airflow as an ETL workflow in order to populate our database from REST API sources I will start by explaining ETL processETL: In most companies potentially useful data is inaccessible; one study even found that two-thirds of companies derive little or no tangible benefit from their data Data tends to be locked away in silos legacy systems or rarely used applications ETL is the process of making this data available by collecting it from multiple sources (see diagram above) and subjecting it to cleansing transformation and ultimately business analyticsThe following paragraphs describe the three steps of the standard ETL process: Step 1: ExtractionETLs goal is to produce clean easily accessible data that can be effectively exploited by analytics business intelligence and/or business operations Raw data can be extracted from a variety of sources in particular: Step 2: TransformationThe transformation stage of the ETL process is the most essential operations The most important operation in the transformation step is to apply the companys internal rules to the raw data in order to meet the reporting requirements: the raw data is cleaned and converted to the appropriate reporting formats (if the data is not cleaned it becomes more difficult to apply the internal reporting rules)The transformation applies internally defined rules The standards that ensure data quality and accessibility must take into account the following practices: These operations transform large volumes of unusable data into clean data that you can present at the last stage of the ETL process the loading phaseStep 3: LoadingThe last step in the standard ETL process is to load the extracted and transformed data into its new location In general data warehouses support two modes for loading data: Batch and incremental loadNow we understand basics of ETL process so we can say that an ETL workflow involves extracting data from multiple sources processing it and extracting value from it storing the results in a data source (data warehouse data lake …etc) so we can consume(processing visualization…) by other consumers ETL processes offer a competitive advantage to the companies which use them since it facilitates data collection storage analysis and exploitation in order to improve business intelligenceWHAT IS AIRFLOW:Apache Airflow is an open-source tool to programmatically author schedule and monitor workflows Developed back in 2014 by Airbnb and later released as open source Its a scalable flexible extensible and elegant workflow orchestrator where workflows are designed in Python and monitored scheduled and managed with a web UI Airflow can easily integrate with data sources like HTTP APIs databases (MySQL Redshift Postgres…) and moreComponents of Airflow : Apache Airflow consists of: THIS THE END OF PART 1 OF MY POST WE COVERED THE PRINCIPALS OF ETL PROCESS AND GET AN IDEA ABOUT THE ARCHITECTURE OF AIRFLOWIN THE PART 2 WE WILL TRY TO DO AN HANDS-ON ON HOW WE CAN SET UP AIRFLOW WITH DOCKER
HQ8GufzdPVmWPUCGg7ahTQ,Apache NiFi provides a large and diverse library of processors for acquiring and transforming data and a flow registry for versioning these often intricate data flows B23 uses NiFi across multiple infrastructures and orchestration platforms including Kubernetes Our pioneering NiFi engineering work allows us to programmatically provision data flows using a library of pre-existing best-in-class data flows that we have developed and honed after many years of operational use Thanks to the recent development work on the NiFi-Fn project by Sam Hjelmfelt at Cloudera there is now a direct path to running NiFi Flows directly on Kubernetes without the need and overhead of an administratively complex NiFi clusterThe NiFi-Fn project brings to NiFi the ability to execute pre-existing data flows as serverless applications What this means to NiFi users is that flows can now be started on-demand and run to completion with success determined by the successful processing of all flow files sent as inputs to the flow The B23 Kubernetes Operator for NiFi-Fn offloads the job management of specific flows to Kubernetes An inspiration for our work was the recently open sourced Kubernetes Operator for Apache Spark released by GoogleAt B23 we choose to run container-based workloads on Google Kubernetes Engine (GKE) For us it was important to manage our NiFi flows as first-class citizens in Kubernetes by utilizing Custom Resource Definitions (CRDs) This allows us to create and manage our data flows just like any other resource in Kubernetes Using familiar commands like `kubectl create -f nififn-flowyaml` or `kubectl get NiFiFn` we can create new flows and list running or completed data flowsThe operator will handle the creation of a Kubernetes Job resource and execute the desired flow after pulling it from the registry This lets Kubernetes handle the semantics of retry and cleanup while giving the user control over the logic and execution of the flowThe B23 Kubernetes Operator for NiFi-Fn is open source and Apache licensed It has been tested locally with docker-for-desktop and in the cloud with Google Kubernetes Engine We have many ideas about how to improve this new capability and we look forward to working closely with the NiFi community to develop it further If youd like to try out the operator for yourself head over to the github repository to get started: https://githubThe JIRA ticket for the NiFi-Fn project: https://issuesapache
BxytgRWm5e64ExFUN6D5Qu,We are surrounded by data geeks and engineers who are capable of coding in phyton/r developing streaming processes using spark and designing etl procedures using sqoop/flume etcWhen someone begins to talk about above technologies we suddenly think that this is kind of developer we need in order to keep our data related projects to be carried onBut do we really? Think about a business case where you need to make many transformations in a sequential data flow order in order to meet the data analytics requirements And these sequential flows requires a good understanding of how data is produced in opetational systems layers deep know how of the data structures used in operational applications layers and etcOr think about some orher environments where you need to extract data sets from SOAP web services or RESTFUL servicesFor the first option a given data geek or an engineer had better know the basics of OOP and have a deep knowledge of data structures And regarding the second option sometimes these phyton / r scripts are not enough You need some geeks who can really code in java and/or c#Using third party tools writing some scripts are of course very valuable assets but on my point of view in order to become a good and respected data geek or an engineer one must put his/her hands into the real dirt by coding
8yWqPhQhqSf2YcUnVKLiGn,I have been using Kafka since I worked as a Data Engineer This story will tell about how to install Kafka on MacOS There are many ways to install Kafka on MacOS we can use Homebrew Docker or Tarball file I use the Tarball method because they are light-weight and isolated from the MacOS systemFirst download the Tarball file from hereClick the latest binary version and choose the recommended mirror siteThen extract the Kafka Tarball file with tar command Rename kafka directory for ease of useKafka depends on Zookeeper for cluster management Before we can start kafka instance we need to start Zookeeper instance The Zookeeper is already available in this Tarball file so we only need to start the instanceThe zookeeper configuration file is available at kafka/config/zookeeperproperties Change only the properties below and let the others defaultzookeeperdataDir: Directory path for zookeeper keep persistence data and logclientPort: Port that zookeeper will listenAfter configuring the zookeeper properties we can start the zookeeper instanceZookeeper is ready now we move one to Kafka instance The Kafka configuration file is available at kafka/config/serverproperties Change only the properties below and let the others defaultserverlisteners: Port that kafka will listenlogdirs: Directory path for kafka to save the log datalogretentionhours: Time in hours to set how long the message will availablezookeeperconnect: Set the zookeeper to be usedThen start the kafka instanceNow we can create a Kafka topic This command will create a topic named test--bootstrap-server: Kafka instance address and port--replication-factor: When set to 1 data will only copied to 1 node (cluster mode)--partitions: How many data will be split into partitions (clsuter mode)--topic: The topic nameUse this command to list all created topicsTest the topic with produce and consume messages To produce messages we can use kafka-console-producersh command--broker-list: List all the kafka instances if more than 1 instance sparate with comma--topic: The topic nameUse kafka-console-consumersh to consume the message--broker-list: List all the kafka instances if more than 1 instance sparate with comma--topic: The topic name--form-beginning: Will consume the topic from the first messageFINALLY KAFKA IS READY TO USE !!!But this is not the final story this is the beginning of the journey In the next story I will tell how to use Kafka programmatically~~ Thanks for reading my story and the clap see you in the next story
8juVLYFi2BQmu6vCfNjrDt,I have completed the Data Engineering Nanodegree by Udacity and I thought I would share my experienceLike many other online providers Udacity offer video lessons (which you can follow whenever it suits you) complemented with quizzes online workspaces for exercising and additional material (documentation and references)There is an online communities to seek help or exchange tips a KnowledgeBase for technical (and non) questions and a dedicated mentor (more later…)The web-based portal is pretty good you can resume a lesson at the point that was left and there is some extra material (introduction to Python or other libraries)Every module terminates with an assignment (few days to a week effort) to solve a data engineering problem with the concepts learned during the lessonsKnowledge of SQL and Python is required ideally at an intermediate level at least: in every lesson there is data modeling and Python coding if you struggle with those it is going to be a long painful journeyUdacity bonus: some introductory material (Python Numpy and Pandas) is providedSafely skip this This an intro to the platform and Udacity Career Services (PLEASE!!) Seriously? We come here to learn not to look for a jobA good start with a dive into data modeling with relational and NoSQL databases covering important concepts like denormalization star and snowflake schema and also working with Postgres and CassandraThis block requires to deliver 2 projects: Data Modeling with Posgres and Data Modeling with Cassandra Good stuffAnother good one: designing a Data Warehouse using Amazon AWS computing specifically Amazon Redshift and S3 StorageThis is a juicy section: copy raw data from S3 into staging tables create a cluster to process the data following a Star Schema design `Infrastructure-as-code`is also introduced with some clear examplesPSApache Spark (finally) turns up The theory behind about the tool features and data wrangling are fine but the practical part is disappointingThere are neither clear instructions nor good examples on how to setup and run Apache Spark some lessons have clearly gaps which make the project quite hard A big disappointment especially considering that Spark was one of the attractive elements of this programDesign execute and debug pipelines on Airflow thats good but mainly because the tool is actually amazing Again the concepts are relatively well presented but there is not a lot of info on how to run Airflow rather than relying on the available workspaceThe course closes with the final project where you are supposed to put into practice what you learned from day one Datasets are made available and there are references to a few good sources The project instructions are kind of broad to ensure the student makes assumptions and justifies design decisionsA partially frustrating experience because a lot of the effort has been put into configuring and running the actual tools rather than develop the ETLAnother big let down I was looking forward to soaking up views and tips from an experienced data engineer but it didnt happenThe first mentor cancelled a meeting at the very last minute and did not show up at another agreed time (always evening meetings by the way) the second one did the same so I decided to get along on my ownOverall I would recommended it if you are keen on getting into the #dataengineering arena Despite some pain and unnecessary effort the nanodegree covers all fundamentals and gives a good overview of the available toolset It could be a better but I guess this is true for everything (and everyone)
4887MLGn2r3kTxQCRXxPi5,Whether you are starting your career in the Data Science world or in IT industry or already in one of them  Writing Structured Query Language(SQL) queries has to be one of the things you have to masterSQL is the lingua franca for data Database Developer/Data Analyst/Data Engineer/ETL Developer/Product Analyst/Data Visualization Specialist/etc are some of the roles where SQL is used frequentlyWhether you save it for future use or share it with anyone  the SQL queries need to be in a way that is easy to understand for the reader Reason being  As the queries start becoming complex  the readability decreases if not formatted properly Also  your job may include sharing data along with the queries with a person who may not be well versed with the data and an easily understandable/ appealing SQL query will help them understand the intentions of the SQL query fasterThere are number of things which need to be taken care of while writing SQL queries which are appealing  such as how to select alias names? which fonts to work with? How should the code be formatted? Below details will help you with the sameWhenever working with SQL  always use any of the mono-spaced fonts(fixed-width)This makes the code more readable some of the common mono-spaced fonts are courier new lucida console monaco consolas etc Here are some mono-spaced fonts from google Below is to show how a non fixed width font makes SQL code less readableMost of the editors specifically created related to databases will usually have the default fonts which are fixed width but just in case not  switch it to any fixed width fonts I personally use courier new font for writing sql queriesAliases are given to the table names as well as column names many a times when people write SQL queries  table aliases are often given random convenient names such as abc which does not provide the correct context and is difficult to read Try giving the aliases which are appropriatea Try creating a alias for a table where each word in a table is shortened to at max 5 characters for example  if there is a table named apps you can use the alias apps which is same as table name If there is a table named downloads you can use the alias dwnld If there is a table named boot_errors you can use the alias boot_err try giving the alias which sounds similar to the actual table name but with less number of characters This will help when creating complex queries and using the alias will be more intuitive and natural without having to refer what table alias a or b wasb In case there would be a self join or the same table being used twice or more in the query provide the table alias with something it will represent for example  Lets say you have a table apps containing all the apps listed on app store and table downloads containing details about each app downloaded from the app store you want to have a result set containing device type where first download happened as well as the device type where the latest download happen for all the apps  one way to get this result is you would join the downloads table twice with the apps table  once to get the first download and another to get the latest download see below- giving an alias fst_dwnld to the table data from downloads which indicate the first download  will not only make it more readable but also more intuitive for someone to modify the query and pick some more attributes for the first download: c In case you create a sub query which needs an alias to be given even then do not go with abc  you can go with an alias this which sounds good when you are reading the queryIt is good to have a single space before and after any comparisonFormatting is very important while writing big complex queries you should format queries in such a way that there is spacing at the same position on each line This way once the whole query is created it will look like a straight river flow(spacing) with establishments(all the query elements) on both sides OR something like a railway track(spacing) with establishments on both sidesAll the keywords which define the structure of the query should be on the left side of the spacing and the table specific details should be on the right side What this means is select/from/join/where/or/and etc clauses should be on left side of the spacing whereas table name/column name/etc should be on the right side see below example: As a person working with data sets and complex queries I request you to give an extra effort to formatting and aliasing the query properly and you will see the results(quicker understanding of the complex query when you yourself see it after sometime or to someone who is looking at it for the first time)All the above points mentioned are a result of experience working with data as well as various resources googled throughout the journey All the tips mentioned are not rocket science and are small things when embedded while writing the query itself will improve the readability Try implementing these while writing query itself and not afterwards it will feel cumbersome at start but will come naturally after some practice
DUUiJecoaC5C7LNJxw5VZR,As the third largest e-commerce site in China Vipshop processes large amounts of data collected daily to generate targeted advertisements for its consumers In this article guest author Gang Deng from Vipshop describes how to meet SLAs by improving struggling Spark jobs on HDFS by up to 30x and optimize hot data access with Alluxio to create a reliable and stable computation pipeline for e-commerce targeted advertisingTargeted advertising is the key for many e-commerce companies to acquire new customers and increase their Gross Merchandise Volume (GMV) In modern advertising systems for e-commerce sites like VIPcom analyzing data at a large scale is the key to efficient targeted advertisingEvery day at Vipshop we run tens of thousands of queries to derive insights for targeted ads from a dozen of Hive tables stored in HDFS Our platform is based on Hadoop clusters to provide persistent and scalable storage using HDFS and efficient and reliable computation using Hive MapReduce or Spark orchestrated by YARN In particular Yarn and HDFS are deployed together where each instance hosts both a HDFS DataNode process and a YARN NodeManager processIn our pipeline the input is historical data from the previous day Typically this daily data is available at 8 AM and the pipeline must complete within 12 hours due to the time-sensitivity of targeted ads Among the various computation tasks Spark accounts for 90% of the tasks and Hive takes up the remainderWith Alluxio we separate storage and compute by moving HDFS to an isolated cluster Resources on the compute cluster are scaled independently of storage capacity while using Alluxio to provide improved data locality by making additional copies for frequently accessed dataThe major challenge when running jobs in the architecture shown in Figure 1 is inconsistent performance due to multiple reasons Under ideal circumstances it takes about 3 minutes to complete a single query However the time can vary as much as 10x up to 30 minutes as shown in Figure 2 This variation in performance makes it difficult to meet business requirements as operations are rarely completed by 8 PMAfter some preliminary investigation we found that data locality was the main culprit With a large number of nodes in the cluster it is unlikely that the data needed by a computation process is served by the local storage process Remote requests from other storage processes created bottlenecks on certain data nodes All these factors contribute to a high variance when retrieving data from HDFSIn looking for a solution our goals to improve data locality throughout the cluster are: We chose to integrate Alluxio as a new solution; Figure 3 shows the new architecture after deploying AlluxioIn the architecture with Alluxio the compute cluster is co-located with Alluxio and is separated from HDFS This ensures compute jobs have no impact on HDFS I/O and vice versa In order to keep the compute jobs compatible with the new architecture we copied the existing Hive tables The new tables point to the same underlying data in HDFS through Alluxios UFS to avoid any inconsistenciesIn addition to the recommended Spark configuration in Alluxio documentation we also added following to the Spark configuration file: With this setting the data passes into HDFS through Alluxios mounted UFS When storing Hive table data to Alluxio modify the Hive table metastore to point it to Alluxio by setting it to alluxio://zk@zkHost1:2181; zkHost2:2181; zkHost3:2181/path With this configuration the data in Alluxio can be accessed by SparkFigure 4 shows the compute results after deploying AlluxioTesting environment: 192TB SSD 40 cores 10000M/s network speed 10 local machines •  100 SQLs each SQL executed 5 times: 2) 500 SQLs each SQL executed 1 time: As shown most queries with Alluxio improved by at least 10% with an average improvement of around 30%Our Hadoop cluster runs more than 700K ETL jobs Network CPU and disk are all saturated for a good part of the day Workload fluctuation from numerous exploration jobs is especially difficult to predict and takes a heavy toll on the system as a whole These exploration jobs cause significant sporadic delays for all ETL jobsTo ensure SLAs for time sensitive applications are met one essential mechanism we employ is to run these in an isolated environment We achieve this architecture by surrounding the main Hadoop cluster with multiple satellite clusters each homogeneously configured to use SSDsThe challenge for this architecture is turnaround time At first it took us weeks to set up a special purpose satellite cluster with carefully planning required for the memory and SSD capacity As usage of a satellite cluster evolved applications needed larger storage capacity Scaling up the cluster was very time consuming as we had to get an additional infrastructure team involved for provisioning new hardwareWith Alluxio adopting an architecture with satellite clusters was much easier; either as a standalone spark cluster or a yarn cluster with node labeling Expanding or shrinking a cluster can now be done within hours instead of days The main storage centric cluster is now offloaded as we can cherry pick hot data to cache on a compute centric satellite cluster and also exploit local HDD if neededBy integrating Alluxio into our stack we can reliably meet our daily data pipeline deadlines for generating targeted advertising The newly adopted technology stack is applicable not only to our specific use case but also empowering data analysis applications across our business At the same time we were able to considerably save on total computing resources utilized at the marginal cost of provisioning additional SSD space The additional SSD capacity is used by Alluxio to maintain the working set of data local to the tasks accessing it The number of copies local to compute is now dictated by the applications access pattern instead of a fixed number of copies for all data as with the previous architecture regardless of the data being recently accessed or notAt the moment Vipshop is only using Alluxio to address the data locality reading problem with HDFS Among the various other use cases of Alluxio deployed in other companies many are also applicable to Vipshop so we hope to continue to explore how to better utilize Alluxio to stabilize and accelerate our data access We look forward to continuing our collaboration with the Alluxio team
EevSpfGgEfNxttcKPtvQzY,For a mobile advertisement to be successful it is important that marketers of Brands be able to identify and define the target audience for their campaigns correctly Displaying an advertisement to the right person at the right time is one of the critical tasks of a DSP (Demand Side Platform) sub-system However within the advertising ecosystem creation and management of these audience segments that the advertisements target are done by the sub-system called DMP (Data Management Platform) A typical DMP is a large scale data store where user activities are stored in structured form That data is processed and each user is categorized into a set of groups called audience segments These segments as well as other user-properties need to be easily queryable through an API A modern DMP should also be able to perform and help in advanced analytics eg look-alike audience creation audience research and insight etcFrom the system design and architecture perspective the requirements of a DMP present quite a few challengesInput to the DMP are audience activity signals converted to structured form These data can come from various channels Eg web-visit SMS payment data WiFi location signals LTE location signals etc These data can come in different forms From structured (payment data) to unstructured (SMS other forms of text data) Some data may require sufficient pre-processing due to low signal to noise ratio Eg WiFi based location signals where a DMP is only interested in significant locations changes or only in walk-ins to the venues of interest Data rate can also vary from one channel to the other While payment data can come at the rate of few hundred signals per second the web-visit data may arrive at a rate 100s of thousands signals per second So high throughput of Insert and Update operations along with the ability to filter and normalize incoming signals at a high rate is an important architectural goal of a robust DMPThere is another non-trivial challenge found in processing these signals Typically each signal is associated with a set a properties of the associated object Eg in case of a venue-visit the venue is associated with a set of properties It can be a fast-food center coffee-shop location in a particular square All these are properties of the object  venue Though hopefully not frequently but these properties can change over time Say a fast-food center became a sit-down restaurant A DMP can either associate these properties with the signal at the storage time by expanding the signal information as it is saved in the database or at the query-time by performing expensive JOINs The former makes the queries easier while the later makes dealing with dynamic changes easier since the association is made at the query time Depending on the requirements this is a design goal as wellA robust DMP needs to keep these signals in its data store for long time Many of these data have seasonal property and goes up and down at different time of the year Eg customer purchase data which typically follows a yearly pattern This seasonal property is an important data pattern that a DMP uses To capture that property this data needs to be persisted in the DMP data store for at least 13 months Some may store for many years before archiving it Of course data from different channels may require different retention policies But in general one can assume that we are talking about storing and analyzing very large volume of data We are definitely talking about an efficient distributed database to store and process this data Some of these data are very valuable and has high reliability requirements Eg user payment history detected venue walk-ins But in general all the data should be kept in a reliable fashion The data reliability becomes more challenging due to high throughput and volume requirements So a flexible scalable reliable data storage is another architectural goal of a DMP sub-systemA modern DMP needs to answer the questions presented in various different structures EgIn order to understand the architectural challenges in addressing the above queries let us assume that the physical data layout is same as the conceptual one A giant table where the row-keys are the user-ids and the columns are representing the user-activity objects Eg physical or online places visited by the user properties of the objects purchased Such a table will likely contain 100s of millions of rows and 100s of thousands of columns although it can grow much larger Now looking at the queries above (1) and (2) has the property that it will scan a large number of rows of a small number of columns (3) has the property that it will scan a limited number of rows but across a large number of columns Traditionally distributed database architectures are optimized/designed for one of these two work-loads Pure columnar storage systems like Parquet+Presto are optimized for the analytical workloads which span across large number of rows for small number of columns Database systems like HBase on the other hand are optimized for querying a batch of rows containing large number of columns in very large tables very efficientlyQuery (4) on the other-hand presents a yet another different challenge where one needs to analyze a large number of columns over a large number of rows This needs to be very efficient Often dimensionality reduction(PCA) or MinHash kind of approximation techniques are applied to reduce the problem to sub-linear time/space requirementsThe volume of the data and the variety of the queries present another architectural challenge and an operational nightmares That is how many different technologies will be needed to be deployed and maintained for the complete DMP solution? Any one who have run an engineering organization or operations team which created and managed large data pipeline clusters will appreciate this challenge So another important architectural goal becomes to minimize and simplify the technology stack In a traditional open source based approach the tech stack may look like the followingThe above goal points to three important aspects of the architectural considerationsA DMP needs to expose APIs that the Brands can use to define their own audience segments and query those audience segments But the most challenging part of API design comes from the requirements of segment definitions The API constructs should be flexible enough to define any segment using those constructs but at the same time should be easily understandable and maintainableIf the DMP supports SQL interface then the above can be framed as a SQL query However the challenge remains to construct the query so that it avoids complex and repeated scans of the data Eg joins larger group-by order-by operations Since the query depends on the database schema the final database where the query will land also needs to be designed carefullyIn case of noSQL databases and where a SQL wrapper is not available such segments can be defined separately in JSON or similar format in plain text Tools like Spark can be used to construct the segments out of a large volume key-value store based on the segment definitionsA robust DMP may be expected to build thousands of segments everyday So these segment building queries should complete in the order of seconds Also these queries should have minimum impact on the on-going update/insert operations in the DMP This brings serious optimization challenges for database design table schema design and query designA DMP is the heart of an advertising ecosystem But the requirements bring challenges from quite a few different directions In order to build a successful DMP care should be given to all these aspects Trade-offs should be made carefully where needed considering the possible outcomesReferences: http://proceedingsmlrpress/v53/ma16
neR25yDoVLxe5zu6rSa8Sr,At Razorpay we have billions of data points coming in the form of events and entities on a daily basis To ensure and enable that we operate using data at its core enabling data democratization has become essential This means that we need to have systems in place that can capture enrich and also disseminate the data in the right fashion to the right stakeholders with high speedThe Data Platform Team at Razorpay strives to leverage this data to improve our stakeholders experiences Our mission is to provide infrastructure to collect organize and process this deluge of data (all in privacy-safe ways) and empower various business units across Razorpay to derive necessary analytics and make data-informed decisions from itThe primary goal of the data platform is to expose high level and business critical data within the organisation through various dashboards powered by Looker Most razors use these dashboards every day to make various decisions on success rate monitoring/ improvement of payments/payouts processed through Razorpay Looker Dashboards also allow real-time tracking and monitoring of various aspects of our business and systems As a result the timeliness of these dashboards is critical to the daily operation of RazorpayPerformance  Our raw data resides in S3 and external tables are created on top of this data using Presto on Hive Analysts currently schedule presto queries to generate aggregate tables and use them to improve their query performances for adhoc needs report generation dashboards etc Even though these aggregate tables improve their experience compared the average query response times were still high and that has a direct impact on their productivity So the system we adopt has to help us achieve interactive analytics and also decrease the overhead of scheduling presto queries by our usersHigh Availability  The system needs to be reliable as critical dashboards weekly and monthly reports ad hoc queries have to be powered using this system We cannot afford to have unplanned downtime as that would have direct impact on scheduled reports and alerts set on dashboardsScalability  We have a fast increasing user base and use cases to serve for example we currently experience tens of thousands of presto queries per day with around 60 80 queries running concurrently for most part of the day Because of high p95(6 mins) and p99(14 mins) values we maintain a 40 node presto cluster to support our workload So the system we choose should be able to handle a similar scale of workloads and improve on the performance and resource utilizationTo solve these challenges we tried to build multidimensional cubes using Apache Spark and Apache Kylin Even though both the systems improved query performance significantly both these systems failed to satisfy our data ingestion and scalability requirements Both the solutions aim at pre-computing the data for possible combinations of dimensions and as the number of required dimensions grows the cube build times grow exponentially and also has a significant impact on storage We wanted Kylin to succeed because we need not do any kind of pre-processing as Kylin can directly sync with our existing HIVE server and build cubes on top of those HIVE tables but because of ultra high cardinality of our data and many dimensional columns being involved in a single cube the build times have been huge and the build process was also complex to understand debug and optimize Even though druid required flattening of data before being ingested into it it was much easier to set up ingest and query data using druidApache Druid has been selected for obvious reasons as listed below: Fast query performance  With predefined data-sources and pre-computed aggregations Druid offers sub-second query latency The dashboards built on top of Druid can be noticeably faster than those built on other systems Compared to Hive and Presto Druid is orders of magnitude fasterScalability  Druid architecture is well separated out into different components for ingestion querying and overall coordination We have found this componentized architecture to be reliable and stable for our workload as it allows us to scale the system easily as needed Druids architecture of separating data storage into deep storage for long term storage of data while loading only the required data into the historical node has worked well for us Keeping the analytical data permanently in S3 gives us disaster recovery for free and allows us to easily manage clustersIngestion  Druid had much less ingestion times compared to Kylin and creating cubes using Apache Spark because it doesnt store aggregates by pre computing for all possible combinations but by indexing the data in own way This also helps us in reduced storage Also Druid has an advanced integration with Kafka compared to Kylin which helps us to create real time analytics stores more efficientlyBusiness intelligence  Druid is commonly used for BI use cases Razorpay have deployed Druid to accelerate queries and power applications Unlike SQL-on-Hadoop engines such as Presto or Hive Druid is designed for high concurrency and sub-second queries powering interactive data exploration through a UI In general this makes Druid a better fit for truly interactive visual analyticsApache Druid is a high performance real-time analytics database Its designed for workflows where fast queries and ingest really matter Druid excels at instant data visibility ad-hoc queries operational analytics and handling high concurrency  druidDruid has a multi-process distributed architecture in which each process takes care of different aspects of a database This design helps in achieving improved fault tolerance and high availability Each druid process can be deployed independently or in the recommended way of organizing processes into servers Each process can be individually scaled/tuned depending on our requirementsDruid is not a relational database even though some concepts are shared Unlike relational databases there is no concept of joins across data sources(druid equivalent of a table) in Druid Because of the data model in razorpay we need to denormalize the tables and make sure whatever columns are required from multiple tables are being included in the druid datasource We run a spark job prior to ingestion into druid so that we are able to denormalize and transform the data in an appropriate manner before the data is ingested into druidThere are 3 groups of columns in druid __time dimensions and metrics __time is a primary timestamp column which has to be present necessarily in a datasource as everything in druid is keyed by this timestamp column Dimensions are those columns on which we filter and group by our queries Metrics are numeric(mostly) columns that can be aggregatedBy limiting its ability druid is able to store data in custom columnar format known as segments Segment files are druids way of partitioning data based on time One segment is created for each time interval this time interval is configurable at the time of loading data into druid For our use cases so far we use a time interval of one day that is one segment/day Depending on the number of rows or size of data in a particular interval we can also have multiple segments for a time interval If the datasets are huge having multiple segments for a time interval can improve query response times significantlyDruid supports both batch and streaming ingestion from various input sources In our case S3 and Kafka are input sources for batch and streaming ingestion respectivelyDruid can roll up data at ingestion time to minimise storage by storing only aggregates Roll up  Metric columns based on the given aggregate function are aggregated across a set of rows having identical dimensions within a configurable query granularity By rolling up data we lose the ability to query individual events roll up has to be disabled if we wish to ingest data as-isMajority of our current ingestion tasks are batch workloads A spark job flattens the relational data to overcome lack of joins in druid and writes to s3 in parquet format Within druid we run native index tasks to ingest data from S3 into deep storageBy setting maxConcurrentNumSubTasks greater than 1 we can run an indexing task parallel But ingesting this way creates multiple segments for a time interval as each task creates a separate segment version and these are pushed to deep storage as-is instead of merging into one segment per timechunk The datasource would then require a compaction task or reindex task to merge those segments to maintain optimal number of segmentsTo avoid compaction or reindex task we split the data based on __time column(we decide which column will be the primary timestamp column before loading data into druid) into a number of splits which takes into account the size of data and available middle manager workers and ingest them with maxNumConcurrentSubTasks set to 1For events we directly connect to the Kafka cluster and subscribe to the topic We currently maintain 1 datasource for each topic and required transformations are done in the tranfromSpec(part of ingestion spec) in druid We have been able to achieve significant roll up ratios in events reducing storage footprint and also improve query response times We use druids inbuilt Kafka indexing service extension which creates a supervisor for a Kafka topic Supervisor is responsible for offset management and task management for a particular datasource Supervisor spawns MiddleManager workers and sequentially submits index Kafka tasks to the workers with specific configurations and ingestion specDruid supports two query languages  Druid SQL and native Json Druid SQL is a built-in sql layer which uses Apache Calcite to translate sql queries to native json queries Our internal micro-services submit queries to the druid router over https using native query language Our BI tool looker uses Avatica JDBC driver to make druid sql queries to the router processBasic cluster tuning guide provided in the official documentation was very helpful for us to tune the druid cluster While tuning druid for performance on scale we need to be mindful of the type of queries that are being made to the druid cluster Managing merge buffers and number of processing threads in historical and middle manager processes is key to achieve desired query response times and multi tenancy Number of group by queries at any given time is limited by the number of merge buffers available in the particular process to which the query is made Number of processing threads in a process(Historical and MiddleManager) limits the total number of segments that can be processed in parallel while achieving multi-tenancy it is important to have an idea on how many segments do we need to be processed in parallel as less number of processing threads leads to higher response times and over provisioning processing threads means throwing more cores into the cluster which leads to unnecessary costsDruid has an inbuilt metrics emitter we push these metrics to kafka and ingest them into another druid cluster which is used only for monitoring to build performance and monitoring dashboards on looker We monitor query metrics ingestion metrics number of segments segment size etc at service datasource and segment level to optimize our storage and manage resources to performances satisfying our SLAsDruid is a big data analytics engine designed for scalability and performance Its well factored architecture allows easy management and scaling of the Druid cluster and its optimised storage format enables low latency analytics queries We have successfully deployed Druid at Razorpay for our use cases and see continued growth in its footprint We were able to achieve p90 p95 values of less than 5s and 10s respectively and our dashboard performances have improved by at least 10x when compared to presto We continue to work towards increasing adoption of druid to improve analytics experience within Razorpayhttps://druidapacheorg/docs/latest/design/architecturehttps://druidapacheorg/docs/latest/operations/basic-cluster-tuninghttps://netflixtechbloghttps://medium
mq8DwPAwGU52jfJv5DKBNm,The paper A Framework for Collecting YouTube Meta-Data is a good read Inspired by the core ideas behind I optimized my YouTube comments collector and release a subset on Kaggle for study use :https://wwwkaggleThe data I release on Kaggle contains comments updated in 2016 together with basic information of corresponding videos I summarized features in this paper For detail I suggest reading the original paperSearch resource provided by YouTube Data API is designed to simulate the search activity on YouTube website that can be easily handled by human Thus it is not scalable enoughThe authors take advantage of another method of Search resource which returns up to a few hundred of videos that are related to the referred video given a video IDInitiated by a small set of seeds video IDs these seeds are potentially able to grow exponentiallyHowever a redundant videos will return when it is related to more than one videos after cultivating seeds over multiple generationThere are two possible solutions to remove duplicate video IDsRestricted by the feature of the relational database an entity in the database must have a primary key to distinguish it from te other entitiesSending queries to database requires accessing hard drive meaning that it costs much longer time than methods that process data in the main memoryIf the database is set remotely we will have the internet speed as a limiting factorHashSet extends the AbstractSet class and implements the Set interfaceAbstractSet only allows unique value stored in it As an inheritance of Abstract Set HashSet will first check whether the element is already in the set: if not simply puts it in; otherwise makes no changeCompared with other kinds of collections HashSet is the fastest with constant time performance O(1) for the basic operations (add remove contains and size)Although ArrayList and TreeSet have some additional functions that HashSet doesnt have these functions are useless in removing duplicate IDsHashSet in memory cannot keep growing unlimited The process will break because of hitting the memory limitIn our experiment the memory heap size allocated to the JVM is 2GB Assume that we initiated with a set of 50 video IDs and retrieve 25 related videos for each of them after expanding 4 times (the 5th generation) we will get more than 200MB data in the memory The 6th generation will break half way since the memory limit is reachedThe authors capped the size of seed videos used for searching with 10000 for each generation When gathered a set of related videos insert them into the database choose 10000 from them as new seed videos and then release the set In this way the database shares the burden of removing duplicates
T7HHfitPqwrb2SgrZQoLLD,The nature of todays enterprise applications environment is that the critical data needed for operational and analytical reporting reside in multiple cloud and on-premise applications A lot of companies have multiple ERPs multiple CRMs and multiple other cloud applications that they have implemented over time or inherited them through acquisitionsSimilar to their transaction systems companies will also have a mixture of modern and legacy BI and ETL technologies The need to bring the data from all these source applications together into one or more data warehouses (ideally one) whether on-premise or on the cloud is more important than ever Business performance analysis can be achieved by having all the required data together in a user-friendly manner for the data scientists and analysts to do their jobEach of these source systems have a different way of being accessed by the ETL/ELT tools Some have API others expose a ODBC/JDBC interface and others output the data in csv JSON or other formats As companies build the data pipelines from these sources over the years each of these are run at different schedules and are triggered in multiple different ways Some get triggered by CRON Jobs others by Windows Scheduler others by in-application schedulers In addition most BI tools like MicroStrategy OBIEE Business Objects or Tableau come with their own set of scheduling tools to trigger report extracts loading cubes or for sending dashboards as emails to usersThe result of running these various ETL/ELT and BI tasks with native schedulers in each of these tools is that there will multiple different kinds of code bases and mechanisms to trigger the tasks and more importantly there will be not be one place to check the success of failure of these tasksIn case a business user says that they didnt get the latest data on their dashboard or email then the operations team needs to check all these different schedules in the BI tools ETL tools database triggers OS jobs or even the source systems to find out why the data was not updated Most probably there will be different teams supporting each of these tools Troubleshooting any small issue will turn out to be a nightmare without knowing where the failure is in the data flow If the BI and data engineering teams are always troubleshooting operational issues new capabilities and solutions will take a backseatThe best way to get the BI and data engineering architecture under control is to run all the operations using a central workflow automation and scheduling tool For this to happen you will need to figure out a way to trigger the various ETL/ELT and BI tool schedules using their respective APIs It will take a bit of technical investment to figure out how to work with each of these APIs In general most modern tools have simple REST APIs but some of the older legacy tools need some research to figure them outOnce you have the APIs understood the next step is to write the code to call the APIs for the various tasks that need to be performed For example you will need an API to trigger a download of employee data from your HR cloud platform like SuccessFactors or an API to trigger an Informatica workflow After the APIs are understood you can standardize on a simple language like Python to write the code to call these APIs There will be a lot of sequencing of API calls that needs to be performed by the Workflow Automation tool  kick off Task 2 after Task 1 is complete etcAfter a detailed search of both commercial and open source products like Airflow I landed on a product called JAMS Scheduler by MVP Systems Software With JAMS we were able build out a complete workflow automation solution which was able to call various ETL/ELT jobs using their respective APIs We were able to chain various processes so dependent tasks kick off when others complete BI tool tasks like kicking off an hyper file creation in Tableau or emailing dashboards can be run after the ETL tasks are doneIn addition JAMS has a great UI for monitoring tasks as well as a detailed notification and alerting mechanism in case of success or failure of tasks In BI one of the critical piece is the scheduling and triggering capability For example we had requirements to trigger jobs at 530 PM Malaysia time on Workday 1 3 and 5 of every month With JAMS these kinds of triggers are very easy to setup without writing any code JAMS has an intuitive interface to build workflows with variables and parameters You can call python code power shell database procedures SQL statements all in the same workflow Once you have setup the job to run it runs very reliably You have the capability of putting jobs on hold during maintenance windows set priorities for jobs and re-trigger them on failuresOne of the most important aspects of JAMS is that I can download a free evaluation copy and test out all the features for 30 days During this time I got great support from their sales and technical folks so I can complete the POC and show it to various stakeholders for buy inOverall it seems to be a well thought out stable and easy to use product which we have gotten to rely on for automating a lot of manual tasks in our BI and data engineering operations
86k6FEnG3aWkDJRtvra7RL,Le mieux est lennemi du bien or loosely translated to Perfect is the enemy of good is a saying that most attribute to Voltaire Its a cautionary phrase about the perceived value of chasing perfection versus the reality of it The phrase is something that can apply to numerous domains including Data EngineeringSuggesting that perfection is not the goal often invites skepticism from data engineers After all developing data products requires precision and a detail-oriented approach Further most Data Engineers have been a part of rushed implementations only to have it come back to haunt them Sprinkle in a natural aversion to risk and there are multiple reasons why people would challenge the notion that stopping short of perfection is a good ideaHowever this is not a plea for shortcuts Instead Im advocating for delivering faster and only implementing the valuable featuresThe concept of delivering faster is similar to Agiles principle of shipping a Minimum Viable Product (MVP) The value of quickly putting a product in the hands of a user cannot be overstated For example if your leaders require three metrics to run the business and you have only one completed ship it Driving the organization with some insight is always better than driving with noneThe only thing that might be worse than waiting to ship a viable product is developing the remaining features in the pursuit of perfection These remaining features can often take longer to implement than others or provide a reduced value or both For example if adding a dashboard visualization for a single user requires breaking model and pipeline changes the payoff probably does not warrant the effort All of a solutions proposed features should be scrutinized for their projected return on investment but be particularly vigilant about ones at the bottom of the priority listData Engineering is a challenging endeavor that is often fraught with complexity By delivering faster and only implementing the valuable features you can reduce the risk and increase your chance of success
9MqwgbTN7BdSiLE79RyuJ9,When I started learning about Big Data and Hadoop I got excited about Apache Sqoop I was naïve enough to believe that the ingestion of database tables was a very easy taskLittle did I know how far it was from reality Sqoop is an amazing little tool It comes with tons of options and parameters it can do a lot but it cannot do everything It is easy to forget that if you do not think the whole process end to end A single Sqoop command can pull a table or entire database create Hive tables and even enable incremental loads after the initial one is doneThe sobering reality is that you will encounter plenty of small but very irritating issues with Sqoop: I created a mind map for myself (I love XMind) to outline the pros and cons of each Sqoop import option (direct mode for Oracle a standard mode for Oracle HCatalog integration option): It became obvious that I could not do everything I needed in one hop I gave up on the idea to have Sqoop register Hive tables and found myself writing a bunch of SQL statements to create and populate tables This is when I thought there had to be a better way to do all that I would have to repeat all these steps for 100s of tables and I despise boring repetitive work! I knew I could generate all the things I needed (like a proper Sqoop command and SQL scripts) if I had a place to store metadata about source system tables It was the year of 2017 I looked around and I could not find any tools that would do thatConfluent Schema Registry came to my mind at first but it would only deal with Kafkas messages and would not generate any code for me I found a few abandoned open-source projects to store and manage schemas but they also did not address my needs I learned about the Hortonworks initiative to unify Hive and Kafka schemas in a single schema registry but still it would not let me generate things I neededLinkedIns Gobblin project a universal data ingestion framework looked very promising but after playing with it for some time I was not sure it would handle all our use cases without doing extensive coding in Java And I was not convinced that Gobblin jobs would be easy to support and manageI checked commercial products as well but none of them impressed me at the time I did not care much about the user-friendliness aspect of those products In my opinion UI tools will limit you more than the value they bring I wanted to enjoy the benefits of open source projects like Apache Hive Impala Sqoop and Spark We would need a framework that could deal with dozens of source systems and thousands of tables and the last thing I wanted was to use a point and click graphical interface to configure ingestion pipelines for all those tablesIt was also the time when I was tinkering with Apache Airflow project which prompted me to learn the basics of Python While Ive used many programming languages in my career I never had an opportunity to use PythonI fell in love with the language and its awesome community Guiding principles of Python made a lot of sense to me and a vast universe of Python packages was mind-blowing I was impressed by how readable and compact Python code was I had a blast! I felt like I was an 8-year-old kid again learning BASIC on my first computerMetaZoo collects metadata about source system tables: The concept is generic and can be used with files as wellThere are 3 steps to onboard the new source system First a Data Engineer can use the existing or create a new Python class called metadata extractor This class handles collection of metadata from a given database platform such as Oracle databaseThe next step is to process source system metadata A Data Engineer would create a new metadata processor class The purpose of this step is to create a set of derived schemas based on the source system schema and project requirements For example if I need to ingest data with Sqoop and create external staging tables and internal final tables with Impala the metadata processor class will create derived schemas using proper target data types column names and any other logic you need There is no magic here - this is just a custom Python code that has source system metadata at its disposal to create needed schemas and structuresFinally Data Engineer creates a job generator class At this point we have all the source intermediate and final schemas described but we do not know how data flows into these schemas The job generator will create various snippets of code (scripts SQL DDL and DML commands) It will pick optimal arguments for Sqoop select partitioning strategy for final tables and so onNow we can run MetaZoo using a command-line interface and it will collect metadata from source system create derived schemas and generate all the codeMetaZoo will also register new tables in the job_control table to record the status of the ingestion process (more on that a bit later)Note that MetaZoo does not do actual ingestion or processing Neither does it schedule or coordinate work  this is a function of an external tool In our case we decided to use Apache NiFi an easy to use powerful and reliable system to process and distribute data NiFi fits our architecture well with its inherent capabilities to do both real-time and batch processing and schedulingOur second choice was Apache AirFlow but unfortunately Airflow did not support our real-time needsMetaZoo is a simple Python application It does not have a fancy UI just a basic command-line interface (cli)MySQL is used to store metadata and control tables but it can use any database supported by SQLAlchemyEach of our source systems has a unique identifier called source_system_id When we need to ingest another source system data we would: Once this is done we can use MetaZoo cli to populate metadata for hundreds of tables from that source system and generate all the code we need Two commands below will collect and process metadata for 300 tables and generate all the relevant scripts: One of our source systems (Cerner Electronic Health Record platform) relies on Oracles NUMBER data types without explicit precision and scaleSqoop would map such columns into a string type and Impala would not allow using string columns with aggregation functions Users are forced to convert such columns from a string to a number on every single SQL query they executeWith MetaZoo we added a data profiling step performed during metadata collection process MetaZoo would figure out actual precision for numeric columns and whether whole numbers or decimals should be usedOne of the most powerful features of MetaZoo is to allow reprocessing which is a way to make changes in our pipelines apply them to all the tables and then reload data all at onceLet me give a few examples Some time ago Cerner notified us about replacing all the timestamps in 100s of tables and converting them to UTC It would be a nightmare to go back to ETL jobs find and change all the timestamp columns and convert timestamps to a proper time zoneWith MetaZoo we just changed timestamp conversion logic in one place re-ran MetaZoo and generated all the scripts to refresh those columns at once for 100s of tablesA recent example when our analysts realized that a source system with 144 tables had undesired trailing and leading spaces in all the text columns All we had to do is to change 2 lines of code in MetaZoo and kick off the ingest process again It only took us an hourWith MetaZoo we do not need to deal with each table individually or worse use point and click UI to re-configure 100s of ETL jobsIt is a common practice in the ETL world to have a table to store the current state of jobs MetaZoo is no exception and it provides a basic table called job_control to manage the status of the registered jobsjob_control table: By changing a few values in job_control we can kick off historical reload of existing tables or change watermark for incremental extracts if we need to go a few days backAnother table job_audit is designed as an EAV table and allows us to log certain events during execution of jobs such as duration of the job or row count ingested Based on a project we might audit different things and EAV design allowed us to have this flexibilityRemember MetaZoo itself does not do any data processing  this is the responsibility of the other external tool Neither it is responsible to schedule and coordinate jobs  we use Apache NiFi for thatNiFi is a pretty amazing piece of software One of the greatest features of NiFi is that it is easy to extend it to add the features you need And you do not need to be a hardcore Java developer to do thatWeve created a few custom processors in NiFi integrating MetaZoo and NiFi flows: We also used NiFi REST API to populate certain flow variables using metadata stored in MetaZooIve mentioned Sqoop a few times and how MetaZoo helps us build arguments for it Here is an example of a Sqoop command generated by MetaZoo Can you imagine doing something like below and choosing proper arguments for 300 tables? Note how MetaZoo picked a number of mappers (16) split data by a primary key (DRG_ID) and used a bunch of other parameters: Another good example is a CREATE TABLE statement below generated by MetaZoo It may not look like much but this is a great example of why third-party tools with a nice UI wont do this jobOur real-time tables are backed by Apache Kudu Following Apache Kudus schema design document for best practices and for the optimal performance we need to apply optional lz4 compression to certain columns list primary keys in a certain order and place and so forthBut the most challenging task was to pick a partitioning strategy Partitions in Kudu must be defined upfront during table creation time Not only you need to pick columns to partition by but there are also some limitations in terms of a number of tablets/partitions based on the size of the cluster and hardware specification of nodes To obey these rules we needed to estimate the size of the tablesMetaZoo came to the rescue again Our Data Scientist came up with a very accurate XGBoost model to predict the final size of Kudu tables based on the source tables metadata we already had in MetaZoo Knowing the size of a table we can assign a proper number of partitions while respecting Kudus limitations 160 partitions in the example below are the greatest number that was allowed based on our Kudu cluster size: The last example I want to share is an SQL INSERT statement generated by MetaZoo as well Note how data types are being converted fixing a bad job by Sqoop and considering nuances of Oracles Number data type without scale and precision And note how timestamps are converted to a proper time zone while dropping timestamps before the year 1800 It is worth mentioning that timestamps are stored by Sqoop as long integer value that needs to be divided by 1000 to get Unix epoch time that Impala and Hive can understand: MetaZoo paired with Apache NiFi proved to be quite useful! We have ingested 22 source systems and 1000s of tables into our Data Lake in a short period of time while keeping our team small and nimble MetaZoo saves us time and helps us with ongoing support Apache NiFi helps us run code generated by MetaZoo and makes ongoing support easier with custom alerts logging and providence eventsIf we need to make changes in the ingestion pipeline that involves 100s of tables or if we need to reload data from scratch adding a new transformation rule  MetaZoo makes it easyThe only wish we have is to easily cope with source system schema changes but it proved to be quite a challenging task Fortunately this does not happen often with our source systems hence was not worth the effort for us to fully support schema evolution But it is certainly something for us to consider in the futureOriginally published at https://boristyukincom on November 7 2019
UvdXXkf3aSGm8rqMjsMSmh,Enter me a fresh graduate that only knows what software architecture and RDBMS knowledge that were coming from the college When I got introduced into the more practical concept of microservice vs monolithic I was like wow you can do that? How do you even do SELECT … JOIN with so much thing in mind? but I answered my own question not much laterImagine you work in a company with abundant little less than a hundred microservices where each of them has their own database Each is in their own instance in Google Cloud Platform (GCP) we use CloudSQL Not one instance has more than one user-defined databaseThen everyone whose job is to analyze data or to create real-time monitoring/alerting system related closely to ongoing transactions on production (like for example detecting fraud or simply detecting apps downtime) must have their data gathered from those many databases From my junior-level understanding this is what our data pipeline help to do where it extracts required information from many instances and put them in one place (I heard some call it the single source of truth) for everyone to work on Ideally the extracted data put into the one place should already be cleaned and transformedAs a data engineer we maintain our own MySQL database instance From the production databases masters we created our own replica instance setting it up using whatever automation tools available  in my case Ansible and AWX We then make use of the database replica to start working on pipelines to stream microservices data into a more centralized place  in my case BigQueryNow from the perspective of a junior data engineer  whose very first Medium article is the one you read here next few articles are all about my own experience in data engineering This includes stories like how handling managed instance like CloudSQL is sometimes not as easy as doing things in MySQL
Atfr4sgbDhCm4uFsEmEeFV,If you have encountered such a problem please give this article a full readWhile running top command in Unix/Linux systems sometimes you might have encountered a task of type zombieThese zombie tasks are the processes which have been completed but failed to read the exit_status and still has the presence in the process tableBefore understanding zombie task in airflow lets understand the workflow of a task in DAG as shown in figure belowIn the above task pipeline worker awaits for the operator to finish If theres a connection failure between worker and operator or worker kills off unexpectedly while the operator is still running it becomes a zombie processAs an example lets say scheduler adds a task in executor queue to run a Kubernetes pod operator on a node pool One of the workers picks up the task and initializes the task instance It may happen that the worker is not able to send a heartbeat to the DB because of some broken link between the task instance and worker
fwyhT6aB95UbLV6qgQL3TV,Data is definitely not only hosted in the cloud As we data geeks know most of the time we have to load file(s) from local machines to the cloud Lets not focus on questions like: where should my data be located etcJust think that for some reason you have to load file(s) from a local computer to AzureIn this article I will demo how to use Azure Data Factory to load file(s) from my local Windows computer to Azure Blob StorageThere is no magic follow the steps: This is just a demo in this case I used the user name and password option but I recommend to use the Azure Key Vault optionInstall Microsoft Azure Data Factory Integration Runtime this software will create a secure connection between your local computer to AzureI created 6 files with no data and 1 file with 32 MBExecuted the test pipelineFiles loaded inside Azure Blob StorageSpecial thanks to Raza Ali and Manish Viswambharan
kwPyTzwt8xk2X3TBeTU3ST,In December 2018 I had an amazing job at a company I loved doing work I was immensely proud of This made my decision to quit extraordinarily difficult This decision could itself be the topic of a long and rambling blog post but thats a story for another day In this post Ill talk through my experience being unemployed my approach to the job search and how I ultimately made the decision for my next career moveI left my prior job without a new one lined up When my friends heard I was entering a phase of funemployment they all assumed I would be taking off to backpack around the world for 6 months I take that to mean that I do a pretty good job of hiding my true workaholic anxious nature By the time the New Year rolled around I was already irrationally worrying about my employability and felt like I was behind schedule with everything  interview prep networking and everything in-between Soliciting advice from a few friends who had taken extended time off I was able to assuage the rising feeling of panic and settle into a loose routine for my time off I would get a full nights sleep every day spend mornings catching up on the latest tech news and analysis (I read a LOT of Stratechery) and take at least one weekday each week to do absolutely nothing job related (this usually meant riding my bike all day) The rest of the time would be split between catching up with old friends and former colleagues gathering information about companies I was interested in and practicing my programming and technical whiteboarding skillsFrom both my experience as a hiring manager as well as preliminary conversations with a few peers I understood very quickly that it was a buyers market and I could easily make job hunting a full time effort I also wanted to give myself the opportunity to truly evaluate a broad spectrum of companies of all shapes and sizes across many different industries In total I spoke to 26 different companies had tech screens with 11 (withdrew my application from the rest) did 9 onsite interviews and ended up with 7 offers the majority of which were for individual contributor roles as a data engineer I thought this process would be exhausting but it turned out to be far more fun and exciting than I anticipated It was fascinating to learn about all the different organizations and businesses and the unique challenges faced by each of themAs I talked to more and more people I started to develop a better sense of how to extract real signal from my conversations During interviews most folks are either in evaluation mode or sales mode In both cases theyre likely to stick to an HR-approved script My goal in every interview was to build enough rapport with the interviewer that I could successfully navigate the conversation away from the typical clichés I also did my best to ask very similar questions to each interviewer By listening closely to their individual answers I could then evaluate them each in the context of the whole which would often paint a much more telling picture of the organization than any individual answerEvery company I talked to had extremely aggressive hiring goals Most were looking to double their engineering headcount by the end of the year and more than double the size of their data engineering teams More often than not when I asked engineering leaders about their biggest challenges hiring was #1 on the list I began to evaluate prospective companies through this lens asking how will this company differentiate from all the others when competing for talent? Every company had a different angle for this some leveraging recent fundraising events or a high profile consumer brand others leaning heavily on their social-impact oriented mission I tried to understand not only how their answers appealed to me but how they might appeal to the broader segment of job-seekersI learned a lot during my interviews Rather than try and tie them all together into a neat narrative Ill just list a few things that stood out to me as noteworthy: I count myself as extraordinarily fortunate to have had my pick of some of the best technology companies in San Francisco I was looking for a company with aggressive growth a great product and awesome leadership and while many of the companies I talked to met these criteria Snowflake was a clear cut above When I first started using Snowflake as a customer at my previous job I was totally blown away by their product The Snowflake data warehouse was critical to my job as a data engineer and it was obvious to me how revolutionary a technology they had developedThe job at Snowflake was in sales engineering a big change from my prior role as an in-house data engineer As a sales engineer the responsibilities are primarily around evaluating the data architecture of potential customers helping prove out the value of Snowflake within that architecture and scoping and executing on a proof-of-concept The chance to get a glimpse of data teams of all shapes and sizes across the San Francisco tech scene and beyond seemed like a unique opportunityFrom a team perspective I knew Snowflakes sales and sales engineering org fairly well from my time as a customer Both groups were great to work with  their sales engineering lead was enormously valuable in helping us with our initial implementation and the regional sales director struck me as an ambitious driven individual who would likely push me to realize more of my potential This gave me a high degree of confidence in the general quality of the team over at Snowflake which was confirmed yet again during my interview processAs I alluded to earlier I spent a fair amount of my funemployment reading through the back catalogue of Ben Thompsons Stratechery blog Stratechery focuses primarily on consumer technology with decidedly fewer articles on enterprise software especially a product as technical as Snowflake Even so many of the themes he emphasizes over and over when discussing consumer tech apply just as well to enterprise In this light many of Snowflakes initiatives made sense as part of a broader strategy I didnt see any other players in the space operating at the same level and this combination of superior product and thought leadership made it an extremely compelling opportunityIm only a few weeks into my new role as a sales engineer at Snowflake and so far it has not disappointed The energy around what were building both in terms of the product and the business is absolutely incredible
czdF8XZFBRZiAJKVAqM5bh,By: Dovy PauktsysDovy is a seasoned cloud compute expert with vast experience in IoT user-facing systems ETL pipeline designs and on-prem tear-downs He also is the owner and creator of Redux Framework His goal is to provide the best solutions to fulfill the needs of his clientsApache Airflow originally created by Airbnb is an incubator project for Apache This project has only been around for two years and has quickly become a staple in many ETL platforms Airflow is a workflow management system that provides dependency control task management task recovery charting logging alerting history folder watching trending and my personal favorite dynamic tasksApache Airflow offers many tools and a lot of power which can greatly simplify your life However it is not perfect Since its an open-source project that depends on its users to develop there are definite bugs within the system Follow these tips to avoid the pitfalls I had to learn the hardLets talk about the governing force behind Airflow DAGs  Directed Acyclic Graphs A DAG is a collection of all the tasks you want to run or interact with in a process DAGs are written in Python An operator is a description of how a task is performed Operators are easy to program so you can make your own Anything that you need to execute and any way that you need to execute it you can program through a DAGOn the left-hand side of the DAG UI you will see on/off switches Nothing in Airflow will run unless its turned on Even if you see it there and you hit the play button nothing will happen unless you hit the on-switch Make sure to monitor thisA word of warning even if you have multiple Python files if they use the same DAG ID only one will show Be Careful of that It is imperative that they are truly uniqueIn Airflow everything is based in UTC and one thing that is really difficult for a lot of people is the fact that the dates are so different than any other platform If a scheduled task ran right now and just finished executing you would think the last run date displayed would be today The date displayed is actually yesterdayIn Airflow dates are always a day behind and you may have to normalize that because if you run through task backfill or schedule they all have different dates so be awareairflow test DAG TASK DATE: The date passed is the date you specify and it returns as the END_DATEairflow backfill DAG -s DATE -e : The date passed is both the start and end date The end date will more than likely be the one you want Also when trying to backfill remember this If you want to run for 2018-01-02 the start date must be 2018-01-01 or youll have the wrong dateAirflow scheduled runs: next_execution_date is the date of execution Depending on your schedule this may be appropriate Otherwise it may be the END_DATE again Just be careful that youre picking up the correct dateWhen testing DAGs you will need to test in all three modes if you are dependent on dates in any way to ensure youre handling all things correctlyIf youre not careful you can have multiple of the same task running at onceBy enabling both of these a singular DAG could only run one DAG run at a time without any extra parallel execution This fit our patterns of processing better However another word of caution backfill disregards these arguments So if youre backfilling data and need it to run sequentially wait for the jobs to complete firstWe use on_retry_callback to alert us of a delay You can also use on_failure_callback to call a cleanup function or on_success_callback to move files out of a processing queue The only limit is your imagination Decide what would work best for your companyIf a task has been run failed or is in retry mode you must clear it out before you can proceed forward Otherwise the UI will say its been set to run but the scheduler will never run it Be careful with clearing things out you can clear more than you want toWhen you go to the graph view the view you see is based on time If you are looking at a dag with a scheduled run the newest run will appear However if you ran a manual load (pressed the play button) just before this scheduled run the scheduled run will NOT be at the top and you will be looking at the manual run Why? Remember Airflow scheduled runs are a day behind This part of the UI will display the dates accordingly to the START_DATE As such the start date for a scheduled run is a day behind and your manual runs will always be on topBe aware of this so you do not try to look at logs for a manual run when you are debugging a scheduled runRemember just because the UI says it works sometimes it wont Also be careful Not all options in Airflow play nice with each other As we stated earlier concurrency is denied when running multiple backfill operationsFor more information watch the full Apache Airflow WebinarIf you know Python and you want to contribute to making this platform better visit https://airflowapache
X7wKXibjz2QcH8xnB8nNEP,Range and rows are types of windowing in Hive While RANGE is dealing with the content of the column which is ordered by ROWS is about row numbers With ROWS you can window on preceding and following rows With RANGE you window the content in a rangeI was facing an issue with hive windowing functions It was a simple scenario but my hive query never endsThis does pin the last location per userAccording to https://cwikiapacheWhen ORDER BY is specified with missing WINDOW clause the WINDOW specification defaults to RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROWWhen both ORDER BY and WINDOW clauses are missing the WINDOW specification defaults to ROW BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWINGIt means the query above it is using RANGE as default I didnt know thatStrangely RANGE does things so so slower than the way of ROWS does In most cases you probably need `ROWS` and `RANGE` may be the one which is being used as default because you didnt notice it I dont technically know the detail of why it is slower This was actually the issue I was facingWith this small change the query lasted 1 hour lasts 120 seconds now
aoVCbnyWdruzUzcSPGc83S,Hive supports windowing features over columns according to the current row You can see the documentation about windowing functions in Hive at LanguageManual WindowingAndAnalytics Those functions are not going be the topic in this articleAssume that you have a dataset contains null value on some columns and you want to fill those null values with most recent not null value of the same columnAs you can see NULL values are replaced with the most recent not null values of the same column before the current rowNormally with SQL you can calculate at row level If you need to calculate something by using column-level according to the current row this is called sliding For example retrieving previous columns before the current row and sum them and count them etc Retrieving last not null value before the current row is what we need to solve which is described in this articleCOALESCE function provides us to make sure to have the last not NULL value of the field If the value of a field is null the last value which is not null before the current row will be selected
X6Bm7zf4dbyNh97AVq76sV,Google Cloud Platform (GCP) offers many options to satisfy all of your storage needs The choice of one or another means of storage is based according to our needsWe will be using Google Cloud Storage which is Google Clouds object file storage It stores your data (stream videos image web asset libraries etc…) as a blob Its considered as web friendly product because every GCS object has its unique key in the form of a URLWe will also be using another data storage way a Big Data product : BigQuery It is a fully managed (no infrastracture to manage) data warehouse used to analyze data Once your data is loaded you can run SQL queries in secondsThrough this tutorial you will learn more about Google Cloud BigQuery and get the instructions to export data from BigQuery to Google Cloud StorageYou will need :  A computer with a good internet connection   The latest version of Google Chrome Firefox or Microsoft EdgeI First step: Upload data to BigQueryWe LOVE BigQuery because it enables super-fast SQL queries thanks to the powerful Googles infrastructureTo run a query select Compose new query near the top right of the GCP console and in the Query editor text area write the SQL query BigQuery validates the syntax if the web UI displays a green checkmark below the text areaI wrote a simple query nothing complicated till nowWhat we want do next is : exporting the new tableNew_table to GCSIIWe can choose either the first method and use the User Interface this is super duper simple or the second method which is more elegant to automate the processPython lovers can instead write a script that you can find in my github repository : https://githubTo have a clean CLI and to automate the process we write a sh file so we dont have to write all the command linesWe only need to precise the name of the table we are exporting and the file to where we are exporting the table in GCS→ Loading a CSV file into BQ table → Storing a query result into a destination table  → Exporting results from that destination table to GCS using two methodsI hope its clear enough for you Feel free to use the code and adjust it to your needs Thank you for reading me
XnPDSy8XYCTUPfpnP93yom,Traditionally if you want to run a single Spark job on EMR you might follow the steps: launching a cluster running the job which reads data from storage layer like S3 performing transformations within RDD/Dataframe/Dataset finally sending the result back to S3 You end up having something like this: If we add more Spark jobs across multiple clusters you could have something like thisThere can be more use cases for example sometimes we need to store the intermediate result of Spark job A and used the intermediate result as input for Spark job B; sometimes you would have multiple Spark jobs read data from the same dataset multiple times As for now each of the Spark jobs have to read input data from disk then process it What if we can read input from memory? Alluxio is a solution for itAlluxio is the storage underneath that usually collocates with the computation frameworks so that Alluxio can provide fast storage facilitating data sharing and locality between jobs regardless of whether they are running on the same computation engineI will show you how to set up Alluxio 181 on EMR 517 with a bootstrap script and compare the data loading performanceSome more detail is described in the following repository: https://githubUse bootstrapsh script includes download Alluxio 18STEP 2: add the required configurationClick Browse and you should see Input file there with default 128 MB as block sizeNote: You can use any test_data_set with Spark I just pick a test Avro file onlineAs you can see the second execution is faster it is similar as we perform cache But if we close and open another spark-shell since we read data from Alluxio and data is kept in memory it would be faster If you are interested please refer to this great talk  Best Practices for Using Alluxio with Apache Spark which talks benchmark for Alluxio compared with SparkPerformance Comparison (Count job with reading from Disk vspros: 1) faster speed to read data as df than from S3cons: 1) Alluxio persist data first in memory to achieve speed so spark job could have less memory to run jobsWhats not been covered is Alluxio is also a great storage sharing layer for multiple jobs read and write data You could have a Flink job that writes to Alluxio and later used by Spark there are more interesting topics on it This post focus on how to setup Alluxio 181 on EMR and run simple test data on it
6v7zWpWKGhmDikX69y7Xjb,Generally building a Datawarehouse involves: This post covers the first part ie choosing and setting up infrastructure for your Datawarehouse In this post I am going to talk about creating Datawarehouse with Googles Bigquery database Intention is to build an understanding for moving your datawarehouse to Google Cloud This post would help in getting basic understanding on how to get things done programmatically with managed deployments on Google Cloud with respect to BigqueryBigquery database is offered as a database platform on Google Cloud Bigquery is serverless offers scalability on the go With Bigquery you can pretty much concentrate on table structures data ingestion and data retrieval designs and not worry about infrastructure Unlike other RDBMS you dont need to spend much time in figuring out indexes storage backup and other infrastructure administration Yes but you do need to build some understanding on how your queries are getting executed and figure out that they scan minimal data in order to optimize your costing It is also important that how do you design your tables for optimal storageBigquery has different pricing for Storage Long Term Storage Streaming Inserts to tables and Querying You should do a due diligence on number of tables size to tables rate of growth Find out different types of queries generally used and club them in partitioned Vs non-partitioned and then try to estimate your costs based on thatOne of the important success factor for a data warehouse is to provide capability to create reports visualizations and ad-hoc reporting capabilities using business intelligence and visualization tools Bigquery connectivity is available with leading tools More details about bigquery integrations could be found here https://cloudgoogleYou could also use tensorflow and other python machine learning algorithms on top of bigqueryNote: In case you would like to try running code snippets please make sure you have following installedPython 2Google Cloud SDK Installation reference :https://cloudgoogleDataset in bigquery could be thought of as a database which groups all of your tables together You need to have a dataset first to be able to create a table From Data Warehousing perspective you could have few datasets for each of your data mart and one dataset for your EDW use cases Good part is you can always refer tables from different datasets while you are writing an SQL for cross domain analysis Lets say for datamart1 you have datasets stream_datamart1 stg_datamart1 and dwh_datamart1 for streaming data insert tables staging tables and final datawarehouse tables respectively You could define a standard set of datasets that would be created for each of the data mart With only one input as data mart name you could have deployment code which would create three different datasets as explained aboveOnce you have datasets defined you can get started with creating tables Though its very easy from UI/Console to create tables for bigquery but If you are working on a data engineering project you should have table definitions defined in a code repository and they should be deployed in different environments via some deployment framework Also at certain points your table definitions would change so you need to have a managed way of creating and updating tables in bigqueryYou could also specify expiration time partitioning type and partition column require partition filter and data location properties for the table These properties help you to optimally design your tables so that data storage and access costs could be minimizedIn the above example of Orders table we should choose partitioning type = DAY and partition column = Orderdatetime and require partition filter = TrueThis would ensure all data is partitioned on Orderdatetime column and you would always need to put filter WHERE orderdatetime  in your SQLs This would prevent someone unnecessarily accessing all data and thus controlling the cost and resources for queriesYou can also define just partitioning type = DAY but then this would require you to insert data in specific partition by appending partition to table name like orders$20180101 In this case Bigquery creates a pseudo column called _PARTITIONTIME and while querying data you need to use this column like WHERE _PARTITIONTIME= The advantage of this method is that you could easily reinstate a particular partition data You can read more about it on Bigquery documentationFor tables which are temporary/intermediate in nature setting expiration time would result in savings on storage costs Most of the times intermediate tables in data warehouse are temporary in nature so you could ensure that this property is set for all these tables and they do not persist foreverExample: Orders table schema (ordersYou could get more info on table resource here: https://cloudgoogleCreate Service Account with Bigquery Admin privilegesCreate a json service account key and download itBigquery allows partitioning only on Day If you want to partition your data on some other functional column then you could create separate tables for each of them Lets say you want to have partitioning by country in orders table then you would create separate tables like orders_us orders_uk with everything else keeping same You could then use wildcard table feature to query themThis has few limitations on the type of queries that you could use but you can think it as one of the partitioning methods to optimize your storage You can check more details here https://cloudgoogleViews are one of the important ways to represent multiple data retrieval use cases Bigquery supports creating views on tables Views can be created on tables in same dataset across datasets and across projects Bigquery views could be used to control the amount of data accessed and also to control the access for tablesLets create a view to show quickly all pending orders which were placed in last 7 days This a type of view which could be required by one of your integration flowsWe can store this definition to a file pending_orders_last_7daysqlNumber of columns that you project in the view is also important from costing perspective Unless required dont use a SELECT *The other purpose these views could serve is give read only access to data You should create another GCP project for that purpose and create dataset named dwh_datamart1 You should use these views to control how much data is accessed by users (such as analysts report developers) You could create views which allow them to access lets say last 90 days dataYou could structure your code in repository for deployment in following wayIn your env_configproperties file you could define different environment and GCP projects service account keys associated with those projects You could govern what you want to deploy using python SDKs (samples above) in deploymentpy file  Here you have your table and View definitions in respective dataset named folders You should have project names and dataset names as placeholders in your table/view definitions so that you could deploy them in different environments easilyYour actual deployment scenarios may be much more complex but this could give you a starting pointIn bigquery you can control different access levels via IAM for Google Cloud You should try to avoid individual users access you should create groups and give them required access then add users to respective groupsOther level of separation could be achieved by having separate projects for storage and access You create separate project then have respective datasets and views for users to access dataService Accounts: You should create service accounts in both projects to run your deployment and ingestion and ETL code You can create one or multiple depending on different kinds of task performedBigquery offers you an SQL interface on really huge data volumes but it is not an RDBMS so you need to be careful while creating your data integration applications
XrCb6fExwLnYfYKWVft4S4,Recently we came across a problem At work we have a flume pipeline which has 3 components The first agent looks for files in the folder puts it to HDFS delete the original file and converts file names to Avro event those events are then sent to another agent lets call that intermediate agent This intermediate agent has Avro as source and sink It works with an interceptor This interceptor reads files from HDFS and extracts metadata and passes it on to the next agent The third agent has a DB sink that basically stores data extracted from files to DB The problem we had for testing we would copy 1TB of files around 750k files There was a trend when we start to ingest a large number of files Initially ingest was very slow and as files in the folder decreases ingestion rate of the entire pipeline increasesWe checked the code this code was our own java code but was quite similar to the one with file Spooling directory source of flume Flumes source lines of the file where we need our file ingest agent to do other jobs such as copying to HDFS Flume ingest has 3 policies on what file to ingest first YOUNGEST OLDEST and RANDOM These names are self-describing The entire code is something which looks in the diagramAs we can see this This is a thread which goes to folder checks what is the best file to process at this point according to the policy set Picks the file and then process it A picked file is then set as a class variable to make sure it is retained when the next execution of that thread comes in by doing this flume makes all the checks before grabbing the next file for processing such as event related to that file is committed and a file is gone from the directory Yes because someone can be copying the files in the folder so there is a sleep time attached to it If a file is not old enough according to the setting a file which picked for a process can be discarded for the processing This thread is executed like a cron job after a certain interval it comes back for execution and does the same process again Now if we can see the problem in this approach as each thread execution it goes to the folder searching for the file it was the root cause of the above-said problem Now this looks like a data structures problems we see in our interviewsFirst solution we thought to make this class variable which holds a single file name a list so that we avoid going through the entire directory structure again and again In OLDEST consume order say if the file is oldest and once we created a list which is sorted by time a file coming later than that will be new so we do not need to care about those new files In case of RANDOM we do not care who arrived when and we just want to process all the files as soon as we can Our most common use cases were around OLDEST and RANDOM consume orders So to solve this problem we decided to make this list a class variable that class variable should retain the list of files flume agent needs to process Now the problem would come up to pick the right data structure for the list As in the random mode we do not need to sort any data we used LinkedList as offer and peak are in O(1) For oldest and youngest mode we used PriorityQueue and custom comparator which would help us sort the file objects based on the last modified time which gives us log(N) as compared to O(N) in case of ArrayList in the previous implementation This website was a great help in picking the right tool Now in the case of YOUNGEST consume order we do not want system to add files in the list which are already there as we need to go through the entire folder structure anyway For this we maintained a set along with a list by doing this before adding the file in the list we would check if the file is already there in the set or not if set is not used we had to iterate through entire Queue to find out whether or not we want to add the file in the listNow the first problem was solved We have a list of files which are sorted so at each thread execution a system can poll a file from Queue and take it for processing and do the rest of it The system will not go and check the folder in case of YOUNGEST and RANDOM mode of execution by doing that we reduced the IO hit It will just create the file-list at first and keep using it till it is empty Now the next problem was as the entire system is implemented keeping single file execution in mind we had to overwrite a flume method which can not take a filename as a parameter so we had to make filename as a class variable The thing we can do to increase ingestion speed is to reduce the time between two consecutive function calls This was a single threaded system Now to implement this we had a code similar to thisNext problem was to turn this fixed delay to variable delayNow if we have files in the list this delay will be reduced and a process will keep doing its job and if there are no files in the folder and it is just checking folder this delay time will be increased to not kill resources unnecessarilyBy solving this problem we came across the Javadoc for ExecutorService which suggests there can be multiple threads and it can run in parallel Now if we decide to run things in parallel there will be next set of problems We have to make sure all the objects those threads access should be thread-safe and still we do not want to lose the benefit of creating one list and let all the threads use it for processing purpose So from Queue we changed that to BlockingQueue it is a thread-safe option for the Queue in Java Set has no similar implementation but mostly our case involved in reading and not adding to it we could use HashSet directly and can make remove operation synchronizedNow in this operation we made the function which adds files to the list synchronized So that we avoid adding duplicate files to the list and reduce IO hit Next problem is the the class variable of the current file which is being processed There will be multiple threads which will be processing different files so we can not let it be a class variable Java has a solution of ThreadLocal variable for this problem This will now give each thread a different file by PriorityBlockingQueuepoll() to process on and this list will be shared by all threads to get file details and we can significantly reduce IO hitBy making this multithreaded we could ingest more files then what we used to do This helped us to get performance increase to 2 events(file)/second to more than 50 events(file)/secondNext bottleneck problem was in the second interceptor which was extracting meta-data We were using drools Due to some reasons building rules were in intercept() function as we needed to change drools rules at runtime and execute In a production system we could easily avoid this building rules for each event intercept We started building rules in initialize() method to reduce IO hit depending on the environment variableNow with these improvements to the pipeline we could get more than 200% increase in data ingestion for the entire pipeline and with reduced resource utilization There is still scope for improvement and all the suggestions are welcome
ad282qLbFdZ9sdPNzbR9fm,If you are like me a curios never ending learner and believe that software engineering is not just knowing all the frameworks but more about really understanding the bare-bone basics then the following books will help you step up your gameNot relying one short therm hyped technologies but understanding the fundamental principles behind is a more sustainable commitment and will greatly benefit youWithout further ado: Here is my top 5 list of books to master Backend Engineering by understanding the underlying principles: Okay one final note  The following books are language and technology agnostic In the following I will give a quick overview over each book outline the learning goals and how to approach the books The list is sorted with the most abstract (theoretical) books first and the last being the most practical orientedI already assume you read some basic books for effective software engineering Hit me up if I should do a follow up on thatThe book that initially brought me to studying distributed systems It covers fundamentals and is not the most recently published book Hence the principles it explains will last for the next years and decades to come The authors empathise on making resources available and that distributed systems are the last resort to do soYou will learn about the following topics: I highly suggest to work through the first nine chapters carefully The last couple are interesting example of distributed systems but not necessarily important to understand the following books I understand that the book might seem very theoretical to you but DO NOT skip the examples provided after each topic Take your time and practice them This will help you connect the dots in real-world-examplesThere are so many more great books by Andrew Tanenbaum et al (go and read them all :D)This book is the logical next step if you already read the Processes and Threads chapter of the first books It teaches concurrency concepts in a more hands-on way Hence it has to use specific languages but the paradigms taught apply to any languageIf you are not familiar with the Seven X in Seven Weeks concept It aims at teaching you a broad conceptual view on some topic (programming languages databases or concurrency) by giving you hands-on tasks Each chapter is tailored to be solved within a weekend (starting with a swift introduction on friday) However those weeks must not be in a row I would really suggest you work through all the provided conceptsThis one is the shortest book by far but covers an interesting concept in software architecture you might not have encountered yet And even if you did (databases  redo/undo logs application/server log files …) it will give you a drastically different perspective on the abstract concept of logs/distributed logs Books like this are a rare find It covers all the relevant topics while building data-intensive applications without lacking the theoretical fundamentals and is explained in an understandable and easy fashionThis one might sound kinda buzzwordy to you and I admit it would if it was not of such a high quality contentwise It operates at a higher conceptual level and really helps you to reflect on all the theoretical foundations you learned beforeRead the chapters in the order you feel its makes sense (that is how I did it)Please do not hesitate reaching out tot me or sharing your thoughts
SmefUeEbD8YUUMjCYEREgM,I had heard about big data technologies like Hadoop and Spark during my college days but just didnt pay too much attention to it Then I started a new project at work involving Spark and coincidentally discovered Andreas Kretzs youtube channel His description of the field really stuck with me  Plumbers of Data Science I also got to see first hand the importance of using solid engineering to build the data backbone for any sort of analytics business intelligence or machine learning endeavourI have been working full time as a Software Engineer for about a year and have surprisingly got exposure to a multitude of fields But what I found was whatever I was learning was very haphazard and I could not construct a complimentary skill set out of technologies I was working with So since I have now become mainly focused on a Data lake implementation I have come up with a niche / speciality for me to purse which is data (platform) engineeringSome of the key foundational concepts in not just modern data engineering but in computer science are data structures algorithms database design system design and data warehousing To implement these concepts we would need a programming language and a relational database Combining all of these form the base of a data platformWe also have to acknowledge that we live in a modern world where big data and cloud being used across companies from big enterprises to even startups What I have seen as well is that in multiple data engineering case studies is that some part of there data platform always involves Web APIs mostly for integration purposes like data ingestion from source system or for transfer data from data lake to data warehouses or data martsMachine learning is probably the next frontier for most data platforms and so an ML engineer I believe can be the natural progression of a data engineer Most people when they think of ML think of data scientists munging data  using cutting edge algorithms and evaluating metrics to create a sophisticated model and I can assure you the reality cant be far from it Any model no matter how fancy have no value in industry until it makes it into production So it falls upon the data engineer now upgraded to a ML engineer or a data platform engineer to take the model and build production ready pipeline for serving the predictions and automated retraining This requires a good foundational understanding not the most advanced know how of MLAlso some additional skills I think are an added bonus for any Data Engineer would be having experience with a Lucene based search engine like Elasticsearch or Solr CI/CD tools like Teamcity or Jenkins and Git for source controlI hope this article gave you some clarity into the roles of a data engineer / database engineer / big data engineer / data platform engineer etc I will be posting more content on topics related to big data and data engineer so stay tuned
njo7vwBYqCS9Aa9rtaTxnJ,I have seen several posts and tutorials on Delta Lake using Hello World kind of examples where everything works wonderfully However as most of you know the performance of data processing technologies changes drastically as the amount of data that it handles increases Thats why I decided to evaluate Delta Lake in the wild using a real world in production Spark job that processes around 100GBs of data Here I am going to share with you how Delta Lake helped me to fulfill the new requirements for the job but also the disappointments I had along the wayThe job process a typical data chunk that data engineers handle in a daily basis It process daily over 2 years of data (~100GB) and updates the dataset in two ways: 1- Inserts data from the previous day • Updates a few rows from the last 6 months of data that had their status changedAs you might have noticed at least the first 15 years of data processing is completely redundant and this was one the main motivation for revisiting this job since it keeps accumulating more and more data to run This is unfortunately a recurrent situation that data engineers have to handle when dealing with jobs coming from legacy DWHNow let me go through the main considerations I set while switching the job from Spark to DeltaIn this particular use case partitioning by day is the natural step for transforming a job that stores full snapshots to a job that process only the minimum amount of data that needs to be inserted/updated daily Moreover a modern data storage should rely heavily on efficient data partitioning for among other advantages described in my previous post enabling efficient querying of the datasetNow if you decide to use Delta Lake then you implicitly also decide to partition your data otherwise Delta Lake doesnt bring absolutely any useful benefitThis job runs once a day only so ACID transactions are not necessarily a big challenge The main situation that might affect in particular the atomicity of the dataset occurs when the job is mistakenly triggered multiple times in a short period thus concurrent applications are trying to upload simultaneously the same dataset Fortunately Delta Lake already handles it for youAs I mentioned I would like to process only the minimum amount of data necessary daily but still keep old snapshots of the job available for data quality controlIn this case the timetravel feature fits like a glove! Since Delta Lake does not really delete any delete any parquet files but only updates the transaction logs pointing to the files that builds different snapshots of the dataset With timetravel we dont need to have a second process only for reading the latest snapshot of the dataset and storing it in another directory Delta Lake just takes care of everything for youA second very positive point in Delta Lake is the vacuum command which can be used for deleting files belonging to old snapshots that can be discarded What I find particularly great about it is that the vacuum command substitutes the implementation of storage policies which are often defined in places of the code far away from the job definition This brings a tiny bit of infrastructure as code to the job definitionData Lake has also the manifest command which creates a manifest file telling which files should be included or excluded for building up the latest snapshot and storing it in the target DWH where the data is exposed to end usersThis is a critical overview of Delta Lake but at this particular point I did not face any situation where Delta Lake doesnt satisfactorily fulfill the requirements I haveFor processing only the minimum amount of data necessary in my use case I need an upsert operation for inserting the new fresh data and updating rows in the table that had their traits changedUpdating rows can be quite complicated since its an ambiguous operation if more than one row from the source dataset tries to update rows in the target Delta Lake table Of course whenever you have this situation the upsert operation fails because Delta Lake does not know which row from the source dataset it should use in the updateThis intrinsic characteristic of the update operation prevents the creation of duplicates in the dataset thus increasing substantially the data quality Needless to say that this situation is completely overlooked in typical join operations where duplicates will propagate freely and usually increase drastically You can still partially prevent these duplicates in the later case by using the expensive dropDuplicates method in Spark However this will consume a good chunk of the resources you have available for running the job Nonetheless if the job has not full duplicates but almost duplicates that should still not be propagated to the final dataset then dropDuplicates will not help you and you need to manually create a transformation for detecting and removing themHowever even though upsert brings the quality of the dataset to another level I decided in the end not using it Why? Well unfortunately the upsert operation in Delta Lake for this particular use case is terribly slow Surprisingly when I tried it out the job started to take longer to finish than the original job that processed full 2 years of data In fact when I switched to a join-like solution that involved reading the full last 6 months of data from the target table and reprocessing it (to 99% redundantly) for updating the correct rows the job finished 3x faster What most probably made the upsert so slow in this case is that only a few rows distributed over a large number of partitions were being updated consequently Delta Lake had to load A LOT of data for performing the upsert The performance of upsert is much better though if you can restrain effectively the number of partitions that needs to be updatedA second problematic aspect of upsert: It generates a lot of small files in each of the altered partitions of your dataset and unfortunately the developer has really no control over that This is another big blow since having an excessively large number of files leads to poor query performance on this dataset You could still run a subsequent job for compacting the dataset as proposed in https://docsdeltaio/latest/best-practiceshtml#compact-files  However in my particular use case compaction was also surprisingly very slow bringing another big blow to the total runtimeDelta Lake is a great technology that brings ACID transformations to ETL pipelines and a lot of very mature features that makes the life of Data Engineers a lot simpler This alone fully justifies a switch to Delta Lake especially if you are dealing with streaming jobs where keeping ACID transactions might be a lot more complicated On the other hand the performance of upsert for the case where several partitions have to be updated is still very poor (in this particular case 3X poorer) and in the end you still need an extra transformation for compacting the data Thats quite a shame since as discussed before upsert ensures a better data quality awareness of your table However you can get rid of those duplicates by good engineering but tediousBy the way Delta Lake is still quite new and useful use cases are not well described in the internet
VamCKpFSnu3sG3kejHEV8D,Lately Ive been thinking about the Lambda architecture used in modern data pipelines Lambda architectures are designed for systems that contain massive amounts of streaming data that needs to be processed and exposed quickly The architecture consists of two different systems One is a real time pipeline thats not perfectly accurate but is able to handle large volumes while providing a solid estimate quickly The other is a batch process that is accurate but runs on a delay By combining the two you get the best of both worlds  accurate historical data and reasonably correct recent data that will be corrected by the batch job when it runsA simple adtech example is to think of the events that are generated during a real time bidding auction We start with an ad request which consists of everything an ad buyer would need to know before buying an ad  including the time user agent and location of the user The buyer then submits a bid containing the ad they want to display along with the price they are willing to pay If they win the ad is rendered and there may be some follow up engagement events by the user  a mouseover and maybe even a clickWe can think of these 4 events as a sort of funnel  we have an ad request which may have a win event which may then have a mouse and then finally a click The challenge is that there may be hundreds of these events being generated each second and its extremely rare that we would have all four events to join together The likely case is that there was either a single ad request or an ad request followed by a win eventIn a batch system its straightforward  conceptually youre doing a series of left joins while increasing the time window to make sure you capture events that may have trickled in after a cutoff So in the case of us processing an hours worth of data we may want to pull in more than an hours worth of wins mouseovers and clicks to make sure we capture everything The real time approach is similar but subtly different First we need to use a much smaller window since we cant keep hundreds of millions of event in memory Second we need to build in logic to take into account the fact that the events may arrive in different orderAs engineers its our jobs to write code and logic thats as reusable as possible and the Lambda architecture provides an interesting example of how difficult this can be The batch and real time systems are doing very similar things yet the code to do each ends up being different Something Id love to see is some way to move the logic itself further upstream that defines the way these events should fit together and then the relevant code is generated for each subsystem This would allow the actual join logic to be kept in a single place which would make it incredibly easy to add new events and fields as necessary
4i4Z5NDcQ3Yf5jmwAjiMYV,I had the privilege of giving a talk today at DataEngConf Unfortunately the talk was not recorded but you can grab the slides here The theme was going over the evolution of the TripleLift data pipeline from the early days where we were sampling events on the client side to the current iteration of a fully fleshed out Lambda architecture Take a look at the slides and if you have any questions Id be glad to answer them in the comments
ETVoqQGtbjgx3doiXVAtvm,At his keynote at Explore DDD 2018 Eric Evans stated that Domain Driven Design needed to evolve While there have been successes he acknowledged that some organizations have struggled to apply the practice successfully He challenged the community to find ways to refine the training the application and even the core principles of DDD With that in mind I would like to share some observations about my experience trying to bring the often metaphor-driven approach of DDD into the often very literal world of Data EngineeringFirst I should provide a definition of Data Engineering for the purposes of this article This is not meant to be exhaustive or definitive but rather pre-empt any confusion about the nature of the domains to which I am referringData Engineering in the context of this discussion is the process of building systems with most or all of the following characteristics: These characteristics create a series of common follow-on characteristics: These dynamics are all highly technical and I would argue outside of the conventional realm of conventional DDD focus They would generally thought of as implementation details that are meant to realize the model as opposed to core to the model itself Of course this point of view has practical limitations Software development is a complex feedback loop in which the model and the technologies that implement it are in constant tension driving domain-centric engineers to make constant compromises to balance the need for coherent metaphor but respect the constraints of the technology or for that matter physics Highly specialized technical needs such as extremely high volume ingest or very low latency for example impose limitations on the kinds of models we allow ourselves to create Or put somewhat differently we can create any kind of model we desire but then are limited in how it can be bound to the systems they are meant to represent When I described Data Engineering as literal above this is what I mean Engineers in this field are reticent to compromise a great deal to make systems comprehensible when there is so much pressure to simply make these system perform with sufficient reliability and performanceLast year Eric outlined several reasons why he believe DDD sometimes achieved disappointing results among others: I would add a few additional possible reasons why DDD has not taken the Data Engineering world by storm: There are kernels of truth in all of these statements but as you might have guessed by now if I believed that DDD didnt have a place in Data Engineering I wouldnt be taking the time to write this article On the contrary I am convinced that there is much to be gained by an acute focus on domain particularly in systems with high throughput data ingest and query This kind of technical challenge drives complexity to the heart of our software systems that needs to be tackled For that to occur we will need to sharpen the tools for the job and improve our public relations for the audienceIn the pages that follow I will describe my experiences trying to apply DDD thinking in a data engineering-focused domain what has been effective and where there has been resistance what experiments I have run and what I have learned I will discuss my working theories about how DDD might evolve to be more applicable to these types of domains and discuss some mental models and heuristics for applying domain thinking to them right nowGo on to part two
D2UU7euBTQmc42zbUwgX8J,This is a continuation of a series of articles on DDD in Data Engineering First more context see the first part of the seriesLast year I ran a series of DDD workshops with my team The team was focused on a part of the platform that provided our products with high volume data ingest and access One of the systems we work with had become a rather complex distributed monolith as years of tactical changes built up obscuring the intentions of the original design and putting unacceptable constraints on development and operations The team was becoming large and there wasnt a great deal of clarity about how lines could be drawn to decouple different capabilitiesFundamentally this system was dedicated to ingest and query of large amounts of customer data for the purposes of monitoring their IT assets Supported use cases involved both curated experiences and ad hoc query via UI and API If we chose to be reductionist we could have summarized the purpose of the system as write my data and read my data Although this sounded like a very simple domain made complex by the needs of scale and performance evaluation of this possibility left me doubtful Had this provided strictly a straightforward CRUD-like experience (think S3) I probably would have been more convinced but this system provided a complex query experience involving different ways to evaluate and aggregate the data across time; it had APIs for supporting end users querying their own data as well as APIs for teams to build applications that query the data on the end users behalf The ingest and storage functions were arguably more straightforward but involved relatively complex rules that varied by product and account providing customers with the ability to pay for only the data they wished to retainWorkflows to support cluster operations such as scaling decommissioning nodes load balancing file management data locality partitioning failover among others created enormous complexity at the operational tier of the system comparable to the kinds of business-supporting workflows Ive encountered in the core domains of retail medical and transportation systems Without a coherent mental model to keep the complexity contained the system had become very fragile hard to change and hard to understandDuring our modeling sessions it was very difficult to see the forest for the trees The team continually struggled to separate the systems core capabilities and functions from details of their implementation There was some insight and some of the engineers were optimistic that continuing on this pathway would be valuable but progress was laborious and convincing the organization of its value a constant challengeI had seen this occur before as a consultant when trying to tease apart the subdomains of another time-series analytics system that was dedicated to evaluating the performance and optimization of heating and cooling systems for property managers The building engineers who served as domain experts had spent so much time working with this system that they knew as much about the software as they knew about the behavior of the HVAC systems that it was meant to monitor making it difficult for them to step back from implementation details But there was a difference: nearly every concept in the building management domain could be ultimately traced back to something in the real world When I recognized that the domain experts were getting lost in the weeds I could push them to provide a real-world example and they generally could describe it in terms like: the maintenance crew notices that the AC is consuming more electricity over the course of the day but the weather has been cooler so either the thermostat is set wrong or malfunctioning Some terms might have been foreign but the experts could usually translate them into something I could understand: a room in a building a fan a heating unit the weather With this advantage we were able to absorb the domain fairly quickly identify terms that were ambiguous or overloaded and start to see where the lines between contexts needed to be drawnThis time around with a highly technical team the task of distillation proved to be much more difficult in spite of the fact that we were the domain experts Our efforts to create clear boundaries proved to be a much more cumbersome processIn highly technical domain where the language of implementation and model substantially overlap the opportunity for confusion between them is substantial I believe data engineering particularly at the platform level is one such domain When I say server am I talking about the abstract representation of the process our system is monitoring or am I talking about a machine on which our system runs? Or more confusedly am I talking about both since we use our software to monitored itself? Linguistic confusion is possible in any domain of course but the level of rigor required to maintain clear boundaries in an IT domain appears to be much higher This problem was made even more challenging by the fact the bounded contexts my team was responsible contained underlying query and storage of the entities within the customer-facing domain The language of the company is Observability But the language of my team was ingest and query and the boundaries were very blurry multiplying confusionSo how does one maintain this level of rigor? I will candidly admit that Im not entirely sure Chapter 16 of Domain Driven Design Large-Scale Structure provides the concept of the cohesive mechanisms which shows some promise for how to think of a complex technical asset as do the provided patterns of System Metaphor and Responsibility Layers but those patterns however useful are very abstract and fall short of providing a concrete approach Which brings me to the next challenge we encounteredThe community provides many examples both negative and positive of how the necessarily abstract concepts of DDD manifest when put to use However few of the ones I have found are from our domain or are easily transferable to it There are myriad example of event-driven systems but I have found few in the literature or the articles that wrestle with the kind of volume this system requires Similarly while I have found examples from highly technical domains I have found very few that talk about IT-domainsThis has negative consequences for would-be DDD advocates in positions similar to mine: The first is related to the perception of DDDs value Few of the data engineers I work with have the impression that DDD is a tool that can help them I theorize that one of the most significant reasons is the dearth of obviously relevant examples There are only so many articles and books a data engineer will read that uses examples from shipping software before deciding that DDD is an approach suitable for building logistics systems or at least for systems that look much more like themThe dearth of example material is harmful to more than just the DDD sales pitch Enterprise software is complex and nuanced and theres a hairs breadth between an applicable pattern that provides great value and one that leads you down the path to technical hell Although the basic practices and premises of DDD may be applicable the myriad mental models and heuristics that are available in less abstract disciplines are less so The constraints of a data engineer have some unique characteristics and there arent a lot of resources on which to base their thinkingDuring his keynote last year Eric Evans expressed his hope that the DDD community would spend some time characterizing bounded contexts My hope is that I can take the things I have learned and continue to learn to contribute to that cause and increased the availability of useful examples to the IT domain specifically and data engineering more generally To that end I will discuss some flavors of bounded contexts I have come across characterize the relationships between them and discuss some heuristics for applicability To give credit where it is due I would like to point readers to Nick Tunes work His Strategic Autonomous Design talk (https://youtube/mEFbjt_87TU) at ExploreDDD 2018 had a dramatic effect on the way Ive been thinking of late You will probably see this reflected in the kinds of patterns and heuristics Ive been consideringThis catalog is very much a work in progress and I will write about them as they emerge but to give a taste of a few ideas that will be forthcomingI hope these dynamics are applicable beyond IT-related and data engineering-heavy domains but as an engineer on a team dedicated to building and running a foundational data engineering platform I will look at each through that particular lens bringing in other examples from outside if they are availableMany of the pathologies that exist in complex high volume systems seem like they are of the type that DDD is meant to mitigate: poorly factored capabilities code that obscures rather than elucidates intent disparate mental models and monolithic systems that are too big to hold in your head but insufficiently factored to work on effectively as a single unit I think of this as the sweet spot for DDD particularly the practice of clearly articulating bounded contexts and reflecting them in the code the relationship between services and the organizational structure as a whole It is my hope that by writing about this I will invite some conversation that results in making more tools and techniques for understanding domain available to technologists working in systems such as thisGo on to part 3
mvXRndJTvDzEqfJf54e4Rp,TL;DR PowerSQL 0As data analysts data scientists and data engineers are using more and more data the quality of the data becomes critical Your daily forecast will happily output an incorrect forecast given incorrect input data Product recommendations will be misleading if data about some product categories are unexpectedly removedThe more the data is transformed by different teams systems and ETL / ELT jobs the more likely it becomes that each step in the pipeline contains errors and leads to incorrect data The less we test this and verify whether transformations are correct the more errors we can expect More Technical Debt will build up over time - and more specifically - Pipeline DebtWith PowerSQL we are addressing Pipeline Debt PowerSQL 03 introduces ASSERT-based testing which allows you to write a test suite using plain SQL Pipeline tests test data rather than code An assertion consist of a condition and a message It can test anything like a non-NULL assertion to see whether the values are always filled: We would also like to test expectations: statistical properties of our data Such as: At least 70% should have a quantity lower than 10: PowerSQL gives data teams the power to test their data with the expressiveness of SQL In addition to syntax and type checking of your code testing will contribute to robust data transformations and fight Pipeline Debt
nk89xFUWiuUMFVjw24VjUp,"The field of Data Engineering has grown exponentially in the past 5 years For the larger part data scientists are relieved of the duty to code data pipelines and now focus on modeling and algorithms PMs have realized that there is a real need for data engineers  a dedicated team of people with a software engineering background tasked solely with building and maintaining data pipelinesAt the moment there is a lack of structured education when it comes to data engineering Very few colleges offer relevant electives and new grads are rarely found on data engineering teams The most common path to becoming a data engineer is an industry transition from a pure software engineer role When interning on the Catalog team at WalmartLabs responsible for data pipelines of the Walmart site I found that our field lacks guidelines that could guide a young padawan forwardIts time for data engineering professionals to set basic principles that will serve as a template for a gentler start in the career of a data engineer This article suggests several core guiding principles for developing a batch ETL(Extract Transform Load) job and offers practical dos and dontsThere is a lack of structured education when it comes to data engineeringETL jobs running in production take hours or even days to run It is necessary to take preventive measures to reduce the chances of run-time and configuration errors You want to avoid such scenarios as a typecasting error or an incorrect IP address of the Cassandra cluster There is no feeling worse than a job failing 8 hours later because the IP address of the Cassandra cluster was off by a digitIn order to tackle potential configuration errors I suggest creating a separate unit test to cover configuration settings You can avoid most run-time errors by using a statically typed language such as Scala or Java Scala is the preferred language of choice for a data engineer Its less verbose clean and more importantly fully supports functional programming (FP) techniques FP comes in handy to accommodate the reproducibility principleAnother problem that you will encounter in developing batch or real-time data pipelines is inconsistent data (ie you expect an orange but receive a garlic) Prior to applying business logic to the dataset you need to ensure you are receiving proper data through data validation Scala has a powerful feature called pattern matching that allows for a deeper object inspection It can be used to validate your data in a clean and readable manner If you are a Java programmer new to FP languages think of it as a switch statement on steroidsPattern Matching for data validation:You can run Scala code in the Scastie REPL environment""Let's unpack the magic above Our goal is to extract the elements of type Orange from a list of data given to us Weve created an abstract class Data and implemented it with Garlic Orange classes We wont delve into what is a case keyword in this article but in Scala pattern matching can only be performed on case classes Next weve created a function clean() that will take an object of Data class and return itself only if its an Orange Pattern matching allows us to inspect the type of the object and act accordingly A wildcard _ in this context stands for everything else and can be used effectively to filter and log all of the unexpected data Finally weve performed a transformation from a list of Data objects to a list of Oranges by applying clean() function on each element of the list Our code is readable type safe and doesnt use mutationThe safety principle reduces the chances of your data pipelines failing for silly reasons It allows data engineers to focus their time on transformations improving functionality and hence delivering more business value rather than spending time on debuggingBig Data is hard and complicated The jobs you deploy will fail for a variety of reasons You should be developing data pipelines with an explicit assumption that things will go wrong In order to effectively debug data pipelines you must be able to reproduce the results with complete confidence In the context of ETL you should be able to run the job again on the same data and obtain the exact same results as beforeIn order to achieve this you can leverage functional programming paradigm The main component of FP is avoiding side effects at all cost In Scala make it a habit to always use val keyword to instantiate an immutable variable and avoid using var keyword which allows mutationLearn how to write pure functions Most of us come from Object-Oriented way of thinking that encourages mutation and working with state Unfortunately OOP doesnt translate to the principle of reproducibility and thus should be discouraged for building data pipelinesPure function addData() operates only on its input parameters and doesnt access or alter a state outside of its scope Such a function will always produce the same result for the same set of inputsaddDataImpure() operates on a variable s outside of its scope There is always a chance that a different part of the system will access and modify s which will affect the result of addDataImpure() Such a function is unreliable in producing the same result for the same set of inputs because we cant be certain that s will remain the same and wont be changed Notice that clean() function from the pattern matching example isnt pure because it produces a side effect through a println() statementThinking functionally instead of OO isnt easy at first but the rewards are worth it Pure functions are easier to reason with more efficient to parallelize and are simpler to test (QA will thank you) Pure functions combined with immutability implement the reproducibility principle which will increase your developing speeds and reduce the chances of unforeseen errorsIdempotency is a concept from mathematics and computer science that is commonly misunderstood but can provide significant value for our use case Idempotence is a property of an operation that has no additional effects if the operation is applied more than once on the same input An idempotent operation can be applied infinite amount of times on the same input and it will not produce side effects beyond its initial application For example in mathematics multiplication by 1 is an idempotent operation while addition by 1 isnt because it will increment the result every time you run the operation Idempotency shouldnt be confused with pure functions Pure functions simply dont interact with the outside world while idempotent functions can perform IO interact with DB etcIdempotent Data PipelinesWhat does it mean for a data pipeline to be idempotent? An idempotent ETL job will not create any discrepancies in the DB if it is run many times over As a data engineer you could rerun your ETL job infinite amount of times and your data warehouse will remain pristineeither no data will be inserted in the DB or all of the data will be properly loaded Its an all or nothing approach The reason we want our data pipelines to be idempotent is for debugging purposes There is no chance of duplicate or partial data appearing in the data warehouse which allows for rapid prototyping without fear of messing up the databaseThere are two components to be developed when incorporating idempotency in your pipeline First you have to implement a mechanism to check whether the job was run for the first time or Nth time in order to avoid creating duplicate data Second if the failure happens in the Load stage you need to have a rollback plan in place to remove all of the elements inserted during the load stage prior to the failure Third if the job fails for whatever reason during the Extract or Transform stages we simply have to gracefully shutdown the job and log the errorsBy making your data pipelines idempotent you increase the ability to safely reproduce the ETL job which ultimately makes the development cycle faster and more efficientData Engineering is an exciting and rapidly growing field that requires a unique set of skills and knowledge to succeed This article merely gives guidelines to aspiring data engineers to start off their journey Below are the links to blogs and books that I found to be extremely useful when starting offThis is my first post on Medium about a topic I am deeply passionate about I plan on writing more posts on data engineering and diving deep into Apache Spark Please clap 👏 if you learned something newAcknowledgmentsCay Horstmann@San Jose State University for academic rigor in the functional programming classMartene Mendy@WalmartLabs for being the Yoda to my Luke Skywalker Rob Potter@WalmartLabs for giving the freedom and feeling of ownership during my time at Walmart My friends Pranav Patil Emerson Ye and my loving mother for your detailed critique"
9sKic8M8vshJv3M7d4CFLQ,A CTO or a VPE may wonder why do i need a DataWarehouse when i have all my data in my Postgres or MySQL DB If i need to report on my revenue growth in users i can just query it directly Sure that works and you can also have a non Production Slave instance support your ad-hoc queries and DashboardsBuilding a DataWarehouse takes time but always pays off in the long term as it will not only enable your Product Managers & Leadership team to see how their Product and Business is performing it will enable many other areas of the company to use data and make decisionsDataWarehouse is the first step to cultivate data driven decision making The journey of data driven decision making is long and hard but with the right mix of Data Engineers Analysts & Data Scientists every Startup can take smart decisionsWould love to know how your company uses a DataWarehouse
cxrpbgRNabvQpv3QSh5qfa,Web scraping is the process of extracting data from websites All the job is carried out by a piece of code which is called a scraper First it sends a GET query to a specific website Then it parses an HTML document based on the received resultIMDB is an Internet Movie Database that contain information related to films television programs home videos video games and streaming content online IMDB is also including cast production crew and personal biographies plot summaries trivia fan and critical reviews and ratingsEXTRACT : During the process of web scraping we have to make sure that all the data we pick is legal to scrape due to the content copy protection such as multi-level layout using JavaScript for content rendering and user-agent validationsWe also found out that if the data in the websites that we are going to scrape is quite small it is better to write our own scraper and customize it according to each specific website The quality of the output content should be 100% If the number of websites to scrape goes beyond small we suggest using a complex approachTo scrape the data we will right click on the selected web page to get the inspect of elements Firstly we will count the number of movies that contain in one page Then we will start to scrape the first movies elementsFrom the elements we can start to identify the data attributes that contain in each of the movie The attributes that we have chosen is movies name movies poster movies MPAA rating movies year movies runtime movies rating movies metascore movies votes and movies genreSince the result of year will display I or II for different series such as (I)(2017) we need to remove the (I)( and ) in yearTo calculate the average runtime for all the movies on the web page that we want to scrape we need to remove min in runtime for an easier calculationSince there is an irrelevant data in the gross attribute we need to drop it to prevent the irrelevant data disturb our scraped data For example Bird Box movie holds a PG-13 as its gross attribute which should the value should be the amount of value that earned through this movieWe found that there have incorrect data types assigned to some features We corrected the data types and converted them into correct data typeWe also removed the unwanted white space in genreWe also carry out the data visualization to sum up the total number of certificates in each yearThree histogram has been drawn for data visualization
VnfJ3e9k6Vi6DB2Dx3TCGk,I had a recent experience with Spark (specifically PySpark) that showed me what not to do in certain situations although it may be tempting or seem like the natural approachThe bottom line: when working with Spark represent any collection of data youll act on as a Spark structureImagine youre working with various periods of time where each period is a continuous range of yearsThe simple approach becomes the antipattern when you have to go beyond a one-off use case and you start nesting it in a structure like a for loop This is tempting even if you know that RDDs underlie your Spark entities and that youll be switching between them and non-RDD entities because ostensibly all the pieces you need are already there To get records for multiple periods of interest with this approach you end up with the following Here we end up creating an aggregator variable to facilitate the antipatternSpark is lazily evaluated so in the for loop above each call to get_purchases_for_year_range does not sequentially return the data but instead sequentially returns Spark calls to be executed later All these calls get aggregated and then executed simultaneously when you do something later with periods_and_purchases that tells it to finally evaluate In particular results remain unevaluated and merely represented until a Spark Action gets called Past a certain point the application cant handle that many parallel tasksIn a way were running into a conflict between two different representations: conventional structured coding with its implicit (or at least implied) execution patterns and independent distributed lazily-evaluated Spark representationsYoull want to represent any collection of data youll rely on for Spark processing as a Spark structure The fundamental unit of Spark is tabular data instantiated as an object within the Spark framework You want to coerce your input data into that even if it ends up being as simple as a single column We can actually define a schema like we did above just at a different point in the overall flow to achieve a workable solution: We end up with what we originally intended a list of purchases for each period of interest We also eliminated a separate nested function and enclosing for loop in exchange for whatever transformations we needed to perform to structure our periods of interest as a DataFrameYou might find this unpalatable especially from an object-oriented perspective since it can feel redundant or in violation of consistent self-contained abstractions because you end up breaking down some delineation between your structures The benefit though is everything lives in Spark you can think of it in one way and its compatible with the resource management and other behaviors of the Spark framework We also introduce a join where we didnt have one before which seems unsavory since join is a quick path to a combinatorial explosion of data Its the right move here though because again getting everything into Spark takes priority Addressing any explosion should come after that within the language and concepts of Spark such as by avoiding multiple instances of join or by partitioning and reading only subsets of the dataConceivably we could have gotten around our issue by forcing sequential evaluation with an Action or perhaps with cache but that seems unnecessary and more complicated than translating everything to the conceptual language of Spark Being able to think about everything in this one way with one consistent set of possible operations strikes me as the correct approach It reduces cognitive drag helps avoid errors and facilitates developmentIve had a good experience with Spark so far in processing considerable amounts of data in the cloud Its advantage over conventional processing within relational databases is its scaling and parallel processing capacity Ive also had a better experience with its ecosystem compared to other big data processing frameworks (eg out-of-the-box Hive) For instance its been far easier to construct fixtures isolate transformations and take care of other components of automated testing Its also worth noting conventional SQL enterprise solutions like SQL Server are now incorporating Spark as a parallel processing engine but I dont know what the exact environment for that looks like and whether it would have similar advantages in automating tests etc
QycZx85vHNJZPv5Kt9mX3y,This is my first experience with real Data Engineering My tasks were to start with a lot of raw data from different sources seemingly unrelated to each other and somehow come up with a use case and design a data pipeline that could support those use cases Time to put my data creativity hat on The data potential is HUGE in this ageThe majority of Data Science works depends on what kind of data you have access to and how to draw out the most relevant data attributes that can support your use cases and let the ML algorithms do its magic Ive read a lot of very creative use of data to do crazy inferences like using Google house images and street views to infer potential car accidents at certain locations Another crazy use case is combining traffic data with car purchases weather patterns traffic lights patterns to predict traffic What Im most interested in is Cybersecurity and Privacy use cases which I will talk about in my next project Note that all these data are in different forms and come from different sources It would require a lot of Data Engineering work to extract transform and load these into a usable data warehouses for analytics to happenThe data sources provided by Udacity are not that interesting to me per se but makes very good practice and preparation for my next project Using the Snowflake schema Ive designed 8 normalized tables such as airports immigrants state code weather etc … Using these normalized tables to design 2 analytics tables that are useful to consume either for analytics or decision making such as airport weather and immigration demographics During the process I had to deal with missing data duplicates malformed data and so onNow that we got the data modelling out of the way well move on to my favourite topic technologiesFirst off Im particularly impressed at AWS CloudFormation its such an easy and convenient way to deploy infrastructure with minimal effort without spending hours clicking around multiple services trying to link them together The configuration syntax is very intuitive and easy to learnAn EC2 instance is used to host Airflow server and an RDS instance is used as Airflow database to store metadata about DAG runs task completion etc … Airflow is used to orchestrate the creation / termination of the EMR cluster The basic workflow is: At the start of the DAG create the EMR cluster and wait for completion Once completed start submitting Spark jobs to the cluster using Apache Livy REST API When all jobs are completed terminate the clusterFew things to note: There are lots of nuances in configuring Airflow and AWS which Ive learnt the hard way through doing this project Its really good preparation for me to embark on my own data journey Stay tuned for my next projectProject repo: https://github
56bFeFaxwB7DXERzszNxXd,Many organizations have a range of sophisticated analytics using tools like Hadoop SAS data warehouses etc These batch processes have the disadvantage of a potentially long delay between arrival of new data and the extraction of useful information from it which can be a competitive disadvantageSeveral challenges make this transition hard First different tools may be required Databases may have sub-optimal read and write performance for continuous streams Use a log-oriented system as your data backplane like Apache Kafka or Apache Pulsar to connect streaming services sources and sinksFamiliar analysis concepts require rethinking For example what does a SQL GROUP BY query mean in a streaming context when data keeps coming and it will never stop? While SQL might seem to be a poor fit for streaming it has emerged as a very popular tool for streaming logic once you add windowing for example over event time A GROUP BY over windows makes sense because the window is finite It also enables large batch jobs to be decomposed into the small fast increments necessary for streamingNow youre ready to deploy but streaming services are harder to keep healthy compared to batch jobs For comparison when you run a batch job to analyze everything that happened yesterday it might run for a few hours If it fails youll fix it and rerun it before the next days batch job needs to run During its run it might encounter hardware and network outages it might see data spikes but the chances are low In contrast a streaming service might run for weeks or months It is much more likely to encounter significant traffic growth and spikes to suffer hardware and network outages Reliable scalable operation is both harder and more important as you now expect this service to always be available because new data never stops arrivingThe solution is to adopt the tools pioneered by the microservice community to keep long-running systems healthy more resilient against failures and easier to scale dynamically In fact now that were processing data in small increments many streaming services should be implemented as microservices using stream-oriented libraries like Akka Streams and Kafka Streams rather than always reaching for the big streaming tools like Apache Spark and Apache Flink
MNNgjsL9madebPGNa5xe48,- An enterprise data warehouse that solves this problem by enabling super-fast SQL queries using the processing power of Googles infrastructure- Fully Managed- Access Method : webUI command line client SDK third party tools to load and visualize data- BigQuery dataset billing & access can be controlled at project level- BigQuery stores data in the **Capacitor **columnar data format- BigQuery manages the technical aspects of storing your structured data including compression encryption replication performance tuning - Load data :  Batch load  Straeming- Data operations  Copy table  Query table  Modify data using SQL DML  Export data- BigQuery also supports querying data thats not in BigQuery storage- BigQuery resources type  organization  project and dataset- Dataset has tables- Organization and project roles affect the ability to run jobs or manage the projects resources- dataset roles affect the ability to access or modify the data inside of a particular dataset- Avoid select * - LIMIT doesntt affect cost - Do not run queries to preview or explore data use preview options instead- Use  dry-run flag- Calculate query cost from explorer- Use the maximum bytes billed setting to limit query costs- Partition your tables by date- Create a dashboard to view your billing data so you can make adjustments to your BigQuery usage- If possible materialize your query results in stages- If you are writing large query results to a destination table use the default table expiration time to remove the data when its no longer needed- use the default table expiration to automatically delete the data for you- Use streaming inserts only if your data must be immediately available  Hadoop Ecosystem-  HDFS  distributed storage filesystem   Map Reduce  Parrellel proceesing programing paradigm  YARN  Yet another resource negotiater  Resource manager  Hadoop = HDFS + Map Reduce + YARN   Hive  sql like interface on top of hadoop  Hive Metastore  Represent user interface of hive as tables but actually stores data on HDFS Metastore has metadata mapping table for table and its releated location on HDFS- Pig  Scripts to handle unstructrured in-consistent in-complete unknow schema datasets Data clean/transformation tool Spark -  Spark core consist HDFS and YARN to handle data And Spark computation has built-in intelliegnce to fast-cpmputing  Spark has diff lib to work with stream and machine learning  Spark has REPL(Readevaluateprintloop) environment like python  Spark has RDDs in-memory resilient distributed datasets  RDDs are read-only immutable and data has default partitions  Spark lazy evaluation and lineage make it in-memory RDD  Lineage  When RDD is created it holds metadata which consist parent RDD and transformation happend  Types of operation: Transformation and action  Transformation are executed only when action are requested
L7UxnNM9d5RHwXRYLveaRy,In searching for a grab-n-go Spark notebook for ad hoc data exploration & investigation purposes Ive tried Zeppelin Jupyter+Sparkmagic and many other solutions Most seems either not lightweight enough (requires installing and configuring Spark on host) or are hard to configure dependencies and/or version-track (dependency configs and notebooks are in separate locations)Then it comes Almond I was able to hash out quickly a minimum-viable notebook that (1) has self-descriptive dependencies per notebook and (2) require minimal setup so is easy to containerize and share among different projects Heres how I set it up from scratch: The official Almond installation requires coursier If you are on Mac OS and have Homebrew there is a quick and easy way availableFollow the instructions and make sure you have picked a valid SCALA_VERSION and ALMOND_VERSION combination as not all combinations are availableIve made a simple notebook to demonstrate the idea It is self-contained in the way that its dependencies are part of the notebook thus can be customized and carried around easily
2BjdK85ECZ4fFJCKRYhMDY,Google gives you 7 days of history for existing tables and 2 days for deleted tables (source)Important: Get familiar with the code by running it on dummy data before relying on it The article is written to help and the author takes no responsibility for the cost incurred or lost data as a result of using the instructions from this article Also mind that this article is current as of October 2019 (things will likely work differently in the futureIf the cost you incur by querying while restoring is an issue you can use the Cloud Shell Opening it is simple: You will need to convert the time you want to go back to into a Unix timestamp\u200a\u200ayou can find a converter here I am using 1577833205000 as a placeholderSo to restore data from a specific time in the past and save it to a new table: This command works also on deleted tables but only for 2 days since deletion It also works for partitioned tablesFor overwriting an existing table (when the table wasnt deleted) just enter the same table name twice: You will be asked if you want to overwrite the table Just enter y for yesNote that if you are undeleting a table the second command wont work: you need to specify a new name for the table and then copy that table in the place of the old oneYou may want to simply query a table as it was one hour ago and decide what to do with the result later: All you need to do is add FOR SYSTEM_TIME AS OF to the end of the basic SELECT query followed by the specific time you want to go to The provided code offers an out-of-the-box way (no Unix timestamp needed) to jump back an hour from the time the query is runYou can find more about FOR SYSTEM_TIME AS OF in documentationDo you perhaps want to immediately restore the table to a state as it was an hour ago? Here is how to do that: The result of this query will be saved in a table you specify in CREATE OR REPLACE; if you use the same name as in the FROM part you will overwrite the table with the data as it was one hour agoYou can do the same with a partitioned table Below is an example where you have partitioned a table by day using a timestamp-type column in BigQuery and your table requires a partition filter: The WHERE condition as written just makes sure that you are allowed to query across all of the data (as every date is equal or smaller than todays)The examples that are shown above will retrieve the data from 1 hour\xa0ago You can of course use any other number of hours and if you need days or minutes just replace the phrase HOUR with DAY or MINUTEIf you are using legacy SQL you can go back in time using the query below You will need to convert the time you want to go to into Unix timestamp You can find a page to help you do that quickly by clicking here for example You then enter the resulting number after the project:datasettable@ as below ( I used 1577833205000 as a placeholder): Mind that legacy SQL query doesnt support partitioned tablesAt the time of writing unfortunately no To quote Google: After you delete a dataset it cannot be recovered restored or undeleted Deleting a dataset is permanentThats it! I suggest you get familiar with how the functionality to go back in time works so you will know what to do if/when you actually need it
CTFwJg6jVX4JCbZt6uNhxQ,With growing numbers of people accessing data it is important that data platforms are flexible and scalable Hence people switch to cloud to achieve these objectives without compromising on security However the key challenge in moving to cloud-based data platform is in ingestion of the data with a faster and secured approach since most of the data are present across on-premises databases such as RDBMS The cloud-based data lake opens the structured and unstructured data for more flexible analysis Analysts and data scientists can then access it with the tools of their choiceThe conventional way of building a Data Lake involves setting up a large infrastructure securing the data which is a time-consuming process and not a cost-effective approachA new managed service by Amazon web services to help you build a secure data lake in few steps
h8d5rwCrNRbJtJdZ67QncK,In my previous posts I went over how we went about implementing a fitness program at Pandera the need for a custom solution to handle a little friendly competition a supporting architecture and how I would structure a Data VaultNow though were building the pipeline This will take the Strava data and get it into BigQuery As a reference point here is the architecture I outlined in my first post: I previously mentioned my main focus in discussing the code involved in this application is going to be the Strava API call component that serves as my data pipelineWhen we start dissecting the Strava API Call in the diagram we see it handles a few thingsAs I mentioned earlier the webhook doesnt pass the actual event What it does do is say what kind of event it was Activity or Athlete what the corresponding ID is and what kind of action it is Create Update Delete Given that content I handle each message in varying ways depending on those two fieldsI parse the pubsub message out into a few objects as follows: In this function I only really care about activities and from there I handle create and update messages mostly the same and deletes a bit differently: As you can see above if the type of change is a delete I dont do a whole lot here other than generating a hash and take the event time from the pubsub message and insert into the activity_sat table The reason I do that is because I can no longer make a request of the API for that activity because it no longer exists but it is still important to have that reference to it being deletedWhen the record is not a delete so either update or create we make the request out to the Strava API I first verify that my access token is still valid by checking the current time against the expiration of the token I have in datastore If the token is expired I get a new one if not I proceed with the one I have Im using a library called stravalib to help simplify the process of interacting with Strava https://githubcom/hozn/stravalib I am not thrilled with how a few things are implemented in it but it sped things up a great deal Long term I will be removing any dependencies on that libraryAfter I get the activity response back from Strava I use one of stravalibs prebuilt to_dict functions As I mentioned there are some things I do not like with this library one thing in particular is that the to_dict() function strips out some of the ID fields So youll notice I add them back in with the supplemental variableOnce that is cleared I upload the dictionary to GCS I organized files in the following way: That way if someone wanted me to remove their data it is easy enough to do a mass delete and if I needed to reprocess data I could find it easilyFrom there I create my rows for insert and load them into BigQuery This really lends itself to using the Insert call instead of the load but currently I am trying to avoid charges as much as possibleOne thing youll see in the above code is a reference to AthleteActivity I created a class to model take in and model the data with how I would need it for my data vault You can see how I initialize the object then use those attributes to create satellite hub and link structures by calling the respective methodsWith the pipeline built and deployed data is now flowing into BigQueryI had a few bugs crop up once it was implemented for instance activities that were not distance based coming in with a 0 for elevation gain that was causing an error in BigQuery That field is a float in BigQuery however the 0 was acting as an int As always Stackdriver is a huge help in dissecting these issuesThe last thing I now owe the PFC members is a way to look at all this data Join me next time as I run through a build out of a Data Studio dashboard
YZZpZz793qM6ChQMTzd6Dj,"Once upon a time long ago in a galaxy fire away… I was excited when I first heard about the possibility of getting Fivetran However when we finally got it the experience left me wanting moreThe following are just my opinionWhen logging into the site it takes 10 seconds or more for the dashboard to loadThe only avenue of support is through their Submit Request portal which depending on your support service level agreement can take hours to get a reply For example with the Starter and Standard pricing plans their initial response time is within 4 hours for the most urgent issues However if you get their Enterprise plan the initial response time drops to just 1 hourOn the page where you set up a connector there is a link to the Configurations Instructions on the upper right area of the page These instructions are great because they give you step-by-step instructions (complete with pictures) The instructions are detailed enough but they lack basic troubleshooting guidelinesTo quote a reviewer on G2Michael E g2""For example I was setting up an S3 connector and I run into trouble with the regex I used for the file pattern The support technician advised me to change my file pattern to ^zendesk_tickets_* but it didn't work We spent at least an hour going through setting up role permissions in AWS before the technician finally gave up and told me that he'll just get back to me later A few hours later the technician finally determined that the regex needed to be changed to ^zendesk_tickets_\\d*csv""When setting up a connector in Google Cloud Storage I tried to use a similar regex using ^chat-\\d*\\csv However since the files were in a sub-folder it didn't take Instead I had to use *chat-\\d*\\csvThis problem is easy enough to rectify just learn some Java-flavored regex! However it can still be annoyingFivetran boasts that it can Replicate everything with zero configuration and schemas designed for analytics Eliminate engineering busywork while empowering your analysts to prove value However this dream is difficult to realize unless you give said analysts access In most cases this is hardly the case because data engineers like to control their data but then again this is not Fivetrans problem but more of an internal issue so make sure your organization is suitably preparedThe aforementioned points are just my opinionI welcome your feedback and look forward to hearing about your own experience with FivetranYou can reach me on Twitter or LinkedInOriginally published at https://blogednalyncom on May 17 2020"
d3EJgAje76GbKV88PQ7kW5,What it takes to believe in yourself your abilities and potentialI am working as a Data Engineer in London and my learnings are unstoppable After two and a half years of hard work I am where I want to beApplication development has been my role since starting my career and working into different IT companies gave me quite good exposure and was also paid well at work However I wanted to be more challenged research and innovate which was not happening in my previous roles I also somehow couldnt push myself enough to start studying with the same energy and zeal which I had during my engineering daysIt was all going fine before that one day at my workplace which made me realise that I am not so much into what I am doing I felt so low and underutilised On this day I could clearly see and remember my college days back when I wanted to stand out and be uniqueSo I decided to start everything from the beginning and started talking to people who are already working in DataSwitching career path to Data Engineering is not easy I was told by manyIn my opinion nothing is impossibleWith no background in Data Engineering I started reading books blogs and videos I was working in the evening shifts those days and struggled to find time for my studies By the time I am home its already 10 pm in the night I enrolled myself in some online courses and completed my assignments at night and sometimes used my office lunch hours to do some readingMonths later my partner got the opportunity to move to London for work This was the turning point of my career After burning the candle from both ends for six months now I started feeling confident to leave my current job and decided to move to LondonWhat matters is how much determination do you have This journey was not easy but being tenacious in nature helped me achieve my goalMany times we stop ourselves to take risks because we are worried about what would happen if we fail To make your goal fail-proof switch from thinking about failures to thinking about discrepancies between what you hope to achieve and what you might achieveAs long as you continue making an effort there is no room for failure
S9xJAk5VWDVpzUWbxbfX3c,Big data is a hot topic now People are mostly focused on machine learning data analysis report generation etc Thats why you can find many online courses to learn more about data science and data analysis But when it comes to make data useful to the company it requires not only data scientists or data analysts but also data engineers And my goal of this article is to give some introduction about data engineering role to the fresh grads or anyone interested in this roleThere is no straightforward definition One can say data engineer is a software engineer who focuses on building data infrastructure data related applications to increase the efficiency of the data teamAccording to the website Payscale one can get on average 78000 USD per annum as an entry level data engineer which is higher than an entry level software engineerThe responsibilities of a data engineer are quite broad but I will talk about the most common ones: In the real world data is scattered in multiple places it can be in the database on a 3rd party API or even in a CSV/excel file When a data analyst wants to create some report on this scattered data it will be hard to run some queries because different sources can have different structures and different formats One of the key responsibilities of a data engineer is to consolidate all this data sources in one place which can be a Data Lake or a Data Warehouse Data engineers build data pipeline to gather these data and dump in the right destination same as the water pipeline in your house which brings water from the water pump to your tapETL ( Extract Transform Load ) is a part of building data pipeline Basically you will extract data from some source transform the data to make sure all the data source have the same data structure and load the transformed data to the destination which can be data lake or data warehouse Inside transformation stage one can do sanitisation validation as wellDeveloping applications to serve big data is one of the responsibilities of a data engineer For example: building an api which serve content recommendations custom dashboard for data reportingData engineering requires multiple skill sets but its not necessary to be good at all the skill sets for a fresh grad One of the key skills required is knowing SQL as SQL is a core part of any data ecosystem Then data engineer should know some backend programming languages specially Java Python Golang etc These programming languages help to write backend API ETL/ELT code Then some knowledge about popular big data tools are necessary like Data warehouses (eg: AWS Redshift Google BigQuery) Apache Hadoop Apache Spark AWS glue Google Dataflow Apache Flink etc There are other tools out there which helps to build a good data infrastructure like Apache Airflow Kafka AWS S3 Google cloud storage etcDemand for the data engineers is increasing day by day as more and more companies are trying to become data driven And the industry have huge scarce of data engineers So it would be great if more fresh grads come into this field
KKwnfqatPJiHMWAzkTy7nL,"Besides finding a right ETL workflow framework to collect process analyze and deliver tons of data finding the best way to manage deploy and scale it is equally important It is not only the humongous data volume but the level of adaptability required to address continuously evolving business needs that make ETL system complicatedOne of the best ways to implement scalable Data Pipeline is using AWS lambda Moreover Combining Lambdas with AWS kinesis let us build sophisticated real-time Data Pipeline However in many scenarios we face AWS Lambdas limitation Airflow is one of the best tools which helps us to overcome those limitationsThis post is a step by step tutorial for deploying Airflow with celery executor on AWS ECSYou can get all the codes from https://githubAirflow an Apache project open-sourced by Airbnb is a platform to author schedule and monitor workflows and data pipelines Airflow jobs are described as directed acyclic graphs (DAGs) which define pipelines by specifying: what tasks to run what dependencies they have the job priority how often to run when to start/stop what to do on job failures/retries etc Typically Airflow works in a distributed setting as shown in the diagram below The airflow scheduler schedules jobs according to the schedules/dependencies defined in the DAG and the airflow workers pick up and run jobs with their loads properly balanced All job information is stored in the meta DB which is always updated in a timely manner The users can monitor their jobs via an Airflow web UI as well as the logsWhy Airflow? Among the top reasons Airflow enables/provides: Amazon Elastic Container Service (Amazon ECS) is a highly scalable high-performance container orchestration service that supports Docker containers and allows you to easily run and scale containerized applications on AWS Amazon ECS eliminates the need for you to install and operate your own container orchestration software manage and scale a cluster of virtual machines or schedule containers on those virtual machinesI am not going to discuss this in detail but you need to properly setup your AWS environment I suggest to setup a VPC with two private and one public subnet Next during the creation of RDS and Elastic-cache you can create a subnet group by using two private subnetAlso you need to create two security group; the first one for RDS and EC which facilitate the access to those instances inside VPC And the second one to give the public access to ECSIn addition you need to setup and install ECS CLI AWS CLI and Docker on your environmentIn order to use Airflow in production environment we need to setup database As Airflow was build to interact with its metadata using the great SqlAlchemy library we are able to use any database backend supported as a SqlAlchemy backendSince we are using AWS environment lets setup our database on Amazon Relational Database Service I decided to use PostgreSQL but feel free to choose MySQl if you like it moreCeleryExecutor is one of the ways you can scale out the number of workers For this to work you need to setup a Celery backend you can setup Redis or RabbitMQI decided to use AWS ElastiCache to setup Reids for CeleryExecuterI get the docker file from docker-airflow repository and update itcreate-userpy let us to create a Airflow user Update username password and emailentrypointsh responsible to set environment variable required You need to update Redis and Postgreql credentials and host namesWe have everything we need in place to build the image from docker fileThe airflow-on-ecs docker image is needed to be hosted on Docker hub or in AWS ECR I choose to use ECR repositoryBefore you can start you must install and configure the Amazon ECS CLI For more information see Installing the Amazon ECS CLIThe ECS CLI requires credentials in order to make API requests on your behalf It can pull credentials from environment variables an AWS profile or an Amazon ECS profile For more information see Configuring the Amazon ECS CLIWith the following command substituting profile_name with your desired profile name $AWS_ACCESS_KEY_ID and$AWS_SECRET_ACCESS_KEY environment variables with your AWS credentials""Complete the configuration with the following command substituting launch type with the launch type you want to use by default region_name with your desired AWS region cluster_name with the name of an existing Amazon ECS cluster or a new cluster to use andconfiguration_name for the name you'd like to give this configurationThe first action you should take is to create a cluster of Amazon ECS container instances that you can launch your containers on with the ecs-cli up command There are many options that you can choose to configure your cluster with this command but most of them are optionalIn this step we create dokcer-compose file In case you decided to not define amount of CPU and RAM for each container you can easily remove cpu_shares and mem_limit from compose file if you decided to keep it make sure your instance type has sufficient CPU and memoryI suggest to allocate at least 1 Gigabyte if you want an stable environmentReplace the image with the image name you pushed into ECR repositoryFinally we can create a service from the compose file with the ecs-cli compose service up command This command creates a task definition from the latest compose file (if it does not already exist) and creates an ECS service with it with a desired count of 1lets check the running containers by executing ecs-cli ps You can copy and past the Airflow and flower address into your browseryou can easily check any container logs by using following command"
b3J52kkBAMeiHPLSouhyRo,In Part 1 Ive explained every step required to deploy Apache Airflow with celery executor on AWS ECS using EC2 (Highly recommend to read Part1 first)In this post I explain how to deploy Airflow on Serverless environmentYou can get all codes from https://githubAWS Fargate is a technology that you can use with Amazon ECS to run containers without having to manage servers or clusters of EC2 instances With AWS Fargate you no longer have to provision configure and scale clusters of virtual machines to run containers This removes the need to choose server types decide when to scale your clusters or optimize cluster packingWhen you run your tasks and services with the Fargate launch type you package your application in containers specify the CPU and memory requirements define networking and IAM policies and launch the applicationFollow from preparation step to step 9 in part 1Amazon ECS needs permissions so that your Fargate task will be able to store logs in CloudWatch This permission is covered by the task execution IAM role • Using the AWS CLI create the task execution role: 3 Using the AWS CLI attach the task execution role policy: The ECS CLI requires credentials in order to make API requests on your behalf It can pull credentials from environment variables an AWS profile or an Amazon ECS profile • Create a CLI profile using your access key and secret key: Tip: if you have more than on profile and cluster update ~/ecs/confg and ~/ecs/credentials and set cluster and profile name as a default valueCreate an Amazon ECS cluster with the ecs-cli up command Since you specified Fargate as your default launch type in the cluster configuration this command will create an empty clusterTip: ecs-cli up command is able to configured VPC with two public subnets but I suggest to understand AWS network and be you own AWS BOSSIn order to use FARGATE we need to set AWS ECS network mode awsvpc which dictate few restriction in terms of parameters we can use in docker compose fileIn addition to the Docker compose information there are some Amazon ECS specific parameters you need to specify for the service Using the VPC subnet and security group IDs Create a file named ecs-paramsyml with the following content: After you create the compose file you can deploy it to your cluster with ecs-cli compose service upAfter you deploy the compose file you can view the containers that are running in the service with ecs-cli compose service psNice Airflow and flower are accessible on port 8080 and 5555 respectivelyView the logs for the task: You can scale up your task count to increase the number of instances of your application with ecs-cli compose service scaleFargate is definitely more expensive than using EC2 There is no additional charge for EC2 launch type You pay for AWS resources (eg EC2 instances or EBS volumes) you create to store and run your application You only pay for what you use as you use it; there are no minimum fees and no upfront commitments But with Fargate you pay for the amount of vCPU and memory resources that your containerized application requests vCPU and memory resources are calculated from the time your container images are pulled until the Amazon ECS Task* terminates rounded up to the nearest second A minimum charge of 1 minute appliesFor instance in order to our configuration for Airflow we need to spin up EC2 t2large to avoid restarting services A t2large instance cost us $ 6793 per month Using same amount of resources (2 vCPU & 8 Memory) will cost us about 7289 $ for vCPU and about 7320$ for memory The total cost of using Fargate with same configuration of t2large instance will be 14709$ which is more than two times of EC2 costOn the other hand Scaling Fargate is much easier than using EC2
ngHfm8HG7zwAb82NnMF4kt,"The goal of this article is to give a high-level overview of different technologies data engineers use to build together end to end pipelines If you want more of a technical code overview lookout for Part 2There comes a time when the data gets too big and running tasks on our computers can get computationally expensive At that point we could take different routes to get the development back up and runningTodays write up will talk about Big Data and tools we can use to help us work with Big Data What do I classify as Big Data? Well if it cant fit on my computer or on a virtual machine its big data Generally the datasets we find on Kaggle to perform data analysis and machine learning arent too big in size so we never have to worry about the computational cost when performing tasks However in the real world businesses have more than a couple of gigs of data and we need to be careful about how we process and run tasks or else we may run off scheduleWithout delving into the technologies we should go over some concepts and principlesData Lake: Storage system that is used for extracting data from its source Ideally having both data in its original (raw) format and the transformed version in separate sourcesData Warehouse: The main database is data that is validated and ready for consumer use The transformed data from the data lake is loaded into this warehouse The data warehouse can have several databases for example we may have a staging database that validates our data before loading into the main database Some validation checks could be checking the schema business logic or monitoring failuresThe Hadoop ecosystem has many different projects Some of which we will go over I would say at the core we have HDFS and then all other elements are stacked on top Hadoop was meant for large amounts of data where we can store distribute and process data at scale We can divide and conquer tasks by running operations in parallel across these clusters resulting in higher efficiency compared to one cluster doing all the workHadoop Distributed File System (HDFS) is at the base of Hadoop HDFS is the file system that will distribute the data across our clusters HDFS reads and writes files from diskThis is where the processing of tasks come into play Yarn lets us manage how we want to handle data on our clusters You can think of Yarn as the resource negotiator that manages our clusters we can specify different settings according to our needs in YarnMapReduce isnt constrained to be used only in the Hadoop ecosystem there are 2 Key components within MapReduce is Mappers and Reducers These can be thought of as functions that can transform into pieces that we want You can think of these operations similar to Map() and Reduce() in Python Sitting on top of MapReduce can be programming APIS like Pig which is a high-level language that has a similar syntax to SQL We can also use Hive (also stacked on top of MapReduce) which is similar to a structured database that we can run queries on Finally we can use a service like Apache Ambari for simpler management and supervision of our clustersJust like Hadoop Apache has different projects used in big data We will be talking about Spark which is another big data processing tool that has been surpassing MapReduce in popularity over the last couple of years The advantages to Spark over Hadoop is that it keeps data in RAM so it doesnt need to constantly read and write from disk For starters you should know two main concepts behind Spark: RDDs and SparkContext which we will debrief you on soon Apache also has machine learning libraries (MLlib) we can use to gain inference or make predictions I also recommend that you check out Databricks if you want to learn more about Spark They have notebooks you can perform FREE cluster computing on""More of a higher-level API which is great to use for parallel distributed computing And its less complex than other frameworks so youll end up writing fewer lines of code You can think of Dask as a lightweight Apache Spark While Spark may have the upper hand in regards to power and scalability for big projects Dask is great if you're at the beginning stages of learning cluster computing There is an API that works with Python and has a similar syntax to Pandas Numpy and Sci-kit learnHeres a simple high-level overview of different frameworks we can use in the data pipelineThat is an overview of big data concepts and tools Please look out for Part 2 Looking forward to hearing your feedback"
dKTzVBcczboByQN9gmpWwA,With data engineering becoming one of the most sought-after skills in the job market and Microsofts success in positioning Azure as the leading cloud provider for enterprise customers the demand for the Azure Data Engineer certification has seen a steady increase An Azure data engineer as described by Microsoft has the following capabilities: Azure data engineers are responsible for data-related implementation tasks that include provisioning data storage services ingesting streaming and batch data transforming data implementing security requirements implementing data retention policies identifying performance bottlenecks and accessing external data sourcesWhat comes as a surprise for many is that in order to obtain the certification you actually need to pass two separate exams: The difference between both as their names suggest is that the DP-200 is focused on testing your knowledge on how to implement the data solutions (some would call it the more technical exam) and the DP-201 focuses rather on the design/conceptual side of the solution Overall including study time revising the Microsoft documentation and acquiring some hands-on experience with the concepts covered in this certification you should expect to invest between 35 45 hours of preparation for the Azure Data Engineer certification Expect this estimate to be higher if you are completely new with cloud technologies or data engineering in generalFor the actual study materials you can find separate guides for the DP-200 and DP-201 exams in the links below: Good luck in your path towards becoming a certified Azure Data Engineer! If you have any further questions feel free to write a comment below
DJTGAPXTGSDSuJgpNd33Xk,Extract Transform and Load or ETL for short is beautifully simple concept Most data scientists analysts product managers developers or anyone working with data should be familiar with the concept of extracting data from somewhere transforming it such a way that is more useful to them and then loading their new data into a new location to be used laterAlthough ETL is a simple concept to understand in reality it can be quite the opposite Implementing ETL using big data tools can be extremely complex if not unnecessarily difficult Atlassian primarily uses the AWS stack (S3 EMR Hive and Athena) to fulfill its data engineering needs and although AWS provides us a wide range of useful tools that allow data engineers to do their jobs for anyone else sitting outside of that sphere the learning curve can be steepFor those not familiar with Atlassian Atlassian is a software as a service company offering with a wide suite of tools for teams from startup to enterpriseWith such a wide variety of tools Atlassian has some complex data engineering needs Conforming different product data tracking customer events across products and calculating company metrics for each product are some of the challenges faced Internally this translates to numerous data scientists data analysts product managers and data engineers working closely togetherWith differing teams and independent goals it can be hard as an Atlassian data engineer to be across everything nor should they need to be Data engineers should be solving the big engineering problems or providing services or infrastructure that others can easily use However if the services arent there or are not user friendly data engineers can easily start becoming blockersReturning to our ETL theme here at Atlassian weve identified that ETL for non data engineers was becoming a major point of contention Atlassians data lake is exponentially growing and there is a critical need to extract and transform data from our major datasets and load it into smaller subsets which differ depending on each persons use caseThe contention arose from the fact that data engineers are ETL domain knowledge experts but with limited time to solve every use case and not willing to support minor ETLs moving forward Whereas non data-engineers are also time constrained and want to find insights quickly but generally lack the data engineering nuances to ensure their ETL will work or understand why their ETL might not work For the most part the minor ETLs are simple initial CREATE TABLE AS SQL queries with the possibility to also schedule these queries which can easily change to become INSERT INTO (append) type queriesWhen data engineers see a CREATE TABLE AS statement these are the questions that are come to mind: Most of these questions relate to data engineering best practices and are a case of creating correct data definition layer (DDL) SQL statements or ensuring the best tool is used for the right job and linking everything together to workHowever non data engineers or the submitter of the SQL shouldnt need to additionally burden themselves with trying to figure out how to answer these questions or learn these data engineering best practices This is especially true as they should be more focused on the data output and the insights they can find from itIt became apparent there was a clear need for a service that would: So thats what Atlassian has done We think that in an ideal world minor ETL should really just be a case of submitting some SQL and the rest is magically taken care for youLeveraging Spark on AWS EMR Atlassian has built a service that allows users to submit SQL where the output of that SQL gets populated into a table of their choosing The service essentially abstracts away all the data engineering and takes care of it for you in the backgroundUsing a data portal front end users can submit an SQL where the service initially: Users can then select: The service then compiles a Spark submit job that is sent to an EMR Spark cluster that: Should users want to schedule their SQL (to append or refresh) we utilise Airflow and create a DAG that compiles the Spark submit job and sends the Spark submit job on the requested scheduleWith this service now running we are seeing a reduction of data engineering requests on how to create tables and populate them This is the desired output as now there is a user friendly way to self-serve ETLAlthough the transformation service has had successful adoption resource management to ensure everyones ETL will run successfully is still difficult There are cases where our Spark cluster cant cope with how much data is needing to be processed We are looking at introducing ephemeral clusters that would persist for single Spark submit jobs allowing users to specify the cluster size should they need something larger then usualIn the meantime we have also implemented autoscaling on our cluster that triggers on memory use This has proven to be quite successful providing the submitted SQL is reasonable The cluster scales up aggressively should there be required load and gracefully scales down when nothing is running This has the added benefit of cost saving as we keep the cluster size low during quiet times (ie the weekends)Finally education in SQL is still proving to be necessary Optimising users SQL solves 90% of our failures within the service Many users are unaware of how much redundant data they might be scanning with their SQL We are still working on the best way to help inform users of SQL best practices
VXRsxBuwmXqPjNuqpzCHiT,"""First of all I want to apologize myself for not posting here for a long time Recently I moved from Social Miner to iFood and I've been working hard to learn everything as fast and better as I can from themBefore moving to iFood I had to do a technical skills practical test and among a lot of data engineering requirements I had this one to build a REST API to get some metrics so I wanted to show you guys this solution I applied a Spring Boot REST API consuming data directly from the Data Lake at AWS Athena Obs: For security and privacy purposes I changed the sample data used for the test to a new generated oneFor the Athena connection I used this Java library The credentials required are all set in the properties file and called dynamically within the getConnection method as seen bellowThe connection is called and used just like in any other JDBC libraryIts a very simple Spring Boot project consisted by the controller classes where the API routes are set the services classes where the connections are made and the queries are pre constructed the error and response handlers and the data objectsI really liked the performance of the queries it responds just like if it was querying a RDB I made two metrics to materialize the use of the data both of them joining the two sample tables and each metric has its own endpointPs: I choose not to do list endpoints since this API was intended to contain only query metricsThe first endpoint returns the number of orders by state in a given dayThe second one returns the top 10 restaurants by a given customer id""I have a lot more to show about this project but since it's more about code than anything else I prefer to open its code so anyone can browse freely on itThe source code can be found here"
eBRRzKaSowY3owpaAvENx9,Motivation: Its been a while since I pushed new articles about my ongoing projects Most of my previous posts are from an era where I was undertaking research in the Computer Vision field as a Masters student I then made the decision to go and work in the industry From Jan 2019 onwards I was working as a Junior Data Scientist at a solar energy company And From Feb 2020 onwards I made the transition to a Junior Data EngineerComing from a research/academic background I was definitely enthusiastic to work in the industry and see how vastly different it is from academia But I wont reflect on that I thought I would use the part 18 months to reflect on some of the data engineering projects that were undertaken at the companyThis series is likely to be a six parts series so strap in Hopefully this will be a fun series Some of the Data Engineering projects that I hope to reflect on include but are not limited to: Stay tuned
ExFohDYR7hCTmX6wSYxgd5,Quick note: This tutorial doesnt explain how to do this for Linux or Windows I have nothing against those operating systems Heck I was an early adopter of Windows 8 and 10 Ive rolled my own ArchLinux more times than I care to count Frankly I thought the swiftness with which you can do this on Mac was astounding and needed to be reportedI strongly discourage installing via Homebrew (ie brew install docker docker-machine) as Homebrew is known to: More on that in an upcoming postSimply run docker pull mongo from any terminal program on your machine and youll have access to the imageYou dont need to do anything else to begin iterating with a Mongo instance on your local laptopWell my friend look no further Bookmark this page as your quick reference because just below Im going to walk you through how to setup your dataMongoDBs support for loading data is some of my favorite in any database technology Were going to use JSON because the Web breathes JavascriptThat said once its in a containerized environment its not exactly clear how to go about getting your local datas folder to be seen as a Docker volume: a necessary step in your data-driven application developmentFor more on Dockers data infrastructure I encourage you to read this documentation from the official website • Lets assume your have some data in the ~/datasets/test/testloadjson that is formatted like the following: 2WhoaLets break it down: 3mongoimport --drop -d test -c data /data/test/testloadThats it You have created a MongoDB test database that you can use and query with your very own dataFor Linux/BSD/Unix: For Windows: You didnt support <xyz> error and now I cant follow alongYeah me too Its my number-one complaint about tutorials (and I problem I have with many certification courses online but thats a rant for a later date) Honestly I tried to break these stepsAnd the list goes on Unless you suffer from this programmer keyboard this tutorial should work for you Tweet at me if it doesntThanks for reading a little further into my mindUntil then
meWqkHgQCzuJyDumbKdHsp,Ensuring your data is well compacted is one of the easiest ways to improve Spark application performance But what is compacted data? How compacted should data be? And how much of a difference does compaction actually make to the performance of a Spark application? These are the kinds of questions well be answering in this articleFor the purposes of this article we say data is well compacted if it is stored in a reasonable number of files relative to its storage size So for example we could store 100 MB of data in just 1 file This would be an example of well compacted data Conversely we could store 100 MB of data across 1000 files which would be an example of poorly compacted data The difference is in the number of filesThis type of situation is commonly encountered with the output of streaming applications Depending on how often a streaming application writes out data and the rate at which events are being received by the application we can potentially end up with poorly compacted dataFor example lets say we have a streaming application which is receiving just 1 event every second Lets further assume that the application is also writing out data every second Based on these assumptions we will end up with (1 file per second * 60 seconds * 60 minutes * 24 hours = ) 86400 files for just that event stream in one day If we further assume that each event takes up 1 KB of storage space we have effectively (1 KB per event * 1 event per file * 86400 files = 86400 KB =) 844 MB of data split across 86400 number of files This is an example of poorly compacted data as 844 MB of data is a small amount of data that could easily be stored as just 1 filePoorly compacted data is bad for Spark applications in the sense that it is extremely slow to process Continuing with our previous example anytime we want to process a days worth of events we have to open up 86400 files to get to the data This slows down processing massively because our Spark application is effectively spending most of its time just opening and closing files What we normally want is for our Spark application to spend most of its time actually processing the data Well do some experiments next to show the difference in performance when using properly compacted data as compared to poorly compacted dataWell start by launching a Spark shell session: Next well write a quick utility function for calculating how long a given operation takes: Next well generate some synthetic data for our upcoming experiments
6u5EsgFwAdpcZirCC5Sonq,The DataBroker DAO alliance is taking shape more and more each day welcoming yet another member to its community Intellegant based in Brussels helps companies to turn their data into valuable insights accompanying them in the intricate journey of advanced data analytics so as to drive better decisions and enhance business performanceIntellegant literally masters the entire data analytics journey Helping companies in the field of data mapping data inventory and classification data integration data transformation data cleaning and data quality and providing expert resources to guide them through the journey of advanced and predictive analytics But thats not all They help organizations understand regulatory requirements and provide a methodology to embed such expectations into their business processes And they give companies a framework to assess their information systems and set a roadmap to prioritize changesBeing located in the capital of the EU the company is ideally placed to consult on European regulations and compliance with a particular focus on data privacy (GDPR) data integrity and data security Upon assessing your operations their experts will make sure that your systems processes and procedures protect you from data privacy loss and unauthorized disclosureDataBroker DAO and Intelligant complement each other nicely Because trading smart data is one thing but knowing how to interpret these data and what to do with them is an entirely different ball gameThe pre-sale phase of the DTX token sale has started You can participate starting from 10 ETH Register on databrokerdaocom and join us on Telegram if you have any questionsAny questions opportunities or partnership requests are welcome on any of these channels: Telegram: https://tme/databrokerdaoFacebook: https://wwwfacebookcom/DataBrokerDAO/Twitter: https://twittercom/DataBrokerDAOReddit: https://wwwreddit
FuW4S5ZfVtvQej9hkJd5oZ,This guide will highlight the steps we took to set up our pipeline on AWSIdeally youll want to to have set up and AWS RDS Database(PostgreSQL or MySQL or etc) Navigate to https://awsamazoncom/rds/ and launch an instance Make sure to keep note or the RDS Password Username and the Instance ID Once youve done that the next part to keep in mind is that the VPC Security group you choose will need to be the same security group that you choose for the Pipeline The last thing to remember is The instance ID is the DB instance identifier you set up when creating the databaseIf you plan on using the AWS CLI create a user with the necessary roles First add a user at https://consoleawsamazoncom/iam/Then add a group Ive called mine datapipelinedevelopergroup Ive given this group all the necessary policies needed for data pipeline For your case youll probably need just AWSDataPipeline_FullAccess Add your user to this groupAfter taking the steps above were ready So lets start by visiting the AWS Data Pipeline link(https://consoleawsamazoncom/datapipeline/) Log in(assuming you already have an AWS account) and then click Create PipelineFrom the Create Pipeline page youll want to give the pipeline a name and a description For source you have a variety of options but for this guides sake well choose the Build using a template option Choose Load s3 into RDS MySQL table optionSo having moved your raw data sources to S3 in the Pipeline set up page for Input s3 file path click the folder and choose the data source Enter your RDS MySQL password your security group the RDS username the RDS Instance ID the RDS MySQL If your RDS database doesnt have a table set up yet enter the query in Create table SQL query(optional)When your done you can specify the schedule you want your pipeline to run Choose your logging and set up the security access Under security access for IAM Role youll want to choose DataPipeLineDefaultRole for Pipeline role and DataPipelineDefaultResourceRole for EC2 instance role Click blue button on the page to continueOn the next page you can choose to add additional operations to the pipeline At this point youll also want to clear up any errors if any exist Some common errors I ran into were connection timeouts from my RDS Database this ended up being the EC2 Instance within my pipeline not using the same security group Also if you are using CSV files youll want to remove the header column I wasted a few hours until I figured out that was the issueIn the end here is how your pipeline will look Hit save and activate to see your pipeline runTo view details about your pipeline navigate to List Pipelines Hit the dropdown and click view execution detailsOn the next screen youll see the status of this pipeline Hitting the arrow dropdowns youll see your activity logs and some error messages on what failed and whyAs a bonus here is a sample template ive created it also contains SNS Alarms that sends you notifications when your pipeline takes certain actions I will have a guide on setting this up soon Just upload this file using import a definition from pipeline creation
Fmeczkh2Bx3ukDb47PcK9M,#1 : Possibly the integration or ESB team… totally focus on the technology and following the best software engineer standards But they really do not care about the data itself#2 : We know data (most often BI Software) but we feel technologies (especially coding) are dirty / low subjects and contractors can take care of it#3 : you clearly love technologies and data and you are like a kid in a toy shop with an unlimited budget The truth is that you dont know to choose Mastery means simplicity and less is the new more Business people will start to think it is doubtful to spend so much money#4 : great data engineering teams are small (still very surprised by the size of very large data driven company with tons of data) and they have a mix of skills around data efficiency and technology It is the sweet spot and people outside will start to attract them because they have #2 or #3 teams
QDtq8ZSA8z3GNyWuXGyL3B,One day at work I was presented with the challenge of consuming a SOAP service using Azure Data FactoryAfter a lot of research over the internet reading a lot of forums I found no tutorial or tip to perform this particular task Even the support that Microsoft provides to our company did not know how to guide usSome forums even said that it was not possible to perform this task in ADF (Azure Data Factory) and that it would be necessary to use Azure Functions to make the SOAP request through codeAccording to wikipedia the SOAP protocol: SOAP (abbreviation for Simple Object Access Protocol) is a messaging protocol specification for exchanging structured information in the implementation of web services in computer networks Its purpose is to provide extensibility neutrality and independenceThen analyzing the ADF I realized that the task in theory could be done using the HTTP connectorSo long history short this is what I didTo perform this tutorial I used the SOAP service available at: http://wwwdneonlinecom/calculatorAnalyze the SOAP service to be consumedTo find out the characteristics of the service to be consumed I usually use the SOAP UI tool**If you have no particularity with the tool there are several tutorials on how to import a wsdl and make a request As the focus of the post is the ADF I wont describe this part in detailMake a POST or GET request to find out which types of headers and which body model will be passed to the Endpoint: I will use the Add endpoint and pass as 10 and 5 values according to the body image of the requestAfter making the request check the headers that have been passed to the endpoint: This information will be used when we build the Activity in the ADFSelect data format Here select the binary type It is the most basic type of ADFName the Dataset created for SoapDataSetBinary and create the linked service Here you will need to create a new linked service for the consumption of the SOAP endpointIn the configuration tab of the new HTTP linked service:- Set the name to HttpLinkedService- Set the Base URL to http://wwwdneonlinecom/calculatorasmx as described in the UI soap request information- Since the endpoint does not require authentication in Authentication type select Anonymous (If your service requires authentication fill in the user-password fields)- Click create Here by clicking the test connection button it is possible to do a test if the connection was successfulAfter creation you will be redirected back to the creation of the dataset Confirm by clicking OK Your DataSet will openCreate the Sink data set to be used In this step you define where the data is supposed to be transported to In this tutorial I used the Azure Blob Store The creation is very similar to the dataset previously created:- Click on new data set- Select the data format for binary- Set the name to SoapSinkDataSet- Select the linked service to be used In this part I will not demonstrate how to configure the linked service for the azure blob since it does not make a difference to the tutorialThe new pipeline panel will be opened Browse Activities and find Copy Data Activity click and drag to the panelIn the Source tab select the newly created SoapDataSet as the source datasetThe Response file of the request will be available at the folder defined in SoapSinkDataSet Just open it with a text editor to see the contentIf you have any questions or problems feel free to contact meLinkedin
C4hpgYE6ZysTpFupfDJ73x,Microsoft Power Automate (previously known as Microsoft Flow) is a powerful product from MS to automate recurring tasks In this article Ill explain how to use Power Automate to build data pipeline from SharePoint to SQL This pipeline will be used to transfer data from a SharePoint site to an SQL tableThe source is of course a SharePoint online website and the destination is our on-premises SQL Datawarehouse Now without giving too much of a verbose text following are the steps you need to take to establish a Data Pipeline from SharePoint to SQL using Microsoft Power AutomatePrerequisites: Step 1: Launch Power Automate from your SharePoint online websiteYou will see 3 options: Select Create a flow This shows you existing templates provided by Microsoft to get you started with a basic data flow Scroll down the list and click Show More and then click See More Templates For all the things I dont like about MS products like their GUI or the presence of unnecessary scroll bars the existing templates come in very handy for anybody whos not familiar with Power Automate When you are on the page displaying all the templates you should see the below image for reference: As you can see MS has packed a punch and tried to leave no stone unturned to enable a beginner to work with Power Automate without any hassle Here you need to sign into your Office 365 account When you sign in you should see the below screen: Step 2: Click Create from the left sidebar MS will provide you with the following options: The subtexts for each flow in the above image represent what they are used for For the purpose of this article I will show you how to create an Automated flow When you click the Automated flow option you should see the following screen: In order for your pipeline to get started it needs a trigger Think of trigger as a starting point What it needs to get started The trigger can either be manual (in the form of a mouse click) scheduled (in the form of a set time) or automated (in the form of an event taking place) Im going to choose one of the most common scenarios I have faced- When an item is created or modified Basically this means whenever a new item is added to a SharePoint list or an existing item gets modified the pipeline will start This will be the trigger Once the trigger is executed it needs to perform an actionEnter your SharePoint address (should be a URL starting with HTTP) and the list name that contains your items which need to be copied through this data flow This will serve as your triggerOnce this trigger gets created you need to fetch the items from the SharePoint list We will now add an action called Get Items Enter the same site address and list name as you did in the trigger If you need items from a particular column in your list select that name when you click inside Limit Entries to Folder When you click advanced options you see a couple of additional fields which I have explained below: Filter Query: Think of this as the where clause in an SQL statement MS uses ODATA(Open Data) filter query syntax that you can use to filter items you need for this data flowFor example: 1 column_name eq Gaurav (eq = equals to) 2 column_name gt 2020 08 11 (gt = greater than)3 column_name lt 2020 07 11 and column_name lt 2020 08 114This article from Microsoft explains the ODATA filters in advanceOrder By: Similar to an SQLs ascending or descending order Specify a column against which you need the list to be ordered byTop Count: Should you wish to retrieve only a selected number of items from the list specify that number hereLimit Columns by View: If you do not need all the columns to be retrieved by Power Automate you can use this feature to limit your view to a particular set of columnsWhen you work with Get Items from a SharePoint list and if your SharePoint list has > 1000 items the Get Items by default only retrieves 1000 entries After a lot of brainstorming to understand why this was happening I realized that we need to change settings for Get Items block Click the 3 ellipses on the Get Items and turn on the pagination featureThe maximum value that you can enter into the Threshold text box is 100000 This limit is set by MS We will enter this limit in the text box Once done you can save and test your flow Until this step what MS Power Automate will do is to wait until you create a new item in your SP list or modify and existing item Once you do any of the above steps it will retrieve all the items from your specified SP listThe flow has been able to retrieve all the items from our SP list But the task is not done We need to enter these values into our DB table Thats the goal of this pipeline isnt it?! In order to do this a) click New Step > Get rows (V2) under the SQL option (to check if there are any rows in the SQL table);b) click New Step > type- Insert row (V2) under the SQL option; c) enter your server name database name and table name in steps a and b;d) click the 3 ellipses to the right of the Insert row block and Get rows block to check if you are connected to your DB server If not click Add New Connection This step needs to be performed because the Power Automate needs to connect to your DB;e) after the connection is established and all the information is provided from steps a and b you will see a list of columns in Insert row block Each column in this list needs to be mapped with the column from your SP list This is how we tell Power Automate which column value from the SP list goes into which column in our DB tableIf you run the pipeline until Step 5 you will notice that it will only transfer one item from your SP list to your DB This is because we have overlooked a crucial programming logic- if we have an array of n items and we need to read all of them we need to loop over themThus we need to go back to Step 5 and before adding the SQL block we need to add a for loop that will loop over all the items in the SP listApply to each is Power Automates for loop When you add it it asks for the array which it needs to loop over Click inside the text box and you will see Power Automate shows you dynamic content that it fetched from our Steps 1 4Note that in Power Automate every time you use Get items all of those items are stored in an array called value This is the value you need to select and Power Automate puts it inside the for loop Once done we can now drag the Insert row SQL block inside the for loopThe pipeline will now function like this: When an item is created or modified in our SP list it triggers an action that follows it The action fetches all the items from the SP list Once it gets all the items it stores them in an array called value The pipeline then gets rows from the SQL table (should you need to check if there are rows in the table) Afterwards the for loop reads each value from the array and inserts those values in your DB table of choiceIn the above flow you might feel annoyed that the trigger only works when YOU perform some activity on your SP And you need something that would take this responsibility off your hands To handle this exact scenario Power Automate has a different trigger called RecurrenceAs you can see from the above image you can time your entire data pipeline to fire at a defined interval of time If you need the data pipeline to check items in the SP list every day and copy them to DB set the frequency to 1 day (as shown) and you can check whether new records were copied after each day The frequency goes as low as a second and as high as a monthWhen you set the recurrent trigger as shown above be very careful about the timezone you want it to work under Click the advanced options and you can search for your preferred timezone You can also tell Power Automate at what hours or at what minutes should the trigger be fired All these options are present under the advanced optionsWith everything set and done using the recurrent trigger the data pipeline should now look like this: Voila! Your data pipeline is ready to be deployed to your environment
BE58Ni6D4KKPgCLuB5zytH,"Data collection pipeline is always a huge topic in the industryEspecially if you are doing it in large scaleWith low latenciesAnd your data scientists hates Google Analytics because the data is not informative enoughFor years maintaining such a pipeline would give headaches to every data engineers and DevOps engineer but with the raise of cloud services like AWS it could just cost a couple of hours to create a service-based auto scaling pipeline: In this tutorial we will walk through the architecture design of the pipeline as well as the underlying implementationsSuppose a user is visiting our demo website: Its fine if you are not familiar with the services and jargons We will go through them one by oneOne important principle in designing the data pipeline is to decouple between data delivery storage and processing Each component agrees on the data schema only and does not care how other components work Such decoupling is good for maintaining a reasonable complexityAll resources in the tutorial are published into this Github repo: Before start make sure you have these resources ready: Note: Some AWS resources are available in specific regions only The demo is built on the region us-west-2The whole pipeline will utilize a bunch of AWS services Heres the full list: The pricing might look a bit hard to understand but in summary: For simplicity we will define a minimal set of items to collect from usersThose who have experience in using Google Analytics should be familiar with the Category Action Label (Value) design More details can be found in the official helpTo keep things short a bash script is provided to bootstrap the stackFrom the repo aws-real-time-data-collection run the script by: You could replace the value of bucket-sample-data-collection and SampleDataCollection with your choiceThe script does quite a few things but the magics are mainly these two: We used to think S3 is just a dummy storage with little featureThat configuration is AWS Glue table a collection of metadata to describe the data schema and storage format It tells the query engine how to access the dataSuch server-less database abstracts us from operations like cluster scaling or version upgrade It is also cost friendly as they are billed by the size of the data scanned not fixed instance-hour priceThe Glue table schema is defined in this template We will use AWS Athena to query the sample data More on this laterIts a managed buffer to put incoming events into S3 PeriodOK Firehose can do much more than that but thats our primary use caseThe data would have been stored as JSON lines but in the Big Data ecosystem we often use Parquet instead for two main reasons: Firehose allows converting the data from JSON to Parquet just before putting them into S3 (details) In our example the bootstrap script will turn on the optionThe next step is to setup the service to put incoming data into the Firehose streamNavigate to collector/ Here comes a serverless app Serverless is a framework for building well server-less applications That means we only need to define the application logic itself and let the cloud provider to handle all the scaling and monitoringFor the first starters install the tool by npm install -g serverless Also install the dependencies by npm installThen open the serverlessyml and update the constant: The AWS account ID can be found in the support centre or from the output of aws iam get-userLets try the function in local: If everythings alright the output will be like this: The project is ready to go Deploy it by: Heres what happened in the deployment: Copy the URL under endpoints as we will use it laterIve written a very basic web page to simulate the actual client appNavigate to web/ and run a local HTTP server""For Nodejs the simplest way is npx http-server For Python you could use python3 -m 'httpserver'The interface should be self-explanatory: just paste the URL endpoint from the Serverless deployment and do the point-and-click If the requests return success it will be appended into the tableThe data would be digested by our Firehose stream and put into S3 The process typically takes 1 2 minutesIf you found such objects then congrats! The pipeline is working normally Otherwise you could visit the Firehose console → sample_data_firehose → Amazon S3 Logs and look for the errorsWe can quickly verify the content by S3 Select just do it by: The contents should agree with the events we fired in the web app earlierWhile S3 Select is handy for quick content retrieval its features are limited such as only one file can be selected at a time This does not help in querying event in big scaleTo enable more powerful queries we can use AWS Athena the query service that can directly access data in S3Visit the Athena console and here comes an interactive query client Athena can query from AWS Glue tables and we have already defined one at the beginningLets try to type in a query and see what happens (spoiler alert: it fails)Why no records? Turns out defining a table schema isnt enough we have to also create partitions In particular: Replace the value of year month day hour and the S3 pathOne of the solutions is to create a AWS Lambda function that: A sample Serverless project is provided for this purpose Go to partitioner/ and again: The next time when Firehose puts events to S3 the partitioner will be triggered adding the partition (if not exist) automaticallyIf you feel tired maintaining (yet) another piece of glue codes AWS also offer the Glue Crawler which can automatically analyse the schema and add partitionsTo create a crawler the simplest way is to visit the Crawler console and click on the Add crawler with the settings: You could run the crawler manually and expect it to recover the partitionsCompared to the partitioner Lambda function the crawler has these advantages: And the drawbacks: The readers could decide which option would be more suitable to their needsThats it! Weve gone through a long tutorial on setting up a data pipeline from collection to consumption and able to retrieve the data with little latencyYou may notice that we did NOT setup any servers and instances throughout the whole tutorial all are server-less and the cloud provider would take care about the maintenance and scalingI hope the tutorial helpsFinally do remember to clean up the resources: In traditional database we can use all the INDEX and KEY to boost performance This is to tell the database where to look for the data effectively reduce amount of data scannedWe could apply similar approach in Athena That is adding partition key to the Glue table Our sample_data already contain 4 partition keys: If your query contains conditions on the partition keys Athena is smart enough to know which S3 directories to scan only making the query fast and cost efficientWhen handling terabytes of data partition key is always a must"
k2LFmFkfRPWXvxgGoMXiJC,While collecting data in big scale is already difficult extracting insights from them is even more exhausting Fortunately with the rise of high-level alternatives like Apache Beam things are now much easier to manage (though not completely painless)In this tutorial we will guide you through the steps of writing a Beam App as well as executing such an App on a Spark clusterAs a brief introduction Apache Beam is a high-level data processing framework One of the most outstanding features about Beam is its unified data model design That means: On top of that Beam is also runner-independent Beam App can be run inside a local machine in a Spark cluster in the Flink application and even by Google Data Flow a fully managed service that you dont need to manage any machine provisioningWith all the available runners supported by Beam keep in mind that the application code stays the same: write once run anywhereWe will write a Beam Application that: We will develop and test the App in a local environment Once we finish the development we will deploy it to AWS cloud by: All resources are published in the following GitHub repo: We also assume that you have: This demo is run on the region us-west-2We will be building our Beam App using Apache Maven First we have to install the toolsFor Mac users Homebrew installation is recommended: Or if Docker is preferred simply launch a Maven container from the official image: Apache Maven is an automation tool for compiling testing and packaging Java projects As npm in Nodejs pip in Python composer in PHP Apache Maven is yet another management tool to automate engineers day to day workThe official MinimalWordCount example already covered the details This section complements the official tutorial by: Instead of writing everything from scratch its much easier to use a templateThe above command generates a project namely wordcount-app in Scala When the command ends the generated project is ready to be compiled and run by: The next step is to extend this project into a production Beam App Its highly recommended to use Java IDE like IntelliJ IDEA or Visual Studio Code in order to enjoy features like auto import and syntax hintsLets take a look at the pomxml inside the project directoryThe POM stands for Project Object Model which is used for defining project configurations such as which libraries to install how to build and test etc If you are familiar with Nodejs its almost the same as the packagejson albeit in different syntaxLike NPM offers an official repository https://wwwnpmjscom/ for everyone to look for useful packages Maven also maintains a central repository https://mvnrepositorycom/Apache Beam SDK are also hosted in that repoThese libraries will be automatically installed when the App is running in MavenNow we are good to start writing our sample AppOpen the App file src/main/scala/com/example/beam/MinimalAppscala in your favourite editor Then replace the Hello World code with the following Scala code: The official tutorial already documented the basic concepts of a Beam pipeline For more implementation details refer to the annotated codes of the MinimalWordCountExampleSimply run the App by the command (or using IDE): and see it in actionYou will see files are created in the output/ directory Lets check out its contents: Yep its the word count we desired forIn the above codes we are hardcoding everything This is not so goodWe will improve it by allowing the App to accept custom configurations Fortunately Beams PipelineOptions already did most of the heavy lifting only minor code changes are neededFirst extend the PipelineOptions and define our parameters: The newly defined parameters will be passed into the App via CLI Update the codes to accept the input arguments: (Note: the _* is the varargs expansion in Scala Check out this SO Question for detailsWhen executing the App supply the arguments as followsThe Beam SDK itself produces a bunch of logs which describes whats going on under the hood However we have to supply our own logger or it will just show SLF4J: Failed to load class orgslf4jimplStaticLoggerBinderOne popular choice is the Logback Simply add one more dependency in the pomxml: And create a file src/main/resources/logbackxml with this minimal config: This is to simply tell the App to log messages with log level equal or higher than INFO to STDOUT Check out the official manual for more configurationsNow re-run the App You should be able to see Beam SDK messages are properly logged onto the consoleIn the output the custom logs will take the name comexamplebeamApp$ so as to differentiate from the Beam logsWe have to realize that our local machine is quite different from a Spark cluster on AWS To name a few: We will go through the tweaks in order to make it runnable on cloudTo enable writing files to S3 we have to include one more library: When using AWS SDK its always better to specify a default AWS regionNext all we need to do is to specify a S3 path in the format s3://<bucket>/<path> in the argumentBeam will match the file protocol and automagically find the suitable adapterWe will be using the spark-submit command to run the App in Spark That means we have to: (In the Java world this JAR with all dependencies packed as a single executable thing is sometimes called Uber JAR This name originates from the German word Über meaning over above or acrossTo enable Spark support we need to install the beam-runner-spark dependencies And to be safe also include the Spark libraries explicitly: To compile a Uber JAR a popular choice is to use the Maven Shade plugin Not only does it include all dependencies it can also avoid potential conflicts (eg two libraries importing different versions of the same package) by renaming themNote: If you are using names other than comexamplebeamApp update the value in the <mainClass> bracketNow we can compile and run the Uber JAR simply by: In the last step upload this Uber JAR (yes the >100MB bulk) to a S3 bucket you ownTo run Beam App in Spark we first launch a Hadoop cluster with Spark supportAWS EMR (Elastic MapReduce) is a managed service to provision and maintain Hadoop clusters with various features like auto scaling and Spark job management This would be a good place to startAll configurations are provided in this tutorial file except one that requires user input:In the wordcount-spark-runner-example/setup/cloudformationyml look for the LogUri property: Uncomment and replace it with a proper S3 bucket location This step is optional but if theres no log config the EMR will not write any logs and thus it will be harder to debugAfter that run the command to create EMR cluster using AWS CloudFormation This may take around 10 minutes to finishVisit the CloudFormation console click into the DataBeamEmr stack and look for the Resources tabRemember the Physical ID j-XXXXXXXXXXXXX for later useNow we have a working cluster its time to actually run the Beam App in SparkInstead of SSH-ing into the instance and run the App within the terminal AWS provided a neat integration call Job Flow Steps Its basically a sequence of commands to run in the cluster but with better GUI and logging supportTo add a Job Flow Step run the command: The command looks clumsy due to the CLI syntax but basically it tells EMR to run the packaged Beam App JAR (stored in the S3 bucket) with the input argumentsWhen running the command on your EMR cluster replace the values of: If successful the CLI will return a Step ID like the followingIf LogUri is configured correctly AWS will upload all job execution logs to the specified S3 locationIn addition the AWS EMR console also provides a simple UI to checkout the logs: Just click into the cluster select Steps and then View logs: Sometimes this error will happen when running in EMR: The exact reason is unknown but after seeking help from the community(1 2) it seems this error only happen in Scala 212 or laterOne of the possible workaround is to use Scala 211 instead To do so update the <properties> in pomxml: And re-package the projectFinally cleanup the resources that are provisioned in the tutorial: Finaaaaaally thats all the basics for Apache Beam Yes basicsTo be honest the learning curve is still somewhat steep and requires quite an effort to pick up Guess this is the nature of the big dataIn the contrast being able to orchestrate all these resources and to see the potential of a scalable distributed data processing platform would also be fulfilling Hope this tutorial could help
i78VkwZUBAbHAWwg8EJxD6,Introduction: Kylin is currently a leader in open source big data-based OLAP tools The performance of Kylin is much efficient when results need to be displayed in fraction of milliseconds I am not going to talk about its features in this blog If you are interested in Kylin features please check out our colleagues article: https://wwwtigeranalyticsProblem with the existing cluster: Why Kubernetes: How it works: In architecture we have installed Kylin client in Kubernetes whereas Hadoop Hive HBase Spark services run on a Hadoop cluster in distributed mode When Kylins job submitted in Kubernetes pod as a client process request gets submitted to the Hadoop cluster and the job will be executed in distributed mode After job completion a response will be given back to the Kylin client process which is in Kubernetes podArchitecture: Our Hadoop cluster is Google Dataproc and the Kubernetes cluster is GKEHadoop Cluster: We wanted our Hadoop cluster as stateless GCS / S3 is used as the underlying storage for HBaseThe benefit of going with the above approach: 1 To Handle autoscale in case of multiple jobs  Having HDFS as storage and autoscaling the data nodes may lead us into the NameNode Safe Mode error frequently2 To Handle the failover scenario If one node/multiple nodes go down  Replication factor 3 can handle if one node goes down but can lead us into data loss in case of multiple nodes failoverBuild your own Kylin-Client Image: Building a docker image of Kylins client is the key to the entire process As part of the image build process all Hadoop Hive HBase Spark Zookeeper Kylin client versions with proper version compatibility should be installed Without blabbering much lets have a look at the Docker fileFROM centos:73# install jdk and other commandsRUN set -x \\ && yum install -y which \\ java-180-openjdk \\ java-18# version variablesENV HADOOP_VERSION=2100ENV HIVE_VERSION=237ENV HBASE_VERSION=150ENV SPARK_VERSION=245ENV ZK_VERSION=34RUN (cd $APACHE_HOME && curl -O https://archiveapacheorg/dist/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}targz)RUN (cd $APACHE_HOME && curl -O http://apachemirrorwuchnacom/hive/hive-${HIVE_VERSION}/apache-hive-${HIVE_VERSION}-bintargz)RUN (cd $APACHE_HOME && curl -O https://archiveapacheorg/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop27tgz)RUN (cd $APACHE_HOME && curl -O https://archiveapacheorg/dist/hbase/${HBASE_VERSION}/hbase-${HBASE_VERSION}-bintargz)RUN (cd $APACHE_HOME && curl -O https://archiveapacheorg/dist/zookeeper/zookeeper-${ZK_VERSION}/zookeeper-${ZK_VERSION}tar#install hiveENV HIVE_HOME=$APACHE_HOME/hiveRUN (cd $APACHE_HOME && tar -zxvf apache-hive-${HIVE_VERSION}-bintargz)RUN (cd $APACHE_HOME && rm -r apache-hive-${HIVE_VERSION}-bintar# install hadoopENV HADOOP_HOME=$APACHE_HOME/hadoopRUN (cd $APACHE_HOME && tar -zxvf hadoop-${HADOOP_VERSION}targz)RUN (cd $APACHE_HOME && rm -r hadoop-${HADOOP_VERSION}targz)RUN set -x && ln -s $APACHE_HOME/hadoop-${HADOOP_VERSION} $HADOOP_HOMERUN (rm $HADOOP_HOME/etc/hadoop/core-sitexml )RUN (rm $HADOOP_HOME/etc/hadoop/hdfs-sitexml )RUN (rm $HADOOP_HOME/etc/hadoop/yarn-site#install hbaseENV HBASE_HOME=$APACHE_HOME/hbaseRUN (cd $APACHE_HOME && tar -zxvf hbase-${HBASE_VERSION}-bintargz)RUN (cd $APACHE_HOME && rm -r hbase-${HBASE_VERSION}-bintar#install sparkENV SPARK_HOME=$APACHE_HOME/sparkRUN (cd $APACHE_HOME && tar -zxvf spark-${SPARK_VERSION}-bin-hadoop27tgz)RUN (cd $APACHE_HOME && rm -r spark-${SPARK_VERSION}-bin-hadoop27tgz)RUN set -x && ln -s $APACHE_HOME/spark-${SPARK_VERSION}-bin-hadoop2#install zkENV ZK_HOME=$APACHE_HOME/zookeeperRUN (cd $APACHE_HOME && tar -zxvf zookeeper-${ZK_VERSION}targz)RUN (cd $APACHE_HOME && rm -r zookeeper-${ZK_VERSION}tarARG USER=apache_kylinENV USER_HOME=/usr/${USER}ENV KYLIN_VERSION=30RUN (cd $KYLIN_HOME && curl -O https://archiveapacheorg/dist/kylin/apache-kylin-${KYLIN_VERSION}/apache-kylin-${KYLIN_VERSION}-bin-hbase1xtargz)RUN (cd $KYLIN_HOME && tar -zxvf apache-kylin-${KYLIN_VERSION}-bin-hbase1xtargz)RUN (cd $KYLIN_HOME && rm -r apache-kylin-${KYLIN_VERSION}-bin-hbase1xtargz)RUN (cd $KYLIN_HOME && cp -r $KYLIN_HOME/apache-kylin-${KYLIN_VERSION}-bin-hbase1x/* RUN (cd $KYLIN_HOME/tomcat/lib && curl -O https://repo1mavenorg/maven2/de/javakaffee/msm/memcached-session-manager-tc7/211/memcached-session-manager-tc7-211jar )RUN (cd $KYLIN_HOME/tomcat/lib && curl -O https://repo1mavenorg/maven2/de/javakaffee/msm/memcached-session-manager/211/memcached-session-manager-211# hadoop-gcs connector jar to connect to gcs from hadoop  gcs is underlying storage for hive tablesRUN curl -O https://storagegoogleapiscom/hadoop-lib/gcs/gcs-connector-hadoop2-latestjarRUN cp gcs-connector-hadoop2-latest#copy hbase*#add libsnappyso native library  if hadoop distribution doesnt have it by defaultRUN (yes | yum install snappy snappy-devel)RUN ln -s /usr/lib64/libsnappyso $HADOOP_HOME/lib/native/libsnappysoRUN ln -s /usr/lib64/libsnappyso1 $HADOOP_HOME/lib/native/libsnappysoRUN set -x \\ && unzip -qq $KYLIN_HOME/tomcat/webapps/kylinwar -d $KYLIN_HOME/tomcat/webapps/kylin \\ && chown -R $USER:$USER $KYLIN_HOME/tomcat/webapps/kylin \\ && rm $KYLIN_HOME/tomcat/webapps/kylinwar \\ && ln -s $HADOOP_CONF_HOME/core-sitexml $KYLIN_HADOOP_CONF_HOME/core-sitexml \\ && ln -s $HADOOP_CONF_HOME/hdfs-sitexml $KYLIN_HADOOP_CONF_HOME/hdfs-sitexml \\ && ln -s $HADOOP_CONF_HOME/yarn-sitexml $KYLIN_HADOOP_CONF_HOME/yarn-sitexml \\ && ln -s $HADOOP_CONF_HOME/core-sitexml $HADOOP_HOME/etc/hadoop/core-sitexml \\ && ln -s $HADOOP_CONF_HOME/hdfs-sitexml $HADOOP_HOME/etc/hadoop/hdfs-sitexml \\ && ln -s $HADOOP_CONF_HOME/yarn-sitexml $HADOOP_HOME/etc/hadoop/yarn-sitexml \\ && ln -s $HADOOP_CONF_HOME/mapred-sitexml $HADOOP_HOME/etc/hadoop/mapred-sitexml \\ && ln -s $HIVE_CONF_HOME/hive-sitexml $KYLIN_HADOOP_CONF_HOME/hive-sitexml \\ && ln -s $HBASE_CONF_HOME/hbase-sitexml $KYLIN_HADOOP_CONF_HOME/hbase-siteENV TOOL_HOME=$USER_HOME/binRUN set -x \\ && mkdir -p $TOOL_HOMECOPY bin $TOOL_HOMECOPY crontabtxt /tmp/crontabRUN /usr/bin/crontab -u $USER /tmp/crontabRUN set -x \\ && unzip -qq $KYLIN_HOME/tomcat/webapps/kylinwar -d $KYLIN_HOME/tomcat/webapps/kylin \\ && chown -R $USER:$USER $KYLIN_HOME/tomcat/webapps/kylin \\ && rm $KYLIN_HOME/tomcat/webapps/kylinwar \\ && ln -s $HADOOP_CONF_HOME/core-sitexml $KYLIN_HADOOP_CONF_HOME/core-sitexml \\ && ln -s $HADOOP_CONF_HOME/hdfs-sitexml $KYLIN_HADOOP_CONF_HOME/hdfs-sitexml \\ && ln -s $HADOOP_CONF_HOME/yarn-sitexml $KYLIN_HADOOP_CONF_HOME/yarn-sitexml \\ && ln -s $HADOOP_CONF_HOME/core-sitexml $HADOOP_HOME/etc/hadoop/core-sitexml \\ && ln -s $HADOOP_CONF_HOME/hdfs-sitexml $HADOOP_HOME/etc/hadoop/hdfs-sitexml \\ && ln -s $HADOOP_CONF_HOME/yarn-sitexml $HADOOP_HOME/etc/hadoop/yarn-sitexml \\ && ln -s $HADOOP_CONF_HOME/mapred-sitexml $HADOOP_HOME/etc/hadoop/mapred-sitexml \\ && ln -s $HIVE_CONF_HOME/hive-sitexml $KYLIN_HADOOP_CONF_HOME/hive-sitexml \\ && ln -s $HBASE_CONF_HOME/hbase-sitexml $KYLIN_HADOOP_CONF_HOME/hbase-siteENV TOOL_HOME=$USER_HOME/binRUN set -x \\ && mkdir -p $TOOL_HOMECOPY bin $TOOL_HOMECOPY crontabtxt /tmp/crontabRUN /usr/bin/crontab -u $USER /tmp/crontab
5pUrcg8rV6v6nbuRJCmhwP,Many times during the development of Spark applications it is required to generate sample data to test the applications functionality This article demonstrates a simple and quick way to generate sample data from the spark-shell for a Spark jobs functional and/or load testingFirst we need a function to generate random data We may create multiple such functions to generate random integers or characters etc based on your requirement of the type of data and use a combination of themFor simplicity well just create one function by using Scalas builtin utility which generates alphanumeric strings You may also decide to write custom random string generator functions you can find a few examples hereThe function takes length as an argument to generate strings of the desired lengthscalautilRandomWe can use `toDF()` to generate a Spark dataframe with random data for the desired number of columnsSeqfill(4000) creates a collection (Seq) of 4000 rows with random strings generated by randomStringGen() for each columnsparkContextparallelize() creates a parallelized collection from the Seq of rows so that they can be operated on in paralleltoDF() returns a dataframe with the column names mentioned as col_1 etcYou may choose a different file format (parquet orc csv etc) based on the applications requirements
BB4PS3PiNzmrG2LyXKrAPL,A retailer will most likely not win a sale if the price is not right Thus retailers want to compare their prices against competitors and price accordingly On the other hand manufacturers want to police their resellers and ensure that they are pricing at MAP However not all resellers price at MAP and so manufacturers are under pressure from compliant resellers when other retailers dont comply Thus the competitive price intelligence industry exists in retailNot all resellers price at MAP and so manufacturers are under pressure from compliant resellers when other retailers dont complyA Google search shows many competitive price intelligence solutions  from really large firms to startups many retailers who sell on Amazon use automated repricers However the problem with current repricers is the imprecision of their comparison pricing insights If you are selling a pair of Red Nike Air Jordan Shoe Size 9 you want to compare the price of your SKU with the price set by other retailers for exact same SKU You dont want to get the price of Red Nike Air Jordan Shoe size 10 Many repricers struggle to provide an apples to apples comparison to resellers as they cant match like this at an SKU levelThe consequences are bad for users Pricing Analysts cannot reprice correctly Advertisers cant advertise correctly to increase Return on Ads Buyers cant buy the right quantity And Pricing Scientists are frustrated that their algorithms arent generating good results Sounds familiar? As a product data customer this is not fair to youGrowByDatas competitive price solution solves this very problem We use a combination of image and product text attributes to precisely match and tell our customers how many other retailers are selling the Red Nike Air Jordan Show Size 9 for We provide precise competitive data that retailers manufacturers and service providers seek to gain a competitive advantageOriginally published at https://growbydatacom on January 15 2019
3Z5f9UYPwe6xQ6NUBmu4FP,Cloud Dataflow is an excellent tool for moving data within GCP and several blog post has been dedicated on how to perform ETL using dataflow and apache beam Out of those several articles very less are focused on how to move data from bigquery to cloud storage in a specific formatFor the problem I was working on We had an internal ML framework which only reads data in a parquet format And given the fact that BigQuery still not support exporting data in parquet we came up with a dataflow pipeline which reads data from bigquery and converts it to parquet then writes it to GCS bucketI am expecting readers to be familiar with apache beam and columnar data store parquet Also this guide expects some familiarity with beam programming guide If not then I would highly recommend to follow this link and get an understanding of beam programmingFor the sake of this post we will be using public data provided by Google cloud team in bigquery called baseball dataset and within that we will be using games_wide_post tableApache beam provides several different ways to read data from BQ It can read the whole table or query can feed to beam bigquery IO to read certain data within a table We will be using the second method of reading data as it covers the most general use case of reading data from BQ And this is quite simple See below sample script Now from the above table we will be only selecting the first 4 columnsThis is an interesting part of this pipeline This is part is basically taking a Pcollection and using parquetIO it writes the data to parquet format into GCS bucket Things to be noted here is that while writing PCollections into parquet one needs to provide a schema for the data to write into parquet And the way the schema we provide is in the pyarrow schema So lets create a pyarrow schemaThe 4 columns we are selecting are of string string string int respectively And the way we create pyarrow schema is belowNow lets focus on how to write to parquet format into Google Cloud Storage BucketYou can also provide other option while writing parquet like row_group_buffer_size  record_batch_size etc but I am leaving this for the readers they can use based on the different requirementsThe way we have been writing parquet is valid for one type of query What if we change the query then this will fail as the schema we are passing is hard codedThen answer is query BQ table to get the schema and then dynamically generate the pyarrow schema So lets see how we can do thisWhat it is basically doing is mapping the BQ schema to pyarrow schema So lets put everything togetherall done here Above will create a dataflow after the completion of job one can verify the parquet file by opening it using parquet_viwerHope this will be useful for readers do clap for this article if you like it Any suggestion or positive criticism are welcomed
gGrYyWJQUe2JJjqXZYR5Zs,Many times people tend to confuse between transactional and analytical workloadsThis is a core concept in data management and the underlining data store technology is designed to optimize for the target workloadsIf we do not care to optimize the data workload then most apps can be built on top of file management system Instead of storing data on a database imagine for a second that your application stores customers as a file CUSTOMERcsvThis would work for the most basic app but what if the application needs to create or update customer records? Now you will likely have to extract the full file in-memory scan through the file to find customer 665 and re-write the record in question for the update or append the brand new customer 666This works for simple use-cases but you can imagine that it starts breaking down quickly: This is a simple thought experiment into the challenges faced by early days of computing Its exactly based on these challenges that RDBMS (Relational Database Management Systems) were invented  to handle these transactional workloadsRDBMS engines such as MySQL Postgres Oracle and SQL Server were designed to handle transactional workloads They are highly optimized over the many years to scale to ensure transactional consistency to ensure quick responsive queries and to some degree handle concurrent writes really well  all while enabling a simple language for consuming data via SQL In some engines such as SQL Server and Oracle they have specific features to enable OLAP workloads via columnstore indexes and in-memory column store respectivelyYou are now able to build an app on top of a database that can handle the writes and have good response time from the screen as you paint the application with data and you are storing data over time about your customer their purchases the products the deliveries and even application health metricsImagine what the next logical set of tasks will be  something on the line of: Im sure many of you experienced this and you can envision the type of queries we are now writing The challenge with traditional RDBMS which are optimized for OLTP workloads is that fundamentally data is stored in a file system row-by-rowIn the case of extracting the annual sales you are forcing the DB engine to scan the entire transaction table filtering on the year in question and then computing the aggregate sales amount in a single goDB engines have ways to optimize for these types of scenarios by enabling indexing table partitions and using the cost-based optimizer to minimize the amount of table scan but nevertheless you will have to scan row-by-rowOn top of that most cases we likely querying for a small set of columns in a large table in our analytical queries (eg trade date transaction type amount) where it really doesnt need to read the entire rowThis is where columnar (aka column-oriented or column-store) DB engines such as Teradata Sybase IQ Vertica Redshift BigQuery Synapse and Snowflake come into playRow-oriented databases store data sequentially row-by-rowColumn-oriented databases store data sequentially by columns insteadThe diagram above shows how row store databases store data on disk via file blocks by row so the records are stored sequentiallyThink about how your app is painting a screen of a single customer record at a time Data retrieval is optimized by rowsThe trade-off is that if you need to aggregate the entire customer table you are reading every single blockEgThe diagram above shows how the same table is store in a columnar database  each column is now stored sequentially in diskNow the data retrieval pattern is optimized by columnsImagine now that you are now scanning a single column that is 1/6th of the table but also the entire column is compressed (average 4 6X compression ratios)Eg 100 GB Customer table (above) performing a Count by City = 4Technology is fundamentally a balance of trade-offsThis is because they have to write the entire file of the columns being referenced so the traditional operation of row-by-row processing would be much slowerSome columnar engines minimize these limitations by: That concludes the first part of the series around analytical data storeI hope this brings to light the core concepts of data management and to better understand transactional vs analytical workloads and how to optimize for themThe holy grail in data processing is the ability to handle mixed workloads of both transactional and analytical but the industry has yet to achieve this  there are some very interesting players in this space where we see advanced streaming-based architectures but they are far too early to tellIn short columnar databases are the most optimized way of handling analytical workloads but if you have real-time ingestion and retrieval use-cases rely on a transactional databaseThe next part of the series will explain why we chose Snowflake as we explain more into the evaluations we performed and the vision in which we see for the future
8koriFvQdgVuAGP2dVBd4a,Quite a few data architectures involve both a database and Apache Kafka  usually Kafka is used for event-driven apps streams processing and as a big buffer between systems The database is used… well as a database For analytics OLTP apps legacy apps and what have youAnd there is always the question of the order of integration There are basically 3 options: The third option tends to be terrible because of all the failure scenarios Typically the database and Kafka diverge within days of launch leaving everyone in a very painful positionI usually tell people that if they want to have a successful first Kafka project they need to get some data for their event-driven app or stream processing jobs and since databases already have data CDC is a great place to get startedBut I always said that eventually  when you gain more confidence and experience with Kafka when you have enough credibility to get other people to rewrite some apps… Eventually youll want to write to Kafka first and from Kafka to the DB (if at all)There is a good reason for that recommendation: Writing from Kafka to a DB is much easier than going from DB to Kafka Kafka was written as a data pipe and the DB wasntBut I recently ran into a use-case where the long term recommendation is to write to the database first And I thought it is a good pattern and others may want to know tooHeres the (heavily modified) use-case: You are an online store If you ever used Instacart imagine that Users can create an order and add items to an order They can then schedule the order or cancel the order If the order was canceled you cant add items to it Even if you are logged in on another device that has the order open Users need to get an errorThe traditional implementation involves a database with referential integrity constraints also known as foreign keys In fact orders and items is the classic example for these constraints The order table will be the parent the items table will be the child and if you try to insert into the child table when the parent doesnt exist  youll get an error Show the error to the user and you are doneNote that the DB does quite a lot for you: it enforces serializability locks your logical constraints etc If the DB is distributed (Vitesse Cockroach Spanner Yugabyte) it does even moreIf you were to go Kafka-first… well it isnt impossible But all those responsibilities now belong to you as a developer And if you are thinking there may be multiple webservers handling user requests and passing them to Kafka you have to solve fairly challenging problemsIn the distributed webservers scenario the best you can do is use stream processing or a database downstream to validate the consistency of the data (consistency in the original ACID meaning) and send the actually we cant deliver this item because you cancelled your order message later Not a great user experience If you have a single webserver you can cache the open orders or something and implement your own validationPutting the database first and using CDC to send events to Kafka and using Kafka to notify other applications (shipping loyalty points) about the event simplifies the design considerablySo next time you have logical constraints on your data think whether putting the database first will make things simpler
KkFYd3fadKmZhzNFi64xGv,NYC City Planning(DCP) digital transformation continues! The process for generating the PLUTO database NYCs most comprehensive parcel based land-use ad zoning dataset has been revamped and modernized Bye bye to obscure mainframe data processing and welcome to transparent modern cloud-based relational database processing Boasting of significant time savings the database can now be generated in a day as compared to six weeks Also a significant reduction in manpower needed to compile source data and generate the databaseShout out to the talented DCPs Enterprise Data Management unit Specifically Amanda Doyle (Data Engineering Team Lead) Baiyue Cao (Data Engineer) Adrian Ferrar (GIS Specialist) and Lynn Seirup (Director Enterprise Data Management)Database generation script was developed in the OpenJoin us at the 2019 Open Data Week kickoff event School of Data as we conduct an interactive user design workshop to envision improve transform PLUTO into new forms for a modern tax parcel dataset
ARcovhu22BEvFHwzXdiLKB,Airflow is a workflow management tool built by Airbnb This open source project has been a very important tool in our organisation At first we used that only for our ETL but now weve expanded its usage to almost all scheduled jobs that we haveThe power of this tool are its capability to schedule jobs and make the execution distributable across instances When were talking about schedule sometimes we have to deal with unexpected problems that affecting the jobs schedule Its relatively okay when the job is behind schedule for the unit of minutes but what if its left behind for hours or even days? Its frustrating Were aware that theres no easy way to deal with this situation it still requires effort and time So this time I would like to share about our effort to at least make it less painfulTo give you a little bit of context airflow actually have so many important components but to simplify my explanation I will only talk about DAG Task Instance Scheduler WorkerImagine that you have a group of processes running in the background and scheduled in every 5 minutes However with 1 instance you dont have enough resources to execute all of the processes so you decided to make it scalable by executing certain processes in certain instancesThat actually summarise all about the 4 terminologies The group of processes is known as DAG (Directed Acyclic Graph) and each of process is known as task instance On the other part of the system airflow has a scheduler that actively create a schedule for a DAG to execute all of its task instances which is known as airflow scheduler The part of airflow that allow us to execute programs remotely is known as airflow workerBelow is the example of a DAG contains so many task instances and designed in quite complex mannerIf you want to have a deeper knowledge about airflow you can go to this pageAs I mentioned earlier we mainly use this platform to execute our ETL At first our ETLs were registered as cron jobs However we found it hard to manage and monitor the status of the jobs We had 2 options at the time they were airflow (this was our CTOs suggestion) and luigi Thus we decided to use airflow because it could fulfill what we needed and then we moved all of our ETLs from cron jobs to a few of DAGsFast forward to present days now we have 46 DAGs with ratio 186 task instance per DAG Very low isnt it? Yes we learnt from the past that complex DAG would bring unnecessary problem which I will explain in the other sectionNot only for ETL now we use it for any processes that should run with schedule like data synchronisation data archiving mail reporter etcWe also made it scalable by utilising the airflow worker It relies on celery as a message channel between instances With that we have 4 instances to work on all of our tasks under those 46 DAGsThe main task of airflow scheduler is to create a DAG run DAG run will create a record that allow the task executor to know that it can execute the tasks otherwise the worker will do nothing So what will happen if a DAG run is marked as fail? Well it depends on your DAG configurationWhen you write your DAG you need to provide depends_on_past config that receives boolean object Therefore when you set it with True your next DAG run will always be marked as fail and that we call it as deadlock state But when you set it with False it will ignore the previous DAG run stateAnd then you might ask a question: in which condition should we set it to True or False? Good question and I do have the answer but not this time Ill answer that in the other postLets look at the condition where you set your depends_on_past with True and you get into a deadlock state There are a lot of reasons as to the cause of the failure It could be your code are crappy It could be theres something wrong with your database It could be your data stream system just ingested a very bad form of dataSometimes right after you fix it your scheduler just have made dozens of DAG runs This is the situation where your DAG schedule is left behind the actual schedule So should I just wait for it to catch up with the schedule? You can do that but I dont recommend it if your DAG is far behind the scheduleLets say I have a DAG named users_etl that contains only 1 task The task goal is to aggregate the user information based on time and platform Its scheduled to be executed in every 5 minutes One day my DAG was scheduled to run at 12:00 PM but then marked as fail for some reasons Of course when it failed I received the alert and immediately took an action However by the time I fixed the program the DAG should have executed the schedule for 02:00 PM This means that I need to catch up 24 schedules If I just let it catches the schedule by itself it might need around 72 minutes to get to 02:00 PM because it needs around 3 minutes to finish a DAG run In other words if I do nothing to accelerate the process it will need roughly almost 3 hours to catch up with the scheduleInstead of waiting for a very long time I did this to my DAGI understand that to do those steps also require some amount of time but its still a lot less than waiting for it catches the schedule by itselfAirflow is a great tool with important features to manage scheduled jobs and I found it very useful for us Despite of some challenges that we faced when using airflow I think its one of many ways to master the toolHere are some advices to have a good experience with airflow: Hopefully those are some good advices and valuable for youI should thank Rencana Tarigan and Septian Hari Nugroho for their experiment with the airflow backfill commandOriginally published at hafizbadriecom on December 12 2016
AavvQ2BBNwgbNkqWL3yqSn,People often do not realize how Big Data affects their lives But every person is a source for big data Big Data specialists collect and analyze digital traces: likes comments watching videos on Youtube GPS data from our smartphones financial transactions behavior on websites and much more And the interest is in finding patterns by mining the big data sources Understanding these patterns helps optimize advertising campaigns predict the customers need for a product or service and assess the mood of users According to surveys and studies Big Data is most often implemented in the areas of marketing and IT and only after that research direct sales logistics finance and so on Special tools have been developed for working with big data The most popular today is Apache SparkApache Spark is an open source framework for parallel processing and analysis of semi-structured in-memory data The main advantages of Spark are performance user-friendly software interface with implicit parallelization and fault tolerance Spark supports four languages: Scala Java Python and RThe framework consists of five components: the core and four libraries each of which solves the specific taskSpark Core is the basis of the framework It provides distributed scheduling scheduling and basic I / O functionsSpark SQL is one of the four libraries of the framework for structured data processing It uses a data structure called DataFrames and can act as a distributed SQL query mechanism This allows you to perform Hadoop Hive queries up to 100 times fasterSpark Streaming is an easy-to-use stream processing tool Despite the name Spark Streaming does not process data in real time but does it in micro-batch mode The creators of Spark claim that the performance suffers from this slightly since the minimum processing time for each micro-batch is 05 seconds The library allows you to use batch-analysis application code for stream analytics which facilitates the implementation of the λ-architecture Spark Streaming seamlessly integrates with a wide range of popular data sources: HDFS Flume Kafka ZeroMQ Kinesis and TwitterMLlib is a distributed machine learning system with high speed It is 9 times faster than its competitor  Apache Mahout libraries when testing with benchmarks on an alternating least squares algorithm (ALS)GraphX is a library for scalable graph data processing GraphX is not suitable for graphs that are modified by a transactional method: for example databases Spark works: among the Hadoop clusters on YARN running Mesos in the cloud on AWS or other cloud services completely autonomousIt also supports several distributed storage systems: HDFS OpenStack Swift NoSQL-DBMS Cassandra Amazon S3 Kudu MapR-FSThe first framework for working with Big Data was Apache Hadoop implemented on the basis of MapReduce technology In 2009 a group of graduate students from the University of California at Berkeley developed an open source cluster management system  Mesos In order to show all the capabilities of its product and how easy it is to manage the Mesos-based framework the same group of graduate students began working on Spark As planned by the creators Spark was supposed to become not just an alternative to Hadoop but also surpass it The main difference between the two frameworks is the way data is accessed Hadoop saves data to the hard disk at each step of the MapReduce algorithm and Spark performs all operations in RAM Thanks to this Spark gains performance up to 100 times and allows data to be processed in the stream In 2010 the project was published under the BSD license and in 2013 it was licensed under the Apache Software Foundation which sponsors and develops promising projects Mesos also caught the attention of Apache and came under its license but did not become as popular as SparkAccording to a survey conducted by the Apache fund in 2016 more than 1000 companies use Spark It is used not only in marketing Here are some of the tasks that companies do with Spark Insurance companies are optimizing the process of claim recovery Search engines identify fake social media accounts and improve targeting Banks predict the demand for certain banking services from their customers Taxi services analyze time and geolocation to predict demand and prices Twitter analyzes large volumes of tweets to determine the mood of users and attitudes towards a product or companyAirlines build models to predict flight delays Scientists analyze weather disasters and predict their appearance in the futureWith the development of the need for collecting analyzing and processing big data new frameworks are emerging Some large corporations develop their own product taking into account internal tasks and needs For example this is how Beam from Google and Kinesis from Amazon appeared If we talk about frameworks that are popular among a wide range of users then besides the already mentioned Hadoop you can call Apache Flink Apache Storm and Apache SamzaApache Spark is the most popular and fast-developing framework for working with Big Data Good technical parameters and four additional libraries allow using Spark for solving a wide range of tasksOf the unobvious advantages of the framework  numerous Spark-community and a large amount of information about it in the public domain Of the obvious drawbacks  the delay in data processing is greater than that of frameworks with a streaming model
2pUp6mgJNpz3BEMPh3VpeV,The AWS Certified Cloud Practitioner course is the recommended beginning step to understanding cloud computing on Amazon Web Services (AWS)The AWS Certified Cloud Practitioner examination is intended    to effectively demonstrate an overall understanding of the AWS Cloud   This understanding covers: The AWS Certifications come in sequential steps with added complexity and specialisation at each subsequent levelIf you plan to improve your capabilities in Data Engineering as I do your path will most probably follow the Associate and Professional Architect certifications and finalise in a specialised Data Analytics certificationThe Certified Cloud Practitioner exam itself costs $100 plus a $20 administration charge (~£95 altogether) and can be sat either remotely or on-site at a testing centreThe pass mark is 70% with 65 multiple-choice questions to be answered within 100 minutes of allocated timeAfter having spent a great deal of time watching Best Practice data engineering videos on Youtube the prevalence of Platform as a Service (Paas) cloud infrastructure in production architecture is obviousThese platforms are almost always either: AWS Google Cloud Platform (GCP) or AzureAWS has been the market leader since the beginning of the Cloud as a Service in 2006 and continues to maintain this dominanceWithin the field of Data Engineering there are a limited number of qualifications that have a reputable base and have a definitive outcome with an official proctored examThe AWS Certified Cloud Practitioner exam provides a demonstrable knowledge base from which to conduct Data Engineering projects within the most sought after and effective service providers platformAmazon suggests several official routes for exam preparation These consist of: On top of this there are many external materials such as books and online courses
7moFCNDwMFPuEbFAHE9ieS,There are many times when you want to connect batches of data collection or pre-processing and perform a series of processes at once For example you get an image from S3 resize it and then throw the resized image to the API Like that And in many cases the tasks are dependent on each other and you need to make sure that you have a good execution plan In short you need to create programs and resources to properly express do B when A is doneSometimes the data processing workflow is called data pipeline If youre in a mature team you might use useful tools like Airflow or Luigi to create a highly available and highly efficient systemIn this article Ill introduce a simple configuration that you can use when you want to create a data pipeline for now Id love to hear your comments on how you would use itActually Also the specific code of Terraform will not appear in this article This is because the volume of the article will be too much and I thought it will increase the burden for you to read it If you want to read it you can read it while looking at the linkThe structure is very simple In short there are only three things we are doingIn the remainder of this article we will explain the first two of these main points in more detailWere talking about adding the following resources to AWS BatchOne compute environment and one job queue are sufficient; one job queue is shared by two job definitions reducing the resources to be managed In this case were keeping the small number of vCPUs for computing resources but its easier if we make this larger and select the OPTIMAL instance type so we dont have to be too aware of the instancesYou can also use SPOT instances to optimize costs Im afraid that I didnt use SPOT instances in this codehttps://awsamazonI couldnt find anything about managing batches with Lambda when I looked into itHeres an article that I found helpful in that case It was exactly the right article for this case (I have to admit I mostly mimic this onehttps://engineeringdoor2doorIncidentally they also said the followingSo we expected there was a configurable cron to schedule the jobs But there we couldnt find a way to schedule the Batch jobs The only Information the docs provide is this short paragraph describing job scheduling: I was in the same situation and it was very difficult for meTheyve solved the problem by asking AWS support and I like the fact that youre willing to share their implementation As mentioned in the above article there is a similar service called StepFunctions which is a distributed application coordination service so depending on the use case StepFunctions may be easier to usehttps://awsamazonNow what we are doing is very simple For the sake of simplicity lets say that JobA is the job you want to execute first and JobB is the job you want to execute laterIts easy Since were submitting the job to the job queue it doesnt run immediately after submitting the job If there is a dependency the batch is put into a pending state and execution is put on hold for a whileNow the batch is executed in series CongratulationsIn this article Ive presented a configuration that uses AWS Batch + Lambda + CloudWatch to do serial batch processing on a regular basis The implementation is done with Terraform I used Terraform Cloud and Plan and Apply were done remotely Its quite useful and recommendedBecause I thought the management of Lambda part tends to be careless CI/CD is included in GitHub Actions to the extent of excuse I think its good to keep the code working Ive also made CI for each function to run on the assumption that lambda will be addedThank you for reading this far See you soon
iyzGLAtzwrZW4KADPecZ2o,"So I have been working in Business Intelligence for around a year now mostly data wrangling pipelining and visualizationI have been using a combination of Looker & Google Data Studio for data visualization Hadoop Hive & MySQL as datastores and NodeJS BASH & Oozie for automation The reason I ended up working with these specific platforms is a combination of personal interest and what available for me to use at my companyI mainly started working in a BI function as the task I was actually hired to perform was not very well defined and did not seem to make sense from an organizational perspective Where I felt I could add the value was in ad hoc data requests that sort of came organically with individuals I was working withAt one stage there was the feeling that we could just throw a dashboard on top of an entire area I was working on as there was no way to access this data bar ad hoc queries which ran on our distributed Hadoop clustersIm not sure whether my manager knew when he hired me that he wasnt going to be able to present a coherent vision of the team or my individual function or if its related to factors outside of his control like acquisitions and layoffs but what i do know is that there are more bullshit jobs than there used to be The book Bullshit Jobs by David Graeber has some nice graphs that places this nicely in a historical contextOne thing that kept on tripping me up throughout this entire process was access This included access to: I guess this differs from company to company so not sure how much advice I can give besides factoring these sort of delays into your project planning phase These are very indeterminate factors so if you know the requirements upfront it might be worth starting this task before any othersThis part was not straight forward at all Our company is using Hadoop for data warehousing and Oozie is everywhere in the documentation presented as almost synonyms with data pipelining and automation It turns out that learning a new XML based workflow language might be a mistake when you can just use a scripting language you are used toAt the time we were persuaded into using Oozie because it was presented to us as the official platform for building data pipelines on Hadoop""It may be worth breaking out your requirements explicitly like this and really analyze your options if you don't want to go down a rabbit holeIf possible spend more time identifying the business needs and try to model out your idea in the most basic tools While you are using something like Excel or Google Sheets to illustrate the value of your solution you can also be thinking and estimating around access automation and visualization"
NPQiEwQF2GfG4oEugc7NQT,Hi guys everybody is busy leaning in Apache Spark no doubt it is one of the most hottest tech in todays scenario for solving big data problems very fastBut I was curious for why and where for Apache Spark to get better understanding of this software or platformNow when I research on Internet that why Spark for solving big data problems it always give me same answer that it is doing in memory processing at distributed level okay thats good but its not what I asked internet gave me the reason why spark is most widely and is a alternative/replacement of Hadoop MapReduceAfter digging about it I finally got a satisfactory answer so lets startInitially we had computers with single processor and as the time passes away hardware is being improved with decrease in its cost so hardware guys keep on increasing the potential of single processor so our code were also working fast as new processors were able execute more instructions per seconds to but soon they were not able to deal with heat dissipations so to solve this they come up with increasing cores in a CPU rather than focusing on single processor so this lead to the change in the programming model now to make the code work fast we need to design it to use multiple core means more threadsAs we all know that in todays scenario amount of data is very large and hence single computer which may be of best configurations can not solve those use cases on its own so we need thousand(exaggerating) of machine to work together to win over the problem for this Google MapReduce first came into sight which was solving this problem using distributed programming model but there were 3 problems : 1- It is very slow as it writes intermediated result into disk (this make MapReduce slow) • For solving machine learning problems we need to do 10 to 12 passes over same data and in MapReduce programming approach we need to write 1 MapReduce job for each pass which is very inefficient • Programming MapReduce job were very difficult and if one is able to write them then it is very difficult to debug them in case any thing goes wrongSo for solving these problems some people at UC Berkley started working on project called Spark Initially they write a functional programming API through which they can express multistep application collectively (solving problem number 2) Then they implemented this API on a model which allows in memory sharing of the data (solving problem number 1)Then they saw that for solving Big Data problems there are several tools and technologies available but those were masters in their own specific fields like Apache Storm was good in Streaming so their aim was to develop a unified framework where the user can solve the Big Data problem without worrying about integrating tons of softwaresApache Spark  unified computing platform with set of Big data librariesUnified  spark was built with a philosophy in mind that it can be used to solve a variety of workloads like Batch Processing  Stream Processing Machine Learning Deep LearningComputing Platform  spark is computing platform which means that it focus only on computation of problem and worrying about storage this is what it make it different from Hadoop which have computing model MapReduce and Storage Model HDFSSet of Libraries  It contains various libraries for solving different kind of problems which are Batch processing  Machine Learning etc and also it follows extensibility philosophy which is that 3rd party developers can also create their own libraries for solving various issues
Ti8VePW5dR7eXByV3mszZA,In this post we would like to share why and how HK01 migrated its data warehouse from a single Redshift cluster to a data lake solution by utilizing Redshift Spectrum From our experience this infrastructure is more scalable in terms of storage space and computational power It allows us to process 20x more data with about only one-fourth of the original costTo start with the first generation of the data warehouse of HK01 is described as in the figure below We have web/app data streaming through API Gateway buffering at Firehose and landing into our Redshift cluster as well as having a backup copy at S3Such an infrastructure serves our demands well Applications setup scheduled job to look up data that they need Data analysts and scientists run ad-hoc queries for analysis We also set up data dashboards which present real-time analytics resultHowever as our team size and data size grows Scalability issues haunt us time after timeOccasionally we find that the loading of Redshift cluster spikes to 100% downgrading the performance of all upcoming queriesThe source of loading spikes includes: It is challenging to do load balancing on the Redshift cluster due to the bursty and unpredictable nature of our queriesThe performance downgrade ends up lowering the productivity of everyone Data analysts retract from doing heavy table scans Redshift cluster triggers loading alerts which distract dev-ops Users who rely on real-time data lose patient on our data dashboardAt first we tried to resolve this issue by adding new Redshift clusters to separate production use cases from analytical use cases By isolating analytic and production environment we hope to maintain a better SLA on our data product but it didnt go wellScaling up Redshift clusters is not a feasible solution as it would introduce a high cost with resources being under-utilized for most of the time Our daily data volume has increased by 20 folds since our first launch one year ago To scale horizontally for better query performance we are looking at the scale of 20 times of the current cost Not to mention that the cost is further doubled by maintaining both production and analytical clusters As our data usage grows we are expected to triple or quadruple the number of dedicated clustersHow about fine-tuning our data structure writing better SQL queries to make them more efficient? This is not a trivial task at all Our ad hoc queries vary in every possible way and our projects evolve quickly A finely tuned data structure would quickly become irrelevant On the other hand enforcing best practice and regulating the SQL queries style is doomed to be incomplete For most of the time performance gain by these practices is minimal anywayTo make things worse having multiple clusters introduce a new issue in our data pipeline namely data consistencyConsistency issues arise when we have a lot of data preprocessing and data importing logics running within each Redshift clusters One has no way to be sure whether two tables preferably having the same data are really the same after a few days of upserting records in different clustersIn reality all sort of accidents could happen in production For instance Redshift clusters go down for maintenance the failures of schedulers database connection going down API errors unhandled/uncaught exceptions polluted data sources etc It is a huge challenge to backup monitor and backfill data with the current settingA critical observation is that by offloading data to S3 we free ourselves from the only expensive solution of scaling up Redshift cluster and open up alternative options like AWS Athena Spark Presto / EMRRedshift Spectrum works by letting Redshift clusters offload its query to Amazon Redshift Spectrum layer This layer is managed and scales automatically by Amazon according to the query usage Price is charged based on the amount of data scanned in S3 which is especially suitable for ad hoc queriesWith Redshift Spectrum as a query engine we are able to query both data in S3 and Redshift through our Redshift cluster The more the data in S3 the lesser the loading in RedshiftHaving a single source of truth in S3 also resolved the data consistency issue and make data management easierThe migration of data from Redshift cluster to S3 could be done progressively We dont have to abandon any existing infrastructure for Redshift Spectrum to work Instead we could slowly fade out Redshift at our own pace and scheduleTo make Redshift Spectrum works Redshift Spectrum needs to know the schema of data in S3This is where AWS Glue comes in Glue provides data crawlers to discover data schema Crawlers support most of the common data format like CSV TSV JSON Parquet etc Detected data schema are put into Data Catalog which is accessible by Redshift Spectrum and other servicesFrom our experience most of the time the crawler returns data schema accurately where no extra effort in fixing the data schema is neededIn Redshift execute the following command to grant the access to Data Catalog databaseWe already have the minimal setup ready for Redshift Spectrum To squeeze every bit of performance out of Redshift Spectrum as well as further cutting the cost some preprocessing on raw data could be doneWhile there are many more other data formats we picked parquet with snappy compression due to its popularity and well-tested benchmark resultsParquet is a columnar data format which allows us to scan only the necessary columns Data engineers do not have to make any difficult decision on whether we should keep or drop a column when designing table schema This is because unused data only introduce a slight storage cost in S3We utilized our batch processing data pipeline to convert raw data into parquet format hourly which is regulated by Apache Airflow The usage of Apache Airflow itself deserves a separated post Roughly speaking a well-written Airflow job guarantees the correctness and accuracy of data At the same time it provides a simple mechanism for us to backfill data whenever we wantedThe latency of raw data conversion is unsatisfying Surprisingly AWS Firehose recently supports outputting data in parquet format This means that we could query real-time raw data in parquet format directly with Redshift Spectrum! We are excited to try this outAnother way to improve query performance and reduce data scanning cost is to partition data into smaller piecesData partition in this context is just a fancy way to say that data is organized in different folder of S3 Redshift Spectrum is smart enough to scan only the data within a certain partition required by the queryWe partition our data mostly by date This prevents a full table scan from most of the ad hoc queries Moreover partitioning data by date allows us to write smaller and more flexible batch jobsFor example in a few occasions when we could pin down data errors within a certain date range we are able to simply update the logic of our data processing script and backfill data within that date range with Apache Airflow This makes our whole infrastructure far more durable for errorsDespite we are still on the progress of moving our data from Redshift cluster to S3 the performance gain and cost reduction are already very significant The hurdle of using Redshift Spectrum is very small for users who already have Redshift cluster set upCurrently we still rely on Redshift to serve application who has a real-time requirement for data Everything else could be done with S3 data lake and our batch processing pipeline managed by Apache AirflowWe look forward to sharing with the readers our batch processing pipeline in the future
B5E8EdhqjDqFDvNHDzMPMG,A quick overview of the main changes introduced to the Hortonworks Data Platoform moving from version 26 to version 31More than one year ago two of the most popular Hadoop distributors Hortonworks and Cloudera announced their merging The new Hadoop distribution is known as Cloudera CDH platform and will be shipped with a combination of the best services from the two worlds But chances are that you are going to work on some Hortonworks 3x distributions moving from 2x before getting your hands dirty with the new Cloudera & Hortonworks generation of Hadoop distrosTo help you with this in this article I am going to share with you my personal experience in planning and executing a Hadoop cluster upgrade from HDP 26 to HDP 31 We are going to review new features introduced in Hive and Spark with particular care of the changes that can potentially impact your platform the mostHDP 31 ships with Hive 3x versions Heres a list of some interesting features introduced: Views are sometimes a nice option to keep your access patterns independent from the raw data being stored With the introduction of materialized views you can now do it more efficiently pre-calculating queries results into actual tables More than that the query engine will take advantage of them by rewriting the queries that could potentially benefit from pre-computed data (thats the default behavior but you can disable it per-views or globally)Currently views lifecycle management is up to you Materialized views data is updated when you issue a: You can also create partitioned materialized views to further enhance queries performances if the access patterns suits a particular scheme Rebuilding a partitioned view is done incrementally ie not all your partitions will be computed if their content has not changedThe other important parameter you have to be aware of is the time after which the views content is considered stale and therefore wont be used for query rewriting You can set this with the property for example: Storage options have changed dramatically leaning towards a greater use of ACID tables for which precious features have been introduced in the past and have been refined (updates transactions …) In the following table you have an overview of the available options: CRUD transactional tables are available only for the ORC format You can have automatic compactions of tables if they are of the managed transactional type This is desirable if the write pattern on the table will lead to a very sparse underlying storage situation (the famous small files problem) You let the metastore decide whether to perform compactions on a table that is carried out with MapReduce jobs running on YARNAlso another crucial aspect to consider is that if you decide to go with managed transactional tables you will not be able to read data with the SparkSQL interface In fact at the time of this writing SparkSQL is not able to read these tables You can still process your data with Spark but you will need a special connector the Hive WareHouse Connector (more of this in the following paragraph)I encourage you to take into consideration all the possible aspects and go with the best tradeoff between the manageability of your platform and the required tweaks needed to applyOnce you decided a storage option be sure it fits your access patterns No big changes here if you already familiarized with LLAP (introduced around HDP 26) JDBC and ODBC are still there for you to do JobInstead you have to be careful if ACID table is your preferred storage solution and you would like to access data with the SparkSQL interface This interface is not able to read from ACID tables v2 (the ones introduced in Hive 3x) Moreover Spark will now have a dedicated catalog for table metadata You will find all the details in the following sectionHere you have a table summing up the characteristics of the most popular interfaces to your data: As you probably understood the main changes introduced in Spark influence the way we can access Hive dataFirst Hive and Spark will now have independent catalogs that allow you to access table data or for example register a temporary table Ensure your Spark configuration is pointing to the correct catalog before deploying your applications The setting is controlled by the property metastorecatalogdefault  and can be set to hive or sparkThe other big change mentioned earlier is the fact that the SparkSQL interface is not able to read ACID table v2 For this task you can rely on a new Spark interface to Hive Tables called Hive Warehouse Connector (from now on HWC) that can work seamlessly with SparkSQLThe HWC comes with some limitations: You will need some extra configuration on your applications to let the HWC work First some spark properties you can set per application on your submit script or globally on your configuration: Also locate the hive-warehouse-connector-assembly jar library containing the actual implementation of the HWC and ensure it can be loaded by your applications for example with the jars parameter in the submission script: One thing to keep in mind is that once you commit with the HWC the Spark applications will occupy available slots on LLAP and will affect the maximum number of parallel queries you can run Arrange your YARN queues and LLAP configuration properlyHere you can find an example of HWC usage with some scala code: Upgrading a Hadoop Cluster from HDP 26 to HDP 31 can be a hard task Modifications introduced to two of the most commonly used services of the stack Hive and Spark can have huge repercussions on your platform Focus on the big picture prioritize your user and platform requirements think thoroughly each choice and ensure it fits the goal I hope that this little information would be useful
aUtSWmS4CDnnhuyWSLhMRE,In this blog we will review how easy it is to set up an end-to-end ETL data pipeline that runs on StreamSets Transformer to perform extract transform and load (ETL) operations The pipeline uses Apache Spark for Azure HDInsight cluster to extract raw data and transform it (cleanse and curate) before storing it in multiple destinations for efficient downstream analysis The pipeline also uses technologies like Azure Data Lake Storage Gen2 and Azure SQL database and the curated data is queried and visualized in Power BIStreamSets Transformer is an execution engine that runs on Apache Spark an open-source distributed cluster-computing framework and it enables data engineers to perform transformations that require heavy processing on entire data sets in batch or streaming modeThe pipeline is configured to run on Apache Spark for HDInsight as show above and its configuration details such as Livy Endpoint credentials etc are passed in as pipeline parametersHere are the details of the data set and pipeline components: Here is a brief overview and some patterns of different data lake zonesThis zone stores data in its originating state usually in its original format such as JSON or CSV but there may be use cases where it might make more sense to store it in compressed format such as Avro or Parque As this zone normally stores large amounts of data something to consider is to using lifecycle management to reduce long term storage costs For instances ADLS Gen2 supports moving data to the cool access tier either programmatically or through a lifecycle management policy The policy lets you define a set of rules which can run once a day and can be assigned to the account or filesystem as well as at the folder levelThis can be thought of as a filter zone that improves data quality and may also involve data enrichment Some of the more common transformations include data type definition and conversion removing unnecessary columns and data enrichment by adding new columns and combining data sets to further improve the value of insights The organization of this zone is normally dictated by business needs  for example per region date department etcThis is the consumption zone optimized for analytics and not so much for data processing This zone stores data in denormalized data marts or star schemas and is best suited for analysts or data scientists that want run ad hoc queries analysis or advanced analytics As storage costs are generally lower in the data lake as compared to the data warehouse it might be more cost effective to store granular data in the data lake and store aggregated data in this zoneLets see what the data columns datatypes and the transformations look like in preview modeRemove fields that are not required for analysis: employeeID and ordernumUse Spark SQL expression unitsold * unitprice to calculate revenue and store the result in a new field revenueAggregate total orders count and total revenue by regionA pipeline is run as a Job Jobs enable scaling and running multiple instances of the same pipeline and the job metrics can be viewed in the bottom paneOnce the job runs successfully the cleansed and curated data is available in ADLS Gen2 and Azure SQL database destinations respectively as shown belowHere we see that as per the configuration in ADLS Gen2 destination the data has been partitioned by region (Not shown are the parquet files in each folder) This data can then be analyzed or used for further processing using ADLS Gen2 origin in StreamSets Transformer or accessed directly from Azure Databricks or Jupyter notebook in HDInsightShown below is the database table dbosales that was auto-created from StreamSets Transformer pipeline This table contains the aggregated summary of total orders and revenue by regionOnce the curated data is available in Azure SQL database as shown above it can be queried and visualized in Microsofts Power BITo get started open Power BI and click on Get data on the top navigation and then select More… >> Azure >> Azure SQL database Then clicking on Connect will present a connection dialog to provide credentials to connect to the Azure SQL databaseOnce connected to the database navigate to the table(s) ( dbosales in our case as shown below) and load the dataOnce the data is loaded and a is model created Power BI makes it really easy to visualize and analyze the data As shown below the chart displays total orders count and total revenue by  Note that the sales model was created in DirectQuery mode in Power BI so reports and dashboards like this one can deliver near real-time / latest data without having to refresh the model For details and other modes refer to the documentationWhile there are different ways to dissect and analyze data hopefully this blog gives you ideas on how to use some of these tools you might have at your disposal in order to make better data-driven decisions fasterLearn more about StreamSets For Azure Marketplace and StreamSets TransformerOriginally published at https://streamsetscom on July 21 2020
Mwv6mvDwwiqj9jwVkZz8nr,Data Skeptics Advertising Attribution with Nathan JanosLooks like an advertising is really complex problem Trying to understand on how advertising works requires a lot of assumptions in your model however it works for common use casesBI is DeadIts all about changes over time and how you (Gartner) define a term so in short BI doesnt dead its alive however tools are changing rapidly so Gartner decided to go ahead and redefine termsApache Airflow and the Future of Data Engineering: A Q&ALooks like its a growing open source project not a Airbnb only stuff I think it make sense to keep an eye on it and maybe do some experiments with big data like public open source repos etcKeen IO SQL in beta The Startup Founders Guide to AnalyticsIts a good guide for entrepreneurs who want to evolve their startups relying on metrics and statistic Very hard thing here is to not overengineer because it would drain your money energy and timeSoftware Daily Software Architecture with Simon BrownSoftware architecture is hard especially in agile world where you need to concentrate on delivering sprints features however Simon Brown says that its possible to do architecture in agile manner Its still about requirements documentation and structure of your code about importance of architect role in a team and how to describe and talk about architectureWeb Fundamentals by Google DeveloperCollection of web development best practises Well organized and not boring to readLighthouseLighthouse is a open-source tool to check your front-end app for PWA Performance etcPolaris - Interactive Database VisualizationJust found out that Tableau was build on top Polaris which is project developed by Stanford Computer Graphics LaboratoryFishtown Analytics NewsletterCreate a Data-Driven OrganizationSoftware Architecture for Developers
GAHydvPuuqacUcN5BuDn4B,Unstructured data is omnipresent in a business and really hard to extract value from it GCP has a wide range of ML solutions depending on business resource and needs
UcnDv7v4TAnZfEytYLjvXE,As a data engineering enthusiast you must be aware that Apache NiFi is designed to automate the data flow between multiple software systems NiFi makes it possible to understand quickly the various dataflow operations that would otherwise take a significant amount of timeIn this blog we deal with a specific problem encountered while dealing with NiFi It has a feature to control the number of concurrent threads at an individual processor level But there is no direct approach to control the number of concurrent threads at a process group level We provide you with an approach to help resolve this challengeAs mentioned NiFi provides concurrency at an individual processor level It is available for most processors and this option is available in the Scheduling tab called Concurrent Tasks But at the same time there are also certain types of single-threaded processors that do not allow concurrencyThis option allows the processor to run concurrent threads by using system resources at a higher priority when compared to other processors In addition to this processor level concurrency setting NiFi has global maximum timer and event-driven thread settings Its default values are 10 and 5 respectively It controls the maximum number of threads NiFi can request from the server for fulfilling concurrent task requests from NiFi processor components These global values can be adjusted in controller settings (Located via the hamburger menu in the upper right corner of the NiFi UINiFi sets the Max Timer Thread Counts relatively low to support operating on commodity hardware This default setting can limit performance when there is a very large and high volume data flow that must perform a lot of concurrent processing The general guidance for setting this value is two to four times the number of cores available to the hardware on which the NiFi service is runningNOTE: Thread Count applied within the NiFi UI are applied to every node in a NiFi cluster The cluster UI can be used to see how the total active threads are being used per nodeTo customize and control the number of concurrent threads to be executed within a processor group use the NiFi Wait and Notify processors Using notify signals created by the Notify processor the number of threads to be executed within a processor group can be controlledHere is a sample use case  create a NiFi flow that processes input flow files in batches For example process five inputs at a time within a processor group and always keep those five concurrent threads active until any of them get completed As soon as one or more flow files complete their processing those available slots should be used for the queued input flow files To explain further if there are 40 tables to be processed and only five threads can be in parallel within the actual processor group it would mean initially 5 tables have to run concurrently by taking 5 threads from the system resources and the remaining 35 tables will get a chance only when any of the previously running threads gets completedThread Controlling Processor Group(PG-1): This is the core controller that manages the number of concurrent threads It decides how many concurrent threads can run within the processor group PG-2Actual Processor Group(PG-2): This is the processor group that performs the functionality that we want to parallelize For example it can be a cleansing/transformation operation that runs on all input tablesCode base (XML file) for this NiFi template is available in GitHub  https://githubAs mentioned this functionality is achieved using Wait and Notify NiFi processors PG-1 controls the number of flow files that get into PG-2 using Gates (Wait processor) This is nothing but signals created by Notify In this NiFi template there are three Gates and you will see how they work belowPG-1 triggers 3 flow files via the input port and each of them performs certain actions • t flow file: It triggers Notify processor to open the Gate-1 (Wait processor) and allows 5 input flow files (configurable) and triggers the Notify processor to close the Gate-1 • d flow file: It triggers 5 input flow files to Gate-1 which was opened by the previous step and reaches PG-2 • d flow file: It triggers any remaining input flow files to Gate-2 (Wait processor) and it acts as a queue This Gate-2 will get notifications if any of the previously triggered flow files complete their processing in PG-2 This Gate-2 releases remaining input files one at a time for each Notify signal it receivesInitially 5 flow files will be released from Gate-1 leaving 35 flow files queued in Gate-2 When one or more flow files get completed it will send the notification signal to Gate-2 Each notify signal releases one flow file from Gate-2 This way it ensures only 5 concurrent threads are running for PG-2Once the last few input flow files (36 37 38 39 and 40 in the above example) get processed by PG-2 it triggers signals to Gate-2 (which is a queue) that there are no further input files to be processed These additional signals can lead to additional runs within PG-2 when a new set of inputs arrive This is avoided using a special Neutralization gate that by-passes all these additional flow files from getting into Gate-2The above example was to just for one requirement ie processing various input tables that are received concurrently What if at an enterprise level this cleansing process has started being recognized and all source tables from the various applications are asked to be processed in this processor group PG-2Let us say App 1 is for processing sales tables and App 2 is for processing finance tablesAll one would need is a small modification in the Wait/Notify release signal identifier This would involve making the name of signals as attribute-driven instead of hardcoded signal names It supports NiFi expression language Hence by making the release signal identifier as attribute driven one can control threads within a processor group at an application-level granularityBy default the expiration duration will be 10 minutes which means queued flow files will wait in the queue for the notify signal until 10 mins then these flow files will get expired The expiration duration needs to be configured in order to avoid the flow file expirationDebug vs Actual Execution mode: During development there will be situations to simulate signal files using generate flow file processors and that can lead to orphan signals waiting for processing when there are no input flow files In the subsequent run input flow files will get processed using orphan signals that were already there during the debugging stageIn the above scenario if any of the flow files go into the success queue there is some unused signal from the Notify processor which is mistakenly stored in Wait processor while developing or testing In the above image it is clear that 18 notify signal are triggered by mistake To make sure that Wait processor is working as expected all the simulated flow files should go to the Wait queue not to the success queue If the debugging step is skipped it may lead to run a higher number of parallel threads inside the actual processor group than expectedBy using Wait/Notify processor the number of concurrent threads can be controlled inside the Processor Group in NiFi This will help one to build a complex data flow pipeline with controlled concurrent threads As you will certainly know by now NiFi is a powerful tool for end-to-end data management It can be used easily and quickly to build advanced applications with flexible architectures and advanced featuresYou can read about real-time NiFi Alerts and log processing here
GkB64QS5nWxNaFrLKXBoDg,NiFi is a simple tool but it becomes complex when we fail to capture logs for complex pipelines As you are aware logging plays a vital role in debugging By default NiFi logs are logged in nifi-applog for all the applications which are running in NiFi instances but if you want to track the logs only for your specific application it is not straightforward with the default settingsBy using the NiFi REST API and doing some configuration changes we can build an automated pipeline for log capturing in Spark tables and trigger email alerts for errors and statuses (started stopped and died) of NiFi machine(s)In a nutshell the process involves passing the application root processor group id as an input to the Python script write_my_processorspy and getting all the processor groups inside the root processor group using a REST API call There may be multiple applications running in a NiFi instance but it is enough to just input the processor group id from where one needs to track it Then iterate each processor group inside the root processor group of the specific application to get all the NiFi processor ids specific to it and write into a pickle file which serializes the output list to a disk for later use by another Python application Pickling the file has a distinct advantage when one keeps on overwriting the same data but with mild modifications Then we use tailFile NiFi processor to track the customized log file(my-applog) in real time and use python script read_my_processorspy to filter the logs which are related to the application using the previously written pickle fileHeres the detailed step-by-step approach: Step-1: Its a good practice to not touch the nifi-applog as this is used by many processes in NiFi So create a separate log file for your application by adding the following lines in your logbackxmlStep-2: Get the application root processor group id either from the NiFi web UI or use REST API from the command line interface: NiFi template(xml file) used in this blog is uploaded in the GitHub repository  https://githubcom/karthikeyan9475/Nifi_Automated_AlertsAndLogging Upload this template to your NiFi instanceStep-3: Tail my-applog and capture the logs related to my-app then send email alerts for Warn Error and Fatal Also all log level information gets written into HDFS On top of that Hive table will be created Query this table using Spark for debuggingNote  Schedule the tailFile processor to run every 10 seconds else it will keep on running by launching more number of tasks which may occupy a lot of memory Here is the Python script which needs to be called by ExecuteStreamCommand processor in NiFiStep-4: Create a Hive table on top of the HDFS directory where putHDFSprocessor is writing NiFi parsed logsNote: If the Hive/Spark querying process is slow it is due to a huge number of small files So periodically do compaction operation on Hive tables or use Spark coalesce function to have the file sizes & counts under controlStep-5: Use Spark or Hive terminal to query those log tables for debuggingYou can use the processor_id in the above logs to directly jump into the respective processor NiFi UI page by searching it in the top right corner in the UI shown below By doing this debugging becomes flexible and logs are now under your controlNote: Use LogMessage processor to create customized log messages for all log levels (debug info warn error fatal) This makes debugging better These logs will be recorded in nifi-applog and my-applog so that one can query the log table to track when the particular part of a NiFi complex pipeline is completedStep-1: To set up email notifications for NiFi machine status update only two configuration files bootstrapconf and bootstrap-notification-servicesUncomment the lines below in the bootstrapconf file: Step-2: Edit bootstrap-notification-servicesHere are the emails received when NiFi is started and stoppedSubject : NiFi Started on Host localhost(162234117Apache NiFi has been started on host localhost(162234117244) at 2019/03/04 20:43:55854 by user karthikeyanSubject : NiFi Stopped on Host localhost(162234117Apache NiFi has been told to initiate a shutdown on host localhost (162234117244) at 2019/03/04 14:56:22925 by user karthikeyanThis will help one build a simple automated logging pipeline in NiFi to track complex data flow architectures NiFi is a powerful tool for end-to-end data management It can be used easily and quickly to build advanced applications with flexible architectures and advanced features
W2NHVsNff52hJf3CbRJ7m7,The Snowflake Connector for Spark enables using Snowflake as a Spark data source  similar to other data sources like PostgreSQL HDFS S3 etc Though most data engineers use Snowflake what happens internally is a mystery to many But only if one understands the underlying architecture and its functioning can they figure out how to fine-tune their performance and troubleshoot issues that might arise This blog thus aims at explaining in detail the internal architecture and functioning of the Snowflake ConnectorBefore getting into the details let us understand what happens when one does not use the Spark-Snowflake ConnectorWhen we use the Spark Snowflake connector to load the data into Snowflake it does a lot of things that are abstract to us The connector takes care of all the heavy lifting tasksThis blog illustrates one such example where the Spark-Snowflake Connector is used to read and write data in databricks Databricks has integrated the Snowflake Connector for Spark into the Databricks Unified Analytics Platform to provide native connectivity between Spark and Snowflake The Snowflake Spark Connector supports Internal (temp location managed by Snowflake automatically) and External (temp location for data transfer managed by user) transfer modesThis type of data transfer is facilitated through a Snowflake internal stage that is automatically created and managed by the Snowflake ConnectorExternal data transfer on the other hand is facilitated through a storage location that the user specifies The storage location must be created and configured as part of the Spark connector installation/configurationFurther the files created by the connector during external transfer are intended to be temporary but the connector does not automatically delete these files from the storage location This type of data transfer is facilitated through a Snowflake internal stage that is automatically created and managed by the Snowflake ConnectorUse CasesBelow are the use cases we are going to run on Spark and see how the Spark Snowflake connector works internally-  1 Initial Loading from Spark to Snowflake 2 Loading the same Snowflake table in Overwrite mode 3 Loading the same Snowflake table in Append mode 4When a new table is loaded for the very first time from Spark to Snowflake the following command will be running on Spark This command in turn starts to execute a set of SQL queries in Snowflake using the connectori) Spark by default uses the local time zoneii) The below query creates a temporary internal stage in Snowflake We can use other cloud providers which we can configure in Sparkiii) The GET command downloads data files from one of the following Snowflake stages to a local directory/folder on a client machine We have metadata checks at this stageiv) The PUT command uploads (iev) The DESC command failed as the table did not exist previously but this is now taken care of by the Snowflake connector internallyvi) The IDENTIFIER keyword is used to identify objects by name using string literals session variables or bind variablesvii) The command below loads data into Snowflakes temporary table to maintain consistency By doing so Spark follows Write All or Write Nothing architectureviii) The DESC command below failed as the table did not exist previously but this is now taken care of by the Snowflake connector internally It didnt stop the processix) The RENAME TO command renames the specified table with a new identifier that is not currently used by any other table in the schemaThe above Spark command triggers the following 10 SQL queries in Snowflake This time there is no failure when we ran the overwrite command a second time because this time the table already existsii) The SWAP WITH command swaps all content and metadata between two specified tables including any integrity constraints defined for the tables It also swaps all access control privilege grants The two tables are essentially renamed in a single transaction  The RENAME TO command is used when the table is not present because it is faster than renaming and dropping the intermediate table But this can only be used when the table does not exist in Snowflake This means that RENAME TO is only performed during the Initial LoadThe above Spark command triggers the following 7 SQL queries in SnowflakeNote: When we use OVERWRITE mode the data is copied into the intermediate staged table but during APPEND the data is loaded into the actual table in Snowflakei) In order to maintain the ACID compliance this mode uses all the transactions inside the BEGIN and COMMIT If anything goes wrong it uses ROLLBACK so that the previous state of the table is untouchedThe above Spark command triggers the following SQL query in Snowflake The reason for this is that Spark follows the Lazy Execution pattern So until an action is performed it will not read the actual data Spark internally maintains lineage to process it effectively The following query is to check whether the table is present or not and to retrieve only the schema of the tablei) First it creates a temporary internal stage to load the read data from Snowflakeii) Next it downloads data files from the Snowflake internal stage to a local directory/folder on a client machineiii) The default timestamp data type mapping is TIMESTAMP_NTZ (no time zone) so you must explicitly set the TIMESTAMP_TYPE_MAPPING parameter to use TIMESTAMP_LTZiv) The data is then copied from Snowflake to the internal stagev) Finally it downloads data files from the Snowflake internal stage to a local directory/folder on a client machineSpark Snowflake connector comes with lots of benefits like Query push down column mapping etc This acts as an abstract layer and does a lot of groundwork in the back endOriginally published at https://wwwtigeranalyticscom on June 25 2020
f5szU3koT9hmvvSWZKVqSV,There are many discussions and speculations around idea of a perfect future travel startup One commonality stands out  there was and there is a cricial mass of countries and destinations Below the critical mass you are simply not in the leagueIf you go with a solution that has content for only 150 cities or even 1000 cities you will never be able to gain sufficient traction The critical mass number as per our calculation believe it or not is around 20000 cities across the world If you start with 10 20 cities like Plnnr did you may be liked by a few but not used by many The startups that have managed to get traction have figured out a way to scale contentWe are pretty sure (and this is also confirmed by numerous talks with users) that critical mass of countries is important To prepare for organic traction we need to cover ~50 countries and ~100 clusters and regions of them It corresponds to ~100000 cities towns cool villages and destinations which are not populated places Examples of clusters are Nordics DACH Benelux New England Southwest US Examples of regions are Catalonia Provence Northern CaliforniaUnknowns still present like which axis to select for scaling: countries or cities? It is top down vs bottom up data-wise and people-wise Airbnb scaled by citites Uber scaled by cities Fever is scaling by cities But maybe tomorrow its better to scale by countries? Bleisure trips are city-bound while adventure trips are country-bound (region/cluster-bound) Big cities are the beaten path while unknown towns are pearls off the beaten path Then big city with new experiences could become not the beaten path againKnown is the need for atoms  to give tourists a better search for trips than Google Travel Guides can do  we must build the smarter thing from the raw data atomsWithin cities there are places POIs landmarks name it They are part of trip plan The most Google searches by travelers besides lodging and transportation go into things to do category POIs together with experiences fulfill travel itinerary They could be located inside or outside cities (out of the city places are often called destinations) Cities (and also towns villages) could be small and big (Sydney is up to 100km wide) they could have complex hierarchy (NYC consists of 5 boroughs Tokyo consists of 23 wards)We simplify the data model to make it work on any country to be able to scale In our model there are two levels of hierarchy  container (usually the city) and place assigned to single container The first level of routing is between containers/cities We plan to add next level  intra-city routing between POIs as soon as we implement hotel bookings so we know where traveler stays so we can plan his/her perfect days from/to the roomTo match POI with City there are lots of non-trivial edge cases Small towns that do not have major attractions are the POI by themselves National parks standalone beaches places like Hoover Dam do not belong to specific city around them so we create synthetic container around themWe have not plugged any experiences in but we have analyzed multiple inventories of them: GetYourGuide Fever Viator Klook Peek Musement Airbnb have great experiences but they dont provide them via API Experiences have locations prices durations themes safety availability seasonality and other attributes It seems that linking experiences is orders of magnitude easier than integrating POIs because experiences are already structuredAssignment of places to containers would be straightforward if there is single version of truth ie data source that provides such relations It appears that there is no such data source Worse than that there are many data sources which provide conflicting information For example Twelve Apostles are in Port Campbell National Park in first data source in Port Campbell in second and standalone in third Our algorithm tries to resolve such inconsistencies taking into account many factors such as: We use Wikipedia Wikivoyage DBpedia MaxMind GeoNames US Government Datasets and dozen of smaller data sources A note on commercial data sources must be put When hunting for applicable data sources we are looking for world coverage travel focus open formats (integrability) and permissive license We are not using Pitney Bowes as their data is mostly limited to US & Canada Facebook Places seem to not provide any export options and Foursquare Places data is very noisy (it reflects people checkis vs places worth experiencing once in a lifetime)Some readers may remember Triposo machine-made travel guides for smartphones made by ex-Googlers and acquired by Musement in 2017 The dependency graph (slide 28) of data sources looks impressive Some of those data sources are used by us however we try to keep our graph simplerWe did few recent updates to our algorithms by empowering it with additional data We use WIWOSM data and point in shape algorithm to determine possible errors in Wikipedia geo data If the location of entity is outside of defined area there is potential error In most cases we just go to wikipediaorg and edit the article so we bring value back to other peopleWe use our custom geo validation tool to show the containers from the knowledge graph on the map Far away places usually are signs of wrong coordinatesAnother improvement is in matching of Wikipedia data with OSM The OSM entities have wikipedia tag but its coverage is rather low especially for non-English speaking countries This is the basis for OSM-WP mapping We use OSMNames dataset with over 21 million place names for the whole planet mapped to OSM to enrich the mapping After containers are mapped to OSM we extract city boundaries from OSM It is our master data for cities During pipelining of knowledge graph each entity (POI city or experience) which has lat/lon location matches against list of cities The point in shape match is considered as high level of confidenceIf there is no boundary around given POI we look for clothest distance cities The following information is fuzzied to derive the decision: Most of data pipeline is done by the algorithm In exceptional cases we do human curation For some countries especially those with poor OSM coverage we recently introduced custom geo sources For example for Australia we explored large list of data sources and end up with ABS Statistics as most reliable data for city boundaries Watch carefully its Australian continent below with cities There are no populated places or destinations in the huge desertsExamples of other projects that we explored include Global Rural-Urban Mapping Project (GRUMP) v1 It is research work resulting city boundaries reflect where people live vs formal admin boundaries In reality this data appears to be very noisy low-res and conflicting with admin areas a lot Example below shows Newcastle Australia from both sourcesAmong others Natural Earth appeared to be useless for this particular task as there are no city boundaries only city coordinates and country boundaries We are continuing to explore additional more reliable and global data sources to continuously improve the data qualityWe have list of ideas in the pocketThe optimization problem that we are solving could be cracked by AI Maybe quantum computing partially help in the future For reaching good levels of smartness AI must be verticalized for some industry or even a niche Full stack is needed for vertical AI In this post we decribed low levels of raw data By letting us know what rocks and what sucks you help us to build the best travel database Some day we could open source it back to all of youMeanwhile we and machines are working on the big scale We work on covering the critical diversity of the travel searches Our mission is to give you better multi-day trip plans than those based on actual visits by Google Travel Guides They show how the world worked ie how people traveled in the past We will show you how the world could work how you could travel better in the future More curiosity in any geo any time any money
75vDxB5NvL9rF25a7CeN3X,Data Engineering by definition is the practice of processing data for an enterprise Through the course of this bootcamp a user will learn this essential skill and will be equipped to process both streaming data and data in offline batches
4eFP4xMgYD5X3McebxXue4,If youre using Amazon Redshift youre likely loading in high volumes of data on a regular basis The most efficient and common way to get data into Redshift is by putting it into an S3 bucket and using the COPY command to load it into a Redshift tableHeres an example COPY statement to load a CSV file named filecsv from the bucket-name S3 bucket into a table named my_tableSimple enough right? What about that pesky <authorization> line? We need to provide Redshift with the necessary credentials to access the S3 bucket assuming the bucket isnt publicAs an aside PLEASE ensure that your S3 bucket isnt public It happens but Amazon has added features to make it less likelyThere are a few ways to mange such an operation securely in AWS but for the case of using using COPY to ingest data from S3 into Redshift I suggest using an IAM RoleIAM (or Identity and Access Management) is an AWS service you can use to securely control access to AWS resources including S3 and Redshift You might already be using an IAM user to log into the AWS console or use the command line tools Most organizations issue IAM user credentials to individual employees and applications Each IAM user only has the permissions it needs and can be disabled or deleted without impacting the root accountIAM roles are a bit different than IAM users but are used for similar reasons Heres how Amazon describes IAM roles: An IAM role is an IAM identity that you can create in your account that has specific permissions An IAM role is similar to an IAM user in that it is an AWS identity with permission policies that determine what the identity can and cannot do in AWS However instead of being uniquely associated with one person a role is intended to be assumable by anyone who needs it Also a role does not have standard long-term credentials such as a password or access keys associated with it Instead when you assume a role it provides you with temporary security credentials for your role sessionhttps://docsawsamazoncom/IAM/latest/UserGuide/id_rolesThe first step is to create an IAM role and give it the permissions it needs to copy data from your S3 bucket and load it into a table in your Redshift clusterNow that you have a role with read-only access to your S3 bucket its time to associate it with your cluster Doing so gives it access to load that data into a tableNote that your cluster will take a minute or two to apply the changes but it will still be accessible during this timeNow that you have an IAM role with access to read from S3 and write to tables in our Redshift cluster you can fill in that <authorization> line in the COPY Statement with your ARN info (step 7 in Creating an IAM Role above)In the end it will look something like this assuming you named your role RedshiftLoaderRoleManaging security in your data warehouse can be a challenge so setting a good foundation is criticalDont forget to sign up for the Data Liftoff mailing list to stay up to date on the latest in data science and data engineeringOriginally published at http://wwwdataliftoffcom on May 10 2019
6i5bjUuRAJGvC6nJBNszFB,In reality data integration is just another word for integrating systemsThe main task of data integration is to secure a flow of data between different systems (for example an ERP system and a CRM system) each system dealing with the data with whatever business logic that is built into themVery often data integration transfers operational data from one system to the other for example: Customers Contacts Items Quotes Orders Invoices or Sales historyBut the scope of data integration can also include other types of data for example either traditional master data (such as posting groups and other metadata that secure a consistent system setup and reporting) or it can basically be any table or field that you choose to map between the different systemsIn reality there are no limits to which data that can be relevant for data integration it only depends on your system landscape and the business processes you are followingTo integrate you need to understand what your processes actually are and what kind of integrations youd benefit fromFor example usually a customer in your ERP system will have come from your CRM system They got into the CRM system by being converted by the CRM from a leadThis would result in this data integration flow: The two systems being integrated need to be connected System designs (that is tables fields and other relevant information) are read and stored to allow data to be mappedOnce testing and transfer are done its time to sync your systems This is usually best done in stages making each area live as you go This means youll start benefiting from integration more quicklyWhen youve finished syncing and everything is working as it should its time to switch to support mode This is generally easy to manage with a data integration platform which should send an email notification when something goes wrong along with all the information they need to start fixing itLets begin with a definition: Business Intelligence or BI is a set of practices of collecting structuring analyzing and turning raw data into actionable business insights BI considers methods and tools that transform unstructured data sets compiling them into easy-to-grasp reports or information dashboards The main purpose of BI is to provide actionable business insights and support data-driven decision makingThe initial step is to check the consistency of a dataset by wrangling and munging the dataset to check for inconsistency inaccuracy and performing actions like actions as extractions parsing joining standardizing augmenting cleansing consolidating and filtering to create desired wrangling outputsThe three main steps involved in Data Wrangling is: Once youve configured data transmission from the chosen sources now you have to set up a warehouse In business intelligence data warehouses are specific types of databases that usually store historical information in SQL formats Warehouses are connected with data sources and ETL systems on one end and reporting tools or dashboard interfaces on the other This allows for presenting data from various systems via a single interfaceThe process may include converting OLTP (Online Transactional Processing) to OLAP (Online Analytical Processing) to generate meaningful insights by doing analysisThe integration phase of the actual tools will require a lot of time and work by your IT department There are various structural elements of a BI architecture you will have to develop in case you want to create a custom solution for your business In other cases you are always free to choose a vendor from the market that would carry implementation and data structuring for youFormed into digestible thematically related chunks of information in Online Analytical Processing cubes or data marts the data is finally presented via a user interface of BI tools That is where descriptive analysis brings its value to the end-userSet the objectives KPIs and requirements: The first big step after aligning the vision would be to define what problem or group of problems you are going to solve with the help of business intelligence Setting the objectives will help you determine further high-level parameters for BI such as: Along with the objectives at that stage you will have to think of possible KPIs and evaluation metrics to see how the task is accomplished Those can be financial restraints (budget applied to development) or performance indicators like querying speed or report error rateLinkedIn: https://wwwlinkedinhttps://githubFind more about me on :https://jayshiljainnetlify
GYFPmNEa8PKrg9iwCod6EW,This will be my first post as I journey through figuring out Apache Flink As there still isnt a lot out there with regards to using Flink I figured that writing this will both help me get a start on writing (which Ive wanted to do for a while now) while also providing me with something I can refer back to in the futureWhile not my primary programming language our company uses Scala so all code snippets will be in Scala (unless otherwise specified)Getting right into things  one of the useful features that Flink provides is the Table API It allows the ability to perform SQL-like actions on different Flink objects using SQL-like language  selects joins filters etc This post will go through a simple example of joining two Flink DataStreams using the Table API/SQL
M7k2sBLHHLeRJDVLeWPZtU,https://mediumThe number one most annoying thing about Glue is that the startup times are extremely slow It can take up to 20 minutes to start up a Glue job (but can take a little less time if you had run it recently) and that is not counting the time it takes to actually run the job Compare that to the startup time of GCPs Dataproc which typically takes around 60 90 seconds This means that debugging a Glue job can often be a long arduous process in which half of your time is just spent waiting for the job to start running This also means that if you were proud getting your spark job down to a speedy 5 minutes it could actually end up taking a total of 25 minutes to runAnother thing I found annoying was that if you wanted to launch your Glue job in a VPC to talk to an EC2 instance that you also had launched in your VPC there is no obvious way to do it You cant simply provide a subnet as a Glue job argument when you launch the job Instead you have to create a fake JDBC AWS Glue Connection (by fake I mean you can specify anything for the JDBC URL because you will not actually be using the connection in your Glue job) and then launch your Glue job with the Glue Connection attached to it I went over how to do this in my article hereMy issue with number two was also a result of this next annoyance  sparse documentation The VPC issue would have not been so bad had it been documented but the only way I was able to figure it out was to contact AWS Support It seems like due to the lack of Glue popularity usually Stack Overflow did not even have the solution to most problems However even simple things like the CLI documentation for Glue seemed to have less TLC than the CLI documentation for other AWS managed services (like AWS Elasticsearch) which often have examples of CLI usage and output something that the Glue documentation does not haveThe inability to name jobs was also a large annoyance since it made it difficult to distinguish between two Glue jobs Glue only distinguishes jobs by Run ID which looks like this in the GUI: But imagine you have a Glue job to load different s3 datasets into Redshift and you need to launch a Glue job for each s3 dataset to load By just looking at the GUI it is hard differentiating which Glue run belongs to which dataset If a Glue job failed the only way to figure out which dataset it belonged to would be to click the Logs or Error Logs and ctrl+F for your datasetSmall things like this dont seem to be a big deal initially but in the long run they slow down development time due to the extra clicking and also create friction in debugging and analyzing job runsHeres a screenshot of the bad UI: Here are reasons why the above screenshot is annoying: Sometimes the jobs will take incredibly long to start up only to fail with Resource Unavailable This error is an internal AWS error and occurs when AWS does not have enough resources on their end to run your Glue job in the region The only way to solve this is to wait and try again in a couple of hours or maybe the next day in hopes that it will work There are also some other obscure and non obvious ways to get by this issue detailed in this Stack Overflow link But this seems to be an ongoing AWS issue that they have yet to fixYour Glue job might have failed with this error before: Container killed by YARN for exceeding memory limits 57 GB of 55 GB physical memory usedI had this issue several times and the way I was able to fix it was to increase the memory detailed here This fix involves setting the --conf flag which they say in the official Glue documentation not to setHowever AWS support will tell you to correct the issue by increasing the memory via the --conf flagWhile I was developing with Glue I ran into many small problems that all contributed to a painful development process and friction in debugging and usage These ranged anything from small user experience annoyances to contradictory documentation and advice to large holes in documentation on how to do basic things The service would be fine as a part of non-critical workflows or a solution to small problems but I would say proceed with caution if you want to use it in critical production services
EQXm5kEPryz7MVVdWjMoVS,Data Engineers assume a basic job in mining helpful business bits of knowledgeA Data Engineer is answerable for ingesting information from various information sources into a focal storehouse for example an information lake/distribution center They are likewise liable for setting up the data pipeline so information can be customarily brought into the information lake with the least hindrances issues and information misfortune as could be expected under the circumstancesData engineers are likewise liable for data cleaning and data sorting out the (information quality and information changes) to guarantee that it turns into the single wellspring of truth Different data engineer jobs incorporate including a layer of quickening agents head of the information  particularly if it is enormous information  so it can all the more effectively be utilized by downstream buyers and in specific cases recording the information$65000  $132000 every year Source: PayScaleData Engineer Skills in Python SQL Apache Spark ETL (Extract transform load) and Apache Hadoop are associated to pay that is beyond averageThis job is getting progressively basic not just in light of the exponential increment in information (in addition to related information outside of an organization) yet besides on account of an exponential increment in the comprehension at the official level that noteworthy basic business bits of knowledge can be mined from this informationThis job isnt equivalent to an information researcher While an information engineer ingests information from different sources and guarantees that it is spotless and secure an information researcher is a buyer of this information The principal obligation of an information researcher is to uncover significant pieces of data from this collected information or perform further developed prescient examinationMost junior-level professionals involved in getting into a data-related job and career start off as Data analysts Falling for this role is as easy as it gets All you require is a bachelors diploma and good analytical consciousness Substantial technical abilities would be a plus point and can give you an advantage over most other candidates Other than this companies expect you to understand data handling data modeling and data reporting methods along with a solid comprehension of the businessData Engineer either procures a masters diploma in a data-related domain or collect a good volume of experience as a Data Analyst A Data Engineer requires to have strong technological experience with the capability to design and combine APIs They also need to know data pipelining and performance maximizationThe abilities expected of a data engineer have advanced after some time A couple of years prior the primary aptitudes were SQL OLTP OLAP Data warehousing and so forth At that point came the time of huge data and Big data Hadoop and the abilities expected became HDFS Hive Pig and different individuals from the Apache stack Presently that cloud suppliers are starting to get a decent grasp available information on the oversaw information administrations from cloud suppliers are turning out to be the focal point of the audience These aptitudes are alluring because they help in the information specialists capacity to rapidly gather important informationEverybody can discuss their previous professional training To stick out you could consider having a recognized arrangement of arrangements/code outside of your work as in GitHub for instanceIn a meeting data engineer work searchers ought to expect inquiries around SQL (halfway and progressed) Aside from that they will probably be posted inquiries on the entanglements to remember while planning a cutting edge information lake how to guarantee great information quality in the lake AWS/Purplish blue/GCP oversaw administrations for information and examination information administration security and building semantic information shops from the lakeRead more at https://jobsriser
8FJwaqxSP7byiyPBXeJTuc,I dont know you but I do use pandas a lot in my work But since you are here I am pretty confident that you met some problems in using pandas in lambda function Its okay I am here to helpLets start from scratchGo to your AWS lambda console please click the orangecreate function on the right cornerNow you are directed to below page Please type whatever name you like in the function name box and select Python38 in runtime box Feel free to do the same as me :) Click the orange Create function in the right cornerNow you are all set with your hellopandas lambda functionAlright lets find pandas Since AWS Lambda is using Linux system we cant use normal pip install stuff to prepare the package if you are using MacGo to this page https://pypiorg/project/pandas/#files find these pandas numpy pytz xlrd(if you want to read excel file in the lambda function)Create a folder structure like this and unzip above packages in folder site-packages: zip the python folderNow you have a pandas package preparedNow open your AWS lambda console on the left bar click the Layer and create a layerFeel free to name the layer in your own way Ill take pandas here remember to select Python38 in Runtime Now upload your pandaszip hereNext lets go back to function click Layers in the middle and add pandas as your layerSelect the layer you created aboveTry import pandas and pandas function in function codeDo you find we need to do click a lot in this whole process in the next tutorial Ill show you how to use a python script and command line to replace the above effort
azZ9D4yF7RLCTfLCrEeJC4,In one of his latest videos Why coding isnt enough the TechLead used the expression Data Gatherers to refer to modern software engineers somehow underlining that the times of the arts and crafts of coding are behind us leaving software engineers with the task of gluing pieces togetherMost algorithms these days theyre not even handwritten anymore theyre powered by machine learning trained on millions of data points so software engineers are not even writing their own algorithms anymore theyre just tuning models and gathering the data Weve become data gatherersAnswering this question required a visit to Wikipedia: A hunter-gatherer is a human living in a society in which most or all food is obtained by foraging (collecting wild plants and pursuing wild animals) Hunter-gatherer societies stand in contrast to agricultural societies which rely mainly on domesticated speciesBummer it was fast we dont need to go that further away to see that describing software engineers as gatherers does not holdFirst the foraging aspect is problematic: if most data can be considered wild ie unstructured (acquired from text corpus or audio/video recordings) the acquisition processes cannot be considered as foraging because of the massive scale of the collect (think about the flood of IoT data streams) and their automation Not foraging but industrial data harvestingSecond the mere existence of data farms built by Google Facebook and other giant companies is a clear sign that data have been domesticated and that we entered years ago the era of the industrial agriculture of data when more and more intelligence is put behind the creation of data silosThere is a very interesting field of cognitive linguistics investigating conceptual metaphors In their seminal book Metaphors We Live By (1980) George Lakoff and Mark Johnson stated that the working mechanism of conceptual metaphors lies in the fact that they are mapping across different conceptual domains An implication is that if there is a strong cohesion of the concepts in a source domain the target concepts mapped through a conceptual metaphor should also expose a strong cohesion This is what makes a metaphor holdsWe havent dived deep into the concepts of subsistence strategies used by humans but it seems clear that the metaphor holding the strongest for software engineers is data farmers and not data gatherersYes we need more than pure code for a product Yet its not by assembling cutting-edge technologies that a software engineer is going to bring value to a company What reading IndieHackers can tell us is that minimalism and old boring techs like PHP can be very profitablePassionate ownership happens when engineers grow whole products like data gardeners
aNufG2xwwrWWFjuCqMbQdr,When I said brand new I actually mean total noob I almost 180 my career I studied international relations in my undergrad worked as a sales coordinator as my first job got my master degree in business analytics and then landed this data engineering job I loved everything along the way and happily engineer my data now but I also know I have so little fundamentals about engineering as of now My path started from the very beginning of everything and I want to write down everything
NVstntXmH8TZ9VxKTDMtNA,Data Engineering is definitely one of the most demanded jobs in todays world As the data grows the need of Data Engineer grows and with the new technologies becoming common like Spark and Hadoop companies are looking to hire people who can do the data handling job efficiently I personally believe there is still short of Data Engineers in the market and its still a good time to think about it if you are interested in pursuing this fieldData Engineering is actually not a new name it has been in the market for decades but was never used the way we see nowadays Back then Data Engineers job was also to handle manage transfer data but the difference was in the technologies and the size of data Due to the small size of data companies used old-school SQL systems which was enough at that time to process the data but with the rise of data in the recent years there was needed a better solution Few of the most common technologies you will hear are Spark Hive and Presto they are cheaper and fasterThis article is going to answer the most common questions I see every other day on QuoraTypical questions are: A typical path to be a Data Engineer includes few important things: It shows how passionate you are about the data one must be working on the type of data he/she loves for example looking into health care data might not be interesting to you so you better look what you like There is almost every type of data out there in the market Top examples include; healthcare financial real estate ad tech social media etcIn this space technologies are still not that matured companies are still adapting improvements are coming every year so if you want to be in this field you will need to be pro active in updating yourself with new tools and featuresAWS and Google Cloud these comprised of various technologies that help to build a fully scalable and reliable system Compute Instances like EMR (EC2) databases like Redshift and query services like Athena are some common applications used by Data Engineers Also everyday improvements and new tools are coming out by these platforms so you have to keep an eye out thereHadoop ecosystem (open source) that includes top tools like Spark Hive HDFS are very common in the market Commonly Spark is used to process large amount of data but SQL still holds its worthy place tools like Hive and Presto (Athena in AWS) use the same SQL to query from a filesystem even Spark supports SQL as well known as Spark SQL concludes that SQL is still worth learning There is a new Spark release coming up known as Spark 30Airflow and Luigi are the most common schedulers in the market at the moment Both Airflow and Luigi are open sourced projects by Airbnb and Spotify respectively they do the same workflow management but have a different approachOld School database systems are likely to go away pretty soon as the new data warehousing technologies like SnowFlake are emerging very quickly AWS Redshift is still popular among companies though its costly and lacks scalabilityYes! Data Engineers are programmers as well they write code to support data Write pre-processing data logics ETL schedulers and much more Thats why most of Data Engineers were used to be Software Engineers and companies might prefer a Software Engineer as a Data Engineer instead of having a fresh oneTop programming languages used in this field are Scala Python Java Spark is written in Scala and is supported by all three languages mentioned while schedulers are usually written in Python like Luigi and AirflowIf you are looking for a job then typically these are the minimum set of skills a Data Engineer must have\xa0based\xa0on\xa0my\xa0experience\xa0while\xa0looking\xa0at\xa0hundreds\xa0of\xa0job\xa0rolesThis was first published on my blog few months ago if interested dont hesitate to have a quick glance at my blog
jxf9hSwswPGUN3h8USE88k,Data engineering is the foundational base of every data scientists toolbox After all before we could produce any meaningful analysis that adds business value data must be obtained cleaned and shaped Thus a good data scientist should know enough about data engineering to understand his role and evaluate his contribution to the need of the companyDespite the critical nature of data engineering only a fraction of the educational programs appropriately emphasize this topic on an enterprise level The lack of emphasis on data engineering in online education leaves students at a disadvantage when they begin their quest for a data science role More often than not the skills that future data scientists require are acquired while on the job Yet how could these skills be earned when most of these burgeoning data scientists didnt have the opportunity to learn on the job This conundrum leads to longer job searches and wasted resources for the candidates and the hiring companiesI am writing this guide to help close this gap and help our fellow data scientists improve their foundational skillset by providing a real-world example using enterprise toolsThis post is the first of four illustrating a real-world example of enterprise-level data flow At the beginning of each post there is a brief high-level introduction to the aspects of the data flows we are developing and the different technologies being usedBefore I begin the example I would like to share the Data Science Hierarchy of Needs Similar to the regular human of the Hierarchy of Needs a company should focus on the lowest level of needs before advancing to the next level Not saying that every data scientist needs to be an expert in data engineering but one should know the basics to reduce the most friction within the unitAirflows workflow is represented using the concept known as DAGs or directed acyclic graphs which is just a collection of directional dependent tasks Each node in the graph is just a task and the edges define the dependencies within the tasksThere are different type of operators task: This guide assumes one already have Docker installed If you did not install Docker and Docker-Compose for your operating system please do so I first create the local development version of the airflow dockerBefore we start please take a look at the GitHub documentation of docker-airflowLets create a new folder and clone the above GitHub RepositoryUse the command below to create the docker image with GCP dependenciesOnce the image is built we can create the docker-compose fileYou may need to change the ports depending on which port is allocatedWe need to set up the google credential and register a connection We can add the GCP connection via the airflow WebUI like this: However we can do better by adding the connection using the internal PythonOperator We can build our first python operator to add the GCP connection The code is self-explanatoryOnce you activate this dag check under Admin -> Connection to see if the connection has been changedOnce we see change we can continue this guide The next thing I will be doing is uploading NumPy generated Dataframe into Google Cloud StorageI am not covering how to create a Google Cloud bucket in this post but there is some help documentation available on the Google Cloud Platform official pageOnce you have the bucket created lets start codingFirst we need to install the correct library First create the requirementstxt file and input the required libraries Then add the following line to the docker-compose fileOnce you complete this we can start writing the dag One needs to first create the hook from the GoogleCloudStorageHook() function For more information you can refer to thisQuick Note: Google Cloud Storage does not allow modification of a file if you want to modify any file you need first to open the content then upload the current to replace itIf your confirmation is correct you should find a folder name in Google Cloud StorageNow lets read in CSV file in another dag and do a transformation to itThis time I will not use the airflows built-in google_cloud_hook; instead I will use the official googlecloud library from Google Make sure you input your credential and bucket informationWe do not need to download the file and instead we can just load the object using the ByteIO buffer and load the object into pandasOnce you run this dag you can check results in the airflow logs and your google cloud bucketNow lets link everything we did together with this finale lets create a sensor operator to detect one of the files Then have it trigger a function that deletes that fileBy using gcs_sensor functionality from airflow we can detect if a file is in our bucket Once we detect the file we can trigger delete functionality To see if the code works properly you can check the google bucketOur first tutorial ends here! In the next blog I will go over the more advanced topic of Airflow and cover topics such as custom sensor/operator and how we can use it to scrape twitter data dailyIf you like this article please like share and subscribe for more content Thanks again
UtCRGArNnHmXwsNvmjqzY4,In the earlier article I talked about the various transforms involved in designing the flows in SAP Data Services  SAP DS Transforms Here I will discuss about the connection from SAP ECC to SAP Data Services via DataflowsSAP ECC(ERP Central Component) is an Enterprise Resource Planning(ERP) software which comprises of several modules like Finance CRM HCM etc The modules integrate within each other to create a full fledged solution Its relationship with R3 or S4/HANA can be attributed to the database style(Cloud or On-Premise)Various companies offer ERP solutions like Oracle Salesforce etc ERP is a generic term and can be licensed and the term SAP ECC is for the SAP company alone which offers ERP solutions ECC uses On-Premise databaseThe earlier version was called R3 which has been changed to ECC which has since then grown to many versions S4/HANA is the first and the latest cloud offering by SAP To understand the concept in a simpler way ERP can be referred to as the car and ECC to a BMWWhat is SAP Data Services (SAP DS): SAP Data Services is an ETL tool which helps in transportation of data from various sources to the Hadoop Database or any place as desired Since we are talking in the aspects of Data Engineering this data can be connected to Hadoop and transferred to the same: The point of this article is to connect SAP ECC to SAP DS which is a major source of transaction data in many organizations and a pivotal data sourceSAP Data Services offers 2 kinds of Data Flow processes  Data Flows and ABAP Data Flows The normal Data Flow can be used to transport data from any source and drive it to desired destination ABAP Data Flows work in specific for SAP ECC tables While the normal Data Flows work for ECC as well the difference here lies in the amount of data way of the data extraction and the joins involved in the tableData Services generates SQL code to retrieve contents from the SAP ECC database and most suited for extracting data from a single table which results in performance efficiency Although 2 or more table can be added here the expected output time would not be the same and can result in a timeout errorABAP Dataflow: ABAP Data Flows generates ABAP code as shown in the figure at the top of the page to extract the data from the Database If we want to use more table and have more join conditions this Data Flow method is the most recommended This results in faster execution and does not result in a timeout If the records are too big The ABAP Data Flow has to be used as an input to the normal Data Flow and then proceeded furtherSAP DS is not directly connected to the Database of SAP ECC It is in fact connected to the Application layer of ECC An ABAP code when executed helps in retrieving the data from the Database layer and then transported via the Application layer to the Data Services This is represented in the pictorial format in the topmost of this pageWhen a normal Dataflow is used for the process then a SQL code is generated and then the exact steps are followed like the ABAP Dataflow Apart from the design changes that ensue as per the type of Dataflow used the best practice is to use ABAP Dataflow for SAP sources as it results in a relatively lower time to execute and does not result in the run time error as it would usually happenWay to create an ABAP Dataflow: When inserting an SAP table in to the normal Dataflow the sign for the input table looks like below which implies that it shouldnt be used in this way but can be used anyway(not the best practice)When the SAP source is being used inside the ABAP Dataflow the process looks like below and also implies that the table is being inserted the right way Data Transport option gets activated once ABAP DF is used It works much like the Data transfer without explicitly mentioning the columns and we have to mention just the path where the data will be storedAs shown in the figure below the ABAP Dataflow is connected to the output source and then the job can be executed
FRVRcJCF4M9cA6uueFzizU,As a data engineer Ive worked for the last 2 years in 3 companies which are different in size data architecture etc I wanted to share a broad(and structured hopefully) overview of data engineering that could cover all my experiences at some point Preparing to write I got the word Mental Model from a book called Smarter Faster Better and I realized that is what I exactly expect to build in your head with the following questionnaires that Ive facedMental models help us by providing a scaffold for the torrent of information that consistently surrounds us Models help us choose where to direct our attention so we can make decisions rather than just reactHere what I mean by infra is a charge of the Data System Engineering part except for the main service part Sure you also assure that it is reliable scalable and maintainable If you start to build your data system from the bottom the first(and the most important) question is On-Premises or Cloud? (and then if you choose the cloud Which cloud provider?) Though things are quite different after you made that big choice the general concept that you would concern is similarWhich resource to use? is different in the end but I recommend you to start with the cloud though you look for the on-prem at least you can test it on the cloud so that you can find the best fit for your organization If you choose the on-prem one thing that you should make sure is that your requirement is based on the maximum not the average Analytic traffic sometime peaks 2 ~3 times more than the average or you would need some extra disk for the safe copy(copy and rename) If you care about How to optimize resource usage(or lower cost)? you might be triggered by the cost(if you are on the cloud) or the scarcity So youre gonna start to measure like tagging monitoring because improvement needs measurements That means more on CloudTrail Tagging for the cloud case and more on metrics tracking(Disk Memory & CPU) tools for the on-premOn its core monitoring is for Which kinds of service do we offer and what is the backup plan for each? Normally the services you are in charge of as a data infra manager are: Out of them the first one is usually an unrecoverable task if we lose some and the fail of the other two could be backed up on the application level(eg you can use a rule-based algorithm to back up the fail of recommendation system) So making it reliable to collect the transient data(like client device log) would be the most critical point in your daily jobYou might be already familiar with the basic flow of the pipelining: Collect Move Store Transform Use Here I use that frame to explain the things to focus on when you build one The concepts usually pop up on every phase of the pipeline are Stream vs Batch Push vs Pull And the book Designing Data-Intensive Applications would be helpful to grasp the technical background of each stages conceptsI define collecting as an activity that collects the data outside of the organization into the organizationSo it starts with a prerequisite task like planting a Client SDK(or custom one) Things you need to care are Which information to collect? Is it legitimate? How to format each log? When to send the data in the device(stream or batch)? What if the device is out of internet connection when it tries to send(No need to concern if your business is on the exceptional countries like India In my experience 30% of the logs are sent after a day passed in India)? How to assure the timestamp of each row of a log? Encrypt it or not? Compress it or not? or Which encoding method?Receiving is about API endpoints Similar to the planting part if youre on the cloud nothing to care that much else you would build one on your own In some cases the timestamp of arrival is added to the logs to validate the logs later or use it rather than the timestamp of the deviceAfter getting the logs from the API endpoint or the Operational Servers(with somethings like Facebooks Scribe Apache Flume) the logs would be put into the big and reliable queue so that they could be pulled from it for many other purposes Message brokers like Google Cloud Pub/Sub AWS Kinesis and Apache Kafka are popular choices for this part So it makes questions like Which tool to use as a message queue? Which one is the best fit for each specific situation? Also lots of things to consider and lots of concepts like topic partition offset producer consumer consumer group retention period would pop up in this partConsuming after the queue could be included in this part Some periodic jobs that move those data from the queue to the storage would be a typical one You might need to decompress or decrypt the logs when you consume the logs Tools like Kinesis Consumer Apache Flink Kafka SQL Streaming Spark Streaming can help you Then when that service writes logs to the storage the memorable point of this process is Are you gonna close the past partition with a time limit or not?AFAIK normally we use partition key for the storage with a time frame(eg dt=20191116) so the question above would be more explicit if I say If theres a delayed log for 10 hours and you regularly execute an hourly batch are you gonna add that log to the 10-hours-ago partition and execute the 10-hours-ago batch and its child jobs? Here you might need some strategic level choice because it can affect BI metric change Analytic Discrepancy or prediction model corruption laterAnd before or after this part is a good point to catch the corruption with some stream validation techniquesStream or Batch? The CDC(Change Data Capture) comes up in this part if you want to get stream-like data from the DB(Check this from Uber for CDC) If you take the batch method you can think of Apache Sqoop And you might need some data governance job to filter out sensitive columns for encryptionIt usually starts with basic concepts: If you are on the early stage of the build like Ubers 2014 you definitely dont need to care about the above things and just a pure simple database would suffice Or though you built one like Hadoop stack then jobs on optimization and regulation would pop upFor technical optimization compression(more than the basic support with zstd or lzTurbo) or partition strategy could be on the listAnd you can set up policies like data lifecycle to effectively manage the data assets in your storage though it needs quite an effort to bring all the stakeholders related to the data Some of you might think of AWS S3 Lifecycle Policies and I think that also could be applied to your on-prem environmentI think that lots of words surrounding the data fields flow into this part: Data Lake Data Warehouse ETL etc I put the Suchi Principle on the top because as I look around many types of data systems I realized that it is easy to handle when you put it raw if you afford to cook it from the raw And I always ask myself How I define the stages of the data by its level of transformation? like Bronze Silver Gold case of the Delta LakeNext though I mention it here the idempotency is related to all its parent process That why we need governance which can assure the data quality with a broad strategic view across the parts Sometimes you dont need an idempotent data because users need just a rough statistic index to pursue But you should make it clear whether the data is idempotent or not unless some data scientists would train the model with the data and have some trouble to find out the reason for the discrepancy between the train and the realFor the data lineage I recommend you to set it up because you need it when you backfill or re-execute a set of data and its child-data for some reason by their order And also you need to regularly check the structure of it to maintain your transforming jobs dependency simpleThough the detailed view of the data pipeline is more complicated than the one before if I just simplify things by mentioning current customers of the data system therere three groups: Business Intelligence Analytics Data ScientistsAs I seem the BI group takes care more on transactional data like order count sales with tools like Tableau SQL-based things And the analysts focus more on user behavior using client event logs because they work with UI designers Marketer or some others who are interested in getting an answer for an issue with tools like SQL on Zeppelin or In-House tools R on Notebook or R Studio Python on Jupyter Notebook A/B testing Or you might build a dashboard like ubers deckglAnd finally the data scientists who are the VIP of the service(IMO) take a big portion of the service with intricate requirements Realtime service Feature Store ML Platform could be some challenging topics as an engineerYeah that is what I want to say Workflow management I heavily lean on the Airflow when I make a frame on this part Lots of good sources for the Airflow are out there and theres a recently released book on it tooHere I think that there are two points: Until now Airflow is the most appropriate tool for those purposes though there are many replacementsDG(Data Governance) is defined in the DMBOK as The exercise of authority control and shared decision making (planning monitoring and enforcement) over the management of data assets Though DG has so many sub-topics I just want to use the frame from the book Data Governance by John Ladley On Governance V it said the following: The left side of the V is governance  providing input to data and content life cycles as to what the rules and policies are and activity to ensure that data management is happening as it is supposed to The right side is the actual hands-on  the managers and executives who are actually doing information management The left side is DG the right side is IM It is absolutely essential that you keep this next phrase in mind all through your data governance program: Data governance is NOT a function performed by those who manage informationThis means there must always be a separation of duties between those who manage and those who govern The V is a visual reminder of this This is a key concept that business people understand and IT staff often experience as a problem So you would be on the left side of the V: verifying compliance as an auditor and promoting data access as an architectYou would set rules or policies on AAAA naming conventions the lifecycle of data to maintain the data assets across your company Some sensitive data needs a short-live retention period to follow your local regulation or sometimes you need to audit the access logs on the sensitive data to satisfy the requirements by the law As the concept of data ownership would be more issued more strategic plans on the data assets could sustain the rich background of your data platformAnd finally you should be a data architect whose roles are: I tried to put all the questions that Ive faced for 2 years The companies that I worked for are like the following: I hope this could help someone who wants to grasp which points to focus on when she first gets into the new data environment And Ill update this article regularly to improve the contents as I figure out something more
Mru7PSRVuBiejQLViUyi3b,This came from an article for my graduation(BS) I currently worked as a Data Engineer at an eCommerce company in Korea Here I wanted to give an overview of the modern data system to grasp the general pattern and direction in real use casesIn the recent development of the data system in IT companies many tools are devised individually to satisfy the diverse requirements By suggesting a structured frame to understand those interactive processes and tools we also want to aggregate popular design patterns and tools for technical comparison among things that share a conceptually similar purpose
irXgiwv6RJLLfCogucwrma,Myself Karthik recently I have completed my internship in a start-up based company and I want to share my internship experience and how it helped me choosing what i wantMost of my friends says internship is a slavery I dont understand what makes them think so So I am confused to join as intern But somehow on a positive mind I joined and to my surprise I experienced different things in Internship I have changed my perspective towards itThings you can learn in internship: 1 First and foremost thing is to understand about the company and the work environment (It will be more helpful if you want to join in the same company as an employee) • Exposing yourself to the real time projects where you cant able to find it in most of the colleges • Learning industry standards • You can expose to some niche technologies especially when u do your internship in Start-up based company • You will be ahead of learning path when compared to your friends • No need of buying projects for your final semesterThese are few advantages which I personally felt when you do internship At the same time i want to share my internship experience in Enquero where I have completed my internship recently and joined as a full time employee I want share this because what I heard during my graduation about working in IT industry is very difficultBut what i heard and experienced is completely different in Enquero Yes some companies are moving to give best comforts advantage to their employees and Enquero is one of themEnquero Benefits:: 1 You were never confined to learn or work on one specific technology • You will get any kind of help from your mentors at any time • The most thing which impressed me is their Flat hierarchy structure • No hierarchy from Director to newbieApart from these advantages the three things which I want to mention specially in Enquero Learning appreciation and support are three best things which I seen where I dont feel I could have find out in other companies for a fresherAs a fresher I were never restricted to learn and to work on one technology My learning curve in this internship is so high It is not just about the technology but how we use best technology practices for business problems From the leaders and employees you will learn how to be committed being responsible and accountability of your work and good values Our leader always talks about the best Engineering practicesComing to appreciation it was first started during my interview When no company recognized my research work and Enquero is the first who find my work so valuable and appreciated Your hard work will always be pays offBeing fresher I have done a few mistakes and faced a lot of problems But the people always there to support me and solve the problem immediately with effort The project managers are taking the utmost care and best effort But one thing was sure nothing comes in your way and solve your problems until unless you talk to them You need to ask them and explain the problem in a better mannerEverything has a good and bad pro and cons when coming to Enquero Yes it has a cons but it trying improvise it in a fast manner It will reflect in a quick way to solve it by talking directly with leaders at any time by giving the best comfort to their employees May be that is reason why where most of the employees treats Enquero as their familyEnquero has many things to beat out regular work and go and making it fun filled work environment They had a fun committee where every month they plan some in house fun activities and it has a sports club too like i have in my college daysEthenic Day Celebrations :: Holy Celebrations:: Womens Day Celebrations:: Fun Activities By fun committee:: Sports Club (Foot Ball):: Thank you Enquero for providing good learning with fun filled work environment during internship in Enquero
ZW8dTM6kiWRS4m268YnQKQ,For both data engineers and DevOps setting up a real-time data pipeline is no trivial task yet such architectures are immensely useful for instance real-time recommendation engine real-time geo-location visualizationOne of the most commonly used solution for such pipeline is Apache Kafka Developed by Linkedin in 2011 Kafka has been slowly growing into a mature platform for distributed streaming with good scalability data persistence fault-toleranceThanks to the announcement of Amazon Managed Streaming for Kafka (AWS MSK in short) in November last year setting up a production-grade Kafka cluster that on AWS is not as challenging as it used to be In this article I will go over some of the basics on starting a bare-bone Kafka cluster both locally and on AWS cloudFirstly lets go over some brief overview of the basic components of KafkaApache Kafka consists of six core components namely: Broker Topic Consumer Producer Connector StreamerKafka topic is a category/grouping for messages allowing for: Kafka broker (sometimes referred as Kafka server/node) is the server node in the cluster mainly responsible for: Kafka consumer subscribes to the broker and consumes data from a topic It actively pulls data from the specified Kafka Topic with offset managementKafka consumer group is a set of consumers consuming data from some topics Each partition in the topic is assigned to exactly one member in the group Changes of consumer group members result in partitions re-assignment to consumersOn the other hand Kafka producer is responsible for writing data to a Kafka topic in the following ways: To read from or write to various data sources (eg databases distributed storage) rather than writing a producer or consumer for each data sources Apache Kafka provides us a simpler abstraction namely Kafka ConnectMainly contributed by Confluentio (a company founded by the creators of Kafka) Kafka Connect is an open source connector for connecting to various commonly used data sources for instance HDFS JDBC or NoSQL databases (eg Cassandra MongoDB Elasticsearch)The Kafka Connect component that reads from data sources is called Source Connector and the component that write to data source is called Sink Connector Both connectors make use of Kafka Connect API which is much handier than the Kafka Producer API and Kafka Consumer APIHence using Kafka Connect over Producer and Consumer is recommended unless there are no official connectors for your data source (eg RabbitMQ InfluxDB)Now with our data sources and sinks ready we introduces the final component in Kafka Kafka Stream is the component that allow us to filter transform and aggregate the data from all the data streamsThere are many streaming processing operations available in the Kafka Stream APIs including mapping reducing filtering and window aggregation It is also possible to use other frameworks to process Kafka data including Apache Spark Apache Flink Apache Beam or even AWS LambdaStill if you are looking for more details you can always refer to these resources: In this section I will go over the steps to setup a Kafka cluster locally All the sample codes are available in this repository: The local cluster architecture is defined in the file docker/docker-composeyml which is as follows: To start with please make sure you have both Docker and Docker Compose already installed on your machine Lets clone the repository and enter our project directory: Start services by executing the command make up and services are expected to be running: After the clusters ready we can now read and write to Kafka topics Lets warm up with Kafka Topic CRUD (Create Read Update Delete) in the command line interfaceUsing the Kafka Shell Client we can instantiate a producer / a consumer application within the terminal Opening two terminal shells and type each of the commands in each separate shell: For real-time usage source data would be coming from different external sources rather than manual input from the terminal To automate the data connection to data sources as a proof of concept we make use of Kafka Connect to connect to a mock PostgresSQL databaseIn the connect/dev/ directory some predefined configuration files are already provided Hence we can use the following command to start a Kafka Connect with JSON serialization: Now Kafka Connect should be able to poll records in a database with detected schemas and perform JSON serialization As illustrated above we can see that the messages produced are of the following format in which schema describes the message format and payload contains the actual message content: You may notice that the same schema object is actually embedded in every single messages which lead to a huge waste of storage space Moreover there is no schema validation upon the arrival of each message To address the above problem we will need some more components  enters Schema Registry and Apache Avro Avro serialization and schema registry pair is not a must for Kafka database connection but is highly recommendedApache Avro is a standard data serialization system All the Avro schema are stored in a metadata server with versioning called Schema RegistryLets try Kafka Connect with Avro serialization: To verify if the new data schema is generated let us access the registry schema server through its REST API: Finally to put the icing on the cake we can visit two GUI interfaces to visualize the Kafka cluster and schema registryKafdrop which is a web interface for monitoring a Kafka cluster is available at http://localhost:9000Schema Registry UI a web interface for the schema registry is available at http://localhost:8082A round of applause for our progress! So far so good but it isnt a complete demo if its just running on our local machineWhile AWS MSK is still in its early preview stage (as of January 2019) we will deploy an Apache Kafka cluster with the following components: Before setting up any services a VPC is required as a network interface On top of that we will need 6 subnets: 3 pairs of public and private subnet in 3 available zonesIn case you are not familiarize with AWS VPC and subnets please follow this official AWS tutorial to complete the setup You may use the below suggested values for the required configuration: After creating the VPC and and its subnets we then create the AWS MSK cluster through the MSK dashboardThe AWS MSK cluster will be running in the private subnets of the newly created VPC For the minimal demonstration purpose we have only 1 broker per availability zone (AZ for short)Since AWS MSK is still in preview stage we can only retrieve the created MSK clusters information via the AWS CLI but not MSK dashboard: Now we have a Kafka cluster with three broker nodes managed by AWS MSK Next we are deploying the following components: We will deploy the above components using AWS CloudFormation All the YAML file for this tutorial are created in aws-setup Simply deploy the two services and containers server via CLI: The deployment may take a few minutes You can check the latest progress in the AWS CloudFormation DashboardWhen the deployment has completed we then check the containers server instance created and copy the public IP (containers_server_ip) We shall successfully see the broker statistics via Kafdrop in http://<containers_server_ip>:9000Now the environment is ready and we can try out the data connection on the cluster by a Kafka Connect task First we wrap our Kafka Connect worker properties into a docker container using the script connect/stg/buildsh: Similar to what we did in Step 3 we deploy the Kafka Connect service to AWS ECS using AWS CloudFormation: We shall see created topics with messages via Kafdrop for the production environmentTo clean up the demo setup we remove the CloudFormation stacks via CLI (as the second stack is serving in the first stack we suggest you to remove the second stack first): As in a simplified demonstration environment we are not showcasing the best deployment practice You are welcome to extend our example with: As you can observe it is not trivial to deploy a Kafka cluster on AWS despite the availability of AWS MSK Yet the deployment process should be more and more streamlined as such a new AWS product maturesThanks for reading this long tutorial Hope this gives you more understandings on Apache Kafka and guidelines for application setups For any questions or thoughts feel free to contact me/create issues @https://github
msKBUUL44Ekg5mqs8B3tS6,The American Dream is dead and has been replaced with a Silicon Valley fueled motivation to become more technical As a collective group we no longer buy into the get rich quick schemes especially those of us who graduated during the recession- we have an understanding that its going to take a bit more work and that while becoming a billionaire is a nice goal its much more realistic to try to gain job security insteadSometimes I wish that I had more foresightInstead I was a solidly mid-range student interested in art and literature with a talent for the sciences I graduated high school unremarkably and attending the University of St Andrews where I studied Social AnthropologyThough culture is a fascinating subject knowledge of Amazonian coming of age rituals does not easily lend itself to hireable skills and after graduation in 2013 I found myself working as a secretary at various financial firms and hedge funds in New York City While the work paid incredibly well and the offices were cool I soon became utterly incredibly bored with the daily doldrums of secretary workAround this time I turned to the Internet to entertain myself I found videos on a lion sanctuary in South Africa quizzes to test your knowledge of the Flags of the World and various articles on nutrition and exercise I also found a few programming tutorialsI started with HTML and CSS The results were immediate- you could see exactly what you were building as you created it It was fun but I wasnt really addicted at first The IT guy at one office saw I was teaching myself and offered his help It was a far more interesting way to spend my days than just sitting around signing for packagesAround this time I started losing a surprising amount of weight- not recommended for someone whos already 56 and only 120 pounds I was tired much of the time and was drinking water like crazy When I eventually dragged myself to the doctor they sent me straight to the hospital At 24 I had developed type 1 diabetes and I was in DKA- diabetic ketoacidosis (caused by prolonged high blood sugars)Everything got derailed and I stopped working for several months I left New York and began spending most of my time in Connecticut with my parents I focused on feeling better- working out eating healthy and pretending that everything was going to be alrightWhen I eventually did go back to work I needed something more flexible so I started looking for jobs at tech companies I found one at a remote based firm called Clevertech as an executive assistant Very quickly I was given more responsibilities- I started doing content creation and marketing for various clientsWhile working on a project for a client I was fortunate to meet a Python developer with a passion for teaching As a side project he had started a remote bootcamp for Python development and was about to start his second round of lessons He invited me to join his test case and I gladly accepted I remember many evenings sitting at my kitchen table in Brooklyn coding on video chat with an international groupThe more I dove into the tech community the more apparent my gaps in technical knowledge appeared to be I was fortunate enough around this time to have a supportive network of family and friends that encouraged me to go back to university this time for a Masters in a technical field I continued working in New York while I applied and saved up money for school and in 2016 I began a degree in Information Studies at the University of AmsterdamThough these days a technical degree does not always equate to a good job in the tech field the MsC gave me more confidence in my search for new roles Right out of school I found work as a social data analyst at an NGO Though the work was interesting the company was small and the amount of time I had to learn and grow on my own skills was very limited Instead of leaning further into the company I reduced my hours and found a Coursera course on Data AnalyticsAfter completing this course I found my current company Wonderkind I was originally hired as a data analyst and spent four months creating data visualizations and helping generate reports for the Sales and Customer teamsBut here again I found another path that I had not necessarily intended to follow After four months working as a data analyst the lead data engineer at the company asked if Id like to instead move to data engineering After all I was already working on an engineering project that had piqued my interest- a project in which I gathered data from an API for market research purposes I accepted after some deliberation and havent looked back sinceIn an effort to track my progress and to cement some of the things Ive learned so far Im going to be keeping track of the projects I complete through blog posts
ApE76vXREErEvghwQXNNCw,"Its a pretty honest and intense exam It was a challenge that I had in mind for a while and after the end of the year I encouraged to take the exam and heres the result: The idea of this post is to share some key points notes and study materials for the examThere were some topics that I didnt find in other posts related to the Data Engineer certification so thats the idea of this Take it as a complimentary note for your cheat sheet course etcFirst of all I work as a Data Architect and I have almost 8 months of experience with GCP so in some question that was really helpful because the answers are really tricky and you can doubt of yourself on almost every question the idea is to stay calm mark the most reasonable one and review the question later The idea is dont stay stuck on a question remember that the time is limit For me it took me almost 1 hour and a half to complete the test (you have 2 hours)The exam for me (questions are random so I dont know about other experience) was really heavy on Machine Learning topics There was like 15 question about this Mostly related to: The exam was heavy on BigQuery questions too Questions related to Updates on Bigquery IAM roles Slots Storage etcI have no professional experience working with BigTable so the knowledge that I have was mostly theoreticalKnowing about default indexes and secondary indexes is a must and when to choose Spanner over Datastore Bigtable or CloudSQLI remember just one question about CloudSQL I recommend to know how to export data from CloudSQL to BigQuery on-premises databases to CloudSQL Cloud SQL HA and read replicasAgain the key here is when to choose DataStore over other databases like CloudSQL BigTable BigQuery etcThis topic was really technical and if you dont have experience working with Dataflow it may be a little bit trickyI recommend to understand pretty good the differents between push and pull and what you need to implement a push solutionThis service is glue with other Cloud components so there was some question related to Pub/Sub / Dataflow / BigQuery implementationThis was pretty heavy on on-prem Hadoop implementations and how to migrate to GCPHere its a must to know the differences between every class in Storage The exam questions were really tricky between cold line and nearline implementationsThis cloud component is quite easy to figure out Remember that the Cloud Composer environment runs Airflow and Airflow itself is an Orchestrator tool So when you want to integrate some Dataflow jobs with Dataproc jobs and theres a dependency on each other Always the best solution is going to be Cloud ComposerStudy the difference between Viewer credentials and Owner credentials if you want to share some dashboardsBesides all the key points mentioned I found this exam heavily focused on Machine Learning so I recommend to learn all the most important vocabulary for that like labels epoch neurons hidden layers bias weight learn when to implement a linear regression model instead of classification or clusteringIts always good to read the documentation and stay update to every new feature added to these Cloud Services for example at the moment you can deploy TensorFlow Scikit-learn and XGBoost models to AI Platform but who knows whats going to be in the future the same can be applied to Machine Learning models for BigQueryAnother good key point is to know about Stackdriver Monitor Stackdriver Logging Stackdriver Logging Agent etc I remember that there was a question about installing the Stackdriver Logging agent for a MariaDB on Compute Engine so that can be helpful too""Coursera - 5/10: If you don't have experience with GCP start with this course Its a really good introduction to GCP but IMO doesn't prepare you for the exam""Linux Academy - 7/10: This course is more focus to prepare you for the exam but it doesn't go too deep into every service related to the exam still its a must IMO to take this course and then read the documentationDocumentation - 8/10: The official documentation really helped me to go deeper into some topics like clustering configuration for BigTable Machine Learning models for BigQuery Dataproc Dataflow etc I recommend to give you time to read the documentation because is quite heavy and its going to take a while but the compensation for that is bigBrainCert - 8/10: I would say this practice exams course is the best out there The questions really prepare you for the real exam and you are going to gain more knowledge after reading the explanation for each answer and question I did two tests almost every day for about 2 weeks until reaching between 94-100% every testI hope that this post will be helpful for all of you that are going to take the exam in the future"
BbriHa8FnKuSiBTC7o9SMK,Airflow is a powerful tool for managing workflows with complex tasks and dependencies We recently began using it at my company to replace our existing workflow orchestrator and have appreciated its easy-to-use UI and smooth integration with other components of our pipeline such as GCloud and Kubernetes To save time we decided to run Airflow on Googles Cloud Composer which is Googles managed Airflow solution It runs on its own Kubernetes cluster hosts the UI and accepts DAGs in a Google Storage bucketRecently we were faced with the challenge of building an application to run 1000 queries (through Googles BigQuery) per hour Perhaps the most straightforward way to architect this is to submit every query with a single script But we had a few more requirements beyond just shooting off queries: Thats a lot of features to add to our simple query-launching script Fortunately Airflow offers these features out of the box You can instruct DAG runs to wait until the previous hours run is complete customize behavior when a task fails and integrate with Kubernetes through different Pod Operators (for the purpose of this post Im going to assume you have a basic familiarity with airflow architecture and vocabulary)Early on we faced the question: should we have 1 task that runs 1000 queries or 1000 tasks that run 1 query each? If we want to make the most use of Airflow we need the latter so that we can determine success/failure at an individual query level If a few tasks fail out of 1000 we prefer to rerun only those tasks and not all 1000 It sounds good in theory but first we needed to know if this is even possible on Airflow Can airflow scale to process this many tasks in the given time frame reliably? What options do we have available in our scaling toolkit? I decided to build a quick POC that became the basis for this blog post (spoiler: it succeeded)As a quick caveat there are many other ways we could have designed this with or without Airflow For example we could have managed task failures and retries through a kubernetes job But we saw some benefits to using the Airflow UI and intended on using Airflow as a centralized hub for managing other applications as well In fact we decided to implement a different solution than this  but it wasnt because of Airflows limitationsSo far we have discussed the leftmost box of this diagram One thing to add is that Composer runs on its own kubernetes cluster At DV our applications have been designed to run on an existing separate k8s cluster This means that its actually the kubernetes pod in our compute cluster that is responsible for doing the work (ie pinging BigQuery and instructing it to execute the query) not the Composer k8s cluster The job of each Airflow task is simply to launch this k8s pod and pass it some environment variables that tell it how to do its job We had already written this k8s component and were running it in production with our old scheduler so switching to Airflow was only a matter of changing how this component is triggered There are a few reasons why we wanted to continue running on two clusters mainly: I could go wild in my benchmarking because I could overload and crash the airflow cluster without draining resources from other running applications The downside of this is it adds overhead to the whole process  it takes about 10 seconds to spin a k8s pod up and down and communicate it back to airflowBelow you can find an excerpt of the task definition from the DAG script This defines the Airflow task that creates a k8s pod from a docker image Each run it dynamically generates 990 1100 tasks Because airflow and compute are in different k8s clusters we are required to use a GKEPodOperator (GKE = Google Kubernetes Engine) For the purpose of this test I simplified the duties of my script I cut out the BigQuery part entirely Instead of submitting a query my k8s pod will just sleep for 50 100 seconds (the approximate time that a query would take to execute) I pass that to the k8s pod through the TIME_TO_WORK environment variable From Airflows point of view it doesnt matter if the pod is sleeping or submitting a queryI started out by doing some research on what settings Airflow and Composer offer that we can tuneThe first category is Airflow settings In local mode you can find these in a file called airflowcfg In Composer you can find these under the tab Airflow Configuration OverridesYour configuration page might look something like this: Will this application be more limited by CPU or memory? We can adjust the resources available on the machines running Airflow to better match our ideal cpu:memory ratio Composer clusters are comprised of virtual machines which the user selects from a preset list offered by Google Certain machine types are more optimized towards cpu or memoryI started out by firing up Composer and creating a cluster To start I chose the smallest number of (3) and weakest (n1-standard-1 which have 1 vCPU and 375GB RAM) compute instances I started running my DAG but it barely ran Tasks were failing left and right and each hours dag run took several hours to complete My first instinct was to increase the cluster size The performance hardly improved and runs failed We can tolerate tasks failing (because they retry automatically) but if runs are failing this means queries are not getting executed Next we should try larger instancesThere are way more types of machines available than is feasible to test so I decided to test 3 types: n1-standard c2-standard (compute optimized) and n1-highmem (memory optimized) I approached benchmarking with the goal to compare those 3 instance types and to find the settings that result in the shortest process time per 1000 tasks where every task succeeds It costs less to run a cluster with fewer bigger instances so in my benchmarking I mainly used 3-node clusters and scaled them vertically For each machine type I would continue to double the worker_concurrency dag_concurrency parallelism and non_pooled_task_slot_count until processing speed decreased or runs began to fail As noted above I always set dag_concurrency parallelism and non_pooled_task_slot_count to 3 times worker_concurrency because my clusters had 3 nodesFig 2: The relationship between worker_concurrency and tasks/hour Optimal performance of 4000 tasks/hr was achieved with 65 99 concurrent tasks per worker on a memory optimized 8 core cluster (orange) If you are wondering why I tested more concurrent_tasks options for the highmem cluster it was because it performed best and I wanted to understand it betterThe sweet spot was using high memory instances and an intermediate number of concurrent tasks Surprisingly tasks are very memory hungry  each task consumed about 300mb of memory and thats just on the Composer cluster While the other machine configurations with 8 cores were also reliable they were still slowerIncreasing worker_concurrency didnt affect process time as strongly as I expected Airflow isnt smart enough to stop launching tasks if the node has run out of memory  so if worker concurrency was above the machines capacity Airflow will still launch tasks and they will fail Above a certain threshold increasing worker_concurrency has no effect on process time Above a higher threshold increasing worker_concurrency will cause workers to crash C2-standard-8 (compute optimized) could handle higher numbers of concurrent tasks but processed them slower N1-highmem-9 (memory heavy) could process tasks quicker but had a lower concurrent task capacity
7TAUEvJuWaLCvGwnhuLE83,As companies mature in their digital transformation journey old technologies and rules of doing business are being re-defined Capturing customers is no longer enough and companies are focusing on how to keep them engaged with hyper-personalized experiences Theres an explosion of data sources as everyone and everything is connected with mobile devices social media and IoTWhat this means for a business is an exponential increase in the speed & volumes of data that needs to be handled and the necessity for this data to be available at all times under all conditions Because of this phenomenon the demand for modern data centers has increased to support better agility high-performance and sophisticated service deliveryData is no longer highly structured information from a limited number of data sources The reality that business leaders face today is the fact that data comes in several types and is much more complicated to handle coming in from multiple sources The real challenge is making this data relevant for the tech leaders CXOs and the Line-of-business so that they can derive meaningful & transformative insightsSo how are organizations responding to these data challenges? We will be exploring this in detail furtherBefore we explore this topic in detail let us first understand what we exactly mean by data modernization Its simply the transition from legacy data infrastructures to modern ones owing to changing data requirementsWith huge amounts of unstructured data like images audio social media comments clinical notes in health care modernizing the data infrastructure has become ever more criticalAs per IDC there will be a stark growth in the number of digitally determined organizations that are fully equipped with an integrated enterprise-wide tech architecture solution This number has grown from 46% to 90% and has been cultivated by digitally-driven businesses that are becoming more focussed on their messaging and adding richer experiences into the customers portfolioInnovation is being fuelled by the app revolution with next-generation cloud-native apps Intelligent applications digital platforms and technologies are taking care of customer needs round the clock The crux is that organizations are investing in terms of people process & technology when it comes to digital transformationBut one major challenge that arises is the silos in organizations in terms of innovation What we mean is that innovation is happening in different layers in the organization for instance chatbots in the frontend and inventory management in the backend The problem is connecting the dots and finding a common ground for all the individual infrastructures that power these different layers of innovationThe process begins with a single enterprise strategy that lays the foundation for a long term investment strategy The objective is to power technological innovation with a fully integrated organization-wide tech architecture while modernizing the internal IT environmentThis organization-wide tech architecture is fuelled by a modern data & storage infrastructure Data is absorbed by organizations from all sources  internal & external processes API based data streams and more The idea is to make this data available for insight and transformation so that it can be converted into timely action before the value of the data expiresLets dive deeper into how enterprise architects and CIOs can modernize their data management & infrastructure to overcome the challenges that modern organizations are facingMost organizations use a combination of multiple cloud infrastructures Business leaders get to exploit the advantages of traditional data centers as well as public & private cloud architectures A truly hybrid cloud architecture will help you in the followingEfficient and flexible delivery of resources across clouds  Data becomes available across and can be utilized for multiple use cases The hybrid cloud allows the use of a public cloud for backups recovery and data retentionAdminister Infrastructure as a Code  Enterprises can deliver public and private cloud infrastructure as a shared reservoir of software-defined resources made available with APIs These resources can be smoothly integrated with DevOps or as a part of the business processAddress governance & security-related concerns  Integrates your organizations modern cloud infrastructure & traditional data centers to a common governance and security systemOrganizations are turning to AI for data-driven insights to derive new business opportunities for products or markets empowering salespeople to have a meaningful sales pitch and improving internal processesAI and ML-enabled solutions directly enhance the performance of data management which has a transformative impact throughout a business For instance Machine Learning has the ability to define new paths a query can take and thus speed up the process The use of natural language querying can go a long way in helping LOB users employ Internet-like- search to draw valuable inferences on time • % of the business leaders from the companies surveyed said that AI and ML are indispensable to their data platform & analytics initiativeTo manage your organizations financial resources well CIOs have to ensure that their database solutions must optimize the costs while maintaining industry standards and adhering to Service Level Agreements (SLAs)A range of administrative tasks like setup & deployment workload management resource utilization and storage management along with maintenance upgrades and capacity expansion can be automated so that the focus shifts more towards strategic initiatives Choosing data solutions with on-premise and cloud architectures sharing a common codebase can help save time and effort on rewritesA large chunk of the IT budget is spent on storage requirements in the form of hardware hosted & cloud services and managed services But the storage requirement can be reduced significantly using techniques like data compression and multi-temperature data management capabilities which have a direct impact on storage requirements Cluster topology transparency is another efficient way to reduce storage requirements as applications are unaware of the fundamental cluster & database deployment thus speeding up coding & testingSecurity and IT professionals should be ready with answers to some difficult questions like : Cyber-resilient architecture is your key to unlock the security concerns of your applicationA cyber-resilient architecture is designed to safeguard servers whether they are traditional data centres remote storage and as part of software-defined data center Under the purview of cyber resilience all the issues before during and after a threat or an unforeseen event are taken care of by IT leadersOrganizations often struggle with the growing volume variety and velocity of the data decisions inspired by data automating business processes with cutting edge technologies In this scenario it becomes necessary to master data engineering to ensure that the right infrastructure is in place to operationalize data pipelines required to perform analytics on growing volumes of dataThrough our comprehensive analytical knowledge and expertise we embed data and intelligence into our clients business processes to deliver data solutions that unlock new revenue growth and cost efficiencies at unprecedented agility and scaleKnoldus can help you revamp your data infrastructure to enable it to fuel your modern requirements Get in touch with us to schedule a call with our expert or drop us a line at hello@knolduscom
oKj8H6SaVp8PrrYmV5T2yS,This is my first post on medium and I would like to apologize in advance for the length of the post but I just wanted to share my journey I am sure that youll learn something hereIn this post I have shared the GCP Professional Data Engineering exam experience Also shared some study material & practice exams in between the post Do read the post and let me know in the comment section whether you liked it or notMy name is Kshitij and I recently appeared for the GCP Professional Data Engineer Certification exam But this was not the first time I appeared for it I had attempted it back once and I failed And recently appeared for the same exam and I passedBefore starting I would like to talk about my background Currently I work in a Cloud startup which provides cloud-native solutions to the customers and solve their data problems I started my IT journey 3 years ago from a very prestigious MNC service company straight from college I worked as a Software Developer and then I decided to change my expertise in the Data domainInitially I worked in very traditional and orthodox banking technology and I wanted to move out of it So I got a project in GCP for a Data Engineer role I had no background or knowledge of the technologies used or the skills required for Data Engineers My colleagues told me to do a course on GCP I started the GCP Data Engineering Course from Coursera which is a very wonderful course When I completed the course I thought that I know all that is required and I can go for the GCP PDE certification exam I was over-confident I thought whatever I learned is enough for the certification exam I gave some practice tests on Udemy and appeared for the Official GCP Professional Data Engineer exam On the day of the exam I was very confident in fact overconfident that I can clear it in the first attempt When I was appearing for the exam as the questions kept popping the complexity of the questions kept on increasing(As the questions are dynamic and complexity increases) and the topics that questions covered were more than what was taught in GCP Data Engineering Course in Coursera When I reached around 30 40 questions I was broken because I knew I will not pass the exam And so it happened I failed It hurt me a lot I lost the money Moreover I lost my confidenceI was completely heartbroken but I decided to give the exam again But this time I did not let my ego rule over me I knew that I needed more preparation and more self-belief to re-appear for the exam Because in this exam its not just your knowledge but your confidence your skills your patience is tested Then I understood why this exam has so much value and recognitionAfter the blunder in December 2019 when I failed for the first time I decided to focus on the skills and not on the certification exam I started learning more about GCP services and I started working on them practically I did some cross services implementations and lots of hands-on practice I practice on my own did some POCs which boosted my confidence This was the best way to learn any technologyAfter gaining a significant understanding of GCP and a lot of practice and work I decided to re-appear for the exam in June 2020Now I will share what study material was used to prepare for the examI first started revising all the concepts from the previous Coursera course and then I went for the Linux Academy course Please find the link belowCoursera course: https://wwwcourseraLinux Academy Course: https://linuxacademyLet me assure you that the above study materials are very very important but not enough You need more practice and more variety of questions to solveNext you need to follow all the links provided in the Github repoTry and read as many posts or study materials as possible from this link They will help youAnd the most important study material is the GCP official DocumentationGCP official documentation: https://cloudgoogleNo study material is as good as this and there is no replacement But its impossible to read all the topics so some good guys created a google doc which collects some important topics to be covered for the examNow if you follow all the above resources then you are good with your preparation but you still need to practice solving some exam questions For that you can appear for the GCP official practice exam and Linux Academy practice exam Please find the links for the sameGCP official practice exam: https://cloudgoogleLinux Academy practice exam: https://linuxacademyKeep re-appearing for the above exams until you keep scoring more than 95% constantly The purpose is not to mug the questions but it will help you understand the variety of the scenarios covered help you understand the scenarios and how to choose the best correct solutionBelow is some extra exam preparation which is not easily available but is very important These materials turned out to be turning points in the exam preparationFinally just a few days before the exam date go through the below linksI didnt do anything more than this I just prepared all the above linksJust recently in July 2020 I re-appeared for the GCP Data Engineer remote proctored exam I appeared for the exam with the above preparation materials and a lot of confidence and self-trustI will not go into the details of the exam as it was tough and it is supposed to be But honestly speaking before finishing the exam itself while I was solving questions I was super confident (not overconfident) that I will pass this time This was the level of boost I got after systematic preparation and also because I worked hard and trusted my instincts I finish reviewing all the questions (took me around 1 hr 50 mins) and clicked the submit button and when I saw I was passed the message my happiness went over the roof I felt very proud of myself and why should I not? I officially became a Certified Professional Data Engineer dude I was is and always be a big deal for meNo matter what happens whether you fail for the first or 100th of times in this exam or at any point in life you should need to be able to pick yourself up Learn from each mistake and be better than the previous time Because at the end of the line it brings immense happiness and a sense of joy & confidenceMany fail this exam for the first time and are hesitant to re-appear I would like to advise them that if they follow the above path and keep faith in themselves then they will clear this or as a matter of fact any exam/situation in the worldI wish everyone who is preparing for the exam all the very best and keep the faith Trust yourself and youll eventually get there Peace
DG9Nip63dWCouxbHDnHmjc,Theres a lot of hype on serverless in recent years its a nature evolution of cloud computing  the ubiquity of on demand instances and servers has taken the customers appetite to the next level that is on demand function callsWhy do we bring up instances? We need to run applications but we dont wanna maintain actual hardwares The technology and market is mature enough now we dont wanna maintain the instance itself In laymans term serverless is to EC2 as Lyft/Uber is to New Yorker we want the convenience of a car but we dont want to pay for the parking and maintenance of a carIts a very common use case for web applications especially where applications need to handle large volume of requests yet each request is usually very simple computation task However in the case of ETL and OLAP queries which usually has more requirements on hardware and setup (more than a container say a Hadoop cluster) its still a green field for serverlessOne of our product offerings allows customers and our analysts to run all kinds of analytical queries against 20+ years worth of US consumer credit data (the data the calculates everyones credit score) Its big but not huge data in TB range however drilling it down is almost impossible if you just dump into RDBMSThe rest of the post Ill be going through how we (at PeerIQ) achieve serverless data lake/warehouse and running performant OLAP queries serverlessly on AWS working with the above dataAt the beginning I was the one to architect it out and maintain the cluster with very minimal devops support Only took me couple days to realize theres no way we can support long running hdfs on EC2 or even EMR we had to dump the data on some other file systems where it can 1 still work with all the distributed query engines (hive/spark/presto) and 2 no one has to ever worry about it Naturally theres one very good candidate on the horizon  S3 Theres more detailed comparison on databricks One thing Id like to highlight is that  with other integrations such as Glue and Athena not just the cost savings on actual AWS service but also the man hours is huge this turned out to be an excellent decisionEarly in the days migrating metastore (where hive keeps all the table information and spark and presto referencing from) from EMR to EMR was not the easiest thing to do if we bumped EMR version With Glue (basically an external metastore/data catalog) it made things (for a developer from non-big data world) a lot easier to run OLAP query with distributed query engines on any data thats on S3 Theres still caveats with Glue thats either not well documented or theres just bugs with the connector  for example somehow I can drop partition in spark-sql but not in hive Yet the cost savings of having anyone with minimal understanding on how these systems are setup and how things work together being able run queries on any data on S3 with an EMR cluster worth way more than working around these caveatsTip: Dont create your table with location on S3 if its going to be a large table do your things on hdfs then do s3distcp to S3Weve been having great success with is AWS Athena Before Athena was available one of my daily routine is to troubleshoot hive/spark/presto queries for our analyst there are principals to tune the memory settings that I have probably shared too many times its ironed in my brain The analysts job is to write analytical queries and my job make the barrier as low as possible for their queries to work  with Athena no clusters deployment involved most queries work out of box job done! Our analyst is even able to run ETL on his own (or maybe hes just too awesome!) with Athena I am now more concerned with version controlling our ETL queries than them being able to iterate fasterOf course its not done-done Athena still has concurrent and resource limit (a little unreliable to use for production big data ETL) at times its difficult to see why querys erring out or say wed like implement our own UDF (that said it does support lambda expression) Wed always need our own EMR from time to time but once we get the custom piece into processed data on S3 data engineers can enjoy other projects on their handHopefully this gives small orgs ideas to solve big data ETL and analytics problems cost effectively! Not just lowering AWS bill data engineers and devops finally dont have to be constantly trouble shooting the cluster
ahsdbWWFmucWDAXUmZhTA2,Engineers are always looking for bottlenecks to optimize For web-applications the resource requirements are not as complex we can usually rely on a well-designed load balancer Over pasts years Ive been tasked to deal with different types of bottlenecks manufacturing supply chain customer service and of course  ETL Planning resources for data systems usually involves more than a load balancer in many data processing pipelines its common to see some of the steps are more resource demanding while others are simple and quick some needs to be happened in a specific setup (say a spark cluster as opposed to a linux box with python installed) while others dont • % of the time bottlenecks are due to poor resource planning  either its difficult to scale when the demand spike (have too many data processing jobs but not enough core/memory to allocate) or wasting money on under-utilized resourcesClusters are scalable but not necessarily flexible (rarely they aim for flexibility but rather having the ability to scale for high demands) For example databases or data stores are often easier to scale up than down Scaling down often incurs interruptions for running jobs (ECS and EMR clusters for example) This introduces difficulties for organizations with smaller budget and lower fixed demand where jobs are often customized and unscheduled to maintain their infrastructure cost effectivelyEven if we have the flexibility to scale the infrastructure quickly and easily with minimal interruptions being able to customize resource requirements and guarantee the demands are met could further enable us to fully utilize the resources cost effectively In different stages we might want bigger or smaller executors depends on how the backend of the executor is setup sometimes its difficult to guarantee resource available for a jobFor example say we have a 3 nodes cluster running our database resources available will be different when theres one query as opposed to ten queries as opposed to a hundred queries This introduces uncertainties in estimating tasks SLA and eventually becomes the bottleneck when things get busyAnother classic example is not being able to customize resource specifications for different steps In a typical spark application we often mixed map and reduce together as a monolithe application This is a problem because most of the time we need small but many executors in map step and less but bigger executors in reduce step We either give it more memory and wasting resources in the map step or blowing up reduce step in order to fully utilize resourcesOk those are all valid and obvious explanations the real question is  how do we solve them? Instead of being very specific about how we solve those problems I wanted to first share some of my personal philosophies about data engineeringTraditionally we think of ETL as batch processing We bring up big boxes and process a bunch of them at a time But time is changing so is the pricing model of computing resources Instead of on-prem boxes we can get as granular as on demand function runs (AWS Lambda or Azure Function) The more atomic the processing unit is the easier it is on the infrastructure setup the lighter the resources requirement is the more efficient resource allocation could beHumans are risk averse so should our systems be The classic psychology theory: even though taking the gamble might win one some money if the stake is too high one rather not take the chance Even though running one huge job might save us some overhead and complexity to track job status but it could also mean we waited a 3 hours for nothing slow feedback loops is one of the major complaints from data engineers (or to put it differentlywatching paint dry) The quicker a job can fail the easier we can troubleshoot and the happier the engineers will beOk I stole the headline The post is rather irrelevant but I thought its a good headline to borrow Traditionally we regulate our resource demand by scheduling as we have more fixed supply of computation resources As the cloud services evolve with serverless offerings and per second ec2 billing for a little premium we can actually request as much resources as we need This is a very great thing for data engineersJust think about it why is software way more scalable than manufacturing? How do you scale manufacturing? By building more plants How long does it take to build a plant? Depending on the size of the plant but way longer than requesting a new ec2 box thats for sure On demand function calls or serverless container runs (such as AWS Fargate) means we can expand the supply without building a plant but rather renting a manufacturing line for a few mil seconds in another plant Making it unnecessary to have boxes up and running at all timesWith all these cool tools how do we scale as fast and flexible as we can with minimal interruptions? Instead of managing resources with timetables or setting up weird pools of resources and load balancer Theres this great invention of queues If you go to stores people get in queues to checkout no matter how many customers are in line the cashiers are business as usual The customer wont get less of a cashiers attention if there are more demands (no weird pool of resources) the manager doesnt have to recognize how many things a customers buying and assign to those who buys more to the less busy cashiers (no load balancing) Instead if the queue gets very long without any interruptions the manager simply has to add clerks to cashiers Same wisdom applies to resource planning in the modern data engineering world The processor should never worry about demand but just focus on its job  processing Adding or removing processors shouldnt interrupt the current processors at all but instead when the demand spikes add the job requests to the queue the orchestrator can monitor the demand (like the store manager) and add processors as neededOne thing I really enjoy about microservice architecture  I can swap the entire infrastructure that a system is running on without anybody even noticing The interesting thing about swapping infrastructure is that (at least for me) really no one cares about where is it running on as long as its running Environment are not as important as you thought as long as it does what it promises and it does not breakFor me at least my biggest joy is to get as much flexibility and scalability as possible by swapping infrastructure with minimal code changes and without people noticing In the following weeks Ill share some of my own observations in terms of cost speed maintenance and issues on my experience moving monolithe ETL running on expensive box to microservice-based mostly serverless ETL infrastructure
7D5SHc6ipidBFXyQ95uZmJ,When it comes to working with bigger data  commonly known as data that doesnt easily fit into one box and one wishes to process in a more parallel fashion one of the most popular choices is SparkOften times data engineers look for ways to distribute data processing over hundreds if not thousands of machines Spark API provides a very easy way no matter how complicated the transformations are think about API similar to pandas but instead of just processing the data that fits one machines memory same logic can scale to a cluster of machines to work in parallelHeres how spark works on a high level  it has this abstract concept of dataframe that actually represents hundreds of thousands of files one defines transformation logic on dataframe when the job compiles it repartitions the data (aka chunk into operable unit) process chunks in parallel based on logic defined accordingly Because developer doesnt have to worry about how to partition the data for parallel processing and resource management its often one of the easiest way to getting started on working with very big datasetIt was also how I first started my journey with distributed computing In less than a month or so of actual development time I was able to implement complex transform in spark on monthly archive with 100 150t data and < 7 days SLA With moderate knowledge in Hadoop AWS EMR its amazing we can just throw money to buy more machine and 150t of data just pumping out without much configuration (obvious an understatement but it stabilized after 3 4 months of iterations on memory and parallelism configs and many days of JVM profiling)There are three key characteristics in my opinion that Spark will no longer the future of distributed framework: The above drawbacks is in general true if you are still developing your pipeline in database except the learning curve (I think its safe to assume you can find more people on the street who knows SQL than Scala+Spark)  at the cost of even less flexibility in terms of scalability and tuningIn general we want a data pipeline infrastructure to be both flexible and scalable flexible meaning its easy to scale it down (we have parts of our process to be serverless where no server is running unless somethings scheduled) scalable meaning you have the ability to scale up without blowing upIn order to make a system very scalable its easier to plan on the smallest possible operable unit That is its easier to make full usage of the cluster resources when each task is tiny than big Think about filling a cup with water (infinitely small) versus chunks of ice needless to say you can fill with more water than ice cubesLast year spring we (PeerIQ) started re-architecting our monolith ETL pipeline I didnt realize we were almost building and designing a different kind of distributed framework from scratch but 3 months later we have a distributed system that is: How we do it is not unique to us its actually quite applicable to most data pipeline I wanna share a few things we learned throughout the processIn order to have the pipeline as efficient as possible first and foremost is to try to define the smallest operable unit of data This provides two major advantages first hardware requirement for one worker that will at most work on one unit at the same time is trivial Secondly like we talked about earlier when the hardware requirement is minimal its easier keep them uniformed and spin up instances for only what you needSo usually in spark or databases the century old trick is hashing meaning when user asks to process large volume of data find the logical field to use to chunk the data up and process in unit and have multiple processes working at the same timeWe dont wanna re-chunk the data every time as theres only minimal changes as the new data comes in if we use Spark or any file based approach I will have to redistribute data across everyday so the new data are resorted with historical data for processing Also because its file based its almost always impossible that blob storage solution will offer you availability at high rate normally we see things error out even at couple hundred requests per second this creates a huge problem to chunk data in the the smallest possible unit with file based approach in both writing and readingOur answer to this is to use Cassandra as a distributed cache to allow us to do pre-chunking (key as the smallest unit and value as the actual data) one added benefit is obviously fast-update as everyday data comes in and fast-retrieval as each worker ask for data only on specific key with specific rangeOnce we have a mechanism figures out what to put on the queue to process all we need is to find a queuing technology that allow for varied number of consumer and auto-scalable workersIn my case rewrite 10k+ lines of code accumulated over years by myself is simply not an optionWe are able to reduce run time to up to couple days (and a couple that just never works) to < 30 mins and cut our cost by 40% with zero code changes on business logic In another similar problem where I used similar approach but use Spark/Presto to chunk the data slightly and use the same Queue+Worker design to get run time from never finish to 5 mins easily also with no code changes Flexibility availability is the key An added benefit is that because its so flexible and easy to scale after we collect enough system logs we started using ML models to predict system parameters (how many workers for one) to be truely zero config (as complex as it is user doesnt care)Thats it hopefully this is helpful for anyone whos looking for ideas to build a scalable pipeline or scale some mystery legacy code base  dont try to rewrite try to rearchitect! Good Luck
SRRJTYA9BGSgxPYzAtiCE5,If youre looking to load data from Azure Data Lake Gen 2 via Azure Databricks into Azure SQL DW over Polybase this article will layout an approach based on Managed Service Identity(MSI) for data transfer to the Azure SQL DWI prefer the MSI method as opposed to passing the account key for the storage account as its more secure ie more granular security wise No doubt we could use Databricks Secrets backed by Azure Key Vault or use Databricks Secrets directly as a means to protect the key nonetheless the key would have permissions storage account wideSecondly key rotation would involve an additional step to change the key in the key store; thus I find the MSI approach better maintenance wise The account key method is documented well in the Databricks page for Azure Sql Data Warehouse as a result Im not covering the approach in this articleAs per the diagram below the assumption is there are multiple zones in the Data Lake we are reading from a particular zone and writing to Azure SQL DW via a Transient Zone which enables high speed Polybase data transferThe caveat is the Transient Zone would have to be cleaned out periodically\xa0as\xa0its\xa0a\xa0temporary\xa0storage\xa0for\xa0in-transit\xa0dataIn my case Im assuming theres a Trusted Zone which contains curated data and theres a Transient Zone to store in-transit data thats being moved between Databricks and Azure SQL DW I prefer to mount the Trusted Zone for easier referencing of paths within the zone when performing data engineering activitiesThere are three connections that play a role in the data transferThe Spark driver connects to SQL DW using JDBC with a username and password ie SQL Auth The JDBC connection string for the SQL DW is provided in Azure portal when you go to the SQL DW PageJust click on the connection strings icon on the left pane for the Azure SQL DW resource Select JDBC tab in the list of connectors/languages to retrieve the connection stringThe SQL connection string would look like below: The encrypt=true setting enables Secure Sockets Layer (SSL) encryption for data sent between the Databricks cluster and the Azure SQL DW instance through the JDBC connectionThe Transient Zone acts as an intermediary to store bulk data when reading from or writing to SQL DW For reading from/writing to Azure Data Lake Gen 2 well use OAuth 20 tokens leveraging a Service PrincipalAs a sidenote the supported URI scheme for Azure Data Lake G2 is abfssNote: Im leveraging Python in the Databricks NotebookFor more details around this approach refer to : Access an Azure Data Lake Storage Gen2 account directly with OAuth 20 using the Service PrincipalSQL DW needs the credential to connects to ADLS G2 when reading and writing temporary data I prefer the MSI approach to the forwardSparkAzureStorageCredentials approach which is used for account key(s)This will require a Managed Service Identity (assuming VNet + Service Endpoints setup) to be pre-configured for the SQL DW server instanceNext I set useAzureMSI to true in the write call in Databricks notebook In this case the connector will specify IDENTITY = Managed Service Identity and no secret for the database scoped credential as a login mechanism to Azure Data Lake G2After launching switch the shell to PowerShell Click Confirm when promptedNote: server-name above refers to the server name where the data warehouse resides • Grant the permission to the MSI in relevant ADLS G2 filesystems /foldersEnsure In this case it would be in the Transient zone so that we provide only the required permissionsClick Save to assign the permissionFurther permissions to the Transient filesystem and folders within it can be assigned via Storage ExplorerNote: The IDENTITY Name must be Managed Service Identity for Polybase to connect to the ADLS G2 Storage Account • Use Notebook session configuration approach to provide the credentials for Transient ZoneWrite the code in Databricks notebook to authenticate against ADLS G2 specifically Transient Zone using service principal created for Databricks to Transient Zone authentication/authorization as per example shown earlierAssuming the dataframe had been loaded from file(s) in Trusted Zone and undergone the requisite data engineering processes write the code in the Databricks Notebook to leverage MSI and for SQL DW to read from TempDir<pre-action-SQL>: T-SQL to execute before loading the table ieReferences: https://techcommunitymicrosoft
EzM6Uafq2H9YHyjDaEoShk,This post is a follow up to the previous post on Databricks exam prep for Apache Spark 24 with Python 3 Im collating notes based on the knowledge expectation of of the exam This is a snapshot of my review of materialsApache Spark 2Various developer blogs that are fantastic There are too many to be called outSparkSession is a driver process that has one-to-one correspondence with a Spark application It is the unified entry point of a Spark application combining SparkContext and SQLContextSparkSession was introduced in Spark 20 to allow for easier programming (with lesser number of constructs) in Spark when working with DataFrame and Dataset APIThe variable spark represents the SparkSession The figure below shows and example output when running the command from a Databricks notebookfrom pysparkspark = SparkSessionbuildermaster(local)config(sparksomeconfigSparkSession API can be used to create DataFrames populated with a series of numbers As mentioned before a DataFrame is distributed across executors in the clusterThe example below creates a DataFrame that has 1000 rows with numeric values between 0 and 999 in increments of 1 which is the defaultdf = sparkrange(1000)df1 = sparkrange(1000)The function pattern is as below: The parameter step indicates value incrementssparkAn example is as below: df2 = sparkrange(210002)A list is a collection which is ordered and mutable Its the most versatile collectionA set is an unordered collection which enforces not having duplicate elements It can be used for membership testing and eliminating duplicate entriesA tuple consists of a number of related values separated by commas and are immutable They are generally used as a sequence typeA dictionary is a set of key: value pairs where the keys are unique (within one dictionary) They are indexed by key and unorderedCreate DataFrame from a list containing tuples
bjo3VQe4ZETs2nr8skDy92,I started out my series of articles as an exam prep for Databricks specifically Apache Spark 24 with Python 3 exam These have now transformed into general notes for learning Databricks and reference when writing codeThis post covers DataFrame and attempts to cover it at breadth more than depth The rationale is to give a wider perspective of what is possible with DataFramesApache Spark 2Various developer blogs that are fantastic and stack overflow There are too many to be called outDataFrame is the key data structure for working with data in PySpark They abstract out RDDs (which is the building block) and simplify writing code for data transformations Essentially DataFrames are table-like data structures (with known schemas) containing a collection of records distributed across executorsSchema determines the column names and their types in the DataFrame Also records in the distributed table are represented as Rows DataFrames can be constructed from a multitude of sources such as files Hive tables external databases or RDDsDataFrame is implemented in the class pysparksqlDataFrame Ill walk through the methods of the class by functional areas followed by propertiesReturns a new copy of the DataFrame with the specified alias as nameSample Call: In the example below Ive create two aliases and inner joined them over two columns One of the use cases for alias is self-joins like below To make it simpler you could just create one alias and self-join to the existing dataframeSelects column(s) from DataFrame based on specified regex of the column name(s) and returns as DataFrame Useful when the column names to be pulled out adhere to a known patternParameter : colName represents a regex of column namesPrints out the schema of the DataFrame in tree format This method is generally used for eyeballing dataframe schema after the dataframe is loaded from file(s) with inferred schema optionApplies a function f to all Rows of a DataFrame This method is a shorthand for dfrddforeach() which allows for iterating through RowsI typically use this method when I need to iterate through rows in a DataFrame and apply some operation on each row For example inspecting string values and do string replacement or string formattingThere are performance implications around this method Its best to not use this method for a large number of rowsThe example below is simplified for brevitySample Call: Output: In this instance print outputs each row of the dataframe to the cluster log They dont show up in the command outputApplies a function f to each partition of a DataFrame rather than each row This method is a shorthand for dfrddforeachPartition() which allows for iterating through Rows in each partitionThis section covers methods that are useful for data preparation workReturns a new DataFrame containing distinct rows in the DataFrame being referenced Its useful for performing SQL UNION vsUNION ALL In this case you would perform a UNION between two dataframes and then do a distinctSample Call: The call below will return total row counts as each row in the DataFrame is unique
2f5kc8y7Ea24cX28tPkMsV,Do you know whats great? Programming I love programming almost as much as I love flying and I have been obsessed with that for the last 24 years of my life There seem to be few endeavors which allow you to quickly prototype test re-design re-test and reorganize your work with little to no cost and with a feedback loop measured in milliseconds Combined with test driven development it allows system-minded thinkers and tinkers like myself the ability to really explore and create at the speed of our mindsMy day job however is not programming; it is data analysis Actually it might be more accurate to say it is developing analytic systems and data flows My primary tool is Alteryx and it is a wonderful data analytics platform The core product greatly simplifies munging and analysis while also providing seamless handoffs to code-based tools such as Python and R for the really advanced stuff It also has numerous SDKs so the coders among us can extend its functionality as needed when the core product either cannot do something or can only do it with great complexity and fragility I enjoy using Alteryx and I believe it helps me be very successful in my career Yet I believe it can be better and we can look to modern programming environments for inspirationAt the Alteryx Inspire conference this year we were challenged to Alter Everything I decided to answer that challenge in my own way: by altering Alteryx itself I want to see Alteryx become more than a beautifully simple and extremely powerful analytics application I want to see it become for data analytics what Visual Studio is for Net; PyCharm is for Python; and IntelliJ is for Java Dart and a whole host of other languages I want it to be an integrated development environment with developer-quality tools worthy of enterprise-level analytic systems development I want a tool that makes my day-to-day even easier than it is now so I can focus on building the most rigorous analytic processes possible rather than becoming distracted with the limitations of my toolsetSo what does this mean in the context of Alteryx and data analytics? I believe we can look to our brothers and sisters in software development for the answer: tests and refactoring Tests particularly those derived via test driven development tell us when the behavior of our system has changed The better our tests the better we can identify when we have broken something by our changes Ill be honest this is a tough nut to crack in data analytics and I have not found an answer I am satisfied with yetRefactoring is the changing of the design or architecture of our analytics without affecting the results they produce It is this process whereby we clean up our design and make it easier to maintain It keeps us from having to rebuild from scratch 2 years later because our once-beautiful analytics turned to incomprehensible spaghetti It is this challenge I want to solve I want to make the refactoring of our Alteryx analytics as easy as dragging a tool onto the canvas Refactoring along with some basic testing facilities already provided in Alteryx is what will give us a developer-quality toolset for building sustainable and maintainable analytic workflows of the futureTo that end I have begun exploring possible solutions in a project I call Refactoryx I am starting with basic refactoring operations and adding increasing layers of complexity to end up with a toolset that if not directly integrated into Designer can seamlessly sit alongside it It has grown a lot since I started almost two months ago At the moment I have a codebase of 4500 lines of code comprising over 40 classes and supported by over 220 unit tests But more important than some random statistics about the codebase is what it can doand perform other smaller refactorings not demod hereThis is my starting point onto which I want to continue adding more complexity I want to keep expanding the types of refactorings the tool is capable of But more than that I want to change how we work I want to see field renaming operate across multiple workflow files macro operations take on more of a project context than a specific workflow context and start introducing the idea of cohesive libraries or project folders Overall I see the direction evolving to support collections of workflows that comprise an analytic system rather than only single disparate workflows These systems would be comprised of many workflows performing single logical tasks made clean and maintainable by splitting chunks of logic into humble little macros This environment is the one I work with in my day job and is the environment where I most often miss the refactoring tools software developers take for granted It is my belief that we the community of Alteryx data analysts and engineers should be able to take them for granted too
CDDWe8wSHpdG8N5UNgjoRv,Most of you might have heard of Apache Spark and PySpark and might be wondering how to execute a simple Spark job In this article I will take you through the steps which will help you run a Spark Job on an Amazon Web Services EMR clusterBefore I get started let me give a very brief introduction to a few of the terms I have used above for readers who may not be awareApache Spark: Apache Spark is an open-source distributed general-purpose cluster-computing framework Spark provides an interface for programming entire clusters with implicit data parallelism and fault tolerance Originally developed at the University of California Berkeleys AMPLab the Spark codebase was later donated to the Apache Software Foundation which has maintained it sincePySpark: The Spark Python API (PySpark) exposes the Spark programming model to Python To learn the basics of Spark I would recommend reading through the Scala programming guide first; it should be easy to follow even if you dont know ScalaAWS EMR: Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark Apache Hive Apache HBase Apache Flink Apache Hudi and Presto With EMR you can run Petabyte-scale analysis at less than half of the cost of traditional on-premises solutions and over 3x faster than standard Apache SparkPrerequisite: 2a Navigate to Key Pairs: Services -> EC2 -> Network Security (in the Left pane) -> Key Pairs • b Create a key pair (Figure 1) • c Save the demokeyppk file to your local driveAdd PySpark script to the Jupyter Notebook • Import PySpark library required for this demo • Create a Spark Session • Create a list of strings (I have listed some inspirational quotes) • Distribute the data across multiple nodes to enable parallel processing • Print the length of each string contained within the list
TUHgAPsLWprfwGiJyBFf6A,On the surface it may seem like a lot has changed in recent years in regards to data collection storage and warehousing The introduction and takeover of NoSQL Big Data Graphing and Streaming technologies may appear to have changed the landscape but there are some fundamentals that remainIn my current role we use Amazon Redshift for our data warehousing However whether we built a traditional data warehouse using Oracle or a data lake in Hadoop the core architecture would remain the sameThe core architecture boils down to some preprocessing and three separate areas (schemas if youre using redshift) called Staging Master and Reporting In this post Ill talk through each in detailUnfortunately not all data is created equally but its data nonetheless and therefore carries valueIn order to deal with the complexities of external data some pre-processing is almost inevitable especially when collection from a number of different sources The main goal of the pre-processing step is to get the data into a consistent format that can be loaded by the data warehouseThis includes but isnt limited to: Once ready you will need a central place to put these files ready for loading into the data warehouseOne example could be putting all files into an Amazon S3 bucket Its versatile cheap and integrates with many technologies If you are using Redshift for your data warehouse it has great integration with that tooThe staging area is the bread and butter to any data warehouseA good data warehouse takes data from many different sources Each data source comes with its own nuances styles and naming conventionsThe staging area is the place to bring all this in  most likely from where you put it after pre-processing (but not always)  and store it transiently until its processed further down the lineLike the loading area at an actual warehouse The place where freights unload isnt the final destination or final form of the materials or products Its just a holding areaIt simply allows you for the first time to have all the data within the bounds of the warehouse ready for further processing and modellingMy personal opinion is that data in the staging area should be as close to raw as possible (again you have to make some changes when pre-processing but that shouldnt change what the raw data tells you) You may even want to keep the original column names and table names the same This makes it easier to trace back when investigating or reporting issues in the sourceThe staging area should also be seen as transientYou should retain data for a selected period of time in the staging area after which it should be purged For example you might keep a rolling window of one months worth of data in case of failed loads or any other investigationThis is the last point where the data should be thought of as raw From this point forward the data should conform to the standards of the data warehouseThe master area is where the incoming data takes some real shapeThe master schema should contain correctly modelled tables that are appropriately named Column names should also be corrected along with their data typesThis makes the understanding of what tables are and what they hold easier inherently improving usability Just like the old school filing of documentsWhen moving data from Staging into Master thought should go into cleaning it things such as: I would also give some time to ensuring that column names for columns that join matchFor example if you have user data from some web logs your user data store in MongoDB and maybe some advertising data about users These sources will all hopefully contain some unique user identifier However they might not all call it the same thingBy standardising the names of columns it then becomes so easy for you or any other user of your data to intuitively understand what data can be joined togetherAs a data engineer this is the ultimate end goalYou have data that is clean and appropriately named to match the business language modelled correctly ready for any exploration or calculations to be done downstreamThe base work is done Weve prepared and ingested modelled and cleaned We now want to expose our shiny new data to the world This is where the reporting layer comes inAt this point if you are using a row based data warehouse in Oracle you might build some fact tables and data marts This is a perfectly reasonable use case for the reporting layer as you can stick any decent reporting tool on top of it and youd be good to goHowever some of these traditional data warehousing techniques have efficiency in mind the suit row based storage solutions like Oracle These systems are efficient at joining data but rows with many columns are inefficient mainly because of the row based approach having to deal with the entire row even if only a few columns are required for the queryIf you are using a columnar based data warehouse like Amazon Redshift your approach should be different Redshift doesnt mind wide tables and denormalising dimensions and facts onto one table is preferred over multiple dimensionsThe benefits of modelling your data this way when using Redshift include: For example lets say you want to report on your customers You have a customer table an order table marketing log table and some web analytics data within your squeaky clean Master layerIn Redshift within the Reporting layer you would build a Customer table This would hold any standard customer data (minus their personal details as they shouldnt be required for reporting) such as their registration date maybe a postcode etcYou could pull in whether they registered on a mobile device or maybe whether theyve installed your smartphone app or desktop appYou could join the order data and build some fact columns such as total spent to date first order date last order date number of ordersThe marketing table you would do the same and create some relevant facts like number emails sent opened and clicked etcFrom the web analytics you might pull in their last website visit date their favourite device most common device type (desktop mobile etc) etcYou get the pictureAll of this results in a super wide customer table with all the relevant dimensions and facts Your analysts can use this to calculate everything from acquisition rates shift in device usages across your customer base high value customers (and any commonalities among them) customer churn and engagement and a whole lot moreAll of this from one place with no joins and most of the heavy lifting done as it should be done using the power of the data warehouseData warehouses dont tend to be cheap and are literally designed to crunch data Make the most out of it do as much as you can here Free your analysts to dig out insights instead of waiting for the less beefy reporting server to do the hard workYou might find that more than just the analysts are willing to use it if you make it easy and quick enoughFollowing this simple approach I believe you can build a fully functioning data warehouse thats not just easy to expand but also understandYou may want to think of your Staging Master and Reporting layers as logical things This may work for you I prefer to keep them physically separate as it not only feels cleaner but allows you to restrict what the end users can use and see from the previous statesLike what you read? Click here to receive articles like this every month
XYdKYxhF7D4T34BScMdteN,Writing code is easy yet building software is hard It doesnt matter how good your code is if the underlying architecture isnt right youll still be left with bad software Maybe not today Maybe not a few months from nowBut eventually youll be left with bad softwareIn this post Ill provide you with another tool for the toolkit that will help shape how you think about building software to ensure its something you can be proud of in years to comeAn Event-Driven Architecture is as its name suggests a way of building software so that events drive everythingIts based on the notion that if everything is an event we then build things that respond to those events and maybe generate further events of their own Its a fairly simple premise but can often be a paradigm shift from how we usually design softwareI can bet that most of the time when you think about building software you think of things in a Request Driven way Dont worry if you do youre not alone I do too In the rest of this post though Ill teach you how to think a different way that will offer you a new perspective when building softwareFirst of all lets define what the usual Request Driven world looks like Lets imagine a simple website that is made up of a User Interface (UI) an API and a Database A fairly common pattern in the modern worldWhen a user interacts with the website lets say theyre creating an account they enter their details and click the sign-up button This event triggers a request to the API It requests that the API create a user with the submitted detailsThis event triggers the API to submit a request to the database to create a new record in the user table The success or failure of the database request is then passed back up the chain to the API who then passes this back to the UI which can then decide the appropriate action based on the responseNotice how often in this scenario that its an event that is the trigger for the request to take placeThe way we think is often Event Driven by default We just dont realise itNow lets look at making this website truly Event-Driven To do this we need to remove the requests out of the scenario Remember that the goal of an Event-Driven application is that the events drive as much as possible If thats the case then we cant have a different part of the application making requests too otherwise they would also be driving thingsSo lets remove the requests from our scenario A user fills in their account details on the website and clicks the sign-up button Now we simply generate an event lets call it a createUserRequested eventWe then need something to react to that event lets call it the UserCreationService This service reacts to this event by creating a user in the database Once done it generates another event called createUserConfirmedThe UI now reacts to the createUserConfirmed event and again takes the appropriate action and the cycle is completeThe change is subtle but its an important one Now our services no longer demand and request things from each otherIn fact they dont even need to know that the other exists at allNow youve built an application with components that are no longer tightly coupled Each component goes about its business raising events when an event happens and then listening and reacting to other appropriate eventsLets take a look at how this subtle difference can have a huge impactThe first and hopefully most obvious benefit that I just touched on briefly is that the components of the application are no longer coupled together This decoupling first of all requires that logical components are physically separatedThis separation ultimately means each component is now easier to maintain and build upon going forward The UI no longer cares about the API it cares about the createUserConfirmed event that the API (or userCreationService) generates It doesnt care where that event comes from meaning you guessed it it can come from anywhereYou now have full flexibility to update change break out or even outsource your userCreationService The UI doesnt care all it needs to do is tell the world that a user needs to be created and wait for someone to reply that its been doneNow were into the interesting part This in my opinion is the most important benefit and one that will unlock the most potentialSince the services are declaring that something has happened (like a user requests to be created) instead of stating what should happen any other service that cares about what has happened can also respond to the eventLets go back to the user creation example The UI raises a createUserRequested event and as before the UserCreationService handles creating the user and raises a createUserConfirmed eventThe UI can listen out for this confirmation event but so can any other service Lets say you want to send a welcome email to new users You create a WelcomeEmailService that also waits for the createUserConfirmed event and when detected sends an email to the new userWe could also have a DataWarehouseService that copies the new user to the data warehouse and a CRMService that sends the user data to the CRM tool All of these things can happen simultaneously totally independently of each other yet triggered by a single eventWeve now successfully moved away from having a god-like API One service that used to know how to send welcome emails and send data to the data warehouse or CRM tool It was therefore tightly coupled as it had to know how to make requests to all of these services so any change in one could break the otherWhat were left with is a loosely coupled system where any service can be upgraded or swapped out without any impact on any other serviceNow this might look to you like more services to maintain instead of just the one I can assure you as a software engineer that I know which scenario Id prefer and its the multiple smaller services optionUp to this point this post has been fairly conceptual In the final part Ill talk through a common technology choice for implementing an Event-Driven architectureIve written about Apache Kafka before in my post What is Real-Time Data? which is a great place to start if you have no prior knowledge of Apache KafkaIts a highly recommended tool for event-driven applications mainly due to its reliability flexibility and scalability (all of which I discuss in the above post)The basic premise of Kafka is that you can send messages via queues (called Topics) Kafka deals with all the complexity that comes with managing queues ensuring high throughput of data etc All you need to do is create Producers that add messages (events in our example) to a Topic and Consumers that read these messagesSo lets see how youd implement itGoing back to the user creation example The UI service would act as a Kafka Producer and create userCreationRequested events on a Kafka Topic You could have a single topic dedicated to these events lets call it user-creation-requestedThe UserCreationService would first of all be a Consumer that listens to the user-creation-requested topic and for each new message would attempt to create a user with their submitted details On success it would then also be a Producer that adds UserCreationCompleted events to a separate topic lets call that user-creation-completedOur UI service would now also act as a Consumer and be listening to the user-creation-completed topic awaiting confirmation that the user has been createdOur WelcomeEmailService would also be a Consumer listening to the user-creation-completed topic and for each new message simply send a welcome email to the userWhere Kafkas separation of producers and consumers comes in really handy is when services failImagine that the WelcomeEmailService is down and were using the Request Driven architecture The API would have tried to send the welcome email failed because the service was down then what? Log an error and continue maybe? Retry once or twice possibly? Either way the user wouldnt have received their emailBack in the new and improved Event-Driven Architecture Kafka is great for dealing with this problem Each Consumer is in control of what they consume and how fast they consume it Kafka simply stores an ordered list of the messages and each consumer reads each message one by one and remembers their place in the queue This means that even if the WelcomeEmailService was down for an hour once restarted it would simply pick up where it left off from sending all outstanding emails to users that were created in the last hourNo other services are affected they dont have to care that the WelcomeEmailService was down they can continue consuming the messages at their normal rate The WelcomeEmailService can catch up every user gets their welcome email and we all lived happily ever afterHopefully this sparks a different way of thinking when it comes to designing your next application The benefits of Event-Driven in my opinion heavily outweigh those of a Request Driven and once you get the bug Im sure youll never look back
YNGF2NMCv9pRDJr2MDiUhu,As Mapan business environments get increasingly volatile our data architectures must be flexible and adaptable to a large number of data in various data formats conditions Besides by having 25 million members we realized that its essential to protect individual privacy To overcome this common challenge we decided to adopt data modeling techniques and data security policies in responding to build our data layersLets take a look at how we taking care of our data! • Raw data refers to any data object that hasnt undergone thorough processing either manually or through automated computer software (based on Techopedia) It can be in the form of files visual images database records or any other digital dataAs time goes by Mapan doesnt only have RDBMSes (MySQL PostgreSQL) as data storage but also have APIs Kafka messages data transaction logs and Google Sheets in which we call it as raw data • Data Lake is a centralized repository contains data items simulating the behavior of raw data in the transactional data pipeline Data Lake retains all data types and schemas the way the data is stored in the previous layer We can store our data as-isTo make it easier to aggregate data items in various data sources in Mapans raw data we store data items to Data Lakes including both totally unstructured and highly-structured data Thats why we need unstructured storage like Google Cloud Storage or Amazon S3 to store our data in any file formatThis level of freedom makes data lakes highly adaptive places and allows for a broader range of analysis on the data thats stored in them Specifically data lakes allow Data Scientists to analyze data that wasnt previously accessible ie logistics tracking customer support call notes etc • Data Warehouse contains data items extracted from varying data sources in Data Lake having undergone data cleansing and transformation to meet the requirements of data users In Mapan we have three sublayers as representative of Data Warehouse and we use this layer as a single source of truth(i) Data Vault is a hybrid approach that combines the best of 3rd Normal Form (3NF) and dimension modeling This data modeling technique enables historical storage data coming into the database by separating the data to three main components which are Hub Satellite and Link tables(ii) Dimensional model is a data structure technique optimized for Data warehousing tools Its designed to read summarize analyze information and comprised of Fact and Dimension tables(iii) Integration Layer is a combination of semantic reporting and analytical technologiesIn this third layer we also have Data Health Check Collections which contains anomalous data items which not fulfilling completeness accuracy and consistency in both transactional and analytical data pipelinesInspired by DAMA UKs white paper on Data Quality Dimensions Mapans Data Governance Team created a Core Data Quality Dimensions as a measurement of data quality These collections will be used as parameters to monitor the quality of raw data by running a Data Quality script in the Data Lake environment • Because Data Warehouse collects data from the entire business its important to restrict control of who can access it Additionally querying data directly from Data Warehouse is a difficult job for anyone who doesnt know about Data Warehouse concept So we decided to create the next layer of Data Warehouse to transform information into specific insights; Data MartWhile a Data Warehouse is built to store data from the entire business Data Mart is built to fulfill the request of data users It often is seen as a small slice of the Data Warehouse Therefore we can use Data Mart to isolate  or partition  a smaller set of data from a whole to provide data access for the end consumerHow we represent the information to data users? We use Metabase and Tableau as analysis tools for business users or operational users to make it easier whether to querying or viewing dataSince we need to protect our Agents data privacy and develop a better way of working Mapans Data Governance created data protection and sharing policy to ensure only the agreed parties can access the agreed data items within the agreed time frameData User: Intends to request data items to perform either operational or analytical tasks in their local environment They can only access the data based on the Point of Difference (POD) between roles such as internal pods like the operational user and external pods like company-levelData Engineer: Maintain & continuously improving data infrastructure so it can provide data across our product to Data Lake storageBusiness Intelligence Engineer: Maintaining ETL processes to improve the capacity of our Data WarehouseData Science Engineer: Develop and implement data products eg regression engine classification engine and recommendation engine They usually use tables in Data Warehouse to build the product but if they need raw or denormalized format they can also access the Data Lake layer
XRtDgpCLjJodAqjMBwVhoX,In a data engineers life there is a chance that we have to work with small files tons of them We dont like tons of small files for our extracting transforming and loading job especially combining with our choice of big data engineering infrastructureIn one of our task we have this use case: a third party vendor uploads one single zip file in a size range of 5GB to 20 GB on a weekly basis The single zip file consists of around 350K smaller size text files After we obtain the zip file we need to do the following tasks: In addition over years there are more than 2500 zip files stored in Azure Blob Container They are needed to be loaded into Azure SQL DatabaseIn a summary we have 2500 10GB  20GB size zip files to extract transform and load into Azure SQL one time and ETL one single zip file on a weekly basisTo give a concrete number it took us 48 hours to ingest 689K JSON files to Azure Databricks Table with a Spark Cluster with the configuration of a standard Azure DS13-v2 machine (56GB memory 8Cores and 2DBU 5 workers)What can be effective and cost efficient way? In my word effective means less code less manual Azure Resources creation and management efficient means it better be faster cost efficient means less Azure Resources used or less time while use those Azure ResourcesId like to share the approach which I think its quite nice and efficient The principal of this approach is to use streaming for unzipping and streaming for ETLOf course this diagram only shows when a new zip file is uploaded to our blob container While dealing with legacy 2500 zip files we have manually created Queue messages for Azure Storage Queue so the following process can get goingWith this pipeline we are able to ingest a zip file in 20 mins with one(1) K8S pod for unzipping process and thirty-two(32) pods for text-2-json process*disclaimer: Our real pipeline is slightly complicated than what I have present but the concept and data is close
8kenH6XMM4QJzjM3TZu3kJ,When I first started as a data engineer writing stored procedure on SQL Server to process data is my primary job After months of writing SQL I picked up a few tips about improving query performances
A25aBat83g4wMVAHcTQdME,The first time I heard about Cloud Data Fusion (CDF) I thought of it as a good tool that would have helped me in past projects where it was required to create Spark jobs on Dataproc and orchestrate them At the time we used an external tool to create the cluster run jobs and delete it after the jobs finished But with Data Fusion I could do everything using one single tool and create pipelines with an easy user interfaceMore recently after discovering CDF I had to put my hands on it working on a client project Ill present my experience with it and how we used it to secure Personally Identifiable Information (PII)Cloud Data Fusion is the fully-managed integration service provided by Google Cloud that helps build and manage ETL data pipelines in a code-free manner It can be used in hybrid and multi-cloud environments and has both batch and streaming capabilities (Enterprise Edition)Cask Data Application Platform (CDAP) is the open source project behind Data Fusion that allows data pipeline portability that means no vendor lock-inAs Data Fusion is relatively expensive to play (180 USD/hour + Dataproc prices) I used and recommend Qwiklabs to help practice pipeline creation and get a good overview of its interface Qwiklabs will provision your own environment and you will not pay for it only for the lab itselfHowever it isnt just pushing data from one side to another Data Engineers need to be careful with the type of data going through pipelines Personally Identifiable Information (PII) requires additional treatment for data security and storage location Always question who should have access to it what type of access and how sensible the data is Always follow the least privilege principle allowing minimum access to itIt is common nowadays to ingest PII and use it in analysis to help decision makingHashing or masking data is the concept of one-way transforming the PII into something non-identifiable for the users while trying to maintain its value Different algorithms and methods can be used for this purpose and the general idea is exemplified below: Suppose we receive the following data through our pipelines: After hashing the PII we would transform it into something like: And thats it! Sensitive information is hidden while being ingestedAn additionally secure step would be to append a salt at the end of the data before applying the hash function We can use a random generated string for example and secure it from people trying to break the hashed dataIn that way we extend the length and complexity of the hashed data making it harder to crackSo our approach is to hash the data and store the real value separately and replace the original data to be used on the Data Warehouse with the hashed value Confusing? It is simple like the diagram below: If the PII is not used we shouldnt keep it However in some cases we are required to store it For example we might need to send an email back to the user but only the application should have access to the users email and not the analysts themselves That is why in this example we are storing the PII separatelyIn this solution we assume for simplicity that data lands on Google Cloud Storage (GCS) and well use it as our initial data source It could also have originated from many other sources supported by Data Fusion Also we will use BigQuery as our Data Warehouse for data analysisCloud Data Fusion offers a Wrangler component that helps clean filter and transform data using simple directives Everything you do on its interface translates to a recipe You could for example add/remove columns parse data and sum columns It is a good visualisation tool for Data CleaningIn our solution we will use the SHA-256 function to hash the data The generated recipe will be: For additional security we will append a salt stored in CDAP Secure StorageCDAP exposes APIs to diverse operations like managing and scheduling pipelines One of them is Secure Storage It is basically an internal storage that encrypts data upon submission and can be controlled via RESTful APIs That is where we will keep our salts and store them by making PUT requests on the Create Secure endpoint for every PI type (email credit card) Example: So we will do it for both for email_salt and credit_card_salt: And now we can use them on our Wrangler with the ${email_salt} or ${credit_card_salt} variablesObs: This is a proposed solution We could also use Google Data Loss Prevention (DLP) to automatically detect PII and mask the data for us but we opted to use Wrangler directives and internal Secure Storage to save costsWhen architecting and designing solutions you will find that it is impossible to design one perfect architecture without any drawbacks There is always a trade-off between using one component over another The same applies to the one proposed in this article and it is important to exercise what can go wrong and what its limitations are Dont be too attached to what you designSome of the drawbacks with this design are: Cloud Data Fusion proved to be a good tool to democratize Data Engineering for non-technical people They can do simple data engineering tasks requiring no deep knowledge on the field CDF is recommended for users with lack of code experience and is ideal for creating and orchestrating simple pipelinesNothing is perfect The easy user interface has some trade-offs Specific and customizable pipelines can be troublesome to create and Cloud Data Fusions price can surprise small companies trying to invest in the data engineering area Additionally CDF was recently released (GA in November 21 2019) making it relatively new in the market but with promising potential and a great feature roadmap aheadWhile I venture myself in other cloud areas and products if I someday come back to check how Cloud Data Fusion is going I would like to see it provisioning other types of jobs as well like Google DataFlow and BigQuery jobs
UvLsiyytHdcdnT78TNhpx7,Id like to demonstrate streaming files with PySpark using the Databricks platformThis is done using 2 notebooks The first one shown below is to continuous create new files to be streamed therefore it should be run first The data will be Apple stock data and we will get the data every second for 5 seconds then place the data in to the directory /FileStore/tables/streamingStock/ for streaming This code will keep running until you stop it manually This will simulate a continuous stream of new data that needs to be streamed by the second notebookNow for the second notebook we will take a look at the streaming code First we will set the schemas we expect Then we will set the input path /FileStore/tables/streamingStock/*csv This is all done belowNext we will set up the stream and check if it is running The schema we set before is used in schema maxFilesPerTrigger simulates streaming one file at a timeNow we can view the streaming in real time and write it to memory which can be accessed with what we set as the queryNameNow we can view some stats of a static dataframe at the point in time of running the below code by using calling what we set as the queryName We can also do some filtering or any exploratory analysis of this dataThis ends my demonstration of streaming files
37sueNaKoqPzNbtiQT5d9A,6:05 AM: Alarm jolts you abruptly away from that REM thing thats so important Maybe you should buy the sleep tracking ring your roommate keeps raving about • 15 AM: Finish your morning scroll taking note of how much more productive tan and talented the people you follow on Instagram seem than you • 53 AM: Realize in a panic that you have 7 minutes in which to dress eat pack and be out in the door in time to catch the last BART which will arrive at your destination in time to catch the last company shuttle • 15 AM: Board said BART in winded/disheveled state Take out copy of The Seven Habits of Highly Effective People Today is the first day of the rest of your life! • 16 8:12 AM: Sleep on BART with no thought towards consequences of leaving backpack full of cherished belongings unattended on seat beside you Drool a little • 13 AM: Hastily stuff unopened Seven Habits into aforementioned backpack and rush off train to catch shuttle • 33 AM: Settle into seat in cubicle Notice all the alerts on Ambari are flashing red • 42 AM: Multiple nodes went down due to thermal fault overnight Ambari suggests Components require restart • 43 AM: Obediently you click RESTART Popup window doubts your choice: Are you sure? Imposter syndrome intensifies • 46 AM: Decide yes you are sure • 55 AM: Peace is restored and green indicators reign • 02 10:04 AM: Read Medium article on Pomodoro technique Turn phone off close email and Skype Well on your way to becoming a 10X Developer You complete yesterdays JIRA ticket and fix that intermittent bug in your PySpark job • :05 AM: Glowing with accomplishment you turn Skype back on Angry messages flood in You forgot about the 9:30 standup This is the fault of the Pomodoro technique surely • :15 AM: Check your phone Youve missed two spam calls and an email from your alma mater asking for donations Its only been two years since you graduated They should give you a chance • :16 10:30 AM: Get a coffee from the break room Scan hopefully for others that you might have a corporate banter with No luck Return to cube dejected • :30 11:10 AM: Attend planning meeting for upcoming project Resolve to inject discussion with some incisive ideas and suggestions for the proposed new architecture • :11 AM: Chicken out • :12 11:30 AM: Internally berate self Resolve to lean in more next time • :35 AM: Go for jog with group of colleagues Get winded but hide it behind short but enthusiastic Mmm! responses to discussion Foiled when asked to explain what Hadoop is • :52 AM: Nobly excuse yourself from run early in order to attend to some work • :33 PM: Purchase bagel turkey dog from workplace cafe Its fun to try new things • 03 PM: Regret bagel turkey dog • 15 PM: Seriously regret bagel turkey dog • 00 PM: Rally from bagel turkey dog experience Log in to Slack and confidently express intention to run end to end test on development cluster • 29 PM: Encounter ValueError: Object too deep for desired array Giggle to self • 00 PM: Continue trying to complete end to end test • 30 PM: Continue trying to complete end to end test • 35 PM: Teammate asks whether testing is done Reply testily • 00 PM: Continue trying to complete end to end test • 39 PM: Complete end to end test Disentangle sticky notes describing encountered bugs from each other Stick said sticky notes on Kanban board • 40 PM: Kanban board is looking a little heavy on the Pending side • 40 4:52 PM: Rearrange Kanban board to give positive if misleading impression of projects progress • 53 PM  5:00 PM: Make insightful and wise arguments in defense of your cases on various Reddit forums • 00 PM: Sign up for six different tech talks and events on EventBrite Resolve to improve networking skills and knowledge of the field • 02 PM: On Amazon purchase a number of technical books with detailed pencil sketches of animals on the front cover Youll start these as soon as youre done Seven Habits • 15 PM: Realize youve been staring into space for 13 minutes and are about to miss shuttle • 17 PM: Catch shuttle much to the consternation of beleaguered shuttle driver She doesnt like you anyway • 42 PM  6:40 PM: Sleep fitfully on BART with Seven Habits on your lap • 45 PM: Take Uber home from station because you are disgracefully lazy Uncomfortably evade drivers increasingly personal questions Worry that your caginess will harm your 489 star rating • 15 8:15 PM: Make avocado toast for dinner Talk on phone with friend whose company is about to IPO He talks about buying a house in the Outer Sunset next year Look dolefully at your avocado toast It would have been better if one of your two roommates hadnt used up all your balsamic vinegar • 16 PM  8:19 PM: Ponder whether East Bay really counts as Silicon Valley anyway • 20 9:30 PM: Watch Fyre Festival documentary Feel freshly emboldened and inspired by their inconceivable hubris Wonder if such inspiration is problematic • 35 PM: Shower and wash self with organic Lavender Whole Foods soap that was delivered to you by Amazon Prime same-day delivery because you are so so hopelessly lazy • :01 PM: Cuddle Seven Habits to sleep with visions of (finally) upgrading to Spark 20 dancing in your head
5LtHpY8RqnDCyUSVju6LDg,I have been working as a data engineer for the past three years and one thing that I have noticed is that there is a distinct lack of readily available resources for preparing for data engineering interviews This is probably partially due to the fact that data engineering as a field is not particularly well defined; the role varies from company to company and domain to domain sometimes tracking closer with software engineering and sometimes more with data science or analyticsWell never fear! The following is a brief list of the resources that I think can be most helpful for preparing for the technical round of data engineering interviews I intend for this to be a living document that I will continue to grow and update over time adding new resources that I come across as well as those suggested to me by others in the field I think preparation can be broadly broken up into these categories: Fortunately online resources abound for preparing for a technical coding interview Heres the rough flow I would take: Ah SQL Seems so deceptively simple Well SQL has tripped me up a LOT in the past I thought I knew it because I could do some INSERTS and SELECTS and knew how to do a basic JOIN or two How naive I was… dont underestimate SQL! That being said it is not easy to find resources for truly tricky SQL problems to work through Here are some I like: Many companies will expect you to be able to design a proper data warehouse given a business use case Luckily there are quite a few resources available out there for brushing up on these skills While a lot of them are more textbook-like try to push yourself to actually work through some real-life use cases by designing a data warehouse for an online store a peer-to-peer marketplace or a rideshare application Sketch out the schema on paper or a whiteboard and build up a process for starting a new problem or design Here are some books and online resources for learning about data warehouse design: Some companies will expect you to have greater experience or familiarity in big data technologies (think Hadoop Spark and event processing technologies like Kafka) than others will There are many books available that cover these technologies in-depth but unfortunately due to the rate of change of the industry they tend to become out of date quite quickly Nothing really beats experience here but the following are some good resources for getting started and/or sharpening up your skills in preparation for an interview: Dont neglect the soft skills portion of the interview either! Good communication and problem solving skills go a long wayI hope this outline has been helpful as a starting point for preparing for your data engineering interviews I also recommend the subreddit /r/dataengineering for great conversations and a helpful community of both newbies and experts in the field Good luck and please let me know in the comments which resources you found helpful or not-so-helpful in your own interview process I would be happy to include them in this article
VRCMFjjmykzMvSXxDr3xEE,Tech Book Talk (the new #tbt) is a monthly (or at least thats the plan…) series where I will review a technical book or text Ive finished in the last monthThe book I selected for July 2019 is Designing Data-Intensive Applications by Martin Kleppmann This is a tome in the OReilly family of technical texts and therefore I had high hopes Spoiler alert: It didnt disappointWho should read it: Professional software engineers and computer programmers especially those starting out in the field (yours truly) or those transitioning from single-node to distributed systemsWhy its great: The book covers a lot of ground but prevents information overload by delivering its messages in concise and clear termsWhere it could be better: Some chapters could benefit from more lead-in to a new topicDesigning Data-Intensive Applications promises to give the reader a (hearty) introduction to the principles of building fault tolerant consistent and robust systems for handling large volumes of data on distributed systems Notice I said large volumes of and not big data? In the preface the author expresses his reluctance to use the term big data: Many of the technologies described in this book fall within the realm of the Big Data buzzword However the term Big Data is so overused and underdefined that it is not useful in a serious engineering discussionDespite this disclaimer in my opinion the book manages to neatly (although indirectly) address many of the inherent challenges associated with the notorious Vs of big data: volume velocity veracity and variety (try making a useful mnemonic out of those!) In my reading of the book I felt that the information presented could be most easily applied to the first three Vs To wit: Volume is addressed in terms of the growing need to handle very large data sets and especially to safeguard against loss or corruption of those large data sets The author talks in detail about replication and partitioning and compares and contrasts common approaches to storing vast amounts of data in a scalable and accessible wayVelocity is highlighted in the need for users to have quick responsive systems to interact with retrieving and writing data with minimal latency or downtime The author explains situations where large-scale distributed systems can hinder or complicate data reading and writing especially in multi-user transactional systems (These sections were some of my favorites since I felt that the challenges of read/write asynchronicity in particular were well-illustrated and his explanations of potential mitigation plans very clear)Veracity goes hand in hand with velocity here; the author shows that in some cases there is a trade-off between data that is guaranteed to be accurate and up-to-date and data that is guaranteed to be low-latency Different systems might have different needs here and the author highlights a few different techniques that can be used when either veracity or velocity are paramountWhat the above information gives you is a foundation for designing a distributed data system The book does not promote one technology or design over any other in outlining how to design said systems; while it does make reference to some of the options available it is not trying to evangelize a particular solution Rather by fully outlining the pros and cons of various implementations and setups it gives the reader a starting point for deciding how they should make decisions for their own unique applications and data requirementsIn a nutshell here is what I loved about the book: One of the chapters that stuck with me most is Chapter 8 The Trouble with Distributed Systems Before reading this book I had only a vague understanding of the downsides of distributed systems It seemed to me that the improvements in data resiliency and service up-time would completely engulf any downsides But reading this chapter made me realize how naive that assumption was I particularly liked this passage: There is no fundamental reason why software on a single computer should be flaky: when the hardware is working correctly the same operation always produces the same result (it is deterministic)… An individual computer with good software is usually either fully functional or entirely broken but not something in betweenThe chapter goes on to elucidate some of the problems that are unique to distributed systems and that catapult them from the deterministic behavior of single-node systems into something more complicated I was struck in particular by the descriptions of what can go wrong with respect to clocks While I have had some personal experience with the headache of dealing with unreliable clocks and conflicting timezones I didnt realize just how badly things can go wrong until Chapter 8 When dealing with very time-sensitive systems something as minute (no pun intended) as the innate differences between quartz crystal oscillators on each node need to be taken into accountWhen it comes to the bad there isnt much to report One thing I would have appreciated is a little more lead-in to the topics at the head of each chapter When a new topic is introduced the book often jumps into the thick of it right off the bat and a longer introduction could occasionally be warranted In particular the Partitioning section goes straight into methods of partitioning and I think it could be well served by running through a brief recap of partitioning and potential use cases beforehand especially as the book can so easily be used as an a-la-carte referenceThe book does not contain any lengthy code samples or instructions on implementing any of the tech it discusses It is more focused on theory so this book may not be the top choice for people looking for a step by step guide or more hands-on contentAll in all I think that this book is not only an excellent resource but also one of those rare things: a highly technical book thats also readable cover to cover
WHSWzVTLAo5ELywSBBDAAp,When your organization practice collecting (almost) everything first and (do) process it later it has to be a time when you need to analyze records that has been sitting in storage for a prolonged time The size of the records could reach multi-terabytes with billion of entries This is when the traditional approach of data processing is at its limitRecently I need to analyze a considerably huge clickstream data with the queries related to device and browser As a python developer my first step is to locate package that can parse user agent from the clickstream and returns the device OS browser and related information I found the right package that can do the job with this library: Python User AgentThe next step is I want to find out how fast this package can parse the user agents so I can estimate the time if I were to run it on a billion records I use Jupyter to prototype my steps (This run in EC2 mlt2large should it matters) With 10k sample combined with timeit magic I can roughly get the average iterationThe average iterations per second is 69930 but Ill round it to 700 Lets do a quick maths here if I were to parse a billion entries: I probably required to have 49 parallel execution to complete the parsing within 8 hours Thats only part of the story I also need to code for scheduler and do the testing for reliability Give or take might cost me few days to finally see the result Using existing tool that can solve this problem would be much more desirableFortunately we can accomplish this task without sweating by using Apache Spark In AWS apache Spark can be found in ElasticMapReduce (EMR) service You can spin on demand cluster from EMR and accomplish the distributed computation requirements without the need to build anything from scratch For cluster configuration we can use Task Node only (well use s3 as HDFS storage) If this is a repeating operation you can utilize the steps options when launching the cluster to automatically run the script and terminate once completedBut for our case we will be using a quick way to submit our pyspark parsing application (lets call this ETL job) through sparks-submit cli and some munging with zeppelin notebookBefore we code our ETL lets take a step back could it be other way to optimize our job? Parsing the user agent is a costly operation but join operation relatively isnt if we can parse only unique user agent and perform a join operation separately would it be faster? Lets find out the total unique entriesWow only 8M unique records out of 1B or roughly 09% We could theoretically speed up the process by 115X If you notice from Zeppelin notebook above the data is already partitioned and in parquet format By partitioning with right numbers and read from columnar storage this will naturally help speed up operation as well Well touch about partitioning later in this article but if youre interested to learn more I highly recommend this partitioning in apache sparkNow were going to prepare the unique user agent into a new file to make our ETL jobs faster by directly reads from parquet  Notice that I also include the count aggregation it might be handy we somehow decided to filter only user agent having certain count thresholdThe descriptive statistics from count above shows half of the entries has occurence below 2 Depending on the scenario you may want to filter out entries less than or equal to 2 or at extreme anything less than 844 Doing so can effectively reduce the total entry to be parsed by twice or 100X respectivelyNow that weve optimized our operation lets quickly prototype simple user agent parser from Zeppelin notebook In order to do so we need to understand the concept of Pyspark function or UDF I highly recommend you to read through this article and understand the concept first before proceedCopy our prototype of ETL script above into EMR Master server (SSH into it): $ ssh emr$ nano testpy$ spark-submit testOpps! We encountered user_agents module not found error The reason is we havent install the package before executing the submit You might be thinking by now how to install the parser package on our cluster imagine if the cluster has 10 task nodes do we need to login to each and every server and execute pip install? The answer is Yes and No both were possible to solve the dependencies but the elegant way is we can zip pip modules into a file and submit together with our ETL script Remember the Task Node we configured with spot instance? At any time AWS might shut it down and therefore the first option were not something we should go after# Lets package our module into a zip file$ pip-36 install pyyaml ua-parser user-agents -t dependencies$ cd dependencies$ zip -r /dependencieszip $ cd # you may choose to move this file to s3 if you need to submit job via livy# optional# aws s3 cp dependencieszip s3://involve-data-science/emr/dependenciesWith py-files option we can now submit the pyspark script and its dependencies$ spark-submit  py-files dependencieszip testBy now it should be pretty much straight forward to code our final ETLThe only part I want to comment is the repartition If you still remember the number of partition for the unique user agent dataframe is 25 I actually tried with this default number and I notice only 8 executors were used But with 36 vcores I can theoretically distribute to at least 36 executors Hence I used 3x multiplier to make the new partition size to 108 Practically from the test I ran this decreased the time to complete the task from 56 minutes to only 31 minutesAt this stage youll have a lookup file and can perform join operation with any pyspark dataframe that have user agent column with minimal cost For sake of completeness Ill demonstrate an example to join it with sales dataWith 5M sales records joining to our lookup dataframe and perform filtering of mobile device and amount aggregation only took 47 seconds You can now inform the business department the top mobile Operating System (OS) is Android with RM 3B of sales followed by iOSWe can do so much more analysis using this lookup however if youre a data engineer you may now pass the next stage to data analyst / data scientistCongratulation if you manage to read until here for final part I think its not complete without cost calculation Lets do some quick estimation how much Ive spent in AWS after all well need to explain whether by reducing the time from 16 days to couple of hours were justifiedIn term of cost we only use S3 for HDFS storage and EMR For storage Ive transformed original data of 1B ~ 1TB into a gzip compressed parquet files with new size of 180GBAll the tasks above should be completed within 3 hours but worse case it may takes longer when we take into account testing and time spent for troubleshooting so just adding it up to 8 hours which is common office working hourThe cost structure for EMR can be found here As for spot instance the price was $00654 per hour for m5xlarge($0048 * 10 * 8) + ($024 * 1 * 8) + ($00654 * 9 * 8) = $10As for s3 I will round up to 200GB to take into account new files that were created to store the lookup and probably some failed testing The pricing can be found here To calculate the usage for 8 hours we need to convert to GB-Month • 0*1024*1024*1024*8/1073741824/744 = 215215 * $0024 = $0Overall our cost should be no higher than $1052 (005 + 1047) By taking the optimistic estimation of 3 hours the cost down to only $395 or RM 1620All price calculations are based on Singapore region Ive prepared the spreadsheet for you to play around with the hours
hgVe7aXHtHR8JgtdtdHaz3,GCP provides an excellent set of managed services for processing large amounts of data with support for polyglot storageCloud Dataproc: A managed Hadoop service for managing Hadoop style workloadsGCS: Google Cloud StorageAlthough there are many references on how one can make use of these managed services and additional services that help in deploying the data processing artifacts There is no clear reference architecture for how one can organize their code automate the building of artifacts and then deployment of those artifacts into GCP environmentThis article tries to address that reference architecture for how to organize your pyspark data processing code base and also automate the deployment of all artifacts required for running the data processing jobAs the old saying A picture is worth a thousand words the picture above is self-explanator except for the place holder for any configurations fileIn my personal opinion and/or experience its better to create a simple python module that has all the configurations defined in a json format that is easily importable inside your code instead of as a text or conf fileThe only buildable artifact is the packaging of all dependencies into a zip file required for running pyspark job Other then that all remaining code requires to be pushed as is into different GCS bucketsA Jenkins job can be setup that pulls the source from GIT to create the zip file and then prepare the code to push it into GCS bucketsIn the Jenkins build machine setup a script that will download all 3rd party packages using pip into the directory where you already have your custom packagesIn the Jenkins build machine install gsutil to be able to push the artifacts into GCSThe following diagram shows end to end workflow once all the artifacts are built and assembled to be pushedI hope this article helps any developers or architects trying to figure out how to design a deployment strategy for their data processing jobs in GCP
iM7sLrjPS8agVq8MNxh2rE,"Brief introduction on Data Pipelines in Azure Data FactoryWe all know that data in its raw and native form is mostly stored in relational non-relational storage systems This data doesnt have any contextual meaning and cant provide insights to analysts and business decision makersData Pipelines are used to process this complex large data into actionable business insights Azure Data Factory (ADF) is a cloud service that is built for handling these operationsThe use case I have been working on relates to Machine Learning prediction of our companys incurred but not reported reserve This is done using the currents granularity as a baseline but eventually getting to more detailed reserves (ie  more frequent reserves certain medical conditions etc)This initiative involved migrating the data sources from the relational storage systems to Azure Storage accounts This involved storing the data in Blob Storage as well as in Azure Data Lake Generation 2 (ADLS 2) storageOnce the data was stored in these storage systems the data then had to be processed on This involved steps of feature generation and predictive modelingThe pipeline was divided into 3 stages: Landing Zone (LZ) Protected Zone (PZ) and Consumption Zone (CZ) The LZ was initial step to store the data from the on-premise system to Azure The PZ was the 2nd step which transferred the data from the LZ (Blob Storage) to the PZ (ADLS 2) The 3rd stage stores the feature engineered data read in from the PZ into the Databricks File System (DBFS)Every ADF pipeline can have a set of pipeline parameters These parameters can be used anywhere else in the pipeline For every parameter we have to define the name type and default value The different type of pipeline parameters include String Int Float Double Bool Array Object and SecureString Pipeline parameters are useful because we dont have to define the values each and every time We will see how this works in later sectionsGetting the data from an on-premise system to the cloud environment is surprisingly not as complicated as people would think it to be! A simple Distcp command from Hadoop File System (HDFS) allows us to migrate the data from our big data systems to the desired location on Azure Once the data is on the cloud it opens up new avenues since this data can be accessed by all services on AzureFrom an ADF perspective we can set up pipeline parameters The first step was to get the metadata of all the tables This was done because we wanted to check if the most recent data is present in ADLS 2 storage account If the data was present we delete the data and load the most recent data from the LZ into the PZDynamic Datasets: Dynamic datasets can be created by making use of the pipeline parameters We only need to define which pipeline parameters should be associated with that dataset and then only change the values in the beginning when we first start making our pipeline This proves to be useful since we wont have to define a different parameter names for every table we run the pipeline onThe parameter names in the picture above have the exact same name as the parameters defined for the overall pipeline The default values are then assigned by invoking the @pipeline()parameters functionThe Get Metadata activity needs to point to a dataset For this we have to create a dataset of type parquet and assign it a file path so that the pipeline knows which dataset to look at We also need to create a Linked Service which refers to the storage account the dataset is stored on This is where we make use of the pipeline parameters A pipeline parameter can be called using the following format: @pipeline()parametersAn important point to note is that the Get Metadata activity also has a field list which acts as an argument This enables the user to use these arguments in the next step The different options for field list include Child Items Exists Item Name Item Name Item Type and Last ModifiedIf this is used in the dataset properties above is referenced as a dataset parameter This can then be used when defining the path of the dataset@dataset()As you can see PZ_RECENT_DATASET_CONTAINER contains the value of the pipeline parameter This allows the the value to be treated as Dataset Parameter and can be used as neededHitting Alt+P opens a window where regular expressions can be written and this window also provides you the list of user-defined parametersThe next step is to check if the dataset is actually present or not in the location provided This can be done using the If Condition activity Each if condition activity needs to have an expression for it to evaluate on The condition should return a True or False value""@activity('Name of activity to be evaluated on')outputIn our case the name of activity will be Get PZRecent Metadata and field_list would be exists This is because in the Get Metadata activity we had specified that we should check to see if data exists in the location given Based on the value of the condition we can define 2 different activities one for True and one for False Pipeline parameters can then be used in those activities as neededEvery activity can transition to another activity based on four different conditions These are Success Failure Completion and Skipped There may be cases where we want the pipeline to continue even on Failure Or sometimes we may not care if that activity is a success or failure and it only needs to have completed successfully for the pipeline to go to the next stepThe Copy Data performs the actual operation of moving the data from source to destination One point to note here is the File Type Path If we want our file path to be dynamic we can make use of the Wildcard File Path This allows us to include all the files in a folder which have a certain extension For eg: *parquet will give us all the file names to be copied which have a parquet extension This can always be adjusted to suit your use caseLinked Service: This defines which service you want to connect to your ADF For example if you want to connect a Blob Storage to your ADF you will need to provide details like Access Key Storage Account Name Container Name etcThe last stage involves pushing the data from the PZ to the CZ The CZ in this case is the DBFS The Notebook activity allows us to trigger a Databricks notebook We need to create a Linked Service for connecting our ADF pipeline to the Databricks resource Once this activity is triggered it will start the notebook and execute all the cells in that notebook The notebook contains code for feature engineering and predictive modeling after which it is stored in the DBFSThis was a very brief insight on ADF I tried to cover as many points as possible without getting into too much detail"
8Qj7jBCta2wXwWfo9d3h9W,Databricks is an integrated analytics environment powered by Apache Spark which let you connect and read from many data sources such as AWS S3 HDFS MySQL SQL Server Cassandra etc We are focusing on S3  Since it is very easy to work withCreating S3 storage bucket is beyond this sectionhere we will read csv format file using spark Dataframe object in DatabricksIn AWS Management Console go to My Security Credentials under your user account name Then  you click Access keys (access key ID and secret access key) section under My Security Credentials which will be provided with option called Create New Access Key once you click that option you will be given access key ID and secret access key please save it in notepad by pressing show access key or download key file for later useSince we access S3 bucket using databricks-backed scope Secrets should be created by putting access key & secret key values in Azure key vault Go to Azure Key Vault in the resource menu click secrets under Settings category Then click + sign (Generate/Import) in the command barGive aws-access-key and and aws-secret-key for name and paste copied access key & secret access key values in step 1 in the place of Value By doing so two times two secrets will be created leave others default and click create you are done with creating secret for accessing storage accountNavigate to properties under resource menu of Key Vault & copy DNS name and Resource ID and save it in notepad This will be used whilst creating secret scopeEx- https://southeastasiaazuredatabricksGive scope name aws which is uniquely identified in the database maintained by Databricks leave Manage Principal as Creator and paste copied values of DNS name & resource ID in place of DNS Name and Resource ID fields Click Create & you will be given with success message Remember scope name or save it in fileAccessing S3 bucketing directly by setting AWS keys in the spark context Secrets utilities of Databricks utilities (DBUtils) will help us fetching sensitive credentials information without making them visibleACCESS_KEY = dbutilssecretsget(scope = aws key = aws-access-key)SECRET_KEY = dbutilssecretsget(scope = aws key = aws-secret-key)sc_jschadoopConfiguration()set(fss3nawsAccessKeyId ACCESS_KEY)sc_jschadoopConfiguration()set(fss3nSpark context is now configured to connect to AWS S3 bucket by above diagrammaticallyLets check whether we can be able to read below df = sparkreadcsv(s3://my_bucket/beatles-diskographywe will see how to mount S3 bucket in DBFS  Since it is a pointer to S3 data is not synced and can be refer to path/to/file like local filesThanks for reading! If you have any queries please feel free to leave a comment
5GwSnw2hQfNnxHrKbA92NL,"""Elasticsearch's shard allocation system can get complicated When we create index or have one of our nodes crashed shards may go into unassigned state Meaning data is there but it is not assigned/replicated to a node to enable processing that shard""Please note that we're using Elasticsearch 654 I haven't tested it on any other versions""Let's revise some terms before going into details Elasticsearch index consists of shards Each shard is a Lucene Index in its internal state and every one of them is assigned to a node to make them enable to index and search Some of the reasons for unassigned shards are listed hereSo How is shard allocated? Elasticsearch has two main components for allocation of shards: allocators and deciders Basicly allocators finds the best nodes to allocate the shard and deciders looks at these nodes and decide whether that node can allocate that shard Allocation of shards are explained well by the Elastic itselfAnother question is what are these deciders and where is the issue in these deciders? Deciders are simply a checking mechanism to check the node Simplest example would be the node has no available storage So the shard cannot be put into that node and this behavior is decided by the DiskThresholdDecider List of deciders can be found here""Currently there is an issue I discovered with one of the deciders which is MaxRetryAllocationDecider and I'm going to demonstrate the proposed workaround When deciders come to a positive conclusion all together the shard can be allocated to that node and the source node starts to copy the shard to destination node Sometimes source node fails to copy In my case the fail was the source node had insufficient memory to process my allocation request Elasticsearch has a Circuit Breaker system which prevents the operations from causing an OutOfMemoryError This system is essential because if it doesn't break the operation Elasticsearch will give OutOfMemoryException and JVM will become not responsive hence Elasticsearch too MaxRetryAllocationDecider counts these kind of errors and after some limit ( defaults to 5 can be set via indexallocationmax_retry option) it stops to request allocation because it continuously fails to assign that shard to destination node and tries another node and fails again (All my nodes were in CircuitBreakingException so no node could allocate) So the shard becomes failed to assign and stays in unassigned state""Elasticsearch allows to retry failed allocations with the ?retry_failed=true URI query parameter Details for this option is here The issue is this option doesn't seem to work with MaxRetryAllocationDecider Even after I use this option ( of course I had to clean my cache first because of CircuitBreakingException)  my shards were still unassigned and the date for failure was same and nothing updated""It shouldn't have said that because I fixed the issue Let's look at the shards allocation""So even though I used the ?retry_failed=true option my failed allocation attempt stays the same the date doesn't change exception remains the same though I fixed it meaning my command affected nothingI created a discussion and it turns out to be an issue to fix Issue is if you have an allocation blocked by the max_retries (MaxRetryAllocationDecider) condition and try and run a reroute command then it will do nothing even if you set ?retry_failed=true so the reroute response will remain unchanged""While the fix is on the way thanks to Elasticsearch developers there is a workaround for now""If we understood the background behavior fixed the reason of failed allocation these steps are pretty straightforward to apply Let's start by disabling the allocation""Second step is to reset the counter which was the cause of the issue""It should return acknowledged: true along with the current cluster state including all shards whether it's allocated or not If you have unassigned shards this may be a good response to discover At this point since we disabled allocation Elasticsearch won't try to allocate the shards So we need to take the 3rd action In my case I had unassigned replicas and I reroute the replica to correct node which has the data so initializing and assigning the shard will be faster""So far so good I have 17 more unassigned shards and I don't want to reroute these one by one Since I fixed the CircuitBreakingException if I re-enable the allocation Elasticsearch should be able to assign rest of the shards itself because we already reset the counter Let's re-enable the allocation which is also the last step in the workaround""Then I check the shards again and safely confirm that all shards are initialized and assigned successfully We're back to green Well played Elasticsearch"
V2rqPmNmGxkjAtmMsXR8kN,"""Important Note: For this guide to work on your indices you have to have your indices with _source field enabled in mapping If you disabled _source field for any reason this code won't work on your indices Elasticsearch recommends have it enabled all the time""Every now and then there are unexpected (or unintended) crashes in Elasticsearch For my case it was a hardware failure during a heavy IO operation to Elasticsearch (Let's just assume I didn't have any replicas or I managed to crash all of the cluster) After some research I found out it messed up with lots of state files of an index (corrupted!) I thought if Elasticsearch uses Lucene I surely can load my data and reindex it using Lucene API""Let's quote two important terms before we start More details can be found here""Let's look at the structure of Elasticsearch Data and try to find these Lucene Indices You can configure the data path in elasticsearchyml with the key pathdataAs you can see there is a folder named eCCAJ-x6SOuN6w7vqr4tGQ under indices directory ( Explanation of these items are completely separate topic For the sake of simplicity just know that state files are generated by Elasticsearch and it is a SMILE-encode file which contains Elasticsearch metadata like an index name node id etc""Let's list our indices by curling our Elasticsearch instance""Gotcha! That's my index where I indexed some malicious IP data for this example Let's look at that directory""And that's the shards of our index OR that's our Lucene Index Instances to loadTip: If your Elasticsearch instances are crashed at this moment you can simply run cat command on state files to look which indexes they belong All of the file will not be readable but at least you should see your index nameImportant Note #2: Before proceeding to the code you should be sure that your Lucene Index is not corrupted In order to check and fix your Lucene index (or Elasticsearch shards) I strongly advise you to use CheckIndex tool The blogpost also have a wonderful information about directory structure you see above""Now we found the Lucene Indices and let's do some coding to rescue our data I will create an example Maven project""Important note #3: You should look at your Elasticsearch instance's Lucene version and use the same Lucene API version here Run the commandcurl elastic-host:9200 and look for the lucene_version key""For the sake of reading simplicity I will write all codes in the main method and throw the exceptions It's up to you how you structure your codeHit Run button and you will see the count of your documents for that shard In my case I have 952 documents in my 0th shard If you list all your primary shards and sum it up it will be equal to your total doc count in your Elasticsearch Index""We can access lots of information using our reader variable such as segment informations and the documents itself What we want is to find these documents Let's code that partTa da! Hit Run button and take a sip of your coffee It should taste better as you see the flowWhat we did is a simple loop and get the corresponding document for that position Afterwards we just get the _source key where our JSON resides It is stored in binary hence first we need to fetch the binary value which is followed by utf8ToString conversionFor further details I strongly advise you to debug the code and inspect the variables to see what kind of information they store and all along follow the Lucene API javadocs"
AKjKh7gX5t27pJi3gqAr8x,Most major projects are collecting an ever-increasing amount of data from emerging technologies such as embedded sensors wearable devices Internet of Things (IoT) and drone-enabled surveying but often lack the resources or expertise to use it effectively to manage risks and control projects It is vital that you create analytics strategies that are suited to the projects unique nature available skills data infrastructure and analytics maturity of the project stakeholders Developing an analytics strategy at the early study phase of the project plays a crucial role in building a rich data-driven culture and practice that can significantly improve the predictability of project outcomesA process in which large sets of data (aka Big Data) are collected organized and analysed to discover useful patterns/findings uncover hidden patterns and trends These patterns provide useful information that can help a project to produce future decisionsProjects can tap into predictive analytics to learn from the past and predict the future or go even further with prescriptive analytics to derive specific advice for decisionsThese analytics applications help project teams ensure their projects are on time within budget and done to specifications Described below are six key pillars that a project organization must support to deploy a project analytics strategy successfullyAnalytics is a project outcome enabler Obtaining a clear understanding of project objectives and key performance indicators (eg schedule and cost performance targets) and alignment with the overall project strategy to achieve the KPIs are crucial in establishing the analytics strategy It helps to define what needs to be optimized what data need to be collected and what processes need to be changedWhen developing an analytics strategy it is essential for you to understand what each stakeholder will require and develop models to ensure they are getting what they need in each stage of the project When projects move through the stage-gate process the granularity of information requirements changes and focus tends to move from estimating and risk assessment to trend and contingency managementDuring the stakeholder analysis you should identify clearly what decisions each one is potentially making based on the requested analytics Set up a series of questions for analytic models to answer This way you can ensure the capture of the right dataAnalytics maturity assessment of project stakeholders including prime and subcontractors is crucial for determining where you should concentrate your attention to create more value for your data A more practical analytics progression is from Descriptive to Diagnostic to Predictive to Prescriptive to CognitiveOnce the stakeholder needs are identified and the analytics maturity level of the project organization is determined obtaining alignment with the project and functional leadership is critical before building the necessary infrastructure For example the technology department needs to be on-board to support the hardware and software deployment and Contracts and Legal departments need to assure contracts allow required data-flow from contractorsData engineering provides data processing and storage infrastructure to support advanced data analytics Successful execution of analytics strategy requires all relevant data to be identified documented processed and made available to appropriate analytics applications It must address how to determine which data is most necessary for supporting project objectives and identify mechanisms for collecting and validating that information As illustrated by the McKinsey Global Institute project execution is typically supported by multiple source-systems and from across various units/departments/programsThe data engineering efforts need to be put in to enable efficient analytics which includes architecting distributed systems combining data sources and establishing data lakes (eg Microsoft Azure Data Lake Qubole Hadoop HDFS AWS EMR and Oracle Big Data Cloud Service) Take advantage of flexible and the low-cost cloud storage technology that can be set it up in just a couple of daysThe data engineering team will select retrieve and transform select data into forms that will work with data mining procedures You can use Robotic Process Automation (RPA) to improve the productivity of these effortsOnce you build the data pipeline the next step is to find and fix data quality problems that could affect the accuracy of project analytics applications Measures need to be put in place to assure the data quality including the meta-data That includes data profiling/cleansing/scrubbing to ensure inaccurate or corrupt data is removed from the data warehouse Additionally it would help if you had data governance policies and procedures with responsibilities assignedSince project data comes from a myriad of sources to reap the benefits of advanced data analytics there is an urgent need to set a uniform project ontology semantics and definitions along with measurements and metrics of how project success will be definedModels are the foundations of advanced project analytics Model users have access to a wide range of modeling techniques Increased functionality and easier accessibility open up analytics applications to so-called citizen data scientists -project analysts and users who have enough know-how to build models on their own However to be successful at it they need at least a high-level understanding of the processes and techniques used in project analytics applications as listed below: These techniques share the same goal: Learning from data Statistical inference is a theory-driven process that cares about statistically significant results In statistics you use all the data to fit each model You choose between models by using a statistic that measures both the goodness of fit and the complexity of the model However machine learning which was developed for Big Data use insights from data mining and learn from a training set on which you fit models and a validation set on which you score the models You choose between models by using a statistic such as the average squared errors (ASE) of the predicted values on the validation data We can start with DM to find out patterns learn from ML algorithms to answer specific questions then use statistics to establish a significanceFew examples of advanced project analytics application areas are listed below: Project delay analysis cost and schedule risk analysis forecast resource demand predicting and diagnosing labor productivity generating optimal plans for workspace conflicts fault detection of engineering structures assigning crew optimality store and analyze unstructured project documents classify and index site progress photographs video analysis of workers behavior towards safety and Virtual Reality (VR) enabled comparisons of As-planned vs as-builtIn addition to developing models you need techniques associated with model evaluation validation and continuous improvementIn todays data-driven environment you need the ability to accurately interpret and showcase vast amounts of project data to efficiently unearth critical insights and inform project managers and other decision-makersCreating daily weekly and monthly status reports are one of the most common tasks of project professionals They are most commonly done using static dashboards while with the availability of visualization and analytics tools (eg Power BI Tableau TIBCO Spotfire) they are now slowly transformed into interactive dashboards They typically contain tables basic charts and graphs describing planned vs actual progress updatesThe majority of these project dashboards lacks the advance predictions and a narrative that explains why it happened and what will happen Below is an example of a progress prediction model developed and visualized using TableauStorytelling with data is the pertinent missing piece in delivering the essence of data analytics to management and other stakeholdersProject managers are ill-equipped primarily due to time constraints to work through the data to unearth the stories  This results in a tremendous amount of analytics efforts unused Project managers and leadership crave useful relevant content that expands their understanding of where the project is heading causes of deviations and helps them better navigate the project By extracting foresight from predictive analytics and delivering a narrative project professionals can provide a real contribution to the successful delivery of projectsThe gap between planning and implementing an advanced analytics strategy is mainly due to shortages in the data science skills needed to build models and put them into action As McKinsey & Companys Ghassan Ziada highlighted the challenge is to find and retain those who understand construction and apply technology using AI machine learning and data analytics to improve productivity To realize the benefits of the digital revolution project owners and contractors investing in data literacy training are as important as investing in technology This level of reliance on data requires everyone in your project organization to be data literateGartner defines data literacy as the ability to read write and communicate data in context including an understanding of data sources and constructs analytical methods and techniques applied  and the ability to describe the use case application and resulting valueFor those who like equations here is one for you: Data science literacy = computational literacy + statistical literacy + machine learning literacy + visualization literacy + ethical literacyThe core knowledge underpinning data literacy skills includes: It is fair to say that attracting data scientists to work in the construction industry is an overambitious goal Project professionals come from various educational backgrounds including engineering accounting and business which are quantitatively skilled Hence it is safe to assume with appropriate training they will be able to transition into performing advanced analytics efficiently Companies who want to embrace the current data revolution need to provide assistance to develop analytic skills Project team leads need to hire the right people who can think computationally analytically and can ask the productive questions Project professionals should be willing to invest time to upskill themselves There are ample free online resources available including some from Ivy Leagues where individuals can learn advanced analytics at their own- phase I will share a list of courses that you can take in a separate postDeveloping an advanced project analytics strategy is an active process that plays a crucial role in building a productive data and insights-driven culture Analytics are only able to establish what a project planning and execution team should do to meet the established KPIs; making decisions is up to the project leadership That is why stakeholder alignment and buy-in is essential to move from intuition to data Decision makers must demonstrate the data-driven approachThe inherent challenge in analytics projects is asking the right questions not necessarily finding the answers A small and focused team with an agile approach works best to define and redefine questions document the requirements build the data pipeline and the models and validate results in a short iterative cycle Citizen analytics tools such as Microsofts newly released Azure AutoML advanced visualization of data and results along with analytics translation skills will add tremendous benefits to providing timely actionable foresight to the successful execution of projects
eWaU3swBgu5HsddSjiR3fv,By identifying risks and managing them in an effective fashion is now of the major concerns of any business When companies can manage their risk in a better manner they can take business decisions in a much more confident manner Becoming knowledgeable in the field of risk taking is extremely beneficial for companies as it can help them not just deal with problems in an uncertain environment but also deal with problems even before they ariseOrganizations face internal and external factors and influences that make it uncertain whether when and the extent to which they will achieve or exceed their objectives Further when companies do not take into consideration the chances of risk they tend to lose direction when their plans do not work out accordingly It is therefore a good idea to add a risk management team/ software technologies to your company as this will help them manage and mitigate risks in a better mannerThe main role of risk management would be to help companies to recognize the risks and come up with plans that will if not remove but reduce the risk of accomplishing goals in an effective manner The risk management team of a company is also responsible for making a list of the risks and determining their level of criticalityThe problem of managing and analyzing data still remains As the amount of data is almost limitless it is not possible for a single person to manually analyze them in any manner Another major problems that exist is that with such exponential amount of data it is difficult to find that data that is helpful and relevant which can get lost in the greater amount of useless and irrelevant data
D5TqkPVJQnE4ksTv8uYy8u,Enterprise Data Integration (EDI) is a technology that help combine and integrate two or more sets of data It includes merging data from disparate systems and delivering the same in a way that it is manipulated and analyzed to support information management activities Data integration has become important in cases of mergers acquisitions and consolidation to provide a different view to the data assets of a companyEvery business relies on IT for accessing integrating and delivering data As data is everywhere in the cloud on premise and in multiple disparate systems accessing the data alone is not easy It has to be cleansed aggregated and validated and put in a better form so as to be useful for the users Unlike application integration that focuses on transaction management data integration resolves complex issues that arise due to data fragmentation IT organizations use data integration in varied ways to driver their businesses and implement real-time reporting and analysis to optimize minute-by-minute operational and strategic decisions Data integration is also being used for big data analyticsTo effectively manage data integration organizations practice a holistic process· Technology: Platform for Enterprise Data Integration: For smooth running of multiple projects organizations need an EDI platform that offers: iiiiii A unified architecture to simplify and accelerate development deployment and maintenanceiv A shared service approach based on metadatav Enterprise-class availability reliability scalability and security· Architecture: service-oriented architecture: Data integration plays an important role in service-oriented architecture (SOA) adopted by many organizations For SOA organizations require data integration platform to deliver share data that is defined by metadata and easily allow inter operation with other IT architecture· Approach: Integration Competency Center (ICC): ICCs have become one of the best practices for EDI They are an organizational approach to increase agility and lower implementation costsFollowing are the benefits of data integration: · Overall data synchronization enable complete view of business entitles· Cleansed and consistent data across sales improves opportunities· Automated processes improve operations due to data centralization· Application integration: Companies enable initiatives through sustainable integration architecture This lowers the cost of ownership· B2B integration: IT departments face challenges to provide B2B integration solutions EDI help reduce complexity and increases flexibility· Big Data integration: Companies use enterprise integration platform with integrated big data movement solution to seamlessly flow the wide range of big data applications· Cloud integration:Cloud integration support B2B integration processes in the cloud as well as file distribution and document sharingfor any consultancy and managed services for data integration visit wwwentradasoft
YLAnQ3WbdmAwppcfnoNGTE,Software certification is a standard procedure implemented in almost all modern companies before adopting a software tool or package The process verifies that new pieces of software will not break the security and privacy policies of the company Security is a complex matter which deals with a wide range of aspects related to any given software product From identity through cryptography to software development practices the list of things to check when certifying software is never-ending Intelligent data management is at the core of any security and privacy policy yet when integrating external software it introduces significant challenges To be able to ensure compliance with security and privacy policies one needs to answer a long list of complicated questions For example What is the data that a system is using? Where is it stored? How is it handled? Which processes are involved? What is the data trajectory in the system? and the list goes on and on Unfortunately many of those details are not available in the product documentation provided by vendors so integration and certification professionals have no idea what will be the actual effect of approving the software as its practically a black box with respect to data Data Catalogs are designed to uncover the real data related behavior of systems in real-time and answer all the questions mentioned above Using a data catalog in the certification process can ensure that certified software complies with the companys high security and privacy standardsDiscovery is at the core of data intelligence insight and analysis  and needs to be both capable and automated in order to successfully address the volume and type of data that organizations collect Effective and sustainable privacy security and governance programs require discovery in-depth: empowering organizations to scratch more than just the surface of their data That means not only finding and identifying more types of sensitive and personal data with greater accuracy but being able to apply context insight and perspective to that data  which then helps inform policy and controls Its no longer enough only to be able to identify regular expressions and common types of sensitive data (like credit card numbers or social security identifiers) Privacy regulations like the CCPA and GDPR have transformed the very definition of personal data  extending it to a much broader set of data taking into consideration things like geolocation friendly names online activity and more Unlike earlier regulations todays data privacy initiatives focus on data that can be related to an individual which means that data discovery solutions need to be able to identify personal data not just by type but from contextual clues and relationships to other data points Furthermore organizations are now responsible for not only protecting that data but monitoring and reporting on whose information it is where it came from and where its going Privacy-centric data discovery (a must for data privacy and cybersecurity in todays environment) requires a multi-pronged strategy to identify classify correlate and catalog all types of sensitive & personal data in an organization  and that strategy starts with automatic discoveryData Catalog is the primary tool for organizing the thousands or millions of an organizations data sets per business need With data catalogs users can search for specific data and understand its context and flow Data catalogs are the core of any data management strategy; they enable data-driven decision making and are often essential for regulatory compliance For example only with a data catalog large organizations can comply with GDPR requirements such as the ability the find and delete all instances of specific customers information within a short period of timeData lineage traces the origins movements and joins of your data to provide insight into its quality Data lineage tools often use a graphical interface to show the datas journey from inception to how its used (ETL databases business intelligence etc); its dependencies to where its joined with other data to whether or not it has been changed or updated Data lineage tools give you more control over your data by allowing error tracking and adjustments when needed Also these tools can facilitate process changes metadata management self-service analytics and data governanceThere are many players in the space of metadata management The following are a few leading solutions that provide a complete automated solution for in-depth data discovery classification correlation catalog and metadata visualization for enterprise data at a large scaleSoftware certification is never complete without analyzing its real behavior with respect to data Modern intelligent data catalogs perform real-time in-depth analytics of software data operations and as such should be used as are a vital component of the certification process
5Kq4npkLcFUKg7tz2jCvi3,Graph databases are not new The web is full of great content that describes in detail all you need to know about these powerful types of databases Unfortunately the term graph databases is a little confusing because under the roof of that one concept live two different types of products property graphs and knowledge graphs which are fundamentally different Surprisingly whenever I speak with a colleague who is not specifically an expert about graph databases I find out that most times that my conversation partner fails to do this basic distinction Too often that simple mistake leads to expensive misuse of a database so I decided to write this article to ensure that you will not fall into this trap and not make that simple yet expensive basic mistakeIf you google graph database most probably you will find information about property-graphs; therefore lets consider it as the default type of graph database Property-graphs databases put the attention on the graph itself which means that what you most care about is how entities are connected to each other and what is the immediate insights you can learn from that connectivity We use these types of graphs every day For example the subway map is a property graph Stations are connected and so by looking at the map these connections tell us how to get from point A to point B The fact that the graph (eg the direct connectivity) is on the front row makes mathematical graph operations such as: find the shortest path native for property graphs use casesThe knowledge graph is the less popular citizen under the term graph database but its by no means less important Here we care less about the direct connectivity between elements but rather we are more interested in the business meaning of the data we store We begin by defining an ontology which is a machine-readable description (ie definition) of the business domain we are working on Then we add the data and apply the ontology to it Finally we use the knowledge graph to infer new meanings and insights from the data and the ontology combined The graph is only a representation method for the relations that make up the meanings behind the data but it is not by itself the focal point as in property graphs Lets consider a simple naïve example: Lets take the domain: Family and create an ontology that defines the concepts and their relations in the domain (Father Son Grandfather etc) Now we can insert specific entities for Bob Marry and George and based on the ontology specify that Bob is the father of Marry and Marry is the mother of George Even if we do not specify any relation between George and Bob the knowledge graph can infer that Bob is Georges grandfatherA knowledge graph is a great tool for data interoperability and computation of deep complex and dynamic business insights Data entities (eg Customer Data) might have different representations coming from different origins but eventually have the same business meaning With an ontology you can define what the concept Customer means in your business and then cross customer data originated from different systems With a knowledge graph databases you can infer relations even if they do explicitly exist in the data based on the ontology Unlike static OLAP analysis executed on traditional data warehouse solutions that require schema definition and lots of data preparation before analysis can occur with knowledge graphs you can dynamically choose how to analyze your data and then find deep insights quickly with minimal data preparationData is represented and queried differently in property graphs and knowledge graphs Gremlins is the defacto standard for querying property graphs and SPARQL is the query language for knowledge graphs Gremlins queries typically include graph operations such as find the shortest path while SPARQL queries will typically include semantic operations such as is-a or has-a Often it is possible to execute graph operation on knowledge graph or compute some sort of semantics on property graphs but the nature of the backend engine is different and the performance of such queries will be bad That is why its so important to understand the nature of the problem before choosing a graph databaseWhen you think about graph databases think first about the nature of the problem you are trying to solve Is it a problem that requires walking the graph where the business value is in the direct connectivity between entities? In that case the Property Graph is what you need Or maybe the solution requires understanding the semantics of the data and inferring insights from it in which case knowledge graph should drive your solution Make sure you dont fall into the trap of polysemy and choose your data infrastructure wiselyIf you want to learn more about Ontologies and Knowledge Graphs I include here some introduction articles that will help you bootstrap your knowledge about it· What is RDF Triplestore?· RDF 101· What is SPARQL?· Video: Introduction to one example of an RDF database
MHZ3xcwkJpBWhsJMZXxo8r,Data modeling is a crucial step in modern data workflows as its purpose is to organize raw data into convenient and efficient forms Data analysts and scientists will find their jobs much easier if a usable dataset is readily accessible Quicker analytics and predictions will then lead to faster insight for business decisionsThe first step to modeling is often to normalize the data which is a process of organization that increases database flexibility by reducing inconsistent dependencies and redundancy Id suggest reading up on this and/or looking up some videos if youre unfamiliar! The problem with a normalized database is that any truly interesting insights from the data will require many JOINS which can significantly slow down the speed of our query as the size of our database increases For instance looking at the schema below most tables are not directly relatedNow what if we need data from even MORE tables after that 4-part JOIN? It would be madness Not to mention it would be an absolute headache to just write the query without any errorsFurthermore real-life databases can have far more tables than just the ones shown in the example above As you can imagine it becomes increasingly difficult to even understand the relationships between tables as our schema growsOne solution to this problem is to perform a denormalization step of data modeling to create a simpler and easy-to-understand schema optimized for ceratin queries The process of creating a star schema involves distilling down our full schema into just relevant features for a particular analytic prupose The general structure of the star schema is as follows: The star schema consists of two types of tables: Lets consider a database of sales for a store We have a fact table Revenue at the center of the schema and four dimension tablesThe fact table is comprised of a composite primary key which is the combination of the dimension table primary keys Fact table non-primary keys Units_Sold and Revenue are the facts were interested in and the dimensions such as Product_Name and Name (branch name) allow us to understand more information about the goods soldFor instance the following query would allow us to calculate the total revenue by product in the year 2010: The star schema is widely used and incredibly useful for business applications It helps us speed up queries that we may run often and clean up what could otherwise be very messy queries among other thingsThere are other schemas such as the snowflake and galaxy schemas that are simple extensions of the star schema
muAbQhhCDFgbrAytKAMUmY,Recently Ive developed quite an interest in data engineering Ive found the silver lining amidst the coronavirus pandemic to be that I get a unique chance to invest as much time as I want into learning Although its difficult to stay home (self isolate!!) every day Im learning many new practical skills that I feel will propel my career forward once we emerge back from these troubling timesIm a recent data science bootcamp graduate who is actively job searching I graduated from college in 2018 so the combination of my skillset straight out of the bootcamp and total professional experience (which is little) makes me eligible for entry-level data analyst positions Professionals in the industry have warned me to be skeptical of data analyst jobs that will mainly have me doing point-and-click analysis in BI tools like Looker or Tableau as those roles will not develop my data science abilitiesAs I spoke with more and more data scientists I often found myself telling them that my favorite part of my data science projects was setting up my ETL pipeline I love getting my hands dirty in the code to extract data from an online source primarily via webscraping or an API I relish the moment I finally fix a bug that had been preventing me from progressing for the past couple hours I am pretty OCD about keeping the code neatly organized legible and well-documentedYou may be familiar with some form of the following diagram: Although I didnt quite understand at first my real interests lie in data engineering I went through a data science bootcamp to figure out I would rather be a data engineerIt makes sense though Even during my bootcamp I was never really too fond of machine learning statistical testing or AI While I certainly appreciate the power of predictive modeling I also realize that I lack much of the mathematical foundation to truly understand and fine tune predictive algorithms I could learn it but Id prefer to pursue something that excites me This is 100% the case with data engineeringWhile many of the skills I learned in my data science bootcamp are transferable there are many technologies to learn For instance data engineers need to be well-versed in ETL tools cloud infrastructure services database administration etc Im able to generally pick up new things pretty quickly but the world of data engineering is pretty daunting given the number of tools in the data engineers stackI thus sought out online resources to aid in this process and found two wonderful and free (during a trial) sites These are Datacamp and Udacity Lets look at these more in detailThis is a hands-off paid subscription service designed to teach data science to anyone What I mean by that is that there are several suggested curriculums laid out for you but you can go through at your own pace to pick and choose whatever content you want to learn in whatever orderDatacamp is a wonderful resource I would highly recommend it to anyone looking to supplement their data science knowledge It covers different topics such as programming fundamentals (in R Python and Scala) math data visualization machine learning Git software engineering fundamentals etc$$$: You can get a 3-month free trial of Datacamp if you sign up for the GitHub student pack For those who are no longer students or who can no longer reap the benefits of their university email addresses one can also get a 2-month free trial through the instructions in this linkOrganization: I consider the basic building block of their content to be a course which is generally a 4-chapter learning module Each chapter takes around an hour to complete and usually consists of an introduction video followed by a mix of instructional videos informational pages of text and interactive exercises that build progressively throughout the chapterMultiple courses are grouped together into tracks which Datacamp puts together to present material in a logical and progressive manner There are two types of tracks skills tracks and career tracks I chose the Data Engineer career track which currently has 75 hours of contentThere are many tracks and I suggest you look into them yourself to check it out The first chapter of most courses are free for exploring even without a subscription! This allows you to poke around before committing to anythingI took a handful of courses within the data engineering track over my first weekend with my trial I took Introduction to Data Engineering Software Engineering for Data Scientists in Python Introduction to Git Introduction to PySpark and JOINS in PostgreSQLMy goal was to get my feet wet into the world of data engineering and I feel like I accomplished quite a lot Each course exposes the user to real-word applications of the tools I was able to learn things like setting up an ETL pipeline using PySpark automating with Airflow familiarizing myself with PostgreSQL and much moreI was very entertained throughout the whole thing which I think is a testament to the strong presentation of the material that Datacamp puts together The interactive nature of the exercises is nice too I have done a little bit of work on Code Academy It felt kind of like that but just more focused to data science topics Furthermore I think the material is split up into very digestible chunks so as not to throw too much at you at once without allowing you to practiceThere are MANY things I could keep learning on this site but given that there are many other resources out there to learn I decided to shop around a bit This led me to Udacity which a fellow ex-bootcamper recently told me aboutUdacity is another paid subscription service Unlike Datacamp you enroll in Nanodegree programs which are similar to Datacamps tracks but are more structured and include actual projects for you to submit for feedback from actual humans Its a strong platform$$$: They have several programs but I chose the Data Engineering nanodegree They say that this program should take roughly 5 months if working 5 10 hours per week I only have one month to use for free before paying something like $400/month for the service which I certainly do not have (especially now) If you do the math this means they recommend anywhere from 100 200 hours total Thus I could theoretically complete the nanodegree putting in between 25 50 hours per week Luckily I dont have a job and coronavirus is keeping me at home while I continue my searchOrganization: The curriculum is laid out into four sections plus a capstone project: Data modeling cloud data warehouses data lakes with Spark and data pipelines with Airflow Im super excited to get through everything I can Each section is broken up into several modules and at least one project to practice all the skills learned You submit the projects upon completion and get some feedback which is a really nice feature that Datacamp does not offerThey also have this thing called Knowledge which is like a forum for students to ask each other questions which can be answered by Udacity instructors or other students Ive found this pretty helpful although most of my questions have been answered through GoogleThe main value to me in this program is the fact that there are projects I can complete and show to a prospective employer My professional experience in the industry is nonexistent so I think that giving solid proof that I can build something is the best way to demonstrate to employers how quickly I can adapt and learn new technologiesAfter just a few days Im close to finishing the data modeling section Im working on a data modeling project to build an ETL pipeline for mock music streaming data into a PostgreSQL database Students have the option to work in a Udacity-hosted workspace or to develop the code locally and submit a GitHub repoI opted for the challenge of developing locally There wont be training wheels forever and I wanted to gain real-world experience developing my code locally and then making available to others with Docker Additionally Im keeping in mind software engineering best practices and trying to minimize dependence on Jupyter Notebooks which were heavily emphasized in my bootcampMy goal is to get through the four sections in this program If I dont get to the capstone project then oh well I can make my own capstone project and ask a more experienced data engineer to give me feedback on it laterI really like Datacamp and Udacity I highly recommend them to anyone looking to do something productive during these coronavirus times especially since there are ways to start out with them for free Plus these are just two of MANY online learning platforms out there Im sure if you look around youll be able to find other free trials or heavily discounted subscriptions in light of the coronavirus
SrvMaukXgtbPkodBXP3X95,At Shazam we run most of our data processing jobs in the cloud via Amazons AWS Data Pipeline One of the main limitations weve found with AWS Data Pipeline over the years is that it provides only two viable means of defining the steps & requirements of each data processing job namely: The first of these was a non-starter for us integrating the UI with our source control was cumbersome and centralising configuration such as database credentials & resource locations was not possibleSo Shazams data engineering team took the decision to maintain our pipelines by writing and modifying JSONBut this JSON configuration is verbose  very verbose! In a meeting a few months back I shared with our data engineers the fact that we had approximately sixty thousand (60000) lines of AWS Data Pipeline JSON configurationMaintaining such a large set of configuration data was an enormous maintenance burden and certain tasks were becoming virtually impossible such as:: I felt there was a fundamental flaw in how we we working: we had sleepwalked into the configuration camp of the age-old code vs configuration debate By using just JSON to define our pipelines we had sacrificed all the benefits that code (and IDEs) can bring Refactoring JSON is practically impossible Sharing configuration across JSON files requires awkward templating And we had no way of validating our JSON without actually deploying itSo the answer was codeWith code we can refactor and share common objects throughout our suite of pipelines With strongly-typed code the compiler can validate our pipeline before deployment And with Scala we can use a domain-specific language to make the code look & feel native to the task at handEnter the AWS Data Pipeline DSL for ScalaThis blog post is to announce that Shazam has recently open-sourced the DSL that we have built and are using in-houseLets start by writing the smallest possible hello world of AWS Data Pipelines: By convention our pipeline definition is assigned to a value named `pipeline` We can therefore define just one data pipeline per Scala objectWe can run this through the provided Data Pipeline compiler by issuing the following command: Here the first argument HelloWorldPipeline is the name of our object the second is the Scala source file to compile If you happen to write invalid code the compiler will issue errors & warnings in the same fashion as the regular Scala compilerThe JSON written by the compiler is now ready to be deployed to AWSIn AWS Data Pipeline it is possible to define multiple activities (such as shell commands SQL tasks EMR tasks etc) and also declare how the execution of these activities should be sequenced Our Scala DSL allows us to express this sequencing with relative ease like so: Take note of the double angle-brackets (>>) on line #23When we look at the JSON output for this pipeline we can see that second activity depends on first activity: Its possible to express far more complex dependency chains with the DSL too such as: Run A first then B then C: Run A first then run B and C in parallel: Run A first then run B and C in parallel then run D: And so onTo avoid repeating yourself the DSL also permits simple definition of certain defaults including SNS alarms which are topics to notify whenever the pipeline failsDefaults are configuration that can be stated once and is then applied to all objects in the pipeline: Similarly a single SNS alarm can be configured for all steps of a pipeline: One of the primary benefits of the Data Pipeline DSL at Shazam is that it allows us to refactor away duplicate code and share common elements between data pipelinesOne way we achieve this is by defining factory methods that have sensible defaults for our platform For example the following declares a much simpler constructor for an Elastic Map Reduce (EMR) cluster: Note that with this factory method we can now define an EMR cluster just by giving it a name instance type and number of instancesSimilarly we have defined a simpler factory method for defining EMR activities that run Apache Spark tasks: The Scala DSL for AWS Data Pipeline is still very much in its infancy and whilst we find it sufficient for our needs at Shazam it is not by any means a complete nor mature tool yet Numerous components of Data Pipelines are not yet supported and there are certainly many features still to be added
duXoPtMB3EVnnR7C63Ziqj,As business transactions keep getting faster near real time data is a necessity for companiesThe process of extracting transforming and loading (ETL) data into a warehouse is time consuming and not flexible enough to keep up with the speed of business Yesterdays data wont detect todays problems In order to be operationally current we need to know what just happened in our businessETL often accounts for 80% or more of a data warehouse project It is the most common bottleneck in getting data available for analysts and decision makers in time to be useful It is also not the most exciting code to write and tends to break with every change at the source or target so it isnt super rewarding for developers eitherWhen we were researching solutions one business intelligence platform vendor stood out from the pack because they use a modeling language to define how the data is transformed and joined at query time This allows simply extracting data from the source and loading directly into a data warehouse with out transformation It is quicker to implement more flexible for changes and more transparent to see what is being done to the data Once the data is available then the next big task is validation and defining the rules of the business While a modeling language doesnt reduce the effort of validation the flexibility of being able to change the model based on what has been learned and not have to reload the data saves lots of time and effortWe ended up using a service for the extract and load and a service for the data warehouse further reducing the effort so one data engineer can do the work of three or four using the old methods We chose Fivetran to extract and load the data into Snowflake data warehouse and then Looker for modeling and visualization There are other extract and load services such as Stitch and data warehouse services such as Redshift or BigQuery but this combination has been very stable and hassle free We stuck to services that were built specifically for the cloud to give us the best flexibilityAfter going to lunch with several people to discuss what we had learned from using Looker for a few years I created a short Udemy course that gives a high level overview with the goal of helping to see under the hood a bit in order to make the decision to engage with Looker for a demo If your company does any data analysis and you have never heard of Looker then you should at least get an understanding of what is availableHere is a coupon that makes it less than the cost of two for lunch https://wwwudemyThis course is not meant as a technical how to but as an introduction to Looker and the Lookml modeling language that makes it unique
njfHAAJDZuDcdZe2LYYwGv,AI is the new electricity says Andrew Ng He continues to say that the only competitive advantage of companies that are big enough (1B market cap) is their use of AIPeople listen The prevalent specialization today is AI and Machine LearningThese facts are true but peoples understanding of them is incorrect Think of this: electricity powers your home etc But who brings it to you?  The electrician!! You usually can only change the bulb but for putting in a socket you invite a certified electricianWhat does this mean to us? That we need to change the focus You can study AI and Machine Learning but to make something practical out of it you need to become an AI electrician that is a practicing AI engineerFor each data scientist there are usually at least three data engineers If AI goes into the depth of knowledge then data engineering is the breadth And it is not easy or obvious Soon we will have enough quickly baked data scientists but they will be fundamentally lacking in the knowledge of computer science and data engineeringOur courses on Practical AI have collected hundreds of students My AI students frequently ask the same question Now what do I do with all this mathematical knowledge? And the answer is that now you go back to the computer science and data engineering drawing board In the next installment I will show you what you really need to study and practice It will not be easy but it will be right
WFvn2aAyiojV6sH7wwzpvL,This article provides a quick overview and feature comparison of three cloud data platforms: Snowflake Panoply and RepodsThe three platforms Snowflake Panoply and Repods are cloud services that allow you to ingest process store and access data in a managed cloud infrastructure Also each platform provides integrated compute and storage resources for data as part of the service These are considered the defining features of what we call a cloud data platform These features allow us to distinguish cloud data platforms from other cloud data services that enable you to present or process data in the cloud but do not allow you to store large amounts of dataAlthough storing and processing data is at the heart of every cloud data warehouse effort this is only the beginning of a far more complex process More functionality is needed if a cloud data platform is to serve as a long-term manageable basis for analytics To provide a good overview of the overall data management tasks involved here we have prepared a list of criteria concerning the data engineering and analytics cycle The three platforms Snowflake Panoply and Repods are not targeting the same audiences and feature sets As such therefore the platforms are not directly comparable in all aspects The comparison of Panoply and Snowflake is based solely on publicly available informationPanoply uses the Amazon Redshift Data Service together with the Elasticsearch Database Amazon S3 Storage and Spark Compute Architecture Amazon Redshift is a scalable database with roots in the PostgreSQL database architecture but with added cluster abilities It runs solely as an Amazon Web Service This Architecture allows for online scaling by adding more nodes to the cluster Different Panoply clients share the same infrastructure Therefore the highly demanding query loads of one client could affect the query performance of another client Panoply manages the position of your database and could potentially locate your database on a separate Redshift cluster With Panoply you should be able to create multiple databases that are Amazon Redshift databases Databases in this sense have separate storage areas but share the same query engine (ie a DBMS system)Snowflakes underlying architecture layers are not disclosed in detail by Snowflake Here we also have an online scaling platform with a clear separation of storage and compute resources Snowflake allows you to create and manage multiple data warehouses in one account You can configure your compute cluster sizes per warehouse in detail and even configure the online auto-scaling for each warehouse In Snowflake you can scale up (=more resources on one machine) as well as scale out (=more machines) both without interruption of service Data warehouses in Snowflake do not share compute resources to ensure the stable performance of each warehouse Snowflake provides direct database access with external tools as if they were local to the databaseRepods underlying architecture consists of native PostgreSQL (version>10) and TimescaleDB for large amounts of time-partitioned data Storage is managed and scaled through dedicated storage clusters delivering optimal IO speed and online petabyte scaling You can create manage and share multiple data warehouses per account For stable query performance the different data warehouse instances on the platform rely on dedicated resources not shared with any other instance in the clusterWe categorize import interfaces into three different sections: IThis is still the most common form of dataIIPlenty of web services with relevant data are available onlineIIIAlthough many organizations store their data in traditional databases in most cases direct database access is not exposed to the internet and therefore remains unavailable to cloud data platforms Web services can be placed in between on-premise databases and cloud services to handle security aspects and access control Another alternative is the use of ssh-tunneling over secure jump hostsIVReal-time data streams as delivered by messaging routers (speaking WAMP MQTT AMQP …) are still underutilized today but are gaining in significance with the rise of IoTPanoply features plenty of import options in all four categories Panoply however can neither pull files from a cloud bucket or SFTP according to an automated schedule nor can it request a RESTful URL according to a scheduleSnowflake focuses on loading files only but allows you to load files from cloud storage including Amazon S3 or Microsoft Azure With Snowflake you can observe the sources for the arrival of new files and automatically load themRepods enables you to upload files load files from S3 Buckets or load data from external SFTP servers including the automatic load of new files Repods does not provide you with a separate interface for all possible services on the web but gives you a generic API to schedule web requests from any kind of service (eg Socrata) You can subscribe and listen to topics on message routers (currently WAMPonly) to ingest data in configurable micro-batches With Repods you can currently import data from all categories listed above except databasesData imported into the data platform usually has to undergo some data transformations before it can be used for analysis This process is traditionally called ETL (Extract Transform Load) Data transformation processes usually create a table from the raw data assign data types filter values join existing data create derived columns/rows and apply all kinds of custom logic to the raw data Creating and managing ETL processes is sometimes called data engineering This is the most time-consuming task in any data environment In most cases this task takes up to 80% of the overall human effort Larger cloud data platforms can contain thousands of ETL processes with different stages dependencies and processing sequencesIn Panoply you use code to create data transformations These transformations either provide virtual data results that are recomputed each time you access the data (Views) or are materialized to save the recompute effort for each access If the data is materialized it has to be manually refreshed to update the information For instance if new data is available in the sources you have to hit the refresh button to execute the transformation Depending on the size of the sources and the complexity of the transformation you may want to store your intermediate results in a dedicated results tableCompanies often need to load new data on top of existing data in a table The key here is the correct incremental load of the new data Depending on the requirements this effort can involve very complex transformations (see the section on Historization below) Panoply does not offer specific support for such historization processesSnowflake has a similar approach to data transformations You can use the Snowflake SQL dialect to implement data transformations in the form of SQL queries and then materialize them in new tables as you see fit In Snowflake you get low-level control over your data objects in comparison to working with pure databases You can create tables indexes views queries or partitions similar to other traditional database systems such as PostgreSQL MySQL Oracle or DB2 On the one hand you can create powerful queries update and insert statements and get fine-grained control over every aspect of your transformations On the other hand however you need to have the know-how of a database administrator to maintain the system  as would be the case with traditional databases Further Snowflake includes the advanced time-travel feature that allows you to query a table as seen at a specific point in time (up to 90 days back) In the Postgres database this feature has been discontinued due to high-performance overhead and complexity All this being said however Snowflake does not come with support for automated historization of data out of the boxIn Repods you can create data transformation with the help of the so-called Data Pipes to transform data from raw data into a specific data warehouse table Transformations are also created using PostgreSQL queries In these queries you do not have to re-implement data insert strategies for each transformation because the actual insert into the target table is always handled by an integrated post-process that applies complex processes such as deduplication key generation historization and versioning This allows you to focus on the business content in your transformations and not worry about database management tasksWhen you have many sources targets and multiple data transformation processes in between you also have numerous dependencies This comes with a certain run schedule logic The automation of processes is part of every data platform and involves a variety of processes of increased complexity Alone for the scheduling of processes a variety of dedicated tools such as Apache Airflow Automate or Control-M have been made availableProcess automation also requires you to manage the selection of data chunks that are to be processed For instance in an incremental load scenario every process execution needs to incrementally pick specific chunks of source data to pass on to the target Data Scope Management is usually implemented by a metadata-driven approach There are dedicated metadata tables that keep track of the process state of each chunk and can be queried to coordinate the processing of all chunksPanoply and Snowflake do not provide any support for automation out of the box and require you to use other tools for this taskRepods uses a data-driven approach to support process automation The system keeps track of all dependencies and resolves them automatically Repods even looks into the transformation results to determine the further course of action Each Pipe in Repods can be automated to run whenever new data is available in the sources and the target is not currently blocked for the insert Repods also comes with a built-in Scope Management process For each combination of source and Pipe a dedicated scope view is created to manage the next chunk to present to the pipe for processing Scope Management can be turned off for each Source and Pipe individually Scopes can be created based on chunk number or based on time rangesLarger data warehouse systems can easily contain hundreds of tables with hundreds of automated ETL processes managing the data flow Errors appearing at runtime are almost unavoidable Many of these errors have to be handled manually With this amount of complexity you need a way to monitor the processes on the platformIn Panoply you can view a history of executed queries and executed Jobs Also you get an alert view listing problems in your sources services and other assets The query log polls the server for updates every few seconds (no real-time refresh)Snowflake comes with monitoring reports that provide aggregated information on the number of active queries in a certain time range and their resource consumption In Snowflake you can access the internal metadata tables with SQL to extract information on any query activity in the system Snowflake also shows historic query execution in the web interfaceRepods offers a real-time graphical overview showing details on every Pipe that is currently executed or has been executed in the past This overview is updated in real time and allows you to drill through thousands of (historical) Pipe executions You can also search for specific Pipe executions in a search drawer Further Repods allows you to analyze the system log with SQL to extract information about a specific Pipe executionThe usability of the tools depends on the targeted audience The main concern here is how easy it is to create and manage objects (such as users data warehouses tables transformations reports etc) in the platform Often there exists a trade-off between the level of control a user gets and the level of targeted simplicity Increased simplicity entails less control All three platforms Panoply Snowflake and Repods make use of a cloud environment and provide user access using a browser-based web application Also all three platforms follow a hybrid approach using code and involving a point-and-click interfaceSnowflake provides the user with a level of control similar to what you would expect from a bare-bones database system Most of this control is handled by writing code in a code panel (imports tables views indexes etc) Major tasks like the creation and deletion of whole data warehouses and users can be handled through web forms The web interface does not respond to activities on the platform and handles updates by refreshing its views in regular time intervals (the query history refreshes every 10 seconds) Snowflake is only available in EnglishIn Panoply the creation of objects is handled via web forms The setup of imports can be handled through web forms as well while transforms and analytics are entered through code panels Panoply is only available in EnglishIn Repods custom transformation logic is handled in code panels The creation of objects is handled through web forms For analytics in Repods you can use either a workbook approach to create custom analytical queries (compare Jupyter data science workbooks) Alternatively you can use a built-in OLAP tool to drill through the data model with point-and-click functions for quick insight You can also use code to create custom web visualizations The Repods web application is a real-time app based on a bidirectional messaging system This means that every user interaction throughout the platform and every backend system activity is immediately visible to all users without executing refreshes in the front end The Repods platform is available in English and German with other languages following soon (Spanish Portuguese etc)This category evaluates the support for user interactions and the sharing of work and data All three platforms allow multiple users to work together in a data environmentPanoply offers one infrastructure environment for all users However users can create multiple Amazon Redshift databases on this infrastructure Panoply has no communication features to facilitate communication between users on the platform There is a limited possibility to provide documentation for data objects and transformations in the platform You can manage teams if you are assigned the roles Admin or EditorIn Snowflake multiple data warehouses with fine-grained access control options can be managed per account Similar to the access control you have on database architectures you can create custom roles with custom privileges on all objects in the data warehousesIn Repods you can manage multiple data warehouses (called Data Pods) per user and provide access for other users (similar to the way the GitHub platform is organized) You can allow users to follow and/or self-register to a Data Pod User interactions are instantly visible to all other users The owner of a Data Pod can assign the access roles Viewer Reporter Developer Admin and Owner Each user can be assigned one of these roles in each Data Pod Inside the Pod tables can be grouped using Labels and each user can receive individual authorizations based on Labels category evaluates the support for user interaction and sharing of work and data All three platforms allow multiple users to work together in a data environment There is no discussion or chat functionality in all three platformsThe need to manage longer data histories is at the core of each data platform effort The data warehousing task itself could be summarized as the task of merging separate chunks of data into a homogenous data history As data is naturally generated over time there arises the need to supplement an existing data stock with new data Technically speaking time ranges in tables are tracked using dedicated time range columns Data historization is the efficient management of these time ranges when new data arrives The most common approach for the management of such emerging data histories data historization is different from data versioning in the sense that data historization is concerned with real-life timestamps whereas versioning is usually concerned with technical insert timestamps (see the section below)Repods provides support for the historization of data out of the box Data historization is applied after the data transformation and before the insert into the data warehouse table The algorithm is maximally space-efficient in the sense that it minimizes the number of records to represent the histories For data that does not change significantly over time this approach can greatly reduce the size of the tables and therefore have a positive impact on overall query performanceThe remaining two platforms allow for the historization of time ranges but these have to be managed by user-provided transformation logicPanoply provides a feature called history tables which will be discussed in the upcoming section Data VersioningBy versioning data you can track data corrections over time for the later recovery of old analyses Versioning allows you to apply non-destructive corrections to existing data When comparing versioning capabilities you have to consider the ease of creating versions and the ease of recovering or querying versions Versioning can be handled on different system levels: a) Create version snapshots on the storage subsystem (similar to backups)b) The underlying database system might come with support for version trackingc) Versioning might be handled by the data warehouse systemd) Versioning can be implemented as a custom transformation logic in userspaceThe variant d) is a complex technical process but could be implemented in all three systemsPanoply provides continuous backups as a built-in versioning option You can recover a point-in-time snapshot at any time in the backup time frame However you cannot query versions in the active system using this method In Panoply you can also create history tables that insert only companion tables to the original tables When updates to the original table occur the system automatically inserts the existing records into the companion table enriched with the time ranges representing the changes This process can be understood to be a versioning process because the managed time ranges are based on technical insert timestamps and not on business validity time stamps These history tables allow you to query different versions of the same data with SQLWith Snowflake you can a) easily create snapshots of all data and b) get the time-travel feature provided by the database system The latter allows you to flexibly query data with SQL as seen at a certain point in time This feature is available only in the enterprise edition and covers only 90 days of history Long-term versioning logic has to be implemented by the usersRepods does not yet provide continuous backups The platform offers versioning support that is specifically designed for the data warehouse use case You can use freeze timestamps to ensure the recoverability of your data as seen at the time of the freeze Also using simple SQL you can flexibly query the data as seen at old freeze times (infinite number of days back) Versioning often comes with the creation of multiple version records during load times This leads to increased table size which hurts query performance To avoid this you can set a second date to prevent the versioning of any data newer than this second date For instance this feature allows you to freeze all data before October while loading October dataData platforms are used to consolidate data from many sources with different identifiers for the respective objects This creates the need for new key ranges for the imported objects and the need to maintain them throughout consecutive imports These new keys are called surrogate keys Creating and maintaining these keys efficiently is no simple taskRepods is the only tool providing extensive support for internal surrogate key creation and management You can select a few columns as the alternate keys and the system will automatically generate a surrogate key from them The alternate keys will then be mapped to the surrogate keys in every load into a Core TableThe purpose of a data platform is to prepare raw data for analysis and store this data in longer data histories Analyses can be conducted in a variety of waysA variety of Business Intelligence Tools (BI Tools) are concerned solely with the task of creating analytical and human-readable data extracts To prepare data chunks for presentation a data platform provides features to create data extracts and aggregates from the larger data stockPanoply and Snowflake provide you with tools to create analytical aggregates that are similar to the tools you use for creating data transformations This involves SQL code editors Snowflake only saves query results for 24 hours Furthermore both Panoply and Snowflake allow you to access the platform with other tools via ODBC (and similar)  secured by username and password You can use all kinds of dedicated analysis tools such as Jupyter workbooks or Power BI to analyze your data However you cannot use these platform resources to run Python or train machine learning models Also you cannot integrate the results of these workbooks into your data workflow inside the platformRepods allows access to the platform via external tools and additionally provides you with their workbooks to create data stories and analytical extracts Workbooks are sheets consisting of many SQL and Python code cards together with markdown cards Besides workbooks Repods also contains an OLAP (online analytical processing) interface that allows you to drill through the data model using a point-and-click approach to create report results The tight integration of workbooks and reports into the platform allows you to include your OLAP reports and workbook cards in your automation to create near real-time reports and statisticsRepods enables you to create custom web assets called infographics right on the platform This allows you to generate custom web visualizations using the state-of-the-art presentation library d3js The system pushes the data from the data platform right into the browser of every viewer using a bidirectional messaging technology In this way Repods enables you to create fully automated data pipelines  beginning with sourcing the data going through ETL transformations and rounding the process off with the visual presentation of the data in the users browser Updates are taking place in near real time Infographics can be hosted on Repods as a very lightweight web request endpoint and can be embedded as an iframe into any websiteTraining machine learning models is a requirement that todays data platforms have to serve Sophisticated methods are implemented using not SQL but Python or R together with a wide variety of specialized libraries such as NumPy Pandas SciKit Learn TensorFlow PyTorch or even more specialized libraries for natural language processing or image recognitionNone of the discussed platforms currently provides a way to conduct data science tasks directly on the platform The current strategy is to access the platform with specified tools and conduct all data science tasks outside of the platform While this opens up a large variety of tools for you to pick from you are also facing the challenge of hosting and managing compute resources to back the potentially demanding machine learning jobsRepods features Python cards in their workbooks to allow for the creation of machine learning models directly inside the platform This strategy enables you to leverage the platform for machine learning using all its computational powerHere we want to compare how the three platforms present their content to external consumersPanoply provides direct ODBC (or similar) access to the platform secured by a username and a password This enables almost all standard Business Intelligence tools like Looker or Tableau to connect to Panoply and use the data Panoply tracks system alerts but does not send notifications to your phone or email for thatSnowflake also provides SQL access for other tools to consume the data inside of Snowflake Additionally Snowflake provides the possibility to export files into Cloud buckets (AWS S3 or MS Azure) Snowflake enables you to set up email notifications for general Snowflake service availability only You cannot set up system push notifications nor can you set up content-based notifications that alert you if for example the number of customers in France exceeds 1000Repods allows you to create API access keys and control access to prepared data extracts within the platform You can then hand out these access keys to external consumers to access those resources via a standard REST request in any programming language For example you can connect the Business Intelligence tool Power BI to the Repods platform to create compelling dashboards File exports of unlimited size are supported via direct downloads from the browser No kind of push notification is currently supportedA data platform is used to implement a lot of custom complexity with many participating users and over a longer periodA data platform is used to implement a high degree of custom complexity with a multitude of participating users over a longer periodThis requires detailed documentation of the user-provided content Here we assess how the platforms support this task Documentation can always be prepared outside of the platform This however implies the risk of information divergence as external documentation quickly becomes outdatedPanoply and Snowflake provide a few one-line description fields in their web forms However full documentation has not been made available yet Other than that you can nevertheless document code in the form of comments within the code itselfRepods allows you to document all tables and transformation processes each in a Markdown panel which is closely linked to the respective object You can even document each column of the core tables which is considered especially important for users of the platform because the column definitions are tightly linked to the meaning of the dataAll platforms require a certain degree of user proficiency Proper documentation detailing the platform features is therefore required for the professional use of a platformSnowflake provides the most extensive online documentation comparable to the way databases or programming languages are documented For Snowflake this is necessary because most of the functionalities are provided through the use of Snowflakes dialect of SQL Therefore large parts of the documentation are concerned with the Snowflake SQL capabilities Additionally Snowflake provides many guides and YouTube videos to get you introduced to the platformPanoply offers less detailed online documentation Panoply exposes the underlying Amazon Redshift tables directly to users for SQL access so the Amazon Redshift SQL documentation is the valid reference for the SQL dialect of Panoply Also Panoply offers general guidance for data warehousing and a few explainer videos on YouTubeRepods maintains separate online documentation for its Web API usage only This includes guidance on how to access data in Repods by RESTful Web Requests from the languages Python JavaScript PHP and through curl General usage documentation is directly embedded in the tool Similar to Panoply the documentation of the SQL code dialect is not necessary because the user can refer directly to the well-maintained original PostgreSQL documentation for that purpose The web app contains detailed explanations as well as short collapsible explanatory texts Besides Repods has published a series of articles on Mediumcom and a few YouTube explainer videosData platform security can be separated into the security of storage (data in rest) interaction (data in transport) and access controlAll three platforms encrypt data in rest and transport All three platforms provide password-based user authentication Snowflake additionally provides the more secure Two-Factor Authentication mechanismPanoply and Snowflake allow you to access the underlying databases through ODBC-like interfaces with direct server connections Exposing database ports on the internet can be considered a security risk To mitigate this you can (and should) use the more secure ssh-tunneling method with a dedicated jump-host to access these platformsPanoply only provides flat monthly pricing packages Snowflake scales prices depending on the infrastructure size of the used platform Repods offers both models: you have flat packages as well as usage-based pricing All three platforms will require you to contact them for the pricing of larger installationsTo test the platform Snowflake and Panoply provide you with a free time-limited test period of their platform Repods offers one small free data warehouse called a Data Pod for testing for an unlimited periodFor more details and up-to-date pricing information please visit Repods Snowflake and PanoplyTo build a complete cloud data platform Snowflake as well as Panoply need to be combined with additional tools to cover the missing aspects of data warehousing Most notably the automation of data transformations is an important requirement of every cloud data architecture Depending on the organizational context user technical proficiency also plays a key role here as all three platforms require basic data engineering know-how and allow users to use SQL codeSnowflake may be a good choice if you are looking for an online database architecture that is used primarily in an analytics context and you do not want to invest in system and hardware administration Snowflake requires database administrators to manage the platform and is addressing larger data environments As stated above Snowflake misses many important features that distinguish a cloud data platform from a database Since Snowflake seems to be quite similar to a general-purpose database in its usage patterns it may also be an option to use a managed database on AWS instead if you do not have special requirementsPanoply is simpler than Snowflake No system hardware or database administration know-how is required to use Panoply Like Snowflake Panoply misses many aspects that distinguish a cloud data platform from a database Panoply appears to be best suited for less complex data environmentsRepods is simpler than Snowflake because it does not require any administrative know-how for hardware system or database Repods offers a more comprehensive cloud data platform experience since the platform additionally covers the aspects of automation historization surrogate key maintenance analytics machine learning and Data Science Repods has a much stronger focus on building a multi-user and multi-data cloud data platform environment than the remaining platforms The Repods management interface is ideal for a project-oriented approach to cloud data platforms as opposed to a big central data warehouse architecture In sum Repods could be described as a data catalog consisting of data warehousesDisclaimer: The author of this article is from the Repods platform
7yQxP5aUgio2nBptUhSKvB,This article provides a quick introduction on how to securely stream IoT data into the Repods cloud data platform To stream IoT data you first need to have an account and a Data Pod in the platform Sign up for a free Data Pod hereIn a first step we are going to set up a basic IoT Messaging environment from scratch Typically you should have some other setup running productively in the field You can simply skip the steps you do not need In our example we are using the WAMP messaging protocolA basic IoT messaging architecture consists of a message router node and a messaging clientWe are going to use crossbar for the WAMP router To install the router on your local computer you have two options If you have a docker installed you can open a terminal and simply do the following to download and run crossbar in a container: If you have a Python3 installation on your computer you can also do the following to get the router running: Now the router is running However the router is running on your local machine only and is not accessible from the internet To temporarily give it a publicly accessible IP address you can use tunneling with ngrok Once you have signed up for this service you can use ngrok for free Download the tool unzip it in a folder and go to that folder with your terminal to do the following: Your output will look like this: Make note of the temporary address you have been assigned In the upper example this would be the following: However your temporary address will be differentTo receive data into the Repods environment we first need physical data that is being streamed For instance you can have sensor temperatures that are being streamed in real-timeThis is how you publish an incremental counter streaming data on a second-by-second basis: You need to have node installed on your computer Now you can start a terminal and install the WAMP client autobahn as follows: Now you can create a file publish_counterjs with the following content: Now you can start the publisher with: The following output should be displayed now: Now our demo IoT sensor is streaming numbers to the router We only need to connect to the router and start listening If you have not signed up for Repods yet you can do this for free here and obtain a free Data Pod to practiceOnce you have signed up wait until your free demo Data Pod has been created You can activate the demo Pod or any other Pod you have created for this demo by pressing the Start button Once your Pod has been activated you can enter the Pod by clicking on the Enter button to set up your IoT ImportNow go to the Import Panel and click the IoT button: Click the small orange plus sign in the Connections list to create a new connection to an IoT routerIn the next step you need to fill out the form as shown below: Enter the temporary URL that was provided by ngrok (see above) in the URL input cell For this demonstration we will use the following: Create a full URL by prefixing ws://and suffixing/ws as follows: Note: The prefix refers to the WebSocket protocol whereas the suffix refers to the standard route to which the router makes WebSocket connectionsOnce you have hit Save you should be able to see your new connection MyLocal in the connections list: Now you can click the green play button displayed next to your connection: After some processing time the button will transform into a stop button (red square) indicating that the connection has been successfully establishedSince one router can handle thousands of different topics we need to set up a listener for our specific topic repodsexamplecounter to receive relevant data This can be done in the Subscriptions list which is located below the Connections listTo create a new subscription item in the Subscriptions list you can click on the orange plus sign: Now you can fill in the form as shown in the screenshot below: A new Raw Table S_MY_COUNTER will be created to store the messages received with this subscription: When you click the checkbox next to your new subscription MyCounter the Data Pod will subscribe to the topic repodsexamplecounter related to the connection MyLocal Every ten seconds or every 100 received messages a new data package will be inserted into the Raw Table S_MY_COUNTERYou can follow this process in real-time by zooming in the graphical representation of the data packages within the Import Panel: Click on any of the grey stripes to inspect the content of the package in the pop-up grid as displayed below: The column payload_args displays the received message content The purple color indicates that the data type of this column is in a JSON format This is how along with structured data you can also store unstructured data You can now also create a Workbook in the Results panel and perform your data query with native PostgreSQL: Now you are continuously receiving data into the Raw Table S_MY_COUNTER
SSzRCGsW6YWmGbWojxXz74,GetYourGuide is the leading marketplace for tours and activities Our visitors look for great things to do while in a destination: from Paris Catacombs tours to entrance tickets for museums and attractions A key component of our mission is to match our customers interests with the amazing products in our inventory That means that we have to be where our customers are and that is often on search engines such as Google or BingAt GetYourGuide we are familiar with Adwords (Googles paid search product) to match visitor search queries with the relevant products in a destination Adwords is a very sophisticated product and also a very data-driven one We recently explored solutions to scale up the way we download and process Adwords reports: a key dataset Adwords provides to monitor metrics such as impressions clicks and more These reports contain a lot of information and our goal was to download and treat them as quickly and reliably as possible Fortunately Google Adwords provides a rich API to do so but because of the large number of destinations we serve the number of languages we use (GetYourGuide is available in 14 languages) and the constraints the Adwords API impose this problem required a creative and scalable solutionBefore diving into it lets review the setup we started with: Our original solution was pretty straightforward A cron job starts a 2-step process on a server where each step is a Python script The first step connects to the Adwords API downloads the reports one at a time and writes the resulting raw CSV files The second step takes each CSV file and applies transformations such as adding columns or filtering rows The final result is a processed or final CSV which could for instance be inserted into a database and/or archivedAs GetYourGuides inventory grew this original solution was taking too long and we looked for ways to speed it up Over time we also identified other important limitations with this setup: Based on these constraints it was pretty clear the solution needed to be parallel and somehow resistant to transient failures We looked to Apache Spark to do this job and found it to be an excellent fit: The solution we developed took the numerous report requests needed and split them in Spark partitions Each partition can (and will) be executed on a different Spark worker Within each partition the work is partitioned again into tasks (eg a specific report download for a specific date) In turn dozens of these tasks are running in parallel on a worker which in turn is part of a cluster of other workers doing the same thingThere were two additional constraints in this system that we had to deal with This nicely illustrates two other important aspects of SparkThe first constraint is the need to authenticate to the Adwords API This initialization step is essential but only needs to be done once However because the Spark driver and its workers are separated from each other one needs to do this initialization step in each worker or each partition This would be too costly and inefficient to do for every task but fortunately Sparks DataFrame API has a mapPartition() function that allows it to operate at the partition level and therefore allows it to run these initialization stepsThen because some reports are much larger than others data skew is introduced For instance with a large skew most partitions would quickly finish but one or two would take a long time delaying the entire job Dealing with skew usually means balancing the work each partition is doing That can be done with or without knowledge of how long each task will take the latter being easier to implement In our case we first attempted to slice the requests differently so reports of different types could be put on the same partition but that made the code more complex as each partitions result schema should be consistent At the end we compared various scenarios but couldnt completely eliminate the problem In the Spark execution timeline example below you can see the problem The download task in red takes some time and still goes on after most of the other process tasks have finishedBesides the fault-tolerance benefits we also wanted to benchmark how parallel this task can be implemented and measure the total job execution time For this experiment we took a sample of the reports data we needed on a daily basis and did some benchmarks with the number of Spark nodesIn this graph you can see that the execution time is reduced by ~66% when using 4 or 5 Spark nodes instead of 1 By looking at the details we learnt that we were bound by the longest download task which wasnt too surprising Nevertheless compared to our initial setup this was a substantial gainDigging deeper on the Adwords download task the result we wanted to achieve was maximizing the download bandwidth given to us for each partition Although Spark runs multiple tasks per worker we found that this wasnt enough to satisfy this criteria and played with the number of download threads within each taskThis graph shows that increasing download parallelism within each task reduces execution time per task by 75% After 4 threads it seems to reach its minimum Also when the number of threads is too high it also risks triggering the Adwords rate limits which will end up delaying the entire task due to retriesThis work has helped scale a critical data pipeline at GetYourGuide while only leveraging existing open-source technologies Apache Spark is a very powerful tool and recent developments such as the DataFrame API makes it even easier to useThe next step in this area will be to continue to address the skew problem by breaking down the reports in more sophisticated ways We also havent done complete profiling of the Spark workers which could highlight where other bottlenecks are (network cpu instance type etc)This article was originally published on insidegetyourguide
C2Ap25ZWJGeWUsenTH7tci,"Update: I created a sample mainpy with requirements that work for Cloud Function environment and a quick summary of this article in the below repoRecently my team experienced the bitter taste of system downtime which caused our major data pipelines to be delayed for more than 50 hours Causes included an insufficient alert system to inform us of the upstream outage (we were questioned by one of our business stakeholders about why a high-level dashboard wasnt showing this weeks sales numbers) an outage of 4 hours on the vendors server side and the wait time for all historic data to be backfilledIn retrospect the first problem  the lack of an alert system  is the most immediate to be solved We would want to send our filtered logs from vendor system directly to our Slack channel so engineers will be alerted We would also like the messages to be reader-friendly instead of just an entire log entry with too much verbosity Im going to demonstrate how we achieved this goal by building a Pub/Sub topic that listens to StackDriver logs and pushes selected log entries to a Cloud Function which then sends alerts in a nicely formatted way to the Slack channel While this solution applies to all categories of StackDriver logs we use our data pipeline logs given by Fivetran to reproduce the business context This example was written in Python 37Before we start tweaking the logs we need to connect Fivetran logging with GCP StackDriver This step varies depending on your choice of vendor but the essence here is to transfer all of the vendor logs into the your workspace on StackDriverFor more info on basic logs filters you can read more here: https://cloudgoogleNow that we have the Fivetran logs nicely flowing in to StackDriver we are able to set up a Pub/Sub topic to bundle up all Fivetran projects and warehouses to one log source This will provide a centralized input stream for our Cloud Function to process in the next step This Pub/Sub can be imagined as a control center for the bi-directional data traffic: We create the Pull delivery in this step and leave the Push action to next step where we build Cloud Function (triggered by this Pub/Sub topic) We are going to create this Pull from Logs ViewerOr create your own filter by going to the Advance Filter: Feel free to customize based on your particular requirements You can expand an example log entry and take a closer look at the log structures: You may create special selector such as: jsonPayloaddatastatus=FAILURE or jsonPayloadlabelslevelValue=1001 • Once you are done creating the query for interested logs hit Submit Filter (or cmd + enter for mac users) Then go ahead and Create Export Give your sink a meaningful name (eg fivetran_{warehouse_name}_{connector_name}_{log_level}) Choose Cloud Pub/Sub as your Sink Service and select Create new Cloud Pub/Sub Topic under the Sink Destination Youre all set to create the first topic! Note that each sink has been automatically created a service account and granted Logs Writer to the topic you just created Theres no extra steps needed • Go to Pub/Sub  Topics and open your newly created topic If you dont see this keep refreshing the page for a minute or so Then Create Subscription to specifically pull from the SD logs Give it a name but make sure to set the Delivery Type to Pull and keep all others defaultVoila! There completes the Pull side of this topic Next we are going to create the Push side from Cloud FunctionSince my team is not using Cloud Functions for Firebase we use our own GitHub repo to set up a corresponding Cloud Repo (https://sourcecloudgooglecom/) and select Add a Repository -- Create external repository Choose your GCP project id and select GitHub as your Git Provider and then authorize Choose your GitHub repo on the next page and connect This step provides us with better version control and is native to GitHub repos (no charge from Firebase) For your GitHub repo setup your repo with a mainpy that contains the Cloud Function that we are about to create and a requirementstxt for any dependencies Now we have everything we need to create the Cloud FunctionCloud Function that is triggered by a Pub/Sub topic will take event payload in the form of {data:base64_encoded_log_message} Here the log message that you wish to test should be the encoded version of one entire log entry • My main""From here on you can filter and create your own customized payload for Slack webhook Make sure to wrap the formatted payload around Slacks template: '{text:some_very_pretty_message}' I personally find jinja2 templates extremely handy when it comes to passing some string parameters into a deeply nested dictionary like the Slack template5 Once youre happy with your filters and formats deploy this function • No action is needed after this but if youre curious like me you could go back to the Topic that you created for listening to SD logs Now that we have created a Cloud Function that is triggered by this Topic we will find two Subscriptions: the Pull subscription that listens to StackDriver incoming logs and the Push subscription that sends the filtered and formatted logs to your favorite Slack channel The endpoint URL for this Push subscription was automatically generated by the Cloud FunctionAt this point we have attempted to (and hopefully succeeded in): There are tons of flexibility in designing the best alert and monitoring system that fits your company and teams needs By connecting StackDriver logging directly with Slack we get instant notifications of any notable logs that we want to see instead of the default StackDriver notificationsIll be writing about our progress in the following blogs as I explore how to bring all Airflow logs in through this channel Ill also experiment the best ways to: This is my first Medium post so I cannot wait to hear your experiences with GCP StackDriver Pub/Sub Cloud Functions and in general anything that excites you in the Cloud Engineering world"
SBsDCXMWD7u6wz3sWgfzkb,Before going further let me describe what I mean by data engineering This definition of data engineering evolved through the yearsBy data engineering I mean all activities related to data analysis and processing without necessary bringing directly a business value These activities are from extracting data from sources to building models The main of them are Big Data Data analysis Data warehousing data mining/ data science /AI by data wrangling processBy software engineering I mean all activities related to software development It can be developing end to end applications to consume data developing graphical tools that can help to accomplish ETL (Extract Transform Load) or ELT (Extract Load Transform) tasks developing front-end applications Software engineering is the systematic application of engineering approaches to the development of softwareA high-quality software will have mainly the following features: Validity: ability of a software product to perform its functions exactlyReliability or robustness: ability to operate under abnormal conditionsExtensibility (maintenance): ease of modification or extension of the functions requestedReusability: reused in whole or in part in new applicationsCompatibility: ease of being combined with other softwareEfficiency: Optimal use of material resourcesPortability: Ease with which it can be transferred under different hardware and software environmentsVerifiability: easy to testEase of use: ease of learning use interpretation of errors etcAs we know software engineering is by far more mature than data engineering From procedural programming to object-oriented programming software engineering has lots of design principles that help software developers/engineers to be more pragmatic and more efficient This is not yet the case for data engineering But as a data engineer you need standards  design patterns principles you need most of software engineering stuffNOThe first difference is a software usually is focusing on business field but the Data warehouse or data lake will process all enterprise data In the following table I present some main and important differences between DE and SELets take some of the software engineering principles and discuss their applicability in DEFor now almost all data engineering experts consultants folks are not from academic cursus dedicated to data engineering so far They are from different fields and mainly from software engineering world and thus brought their coding habits their design principles and so Other than those people we can find data analyst folks database developers former data scientists too etcPrinciples of Software Engineering that may be used in all phases of software development are the following: All these principles must be observed when it comes to develop an ETL or a product for data engineering but at a different levelI will discuss the most known implementations/concepts of these principles and their applicability in Data Engineering namely KISS (Keep It Simple Stupid) DRY (Dont Repeat Yourself)  YAGNI (You Arent Gonna Need It) and SOLID PrinciplesComplex systems cause problems when it comes to their debugging and maintenanceConsume that principle without moderation having your code simple and very understandable is always a good thing as well in SE as DEYou should not write the same code/configuration/data in multiple places The DRY principle promotes reusabilityOne of the main aspects advantage and design principle in Data Engineering is Fault tolerance and fault is reached by replication and redundancy Data are repeated replicated by defaultInheritance and Composition: Context Session should not be Inherited/ComposedDatabase Normalization: To be avoided join are very expensive in terms of execution time and resources consumingAdding all the possible methods to the class from the very beginning and implement them and may even never use them in the future is not good practice Implement only what you need in the first place and in the future if necessary increase the functionalityXP co-founder Ron Jeffries has written: Always implement things when you actually need them never when you just foresee that you need themDont forget that in DE what we focus more is data and execution cost not really the amount of code It is very common to focus on data while starting to implement functionalities and at that point we know about the bunch of stuff well need but not the actual ones Then guys dont hesitate to start creating the classes or functions you will need even without implementationIn object-oriented computer programming SOLID is a mnemonic acronym for five design principles intended to make software designs more understandable flexible and maintainableIll discuss those principles one by one but quicklyIn Data engineering: Its preferable to have one instance of a many-purpose class than instantiate new objects of new one-purpose classes Because of the parallelism created objects are shared/duplicated between workers and will continue consuming memory or other resources as long as they are alive until the GC remove them And GC is one of the hardest things to manage Then instead of Factory Pattern use Dictionary methods based In the dictionary methods-based approach instead of instantiating a new object of a specific class depending on a keyword you will call a specific method of the same class depending on a keywordSE says software entities (classes modules functions etcIn Data engineering: its preferable to modify an entity than create a new extension of an entity Use case statement if statement or dictionaries because of the reasons I mentioned aboveSE says the inherited class should complement not replace the behavior of the base classSE says the programmer should work at the interface level and not at the implementation levelIn Data engineering: Interface are not advisable implement everything inside concrete classes and do not hesitate to extend them if needed But for sure we should avoid tote classesRegression Tests is a type of software testing that intends to ensure that changes (enhancements or defect fixes) to the software have not adversely affected itIn this post I have tried to summarize software best practices and design principles and their applicability in Data EngineeringI have tried to keep it short and maybe less clear for the folks you did not have strong experience in Software Engineering and Data EngineeringI will write another post dedicated to the best practices and design principles in Data Engineering
Q8NWrxpWfESfYLTs7tanAp,Introduced by Airbnb Airflow is a platform to schedule and monitor data pipelines Especially when it comes to having complex pipelines consisting of many steps of task executions and relation among them At WorkGenius we are using Airflow for a variety of purposes such as extracting freelancers performance and skills as well as pre-processing of data for the recommendation modelIn this article first we overview the basic setup to run Airflow including an example use-case Then we address how it can be optimized by different configuration parameters and the internal tools it providesKey concepts of Airflow such as DAG are skipped as they are well explained in the main documentation If you are already familiar with the installation process you can skip and go to the Airflow Tips sectionFor the demo we use Airflow 1103 via pip on UbuntuWhen installing Airflow it requires a directory to store the config and other files and directories (logs database etc) This directory is defined by the environment variable AIRFLOW_HOME and by default it is set to ~/airflow In case you want to change it to another directory export this variable to your desired directory: After installing Airflow the first task is to initialize the databaseThis should produce an output similar to this: Airflow provides two main processes; one is the server which provides a web UI to monitor and manage the DAGs The other one is the scheduler that is responsible to manage schedule and run the DAGs They can be run separately by: By default all the configuration of Airflow is defined in the file airflowcfg under AIRFLOW_HOME directory For example initially Airflow loads some sample DAGs This is set via load_examples variable In order to remove them one way is to set the value to False These type of changes that contains a modification in the database you need to reset the database first: Warning: resetting database will delete all the data regarding you DAGsTo make sure your changes are applied properly its best to stop both scheduler and server before changing the config and run them again after changing valuesImagine you want to provide a report for your online store The report should be generated at the beginning of the month and contains the sum of the prices of the purchases for the previous month Now one year is passed an you want the report to be automatedIn the code snippet above line 13 defines the DAG itself The first parameter is the DAGs title It runs every 1st day of the month at 4:00 am in the morning The DAG start is set to 20180101 The catchup defines if the DAG should run for every interval from its start time until the current time or not In this case we want it to do so; thats why its set to TrueThere are two tasks defined for this DAG first one is a dummy task define in line 18 The dummy operator takes no callable but it can be used to connect different operators and be a checkpoint to manage your connectionsThe main task is defined in line 19 a python operator that takes a python callable The key point here is to provide the context of the DAG to the python method A context is a dictionary containing the information about the DAG like execution time which defines what is the date and time that the DAG is running for The python callable of this example is the get_sum_of_prices method and takes the context as inputFinally the order of the tasks is defined in line 22 where >> shows which task should be finished (dummy operator) before the next task (etl_operator) startsWhen you run the server and scheduler youll see the new DAG in the list By turning it on it will start running DAGs from the start date until the current time The image above shows one successful run for 20190501Depending on the use cases some challenges may arise while designing the pipeline Below there is an overview of some of those challenges and possible solutions for themIf you are using cloud services to host your Airflow especially the pay as you go mode as in Amazon Web Services one important config parameter is min_file_process_interval This value controls how often the scheduler should check the DAG definitions for changes and by default is set to zero (in seconds) This means that it repeatedly checks for changes which can lead to a high load of CPU on your host machine where scheduler is running Changing this to a higher value like 60 (seconds) will solve this issueIf you are new to Airflow you might get confused with the order of your DAG runs mainly when you run the same DAG in both manual and auto mode There are two different behaviors for the DAG-run timestamp parameter In the auto mode the DAG run timestamp is set to the start timestamp of the DAG For example if you set your DAG to run daily at 15:00 when it runs on 20190515 at 15:00 the run timestamp is set to 20190514 15:00The other mode is the manual run in which the run timestamp is set to the exact time when it is runYou may notice the effect in the Last Run timestamp of your DAG For the example above if you run your DAG on 20190515 at 10:00 manually and it runs in the auto mode as every day then youll see your manual run as the Last Run since it has its real timestamp while the auto mode timestamp is set to one day agoAirflow contains several approaches to execute tasks in DAGs such as the sequential executor or the local executor As its name suggests sequential executor runs tasks in sequential order one task at each time without any parallel execution This type of executor should be handled carefully in case you want to use an external listener for your DAG meaning a task in your DAG is waiting for another task on another DAG to be finished In this case if you run the first DAG before the second DAG it will end up in a state that it waits for the other task and since only one task can be running at the same time the other task wont have a chance to run; and your DAG gets stuck in the waiting stateWhat if your DAG runs are dependent to the previous DAG run timestamp In this case you need to access the last (successful) run of a DAG and get its properties like execution time In general Airflow provides two ways of accessing its information; one is the context parameter that can be passed to an operator The context represents a dictionary of the current task including previous execution timestamp The other one is the provide_session annotator that passes a SQL-alchemy session into your methodAs the documentation says this parameter defines how many instances of the same DAG can run concurrently When your ETL is running in a continuous mode meaning that only the newer data is processed through ETL in each run then you might want to have a check in order to avoid having duplicated data since the DAG runs are independent of each other One solution is to set this parameter to 1 so there are not multiple instances running at the same timeIn case your Airflow host is not in the same timezone that your applications are or you have your application in different regions it might be necessary to have your processes run when the load on your servers and data sources are low Airflow supports this by providing a parameter for start_date of your DAG that defines in which timezone it should be running named tzinfo The code snippet below shows a DAG that is set to run every morning at 3:00 am in Central European TimeIn this article we had a brief overview of Apache Airflow and provided a simple real-life example We showed how Airflow can make it even easier to setup and run the process Also we introduced some ideas to some probable challenges in different dimensions while using it such as config parameters and the tools Airflow provides to the developers
f3CvPTfwPxs5aqUrVky48B,Last December I got an interesting bug report from the VWO support team It seemed like the loading time of a particular analytics report was ridiculously slow for a big Enterprise customer Since I am part of the Data platform a flag was immediately raised and I got involved in debugging the issueTo provide a bit of context Ill explain the basics of VWO Its a platform through which people can run targeted campaigns of various types on their websites: perform A/B experiments track visitors and conversions do funnel analysis render heatmaps and play visitor recordingsThe real power however lies in the reporting of the platform All of the above mentioned features are connected together And for Enterprise customers ingesting all this vast data would be useless without providing a powerful platform that can deliver those insightsUsing the platform you can make arbitrarily complex queries on large data sets Here is a simplistic example: Notice the boolean operators Those are provided in the query interface for customers so they can make arbitrarily complex queries to slice and dice the dataThe customer in question was trying to do something which should be reasonably fast if you think about it intuitively: This was a site that got huge amount of traffic and we had more than a million unique URLs stored for them alone And they wanted to search for a fairly common URL pattern that relates to their business modelLets dive into what was happening in the database The following was the original slow SQL query: Here are the timings: It ran on around 150k rows The query planner showed some interesting details but no immediate bottlenecks jumped outLets examine the query a bit further As it can be seen the query does a JOIN on three tables: Also notice that we already have all of our tables partitioned by account_id So there is no possible scenario of one large account overwhelming other accounts and causing slowdownUpon close inspection we see that there is something different in this particular query The following line needs to be examined: Initially I thought that perhaps the ILIKE operation on all those long URLs  we have more than 14 million unique URLs captured for this account  might be causing the slowdownThe pattern matching query itself is taking only 5 seconds independently Matching millions of unique URLs is clearly not a problemThe next suspect on the list is the multiple JOIN statements Perhaps the excessive JOINs have contributed to the slowdown? They are usually the most obvious contender when it comes to query slowdown but Ive not really believed that to be the typical caseAnd it wasnt the case The JOINs are quite fastI was ready to start tweaking the query to achieve any possible perf gains possible My team and I brainstormed 2 tactics to try out: Yep The subquery when wrapped with EXISTS makes the entire thing super-fastBut this was still extremely slowThere was this one little thing that Id been looking at and dismissing all along Since nothing else was working I decided to take a look at it anyway It was the && operator While EXISTS was the performance booster that made things faster it seemed like the && was the only remaining common factor in all versions of the slow query
jCFwDGVnuNgYoGhqHdqHbS,It all started back in 2016 when Agroknow decided to start working on their SaaS product and suggested I join this new ventureThe team had an ambitious dream; to create a data-powered solution collecting translating and enriching food safety data worldwideThe endgame? To offer insights to food companies enabling them to make data-informed decisions concerning their supply chainWe had to crawl and scrape various data sources announcing food recalls and border rejections worldwide This data may come in various formats; just to give a quick overview: multilingual PDF files custom formatted Excel files RSS feeds and of course HTML pagesTime for our first research & respective choice! How to crawl these data? At the time we were really fluent in Java so we decided to go with Crawler4J but we needed to customize it a bit in order to cover our needs Thankfully its an open source project so that was easy Currently all it needs is a yml configuration file and it takes care of the rest More on this subject on a next post when we dive deeper into this part of our platformOk we got the data but we need to store it! We have a wide variety of data each with its own properties Data concerning food recalls that have a velocity of at most under a hundred per day and data coming from sensors deployed on a field level which presented a velocity of thousands per day; no one framework would be able to meet our needs So we decided to split the data based on its velocity Those presenting lower velocity (eg raw html xls) would be stored into a MongoDB instance and those with a higher one into an Apache Cassandra cluster Again more on this part of our endeavor will follow in a next postBut data is useless if you cannot process it so the next part of our stack is our Transformer into our internal schema; of course based on the entity type we are processing To that end we employed into our stack python and PHP (currently marked as deprecated no judge there) scripts as well as a custom Java project all of which take care of the harmonisation of the collected dataTime for our cooler parts of the stack to take action We should identify important terms in our collected data like the ingredient that was recalled the reason behind a recall or the company involved in it This is where data mining NLP NER ML and DL techniques are employed A number of projects and respective API endpoints are deployed taking care of this tasks As far as technologies and frameworks are concerned we have Spring {Boot Data} projects Flask endpoints taking advantage of scikit and Keras classifiers all communicating with Elasticsearch instances and internally trained models Each producing an accuracy score that if above a threshold is accepted as valid response More on the subject will follow on a next postOur collected data is now harmonised and enriched Time for it to be stored into our internal CMS (Drupal was our choice for this) in order for it to be easily accessible by our internal food expert team to review correct and approve for publishingAnd our collected automatically enriched and human curated data are ready to be published over to our production instances of Elasticsearch ready to be queried by our custom developed Smart Search API visualized over to our application layerJust to give you some numbers as to the current state of our Data Platform there are: All of the above stats were generated by the Elastic Stack dedicated to monitoring our whole infrastructureOver the next posts we plan on completing the walkthrough of our Data Platform; a Data Platform awarded by Elastic for its usage of the Elastic Stack throughout our product
fVY3oXsjH7LBxds23L5WpH,Ensuring global food safety depends on data There is data concerning food recalls and border rejections data pertaining to a huge amount of raw ingredients and commodities and data unpacking all the possible hazards behind a food safety incidentWhen a food safety incident occurs somewhere in a global supply chain whether on a market or a customs level simply gaining access to that top-level information is not enough to truly understand the repercussions What about the product brand that was recalled? Which was the company behind the recall? And if you dive even deeper you also need to make sure you also understand the companys involvement in the incidentIn other words accessing comprehensive reliable data in real-time  and being able to properly analyse and harness that data  is critical for ensuring food safety amid increasingly complex dynamic and international supply chainsThe first challenge is one of access  that is actually identifying the relevant data sources for a particular food safety incident and gaining access to them And although a simple Google search: Food Safety Authority in XX may prove successful in some cases language and local regulation can make this more difficult in certain countries or jurisdictionsEven once we have identified the relevant sources actually extracting the data we need may be further complicated A simple crawl will suffice in many cases but there are still plenty of data sources in Africa and Asia where there are restrictionsHowever once we have overcome this and extracted the data we need this newly acquired information is likely to exist across various different formats and languages A number of directories full of htmls pdfs and xls spreadsheets await processing The next step then is to translate these data into a common language such as EnglishWith this information at our disposal we can begin identifying some of the core factors behind the food safety incident These include: We can achieve this through a number of different state-of-the-art tools and technologies For example Named Entity Recognition (NER) can be used to locate and classify information into pre-defined categories such as organisation names or locations Natural Language Processing (NLP) tools can be used to analyse large volumes of natural language data and transform it into useful information Ultimately the goal is to annotate each food safety incident with as much useful enriched information as possibleNevertheless the automatic data enrichment performed by these tools and algorithms is not enough on its own It is still necessary to bring a food safety expert into the process to manually review all the extracted information and either correct it or approve its publishingAnother important point to consider is the possibility of the same incident being announced by multiple different countries and Food Safety Authorities  for example the Rapid Alert System for Food and Feed (RASFF) which operates across the EU might alert to the same incident as a member states individual authority It is important therefore to be able to automatically identify such cases and suggest them to the food expert as possible duplicates Should this suggestion prove valid the relevant information must be merged so that the analysis performed is statistically sound and has as many data properties as possibleNow (finally) the data is ready to be visualised analysed and correlated From there it can support intelligent decision-making and smart strategy across the food safety sectorAnd this is (part of) our everyday life in FOODAKAI! We are focused on continually identifying new sources of food safety information from across the globe applying the most appropriate automatic analysis and annotation in order for the curated data to be presented at and visualised by end users from all around the worldHere we have focused on the official sources of information from food safety authorities Truly comprehensive and intelligent data-empowerment for the food safety sector requires further information sources too Laboratory testing results price data country level indicators consumer complaints food (or animal) borne outbreaks food consumption rates production and trade indicators are also published throughout the world and can provide additional layers of enrichment and intelligenceOn our next post in this series we will dive deeper into these diverse data types and explore how correlating them can further empower the food safety sectorThe food safety sector incorporates myriad different forms of data published by multiple different organisations and authorities all over the world Applying state-of-the-art technology to this data can unlock new insights and intelligence inform better decision-making and generate a safer smart food industry across the globeWe have decided during this times to open up access to our global food safety data insights for all people taking care of producing packaging distributing and serving safe food to all of us so they can better understand the data insights value
PGUuUzvVt9HjCxd8cmgxrU,Many out there have build (Big) Data Platforms or claim to have done soSome have been awarded for their endeavors some have built something even more amazing! Data products are spawning everywhere everydayNow lets take a step back; this may seem like a trivial task for those out there dealing with relational DBs or rather traditional architectures and platformsA simple object = new Object() or findById may sufficeThe answer is one and only one: Many practises may be applied to tackle this problem: Lets review each of the aboveThis is a somewhat safe approach; urls are unique throughout the web so chances are a hash function on top can prove to be succesfullIt however does not come without any drawbacksIt is not uncommon for urls of websites to be generated based on the title of the sourceSo what about updates to the titles? This can lead to updates to the url as well So even though that is a rather straight-forward choice special care should be taken to such updates in order to avoid duplicatesTime for the another approach; an internal identification process This can be implemented either deploying an API endpoint responsible for assigning an ID to each resource collected (if your architecture follows the microservice one) or a simple method/function/bash script if you follow a monolithic approachThe above suggested method has some very important pros; most important of them being its blackbox way of working Once it has been perfected you no longer have to worry about duplicates in your platform or assigning the same ID to 2 different resourcesOf course they exist! First and foremost time should be spent perfecting such a mechanismAnother drawback we should point out is the rationale behind the identification process Basing it uniquely on the collected content can lead to duplicates as described in the previous case Having some kind of complex process involving various factors (possibly differentiating based on the collected source) may prove more suitableLets switch our attention to the most challenging choice available Time to attempt to identify the collected sources ID assignment methodIt is because it requires knowledge of the remote sources tech stackKnowledge of it should be present if one wants to successfully assign a unique ID able to avoid duplicates For instance Drupal assigns a unique id to each piece of content (nid) always present in meta tags and by-default in CSS classes of article tagsHowever not everything is a drawback for this method! If employed correctly one should never worry for her ID assignment; or almost never Care should be taken only when some major migration takes place on the remote sources side a rather infrequent caseThis concludes our analysis over various methods that can be employed as fas as ID assignment in (Big) Data Platforms is concernedAll of the above have pros and cons as is the case with everything out there Similarly to every choice one has to make you should weight these pros and consApply some kind of hybrid approach taking advantage of pros from various methods It is what we have deployed so far in our platform and seems to work wellKnow your data and your sources
SpcDLEHPvezDsexfcVSCu7,In a previous post we have talked about our Data Platform the tech choices we have made throughout its implementation and the challenges involvedWe collect translate and enrich global food safety data This data covers: There is the challenge to collect and process all this data This is covered in a different postAs you can imagine though a number of workflows are involved in the process Tasks triggering one another signifying the collection processing enrichment of each of the close to 200M (taking into account the hierarchical model employed) data points that are present in our infrastructureAll this has already been covered and will be done in more detail in dedicated postsThere is though a main challenge we have not covered; that of the overall orchestrationBack in 2016 when we first started implementing and deploying our stack due to the teams previous experience in DevOps tasks cronjobs were our initial choiceEvery source we track has its dedicated directory in our backend/processing servers and within each of these directories lies a runsh script This is the script that manages all the action Every single task in each workflow triggered is managed by such a script calling other scripts created with the responsibility to handle each taskAnd this runsh is triggered by crontabFor many of you accustomed with cronjobs execution space of each cronjob may come as natural; we had to learn it the hard wayThe above depicted lines of code accompany every single script existent in our (dedicated) serversDepending on the source translation endpoint triggering scripts may be present Text mining or text classification workflows may take place with their respective scripts All initiating calls to the respective projects and endpointsSay we are done with the collection and processing of each data record we now have to let our internal CMS (Drupal) know of the new data Time to sftp over there and upload the respective transformed and enriched records That system will take care of the rest But more on that end to follow in a dedicated postYou have most probaly already guessed itWe should not stress our (already working at maximum capacity) servers any more than they have to; only new data needs to be taken into account This is where our MongoDB kicks in; the place where all the raw data is stored along with a collected on timestamp and a flag signifying whether or not a record has already been processedAlthough it would make our life way easier firing up crawlers every minute towards each of the sources tracked regardless of the publishing rate may prove fatal in our endeavor We need to configure our ETL workflows to be triggered only when chances are new data are presentThis although easily configurable through cron expressions requires some manual laborWe need to dive into our data and identify the rate at which new records are published Only then can we define an acceptable rate at which we can dispach our workflowsThis is the most tricky question of all Implementing and deploying a workflow capable of executing regardless the stress levels of a server is really challenging Our choice at this point was splitting our workflow into atomic operations this ensures that even though a task or a workflow may not complete no data loss will be observed since each new workflow triggered will always check for previous workflows letfoversWe will cover the above on another post since we are currently switching to Apache Airflow for our ETL pipelines however let us stress at this point that all of the above are crucial One cannot have a robust ETL pipeline if the above remarks are not addressed
2Kipps7yBk3BFvu747UmDr,Just cleared the Google Cloud Professional Data Engineer ExamI wanted to share my experience with you all when I started studying was really worried if I can clear the exam as it had been many yearly since I gave any exam so I know many of you will be worried dont worry you will pass if you have gone through study materials properly mentioned in the post Make sure your family is on board with you for this journey as support from family members is mustmy wife supported me a lot during my journey to GCPBefore you start studying make sure you have already decided the track you should choose Please correctly identify which one you should give if youre looking for a job in Google CloudCloud Architect : If you have experience already in server administration of any enterprise data analytics tools or you come from a networking back ground or Linux administrator or windows administrator make sure you check this one before attempting Google Cloud Professional Data Engineer ExamData Engineer: This exam mostly suits the people with good knowledge SQL ETL PythonRDBMS or Java its important to have these skills else you will find it difficult to use this certification latter on in your careerI can be wrong as well but its good idea to check what you want?Now lets discuss the key topics you need to learn for this exam as sharing question is against the exam process just to be clear you will not remember any question once you come out anyways they are too longPlease go thru these topics as they will be important subjects area for exam and also do some hands on! • Google Storage Near line • Google Storage Cold line • Cloud Composer • Cloud Scheduler • Pubsub: Timestamp • Pardo • Pcollection • Transform •  Side input •  Prefetch in Data studio •  Sliding windows •  Session Windows •  DF cancel and Drain •  Stack Driver monitoring •  Stack driver auditing •  Wild Card in BQ •  BQ quota •  BQ sql execution plan •  Partitioning in BQ •  Indexes in all the DB •  ML types •  Recall •  Precision •  Gradient descent •  Feature engineering •  BQ import and export •  Data proc • https://linuxacademyIf you follow the course and remember everything Matthew says in the course your good for the exam If you top it up with some hands on your good to work for any big companyhttps://wwwcourserahttps://wwwcourseraorg/professional-certificates/gcp-data-engineeringIts a good one just like a cheat sheet tells you what to study and good to go thru onceNumber Four : QwiklabsVery good for hands on and really gives you a feel of it all • Linux academy is the best one for that as well2 You can give one which in is in this course https://wwwcoursera
7osDKcP5nNERj5R2sDxkQg,From the need to deliver quality data quickly and continuously emerges DataOps an approach that promises agile data operations for analyticsWith the exponential increase in data volumes at enterprises in the recent years there has been an ever-growing need to leverage data and to streamline it faster for decision-making process For enterprises to adapt to this new-normal and to become data-driven the teams that consume and produce data must collaborate effectively and use data at each step of the process of making every business decision regardless of whether the decision is big or small To achieve robust and rapid insights there should be a continuous and real-time delivery of data for analyticsA faster and agile approach for the delivery of analytic-ready data requires accelerated data pipelines that can ingest test and deploy data rapidly and can handle huge volumes of data quickly and continuously DataOps is a principles-based practice that aims to achieve faster delivery of reliable self-service data The approach needs continuous monitoring of inputs outputs and business logic Speed agility metadata automation and self-service culture are some of the building blocks of DataOpsSelf-service data is a form of Business Intelligence (BI) in which line-of-business professionals are enabled and encouraged to perform queries and generate reports in close collaboration with data analytics teamWhen business users are empowered to explore data and test their hypothesis without much IT help the practice naturally internalizes data in decision-making process Business users can become innovative and propose new use cases for analyticswith the proposed use cases and businesses can see value in data analytics projects This DataOps practice can quickly lead to incredible agility among data teams within organizations The practice improves teamworkSo this shared mindset is important However for all this to be practical the underlying data engineering process should be robust and agile enough to provide analytic-ready data quickly and continuously to its data consumersSuch faster and continuous delivery of quality data can be achieved with the support of metadata Collecting extensive metadata is the key practice of DataOps Maintaining consistency in metadata and capturing schema drifts is crucial Metadata gives information about data Once the collection process begins it empowers data engineers to automate data processes and implementation of thousands of test cases in data pipelines Continuous automated testing will improve data quality and thereby trust in data analytics Collection of descriptive administrative and structural metadata would give us the essential information required to implement automationDataOps is not feasible without automation Highly automated and augmented data pipelines will only deliver faster data enablement As data pipelines grow in number and size the organizations need to set some standards to govern data at various stages in the pipelines Standardization and repeatability are the core components of automation The organization that implements automation is more impregnable to schema drifts and changes in dataAutomated continuous testing is essential in building trust in data Thousands of tests cases can be generated automatically for data pipelines and can be used to test data continuously The tests are simple and additive Whenever a change is made to data pipelines test cases are created in DataOps These tests are the early warning indicators of data quality issuesAs the complexity of data pipelines rise the interconnections in the data elements also becomes complex and the pipelines are prone to more errors Automated continuous testing can help boost confidence in dataFurther statistical process controls ensure continuous monitoring of the data pipelines by analysing the output data Any variations in data outputs can be identified studied and appropriate action can be taken to resolve the issuesAll these practices of DataOps if applied to the fullest can reduce cycle time drastically allowing the business users to dive deep into the data without any waiting time It also encourages collaborative working environment and promotes agility
7HaZVHLBqytefhYwZwEuH8,Oracle SQL Access to Kafka (OSAK) enable external table to read record from Kafka topics Using OSAK we can read data join data with Oracle table and load data into Oracle tableThrough this article I will install and configure OSAK Reading CSV records from Kafka topics Loading these data to Oracle tableFirst download new version of sqldeveloper from linkThen copy orakafkazip file from orakafka directory inside sqldeveloper directoryThen unzip orakafkaAfter unzip file run orakafka_distro_installsh which will unpack kit and create required directories The directory path is pass to command The script will create ORA_KAFKA_HOME on this directoryThem configure APP_DATA_HOME variable by run configure_app_data_homebefore start work with ora_kafkash script we need to Check the java version on database server OSAK need JAVA 1The script orakafkash will use for all the next operationsTo verify OSAK install kit run orakafkaFirst use add_cluster option to create a cluster filesystem configuration directory and generate DDL to create cluster configuration directory in database we pass the cluster name parameter KAFKACLU1If you have secured your Apache Kafka cluster you need to configure security properties on file orakafkaproperties For more information refer to the Apache Kafka Security informationNow we can test our cluster configurationFirst use adduser_cluster option to grant required permissions for database user on Kafka clusterThe command generates DDL to grant required permission we must run task 1 script as sysdbaSecond we use install option to generate DLL to install ORA_KAFKA package in database user and create required directoriesThe OSAK support two of record types: In this demo we will produce comma separated text (CSV) to Kafka topicsBefore data can be read from a Kafka topic register the cluster and assign a unique nameThe next step is to create OSAK views that map to the Kafka topic When the Kafka record format is CSV or delimited text before creating the views you must create a reference table whose shape is mapped to records in the Kafka topicNOTE: The order of the columns in the reference table must match the order of the fields in the Kafka recordif the Kafka record format is JSON_VARCHAR2 a reference table should be passed as NULLFor our demo the table refer to customer table we will create customer table in database schemaNext create view use ORA_KAFKAThis will create view for each topic partition for our demo the topic created with 3 partition we will have three viewswhere KAFKA$PARTITION is the Kafka partition id KAFKA$OFFSET is the offset of the Kafka record and KAFKA$EPOCH_TIMESTAMP is the timestamp of the Kafka record The remaining columns represent the fields in the CSV dataThe ORA_KAFKALOAD_TABLE procedure loads data from a Kafka topic into a database table This procedure creates a view that is used internally and maps to all partitions of the Kafka topicThe view is not deleted at the end of the call procedure So when call procedure again it continue from last pointwe can call this procedure in loop To continuously load Kafka data into the database
Utf3hX27oGkUqEkgZFFNfv,Big data analytics tools are a constantly shifting landscape Most organizations want to be data driven this means collecting your data into a data warehouse where it can be explored The two biggest solutions in the space are AWS Redshift and Google BigQuery Here is what your big data administrator needs to know to choose a solutionBoth of these solutions are focused on OLAP (Online Analytical Processing) workloads These are usually column store databases They are designed for fast analytics not transactions like your products main APIRedshift was launched October 2012 It runs a forked version of PostgreSQL 802 Its changed under the hood to perform better on analytical workloads and is missing some postgres 90 features Redshifts mental model is that of managing a cluster of nodes You need to choose the number of CPUs and storage for your needsBigQuery was launched May 2010 It is a productionized version of an internal Google Database called Dremel It is a serverless model there is no concept of hardware you create tables upload your data and analyze itYou trade speed for flexibilityIn Redshift you are given control over how to tune your big data database Youll want a dedicated person to tweak the data model and fine tune the cluster for the best performance and lowest infrastructure cost The payoff is that frequently queried data becomes cheaper to analyze this results in lower cost of generating reports and analysisBigQuery is simpler Use it if you are not sure what your analytical data model should look like or you have lots of data that is rarely queried BigQuery could be more appropriate if you dont project needing a large analytical team in your organization or cant afford to dedicate resources to build world-class analytics If you are a Data Scientist working with a new dataset you could build a prototype faster with BigQuery Similarly if your organization isnt a technology company or you have Medium Data BigQuery will be easier to get started withBoth Amazon and Google are competing to offer the most competitive cloud This benefits consumers as you can assume pricing will be comparable for similar services The differentiator will be the ecosystems and where your data isData exhibits a gravitational property Say your main products database is on Amazon there are very real cost incentives to use Redshift as Amazon will not bill for transferring the data within its cloud Same goes for loading data into BigQuery from Google Cloud StorageTheres also considerations for analysisAs the most popular datawarehouses in the space both solutions have a mature ecosystem of Business Intelligence tools Note before using any BI tool youll want to build an ETL pipeline that copies the production data into the datawarehouse One exception is if your building a prototype for a stakeholder and its main backend is a datawarehouse BI tools should also be sandboxed by setting up a read-only user at the database level this way your analysts cannot damage your data and are free to experimentMetabase  free open source BI tool Popular with startups as you can install it in a VM for a BI solutionLooker  paid product Uses proprietary SQL extension LookML for enhancing your SQL queriesTableau  enterprise BI solution Heres a demo of its UIGoogle Analytics 360  enterprise version of Google Analytics You used to have to have google cloud costs in excess of $150000/year They no longer state this on their pricing pageOne of the advantages of Redshift was that it runs standard SQL Meaning all SQL tooling works out of the box on Redshift Whereas BigQuery ran a variation of SQL that had its incompatibilities (BigQuery added standard SQL support in 2016) This could explain Redshifts early dominance in the space These days most BI tools support both platforms equally wellGoogle Trends show that Redshift is more popular than BigQuery in the United States It seems that data is viewed as an area to invest in for many organizations As such they are okay investing into developing their datawarehouse and having fulltime engineers work on this problem if it gives them a competitive advantage The fact that BigQuery used a variant of SQL may explain why Redshift was able to overtake BigQuery despite its 2 year headstart Investing in something that is nonstandard is viewed as more risky for an organizationWe see most search volume from the big tech hubs Washington California New York Interestingly in Georgia more people are searching for BigQueryWorldwide interest in BigQuery was overtaken by Redshift and in recent years the gap has been closingIt seems like Redshift is more popular in the United States India and Australia BigQuery is more popular in South America and EuropeDatawarehousing technology is a big decision for organizations Leveraging your data effectively can be a competitive advantage Its not always clear for stakeholders how much to investWhen investing heavily organizations invest in Redshift due to its perceived safety However for smaller organizations or projects time-to-market would be faster with BigQuery First determine what your current needs are and project what they will become Then decide on the tool that is right for youI hope this post helps your big data administrator decide which tool is right for you If you need help assessing your existing software strategy or plan out a new project you can contact meHarry Moreno is a software Consultant with a track record of developing solutions people love If you need software solutions please reach out to him directly on his site harrymorenocom/abouthtml on June 29 2018
VQHxtsb9UysUZgVznDf4MJ,Building ETL pipeline with load monitoring and error handlingWelcome to part 3 of the tutorial series Build a Data warehouse in the Cloud using BigQuery This tutorial demonstrates how to build a real-time (or close to real-time) analytics ETL pipeline using Cloud Functions In Part 2 we wrote a Cloud function to load data from Google Storage into BigQueryOur Cloud function was built on top of the hybrid solution that we completed in Part 1In Part 1 we created a streaming lambda to transfer our data files from AWS S3 to Google Storage In Part2 we wrote a Cloud function to load different types of data in BigQuery We will use it as a foundation for our ETL pipeline which will be responsible for loading data monitoring and error handlingBefore trying this article follow the Python setup instructions in the BigQuery Quickstart Using Client Libraries Part 1: Sync only new/changed files from AWS S3 to GCP cloud storagePart 2: Loading data into BigQuery Create tables using schemas Well use yaml to create config fileYou are here >>> Part 3: Create streaming ETL pipeline with load monitoring and error handlingPrerequisites: Just install Anaconda distributionBefore starting lets think about the architecture of our ETL pipelineThe next diagram demonstrates ETL pipeline flow and itscomponents The solution is very similar to one suggested by Google in this articleThe difference is in details Part 3 adds extra information about how to ingest different data types and formatsWe will be using schemasyaml to create table definitions So once our Cloud function is deployed we can successfuly forget about it If we need to add a new table well just add a new record to schemas fileAnd the third novelty is that we will add a few helper functions to read the ingestion results logging from Firestore collectionsWe will be using BigQuery Python API in order to process and load files You can find usage guides here  https://cloudgooglecom/bigquery/docs/quickstarts/quickstart-client-librariesWe will also need to have a Google service account in order to use Google account credentials We have already done that in Part 1 but just in case you are only interested in this particular tutorial this page contains the official instructions for Google managed accounts: How to create service account After this you will be able to download your credentials and use Google API programmatically with Nodejs Your credentials file will look like this (example): /your-google-project-12345Run the following commands to clone the starter project and install the dependencies: The master branch includes base directory template for Part 1 with dependencies declared in packagejsonAfter you did git checkout part3 you will be in branch for this part with all the Python code we need It is slightly different from Part 2 as we added topics Firestore and move files functionUse your Google account credentials (replace the file and path to match yours) and in command line run this: Then in your command line run these two commands which will create two buckets: If you dont have gsutils installed on your machine you could try using Cloud Shell Great! Now if you go to Google web console you should be able to see your new bucketsNow lets create the topics we need to publish ingestion results into They will be used to trigger two other Cloud functions which will be moving processed files into success/error bucket Run this in your command line: Run this to list your topics: Great! The topics are there nowIn the ideal scenario we would like to have a record of each data file ingestion: Lets create your Cloud Firestore instance follow these steps: Now lets deploy our cloud functions: streaming_staging which we wrote in Part 2 and two extra functions which will be responsible for handling successfull (streaming_success_staging) and error (streaming_error_staging) data ingestion and will move the file into a relevant bucket so we could investigate it laterWe will create a shell script for this This is how our Cloud Function streaming_staging looks: /mainLooks huge but we only added a few functions Check Part 2 to see the differencemainpy contains 6 different functions to process data we have in our source files Each of them handles one of the most popular scenarios to process the data in modern datalakes We described source data files types in the beginning of Part2 article but essentially they are different types of JSON and CSV Have a look at /test_files folder Each file with table_ prefix represents a use case and has its own data structure Also take your time to familiarise yourself with the code or just keep reading and I will talk you through each stepNow lets create a shell script to deploy it /deployshIt is slightly different from Part2 deploysh Well add ENVIRONMENT variable and labels to simplify future deployment and monitoringHave a look at this file Last two commands set labels and environment variable staging For example staging environment variable will be used to create Firestore collection calledstreaming_collection_staging Replace your-project with your project name and we are ready to goNow in your command line run /deploysh staging And after that you should be able to see JSON response with deployment progress saying that your cloud function has been successfully deployedNow lets deploy two functions which will move ingested file to success/error bucket depending on the resultEssentially it is the same function /functions/move_file/mainpy but we will name them differently and each will have its own trigger topic/functions/move_file/mainpy: To deploy the first function run this in your command line (or Cloud Shell): Change staging suffix to whatever environment you use This is just a shell command and we can always wrap it all up into one script with variables in the endNotice that we have just set up a trigger and two environment variables for that Cloud function: Now run this in your command line to check function status: Great If you see this then your function has been successfully deployed: Now lets deploy the second function to move file to success bucket: It is almost the same We just changed the trigger and destination bucket: All we need to do in order to run a test is to copy a file to our source bucket called project_staging_files In your command line run this: Now go to Google web console and run a query: select * from `your-projectstagingtable_7`: Great! Records from your file have been successfully loaded into BigQuery Lets see what we have in Firestore Go to Google web console and select Firestore: Now lets go to our files success bucket We should find our file successfully moved from project_staging_files to project_staging_files_success: Try running cp command again on that file and you will see duplicattion attempt record in Firestore which means everything works as expected: Now lets test error scenario Run this in your command line: We have just tried to copy the same file to a wrong table We named our file meant to be for table 7 as /table-5_data_new_line_delimited_jsonjsonOur cloud function read the file name and decided to insert its contents into table 5 obviously using a method which is not suitable for that dataHere we received an error and file has been left in project_staging_files folder for further investigationNice! Data load pipeline seems to be working fine Time to do some load monitoring ad set up alertsFirestore database we set up earlier makes it easier to find and fix data ingestion errors We can filter them by using standard Python API for Cloud Firestore Query request looks like that: firestore/show_streaming_errorsTo run this query on your machine do the following: 2 Install the Python Cloud Firestore module in your virtual environment • Query the database to get error records: $ python /functions/firestore/show_streaming_errors/functions/firestore/show_streaming_errorspypy will run the query and pull the results out: https://gistgithubBefore it shows the results it may return error asking to provide Google credentialsIn order to get a filtered list of all records since midnight we will use compound filtering like so: We have also adjusted final output function to display all ingestion results: Check the full file /functions/firestore/show_only_todayIn order to execute it in your command line run this: /functions/firestore/delete_where$ python /functions/firestore/deleteCheck file delete_by_nameRun deactivate to deactivate your environmentIn our production environment we would like to keep everything under control and be notified when data ingestion goes wrongOur Cloud function generates logs in Stackdriver logging  So we will create a custom metric for ingestion errorThen we will create alerting policy to notify us by email whenever data load fails • In the advanced filter paste the following configuration • In the Metric Editor fill in the following fields and then click Create Metric • Now go to Create a Policy page • In the Create a Policy window complete the following steps- In the Title field enter streaming-staging-error-condition- In the Metric field enter logging/user/streaming_staging-error- In the Condition trigger If list select Any time series violates- In the Condition list select is above- In the Threshold field enter 0- In the For list select 1 minuteGreat! After saving Stackdriver will start sending notifications every time data load failsDont forget to clean up and delete everything to avoid incurring charges to your Google Cloud Platform account for the resources used in this tutorialWe have deployed a Cloud Function to load data from Google Storage into BigQueryWeve created a streaming ETL pipeline to load data stored in Google Storage in different formatsWeve set up data ingestion status logging with FirestoreWeve also set up Stackdriver Alerts to send notifications by email in case aomething goes wrongStay tuned! I post weeklyOriginally published at https://wwwmydataschoolcom
4oYtTV4xaKVLBTMUkcJpJo,I would like to talk about clickstream data in this blogThere is a lot of answers to this question In my opinion the most important reason is you can understand better your visitors behavior and you can redesign your business model according to dataDivolte: Divolte is a scalable click stream platform It provides a javascript api and rest api for client side and you can store your data on Kafka HDFS or Google Big Data platformKafka: It is a distributed streaming platform and we will send the clickstream data from divolte to kafka directlyDivolte is a collecting server for your clickstream data You can store clickstream data directly HDFS or Kafka It provides a javascript for the client-sideDivolte uses Apache Avro serialization system to store the dataDivolte provides ip2location support and user agent parsing You can define your custom events in the configuration fileFirst you can check out my sample code from github for my configurationYou can find divolte configuration files in data/divolte folderYou will see 3 different files: In my case I customized some values for basic You can configure it if you need it I recommend this documentation: You will find configured kafka in docker fileIf you are ready we can test whether our clickstream data is going to kafkaBuild docker-compose file vi below command: You will see divolte homepage when you visit http://localhost:8290 Actually you can disable this page from the configuration Because you dont need this welcome page You just need javascriptSending clickstream event from divolte: You can run the below command on the browser console to test: 4Now we can connect to kafka server for checking data via this command: You will see different chars in the message because of serialization dont worry about formatThanks for reading
BLtj24jxBj293qBodyq6K6,So I was working with one use case to send a spark DataFrame results over e-mail as an attachment I looked through multiple sources for the same in Scala and couldnt find an appropriate way to send the results in DataFrame over e-mail Finally I decided to work my way through itThe steps mentioned are of Scala but can be used in Java too with minimal changes
VVyCgkC3RUyUfAph3VwwaX,I have been working with Apache Spark for around two years now and considered myself having fairly good knowledge of sparkMaybe one of the reasons I was unaware of the Closure in spark is because I was working with the Spark DataFrame APIs too long and operations at RDD level are like a black box to me Well its still no excuse to ignore a concept as important as closure And trust me it is IMPORTANT to understand closure in spark so that you dont spend hours debugging a perfectly looking code trying to figure out why it is not working as expected Well enough talk lets get startedLooks simple enough right? But this code is WRONG as it might give unexpected resultsThe reason behind this is the life cycle of the variable counter Lets say we have a spark cluster with 1 driver and 2 executor nodes Now for processing of above code Spark will break the above piece of code into tasks to be run on executors For processing the executors also need information regarding variables or methods right? So this information is serialized by Spark and sent to each executor and is known as CLOSURENow each executor has its own copy of the variables and methods and all the updates will take place in that copy only So if my task is counting the elements in an RDD the count in each executors copy will be updated corresponding to the data it is processingNow consider we are working on a single node cluster where both the driver and executor are on the same machine There might be a scenario that execution of the foreach function by the executor will take place within the same JVM as the driver and the executor will reference the drivers counter and might update it and we can get the actual count of the elements of the RDDBut there are a lot of ifs and mights in the above scenario and not how we expect our code to runFor scenarios where the updates on variables are split across various nodes across a cluster and we need a well-defined behaviour of our code We can use AccumulatorsIn general closures  constructs like loops or locally defined methods should not be used to mutate some global state Spark does not define or guarantee the behavior of mutations to objects referenced from outside of closures Some code that does this may work in local mode but thats just by accident and such code will not behave as expected in distributed mode Use an Accumulator instead if some global aggregation is neededSumming up closure is those variables and methods which must be visible for the executor to perform its computations on the RDD This closure is serialized and sent to each executor Understanding of closure is important to avoid any unexpected behaviour of the code So next time before updating a variable in a loop while dealing with spark think about its scope and how Spark will break down the executionI will add the next article to discuss Accumulators and how they can be used to fix the code mentioned above and ensuring a definite behaviour of the counter
3BGZPsKicPxfFT2RpEGdy7,In any data pipeline (say ETL Pipeline) it is important how to extract data from any data source there will cases where datatype could be simple or complex Handling such scenarios where data type and format are complex leads to complex processing I will try to show spark SQL utility functions and APIs which can make your task easyAs a part of Apache Spark 2x in orgapachesparksqlfunctions they enable a developer to work with complex or nested data types ETL in which data are JSON objects with complex and nested structures: Map and Structs embedded as JSON
2YXfMoCnZP5xfUhSecqEjJ,As a Data Engineer one should be able to understand computer science core components then how to store and analyze the data Big Data Engineer should understand how to build end to end large scale applications for both batch and streaming applications When you want to store and process the data Hadoop (Big Data) ecosystem is of great help to build the distributed and scalable applicationsMajor activities by a Data Engineer: Extract and Store the data: To store the the data in distributed manner Data engineer prefers HDFS to start with few nodes in the cluster and increase as per the requirementTransform the data: To transform the the data in distributed way Spark is a powerful tool now a days And for realtime analytics we can use the spark streaming The raw data will be transformed after necessary calculations and store in SQL or NoSQL databases to visulise the data furtherVisualizing the data: Once the data is loaded based on the requirement we can visualize the data using different tools like Tableau Qilkview etc
P4LLp4EYBQueJTghdGE8km,Apache Flink is an open-source distributed stream-processing framework developed by Apache The core of Flink is built on Java and Scala Recently Flink gained popularity over other streaming frameworks such as Spark Samza and StormFlink natively supports Java and Scala But It also supports PythonPyflink is simply a combination of Apache Flink with python This combination allows us to make use of all the computing capabilities of python on Flink and all the features of Flink can be used on pythonSo we have two options to install Flink on AWS Either we use an Amazon EC2 instance download the dependencies or make use of the pre-existent installation using EMR to download Flink The latter is the recommended approach to run Flink on AWSHeres the description provided by Amazon for the EMR service: Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark Apache Hive Apache HBase Apache Flink Apache Hudi and Presto With EMR you can run Petabyte-scale analysis at less than half of the cost of traditional on-premises solutions and over 3x faster than standard Apache Spark For short-running jobs you can spin up and spin down clusters and pay per second for the instances used For long-running workloads you can create highly available clusters that automatically scale to meet demand If you have existing on-premises deployments of open source tools such as Apache Spark and Apache Hive you can also run EMR clusters on AWS OutpostsAmazon EMR is an AWS service EMR stands for Elastic MapReduce As the name implies it is an elastic service that allows the users to use resizable Hadoop clusters and it has map-reduce components for the computationsSo it is an in-the-cloud solution hosted in Amazons data center to supply the needed computing power and on-demand capacity to resolve big data challenges Moreover using Amazon EMR simplifies the setup and management of the cluster of Hadoop and MapReduce componentsI will start by outlining the steps I followed to install Flink on my local device (OS ubuntu 1804) then the EMR setupI used the local installation guide from Apache Flink and the steps were as followed: I have faced an issue while running Flink with the system path configurations for python So ensure that python3 is added to the path and it does not conflict with AnacondaThe cluster will require some time to be ready for usage Once it is ready we can test Flink on EMRIt is worth mentioning that I had to fix Flink configurations to run the cluster due to memory errors using the default configurationsThe default configurations: So what was the issue with the configurations? the size of the task manager heap and job manager heap configurations is commentedso I replaced the following configurations and then the cluster ran successfullySettings required to enable the web interface: Lets test if Flink is working by using the examples provided by Flink: For this I choose the word count batch example This program will return the number of occurrences of each word from a given input file To run a Flink job we use the following command: To run the wordcount example we just provide the jar location to the Flink running command as follows: From the console we notice: Next lets check the web service UI: The web service UI main page outlines information about the submitted jobs the number of tasks and the status of these jobs If we click on the Job we will be directed to a detailed description of the job executed The overview includes information about each task (duration parallelism records sent and bytes) and the DAG diagramThe python example for word_countThe above fails because Flink is trying to use python 27 to run the program which is an incompatible version with Flink we need python version above 35 Lets try to check the python version installed on EMR: So even though the default python is set to python 376 Flink was running python 27So now the code is running successfully what we can infer from the above: What about the web UI: When you dont want to use Flink cluster you can stop the cluster with: I have just started learning Apache Flink so I decided to start blogging about this learning process with a series of blogs covering realtime and batch streamingLooking forward to the feedback or questions I can be reached with Linkedin Email and Github
8tf5dkUxmrsnhU8YmiYSg4,Is coupling troublesome for scaling and decoupling a better alternative over it? Which one to use when is not an easy choice but this article will surely help in gathering more knowledge around these often confused  conceptsA simple idea of bringing a machine with high volume disk for saving data and having enough processing power to process the saved dataThe coupling of storage and computation has revolutionized the way to process the dataThe idea of moving computation near to data helps in reducing network cost and increasing throughput performanceThe RDBMS is strongly coupled and uses tiered storage to retrieve data fastThe HDFS allows the data locality principle to move computation logic near the data-node hosting the dataSpark can use a data co-locator feature to improve server utilization and system throughput by moving computation near to dataHbase uses co-processor to move computation near to dataConsider a Postgres is running on 4GB RAM instance with 160GB SSD Due to the reporting workload and spike in the inflow of data the instance is scaled up to 8GB RAM with 200GB SSDThe reporting workload runs for an hour and the remaining time server is underutilized After purging old data we are still helpless to reduce SSD size ( 200 to 160 GB)At scale the resources are getting underutilizedAchieving an elastic scale with a coupled system is infeasible Having read-replica with data duplication is good if you have a small dataset At scale we need a replica on the same data but with a complex strategy to distribute data and synchronize replica( easier said than done )This is a case of the theory of small scale verse theory of big scale A coupling technique that works wonder on small scale starts creating challenges on a big scaleAssume we have an HDFS cluster running with 100TB storage The analytical team is running a dedicated spark cluster to analyze data The reporting team is running a short term one-off spark cluster to build reportsBoth teams have different clusters with the ability to independently scale without impacting HDFSDecoupling provides separation of concern to each team with an ability to use the best solution for a specific use-caseAlright! This solves the problem and now you might have already guessed that decoupling might be the future of the system design Well you guessed it correctly! But will decoupling replace a coupled system it wont be happening soonA decoupled system is good for offline workloads working on cold data and interactive analysis for second-level latency With some changes the coupled systems can do the same thing but it comes with a higher costA coupled system is good for millisecond throughput such as a production database Can a decoupled system provide such throughput? No it wont unless we have an expressway network with low latenciesUmm Confused? Let me reframe the question will future system would be coupled and decoupled at the same time? Yes it already startedHBase on S3 release on AWS EMR it uses HDFS for WALs and HFiles (ie data) are stored on S3 A single primary HBase cluster optimized for production workloads and you can spin the HBase read-replica cluster to serve reads directly from S3 data all possible due to decoupling We use this strategy successfully in production at CreditVidyaApache Druid uses the disk to save hot data and deep storage is saved on cold dataLets understand concepts through an example by building an OLAP for e-commerce using PostgresThe information for funnel analysis is present in the user_tracking table The good thing is the production database keeps the last 7 days of user tracking historyWe can do this by using a secondary hidden replica database (iNow the team is asked to add a new feature over dataTo enrich user tracking data with all the product metadata that he navigates through or search on a portalWe can accomplish the same task by creating a materialized view by joining user_tracking product_catalogs tablesBut the new feature is demanding changes on source systems ie productionA better way to handle is to do ETL (Extract Transform Load) and move data to a different destination system to manage persona requirements AWS Redshift looks promising for our use case do ETL to enrich data from Postgres and move it to redshiftWe used separation of concern to mitigate OLAP challenges with OLTPNow the team is asked to add more new features over dataWhoo… Now redshift needs to save all the data even less frequent access data With these you can observe cost utilization patterns like pay 70% for 10% use cases spending more on lessThe above problem can be solved by defining the data life cycleIdentify hot and cold data serving hot data through redshift and cold data through Athena / Redshift-spectrum (ie SQL on Hadoop) on decoupled storage such as hdfs or S3Using managed and cost-effective services help you save big time on TCO (Total cost of ownership)The team needs temporary EMR infrastructure to run spark jobs Spark can read data using the Redshift Spectrum Athena EMR Prestodb or S3 The underlying infrastructure is elastic with the ability to scale compute and storage as per use caseAfter completion of work the temporary infrastructure can be tear down without impacting other systemsLet assume we have 1 billion records partition across n database segments each segment is serving (1 billion / n ) recordsYou can use different strategies to distribute rows such as round-robin evenly distribute key-based or duplicationThe strategies are chosen to minimizing the data movement across the segments and getting results derived from the segment itself a classical application of data locality principleSo the decoupled system achieves throughput performance only due to the coupling of segmented data on the serving nodeNo a decoupled storage can be as simple as attachable EBS volume The cloud vendor is using decouple storage as a marketing stuntShould we!!!… Since the production database provides data retrieval in milliseconds the decoupling would add extra network latencyA distributed database an extension of Postgres can provide high read/write throughput MySQL provides a sharding technique based on master-slave architecture All the write request goes to master which can vertically scale and read request is serve by slaves which can scale horizontallyStill both cannot be called decoupled systemsThe coupled system has a single purpose to provide fast access to data In the future we can expect an extension in a database to support decoupling for cold dataFound this article useful? then hit that 👏 button below to spread it around even more
ev2ingYagYAxmCdd8p5jS4,Want to learn more about data platforms or cloud-based solutions? Check out our website: https://wwwinfinitelambdaData sharing  providing read access to data without transferring ownership  has become increasingly common most recently being deployed in support of COVID-19 related projects Its an efficient way to provide third party access to large amounts of data without having to architect a data transfer process This means its well suited to scenarios in which an organisation has preexisting raw data available in its data warehouse It generally also doesnt require much infrastructure setup and usually provides automatically refreshing data so the recipient can move directly to applying quality checks transformations and reportingAt Infinite Lambda we were recently involved in a data sharing proof-of-concept project We worked with an e-commerce company whose email marketing data was residing with a third party who was also managing their email campaigns This third party was already providing some insights on campaign performance but since the data were pre-aggregated our client was not able to arbitrarily slice and dice data perform detailed cohort analysis or integrate the insights into their own attribution model To remedy this the third party offered to provide raw data via a Snowflake shareWe stepped in to model and transform the data into a state that would facilitate reporting and business intelligence specifically to expose it to Looker the clients reporting toolWe received the data in the form of several secure views via a Snowflake share Since these secure views were querying a much larger underlying table they were slow to query (Snowflake secure views bypass certain optimizations used in standard views) The first step was therefore to clone the data into a RAW layer before applying any transformations removing these performance issues for subsequent nodes in the DAG This is represented as Layer 1 in the diagram below dbt is first and foremost a data transformation tool but it is just as well suited to an ingestion task like thisWorking with event stream data meant that we were able to make use of an incremental materialization strategy in dbt  only refreshing new or changed table records after the initial build We also masked PII (personally identifiable information) in the RAW layer but applied no other transformationsBefore modelling the data various quality controls and formatting needed to be applied We began by generating surrogate keys (the dbt_utils package provides a neat way of doing so) and deduplicating the event data in Layers 2 3Layer 3 also handled column renaming casting timestamps booleans and so on Finally Layer 4 housed our normalised fact and dimension tables as well as some pre-aggregated data ready for the reporting layerHeres a high-level diagram of the projects structure: Layering successive transformations like this makes the models easy-to-read and easy-to-follow so that everyone involved in the data pipeline can see what is happening where and how This structure also makes it much more straightforward to debug and make modifications Again most of the models we created were incrementally materialized limiting the number of transformations that happen in each dbt runWe set up the data warehouse in this case Snowflake in such a way as to separate different ingestion and transformation tasks The specifics will depend on the use case but in this case it was important to minimize the number of users with access to PII and to be able to easily monitor the resource usage of different workflows The diagram below represents the structure we settled onHere we have dedicated users for ingesting data transforming and reporting Were showing the structure of our development database here but it is essentially mirrored for the production environmentWe also created a macro that applies a custom row limit in the development environment to limit the resources required for building out and testing our modelsIn this section well lay out the details of how we were executing dbt If you dont need the technical details feel free to skip aheadRunning our first layer with dbt: This executes all sql files inside the 1_RAW folder of our dbt project cloning the raw data from the secure views into our ingest warehouseLets move to our second layer  staging Again executing this layer is as follows: Notice the change of targets Here we need to take data from the RAW_DEV database and write new relations in ANALYTICS_DEVNote that it would be possible to execute different layers with different targets simply by using dbt run but to do this you would need to refer to source models with {{source}} instead of the usual {{ref}} function This has the unfortunate consequence of breaking the lineage in the auto-generated dbt documentation so we preferred not to do itSince our transform_dev database default is ANALYTICS_DEV we have to {{ref}} a model that is in different database we needed to include the following in our dbt_projectyml file: We can then run layers 3 and 4 analogously and no further change of target is requiredIn this proof of concept the models were being run manually In a production environment the setup described above could be scheduled in Airflow and run regularly with a Python wrapperThe final reporting layer results in normalised fact and dimension tables as well as some pre-aggregations Connecting this reporting layer to a BI tool then allowed us to integrate and enrich this email engagement information with the other customer data already available via that BI tool This paves the way for the client to delve much deeper into marketing campaign cohort and persona analysis and run much more powerful A/B tests ultimately driving more value for the businessTo discuss how Infinite Lambda could help your business reach out to contact@infinitelambdacom
iQYVaMHvDPCeLHSMpKJvyk,In this post I will introduce the way I set up the Jupyterlab environment with Docker and useful extensions to help me and my teammates work more productivelyIn the data team semi-technical roles often find its hard to manage a Jupyterlab environment and install new packages… So Docker naturally comes as a strong candidate
aFdvN7RwA5Xe83QxLTQL48,On August 24th I successfully passed the Google Cloud Professional Data Engineer exam During preparation I came across many similar articles which were quite helpful so this is my contribution with the recent experience Cloud platforms and Data Engineering are fast-evolving fields and I saw some changes in the exam vs what I was preparing forThere are many articles on WHY and WHAT so I will just explain my motivation as perhaps it will resonate with someoneWikipedia states that Data Engineering is a sub-domain of Data Science and is about data infrastructure Id further describe it as a mix of three disciplines  Cloud Architecture DevOps and AI / Machine Learning It is a fast growing field which so far remains shadowed by the hype of Machine Learning and AI sub-domains of Data Science However as more organizations deal with data-driven products more people start to feel the pain of getting data to right quality into right systems and building a resilient model pipeline As we get more ML models industrialization becomes more critical So for me coming from ML background and working on data-driven products the primary motivation was to understand the whole pipeline from getting data from IoT sensors to producing business insights to userThere is an alternative angle as well if you come from Cloud Architect or DevOps background A lot of ML/AI logic is now getting abstracted by generic cloud providers AI offerings (eg GCP AutoML and Computer Vision and NLP APIs) and are rapidly becoming a commodity Having good data set now is the main problem not creating the model (well to be honest it was always like this) For folks with strong DevOps or architectural experience Data Engineering may be an opportunity to move into Data Science roles by adding some knowledge around AI/ML to the mixLet me also address here the potential question of why Google Cloud and not <insert your favorite provider>? There is not a a lot of variety in data processing pipelines / products In fact most of GCP Big Data products for instance are managed versions of open source technologies (Apache Kafka and Beam are obvious examples) As a result your knowledge should transfer well to other platforms if you map GCP products to other providersLets talk about the value of experience The official exam guide recommends 3+ years of industry experience including 1+ years designing and managing solutions using GCPIf I use the formula stated above where Data Engineering is a mix of Cloud Architecture DevOps and ML/AI then my only direct relevant experience was 6 years of ML practice and training on top of PhD involving good deal of regressions and statistics 15 years ago You can expect exam to have 5 8 questions on ML/AI aspects but generally they wont be too deep As a result if you come from ML background you can expect up to 15 20% of exam to be easy This being said  I still encourage ML practitioners not to take it for granted and have a deep look at GCP AI/ML products to understand specifications and use casesOn Architecture side  I worked closely with several cloud teams developing on GCP over the last few years Through daily interactions I picked a sense of patterns and anti-patterns After some time you realize that data pipelines are not that diverse in fact there are only a few recommended patterns so if you have seen a couple of working systems on GCP you should not have problems with building a right mental modelFinally the DevOps aspect was probably the most challenging for me IAM deployment Logging and Monitoring aspects  I had to spend quite some time on itSo key takeaway is to leverage your background Keep the three areas above in mind know your strengths and weaknesses and create a study plan correspondinglyCourses may provide a foundation but dont expect them to be sufficient alone to pass the exam I think at best you can become 60 70% ready for the exam after courses aloneThe specialization consists (at the time of writing) of six two-week courses I dont think you can sign up for all of them at the same time so even if you are aggressive it will take you probably at least 6 weeks to finish specialization Courseras monthly cost is $50Overall I found it to focus too much on general patterns It wont give you the level of depth you will need for exam Also when I took it  it was quite outdated which by the way is likely a problem for all courses at the moment Coursera partnered with QwikLabs which if you are not familiar provides a controlled environment to play with cloud platforms and also have the built-in grader These labs are a great way (in theory) to get practical knowledge but I found most of the labs to be fairly basic you dont have opportunity to experiment you are expected to be a monkey following the script (and it does not like if you deviate from it) and labs are full of bugs On three occasions I had major issues with labs On one occasion  I waited for more than a week for QwikLabs to fix the lab grader to pass the course and move on and everyone had the same issueOne main (only?) highlight in my opinion is the first course in the specialization which is on Machine Learning It was a great overview and even after a dozen of ML courses on multiple MOOC platforms I found some of their explanations very intuitive succinct and fit to the exam scope So definitely recommend it if you are relatively new to ML and dont want to go through longer introductory courses like Andrew Ngs for instanceI saw someone recommending this platform and I wish I saw it before spending time (and money) on Coursera specialization If you need to take one course  I highly recommend this platform Matthew Ulasien did a fantastic job as instructor achieving very high information density in the course It covers more products and has deeper depth than Coursera It is a bit more up-to-date than Coursera Labs are less fancy than QwikLabs (still a monkey script) but they actually work and give you exposure to a wide variety of productsOther benefits of this platform include: the full-duration 2hr exam with 50 questions (Ill talk about practice exams in next section) 100+ pages Data Dossier PDF with all notes which you can download (the platform has an interactive version as long as you are an active subscriber) and a dozen of Anki/flashcards collections created by users (need to be an active subscriber as well)Linux Academy is $50/month This specialization has about 12hrs of videos split by a Product (eg BigQuery BigTable) or a concept (Machine Learning / AI) Overall these were the money well-spent and if I decide to continue with cloud certifications Ill likely stick to this platformIll list some resources which I used after courses to finish my preparationThere are a dozen of GCP Data Engineering cheat sheets available on github slideshare and similar resources In combination with Linux Academy Data Dossier they are great to polish/memorize key concepts behind each product However keep in mind that the level of details of these is less than what you get in the exam  which is what you would expect from cheat sheetThis is one of the most useful features of Linux Academy There are about 6 8 decks available some having up to 300 cards You can rank them by popularity the top 2 or 3 are definitely worth studying I had a very busy schedule during preparation and several multi-day pauses so these cards helped me to bring memories back They have the right level of details but wont cover all products Be also aware that quite a few of them are outdated (eg BigQuery allowing Table-level permissions or Bigtable not requiring three nodes for Production)There are only a couple of Practice Exams available First there is an official practice exam from Google (not full 50 questions) there is a short practice exam within Coursera specialization and full 50 questions exam on Linux Academy There may be others on other MOOC platforms but I found a lot of overlap between the three I mentioned so dont expect other practice exams to be very different I suspect what happened is that both Coursera and Linux Academy used some questions from official practice exam and/or from each otherAs such I suggest to use Practice Exams strategically In a way they should become your training-validation-test data sets I had Coursera exam first then I didnt take Linux Academy exam immediately after I finished the course I studied for several days and only then took their exam Finally I left Google practice exam for the end and took it few days before real exam to confirm that I can handle unseen questions with the good level of comfortGoing through a detailed syllabus is a also great way to assess your readiness There are several extended versions available which I wont recreate Here is the good example The idea is to go over every item and assess your knowledge of use cases limitations and key specificationsWell this is a trivial advice but youll actually need to study official product pages Read How-To guides Pay attention to Beta announcements and General Availability announcements Pay attention to product name changes as well Pay attention to quota and configuration changesDue to certification terms and conditions I cannot talk too much about the exam or provide a brain dump Overall I read some people estimating exam to be 20% more complex than training exams which I agree
7E2KSMi8DBDakX5KcXFbdd,Want to learn about Data Engineering? what is it? what are the challenges? Start with this teaser and stay tuned for the next serias of lectures about Investingcom Journey to the Cloudhttps://youtuThe Lecture slides in english: https://wwwslideshare
9tZkXyDn9fynsYvJf7MQtj,Apache Spark and Kubernetes are some of my favorite tools these days And turns out you can run Spark jobs using Kubernetes as a resource manager It sounds cool but due to implementation differences between cloud providers and lack of documentation it can get a bit trickyIve worked with Amazon AWS Google GCE and Microsoft Azure in the past And no matter what people say they all work fine Of course you will find differences but they are all professional production-ready cloud environmentsI tested this integration on GCE and Azure And I decided to share my experience with Azure mainly because it wasnt so straightforward and it required extra setupDuring the process I got into some issues Mainly around networking when running Spark jobs in client mode One important lesson is: When using Apache Spark in client mode with Kubernetes the pods should be able to communicate over the network with the host that is submitting the jobHaving a host submitting Spark jobs in the same virtual network the cluster is running in and that allows inbound traffic from the pods should be enough In this case with Azure the solution was to create a custom Virtual network and a Subnet Once you have these resources you can launch the AKS cluster and the VM that will submit the jobs in the same VNet and Subnet Then the Kubernetes pods that act as executors can talk to the VM that runs the driver program and the jobs can be executed with no issuesThis post contains a bunch of commands In case you want to dig deeper links with more details will be provided Lets get startedAll the commands where tested using an Ubuntu bionic container If you have your own Ubuntu machine you can use it instead This container is only used to create the initial Azure resources before launching the VM that will submit the Spark jobs If you dont have an Ubuntu 18 machine you can use Docker and follow alongLets start a bash session in an Ubuntu bionic container: We need to install the Azure CLI tool to interact with our Azure account along with some dependencies: You can sign in with: Follow the instructions from the login command output You will need to authenticate with your Azure credentials using your browserA resource group will help us to encapsulate all the resources we need for this tutorial Once we are done we can delete everything by just deleting the resource groupLike I mentioned in the introduction we need a custom Virtual Network and a Subnet Now when you create an AKS cluster if you dont specify a Vnet a default one will be created I had some issues with this default Vnet when I tried to create a VM in it after And we need that VM to be located along the AKS cluster Thats why we are creating a custom one hereYou can find more details about the following commands in this tutorialFirst we create a Vnet in our resource group We need to provide the network configuration for the IP ranges and the Subnet name and CIDR prefix Take note of the Vnet and Subnet names We are going to use them in later commandsYou dont need to use those exact values in case you wonder Those are just the default used in the tutorial I linked previouslyThe following command will allow the AKS cluster to interact with other Azure resources This method of managing permissions is very common when you need to integrate Azure servicesFirst we create a Service Principal: Output: Take note of the appId and password properties We also need those for later useTo assign roles we need the resource ids of the Vnet and Subnet created previosly We can use the az tool to run queries for these values: Now lets create the role assignment that we will use for our cluster and that assigns the right permissions to interact with the Vnet Replace the assignee parameter with you appId from the your Service Principal: Most of the parameters to launch the cluster are network related and also copied from the link posted before Remember the cluster needs to be created in the right Vnet and Subnet Replace the service-principal with your appId and the client-secret with your password from the Service Principal: In order to submit Spark jobs to Kubernetes we need to host Spark docker images somewhere Lets use the container registry service from Azure to create our own private registry: Our AKS cluster should be able to pull images from this registry We can also create role assignment for this (more details here): Now we can create the VM that will submit the Spark jobs using the same Vnet and Subnet the cluster was created in: The generated SSH keys will be stored in the ~/ssh directory Copy them in case you want to use them later Remember once you exit the container they will be goneLog into the VM (from here on you can keep using the container or you can use the generated SSH keys and use your regular local environment): Install Java and Spark: Set the SPARK_HOME environmental variable and add the Spark binaries to the path: Lets also install the Azure CLI tool to connect to the Kubernetes cluster from this VM: Log in once again by following the instructions: Docker will be necessary to build and push the Spark images to the container registry: Log out and log in so docker can be used without sudoSubmitting Spark applications to a Kubernetes cluster also requires to have kubectl installed and configured to communicate with the cluster: We can configure kubectl by pulling the AKS credentials: Log in to the container registry so we can push our local Docker images: Now we can build and push the Spark images using the docker-image-toolsh that comes with Spark The following commands need to be run from the SPARK_HOME directory After running the commands the remote container registry will have 3 images: spark:01 spark-py:01 and spark-r:01: There are two important parameters when using client mode: sparkdriverhost and sparkdriverport These parameters tell the pods where the driver is running on The host will be our VMs private IP address: Which in my case is 1024007 And for the port we will use 7778We also need our Kubernetes cluster address You can get it with: Which in my case is https://sparkakscl-sparkrg-e659ac-e06a364ehcpeastusazmk8sio:443Now we can start a spark-shell with the following command: We are using our Kubernetes cluster endpoint the driver parameters the Spark image from our private container registry and the service account created previouslyRun a simple test: You should see something like the following: If you run the kubectl get pods command you should see the Spark executors running as Kubernetes pods while you have this shell opened
XRqMnYCqyCSUr9PyntCFFi,This article is the final part of the Data Migration tool seriesProblem Statement: Migrate records stored in Hbase to ElasticSearch 7Have a look at the diagram belowTo achieve the above objective we will be writing a topology which will pull messages from Kafka The messages contain the data required to create RowKeys These RowKeys are then used in the queries sent to Hbase With the Data that we get back from Hbase we form a batch with it and send it to ElasticSearch to be written as documents on our created indexFor explanations of the technologies mentioned above go through the below articles  you can find the explanations there Also the data that we will be using is from part 1 of the seriesLets add the dependencies: Lets look at the Project Structure: Setup configurations for Flux to deploy the topology: 1- configThe configurations are the same as Part 2 of the series • configWe are using KafkaSpout just like we used in Part 2 Its a Storm-specific wrapper on top of Kafkas consumer client which reads data from Kafka into a Storm Topology We will use SHUFFLE again for Grouping and only one bolt called RecordMigrationBolt If you check out the streams section you will see how the stream goes through our KafkaSpout and the tuples are then passed on to the RecordMigrationBolt for processing Another alternative is to break up our bolt into multiple bolts but I wont be doing that hereBy showtime I mean time to look at the RecordMigrationBoltSo for our bolt we implement the IRichbolt interface which gives us a set of methods The explanations for the set of methods is given in the following articleOur Connection to ElasticSearch is maintained via RestHighLevelClient which is present in version 71So I will directly move on to explaining the execute() method For rest you can go through the comments in the codeisTickTuple(): returns a boolean value stating whether the tuple is a tick tuple or notOkay before running go to run-> edit configurations and set the followingGo ahead and run the topologyIf all goes well you will see an output like the one belowThe above output is from an elasticsearch extension which is available for ChromeHi I am Pankaj Kumar Singh A Software Engineer Developer and Infosec Enthusiast  Thank you for taking the time to go through the article If you find any issues or have any suggestions regarding the post feel free to reach out to me Will be happy to resolve any issues You can find me on Linkedin and GitHub Or just drop a mail to singhpankajkumar65@gmailcom Also pardon me if there are any grammatical mistakes
BniENnTDA6XYgHFHm9mo2v,Hey there Ill be coming up with a series of articles pertaining to a particular use case Hence I have divided this series into modules which are independent to develop and can be later combined to develop a Data Migration toolPlease Note: This article uses kafka version 082Problem statement: Lets suppose we have a Kafka Server already setup for us We would want to read a CSV file and push each line into a Kafka Topic:Okay one term at a time Queue - One must have heard of this line when standing at a bus stop- Please form a queue In the bus stop scenario a queue is a line of people waiting to get on the bus to reach the destination In technical terms  a queue is a line of things waiting to be handled in sequential order starting at the beginning of the line Queue is basically FIFO(First in  First Out) So a Message queue will be a line of messages sent between applications An example of a message in this context could be any data that needs to be processed by an application or something that tells a system to start processing a task  it could contain information about a finished task or just be a plain message:Imagine a Topic as the bus that is carrying the people in our case messages One can also call it a category of messages depending on what we push into the topicJust to elaborate a bit on the architecture of a Message Queue there are client applications called producers that create messages and deliver them to the message queue Any other application called consumer connect to the queue and get the messages to be processed Messages placed onto the queue are stored until the consumer retrieves themSetup: We will be using Intellij to write the code in Java You can follow Steps-3 and 4 from my following article to setup your projectSample Data - These will be the records that we will be pushing as messages on to the topicMoving on lets see how our sample csv file looks like For our use case I have taken a csv containing 500 records Usually in a production environment the scale can easily go to millionsWe have three columns in our sample file - first_name last_name company_nameDependencies: For this article as I am using Kafka version 0821 hence we will import it in our buildgradle fileWe will have only a single file called KafkaCsvProducerjavaLets look into the functions that we will use for our codeWe will be using the below variables and passing it as arguments while running are code Define the following variables in our class globallyLets head on to main(): ProducerProperties(): This method is responsible for defining the Producer Configurations using the Properties class This will return a Producer with the set propertiesTime to read the rows from csv and write messages using the PublishMessages(): This is the how the final code will look Also check the libraries that are used If you do not have auto-import you might have to add it manuallyIn order to run this code we first need to create a topic using the following command on our Kafka server: /kafka-topicsHello-Kafka is our topic nameOne can also check if the records have been published by using the following command in our server: /kafka-console-consumerAppreciate your time in going through my article There are a lot of other optimizations that can be done with the above code such as adding MultiThreading features to optimize the writes to the queue for a larger scaleSo let me know if there is something wrongly explained or if you have any suggestionsHi I am Pankaj Kumar Singh A Software Engineer Developer and Infosec Enthusiast  If you find any issues regarding the post feel free to reach out to me Will be happy to resolve any issues You can find me on Linkedin and GitHub Or just drop a mail to singhpankajkumar65@gmailcom
RWi5thhuYvX2ccNESDbRSS,How to work on database without installing on main system Just use docker and install docker compose But use packages which are on docker main page not in your system repository • Create docker-composeEdit docker-composethis can be useful so add this as linux alias So modify ~/Below there is a screen with example from PyCharm
7AvgLLrp6oF3VeyY3bKJqJ,As a proudly data-driven company dealing with physical goods Stitch Fix has put lots of effort into inventory management Tracer the inventory history service is a new project we have been building to enable more fine-grained analytics by providing precise inventory state at any given point of time just like the time machine in Mac OSXIt is essentially a time series database built upon Spark There are already many open source time series databases available (InfluxDB Timescale Druid Prometheus) and they are more or less designed for certain use cases Not surprisingly Tracer is specially designed to reason about stateInventory evolves over time and every change should be described by an event Its natural to model inventory as time series events However reconstructing the inventory state at any point in time given these time series events is a challengeThere are two kinds of events: stateful and stateless Stateful events contain state at a certain point of time This is a very common type of event For example we can have events reporting SKU (Stock Keeping Unit) counts over time as illustrated in Fig 1 Another example can be temperature reported by a remote IoT thermometer every 5 minutesStateless events contain information about how state changed at the point of time rather than state itself We can rewrite the previous SKU count example with the changes in counts shown as in Fig 2 Take a bank account as another example: every transaction event can have either credit or debit to balance but not the balance itselfSo how can we reason about state at any point of time if we have stateful events? If there exists an event at the time for which we requested we can directly read the state out of it otherwise we need to find the last event right before that time to get the stateIf we have stateless events it becomes a bit tricky as events dont indicate states directly therefore we need to establish some initial state as base then apply the change from the time of that initial state to the time for which we requested to get a new stateNo matter whether we choose to use stateful or stateless events we can generalize the pattern to reason about state simply with two functions: These two functions are pure meaning for a given tn or a tuple of (t1 t2) function I and D will always return the same result This property is important and gives us confidence that we get the result we expectThe difference function is especially interesting in that the returned state transition can be in various forms depending on the use case: Importantly the difference function should be aware of the chronological order of state transition so that if we reverse the input timestamps the returned result should also be in reversed orderOnce we define the operation to apply a difference to a state we can reason about state forward and backwardI can not emphasize enough that the difference function D is very flexible As long as it stays pure and holds true to formulas 1 and 2 it can contain state transition information in any formThe design of Tracer follows the math foundation described above which is actually inspired by the MPEG video compression algorithm Basically the algorithm picks a number of key frames which contain complete information of static pictures at given moments in a video and then encodes changes of motion in between these key frames in a compressed format When a video player opens an MPEG format video it decodes the file by using key frames and applying changes in between to restore frame by frame pictures on screenSimilarly two building blocks of Tracer snapshot (green) and delta (orange) are exactly mapped to the state I function and the difference D function to store state and transition data illustrated in the following graphA snapshot contains the complete information of state at a given point of time If we are going to store SKU count for example a piece of snapshot data can look like thisAccordingly a piece of delta in this case can look like this: Notice this does not store events directly rather it stores the effect caused by events on SKU counts By doing so the operation to apply a delta to a snapshot is simply to summarize the sku_in_delta and sku_out_delta and then add up to sku_in_count and sku_out_count respectively for the same time window As I mentioned before we can be creative to store events in different ways to best serve the way we use TracerTracer is implemented on top of Apache Spark in Scala and exposed to end users through both Scala and Python APIs The returned result is just a standard Spark dataframe so its very convenient to connect Tracer to our existing Spark based data pipelines as well as making it easily queryable through SparkSQL or the dataframe APIFor example users can query Tracer for inventory state with a timestamp or multiple timestamps Or more conveniently they can query with a time window over a certain intervalAASparkContext is our internal augmented SparkContext with additional helpers Once you have that then just initialize a Tracer inventory service object and start sending queriesEasy enough? What actually happens under the hood is that for each timestamp requested (for example at time t) Tracer will check the timestamp and find out the closest snapshot (snapshot at t0 in this case) and apply the correct delta time windows (indicated by the purple part) to the snapshot to build a virtual snapshot which is exactly the inventory state at the requested timestampCurrently snapshot and delta data are stored in S3 just like other spark tables These implementation details are completely hidden to our end users enabling us free to later adopt other kinds of data storage such as Cassandra or in-memory solutionsThis transparency also brings another advantage that we can apply all sorts of optimization under the hood such as how and where we store the internal data and in what structure and how we execute a query all without asking end users to change any code on their sideOne such optimization is indexing An index is a list of pointers to the locations of snapshots When Tracer starts planning to run a query it can quickly identify the location of related snapshots without actually scanning through the contents of other unrelated snapshotsWe originally decided to create snapshot partitions hourly but later realized some delta partitions were huge during rush hours as we had a lot more events happening within certain hours of a day With an index we can distribute data more efficiently by adding snapshot and delta partitions based on data volume instead of a fixed time scheduleWith two simple core concepts snapshot and delta Tracer provides a flexible framework to reason about state with time series data The scope of Tracer is of course applicable beyond inventory: client events stylist events … any event we create or consume where we need to understand how state would transition from one to anotherWith Tracer its even possible to use it for future state! Given a prediction model m and a timestamp t in the future a new I(t m) function can still stay pure by using predicted events
Z4ZfzfZkSoyZ9qBVxfmkc7,After attending several online sessions and courses on various technology served by AWS the ones that enthralled me the most are the utilities provided by the services like Amazon Glue Amazon Athena and Amazon Quicksight Also the power of Serverless Data Engineering is something I already have been trying to learn about and play around with From the knowledge I had from various sessions I decided to build a personal project of Interactive Dashboard using various servicesBeing someone who is working in the field of Data Engineering and Business Intelligence I was definitely amazed by the sheer efficiency that AWS tries to bring about in building BI Dashboards by minimising the workloadIn this article I will try to discuss: Note: This article is not about How to build the dashboard and is more about the Data Engineering behind the preparation of Data which is hence imported to the presentation layer to create the dashboardBefore talking about how to build the dashboard on AWS it is important to understand what the underlying development flow for building a dashboard in a system like Business Intelligence is all about This is something that this article will be all aboutThe diagram below shows what a typical workflow for building a typical BI dashboard looks like It is a very simplified version because in reality you are going to have several other layers like Staging DB different other layers of ETL The in depth understanding of Business Intelligence is not the purpose of this article and hence the basic knowledge of BI required to understand the project is discussed in the diagram belowNow lets talk about the projectHere is the flow of tasks that I used to build this dashboard: The details of scraping is also beyond the scope of the discussion I am trying to haveHowever just for a quick brief  what I did is scrap the weekly box office data of the movies over a year from 1982 2019 from https://wwwboxofficemojocom/weekly/ Using python libraries like requests and BeautifulSoup you can easily do the scraping steps Here is the link to the code which I used to scrap the siteDisclaimer: Dont forget to clean your data before you actually upload it to S3 Trust me that cleaning part took most of the time for this project and this is something you can never avoid and you should never intend toAfter creating the pandas dataframe and converting it to csv I used boto3 to upload the csv to S3 : The code above puts your csv to the S3 bucket called boxofficedumpv1(figure below)Amazon Glue helps in cataloging preparing and transforming the data very easily and it does so serverlessly It also helps create ETL processes customize the ETL processes by providing the python and scala code they automatically build for you For this project Amazon Glue is very crucial and was used to crawl the csv file from S3 to Database create the ETL job to map the csv format data to parquet format and then crawl the parquet data from S3 to the DatabaseTo crawl the data from S3 your crawler needs to be able to access S3 on your behalf This is a good resource to know how you can do that Below are the steps to create a crawlerAdd name of your crawler and leave everything as it isChoose S3 as data store and refer the path of the bucket where you uploaded your CSV file toChoose the IAM Role you created earlier in the tutorialSend the output of the crawler to the database and name the database Click NextThe next page will be a summary and review of your configuration Click FinishNow run the crawler until you see the crawler state to be as belowSpecially wait until you see the Tables addedFrom the tab on the left Database → Tables Here you will see the tabular representation of the csv in the database The name of the table turns out to be same as the bucket nameAs you click on the table you will see that the columns are there and he data have been successfully imported tooIn this way you can use Glue to infer your data residing in S3 in any format  csv json or anything If you care using json you have to use custom classifier to crawl the data to database You can use this link to understand how json data can be used to do the same thing we are doing in this projectNow we are going to convert this data to parquet format using ETL job Why is this so? There are two reasons behind doing so  1 Parquet format of data is a columnar optimised storage that provides smaller file size than json/csv 2 Provides much faster querying when you are going to use Amazon Athena to query the tables In order to create the ETL job click on the ETL → Jobs → Add Job in Amazon Glue consoleWe will try to generate python script So set the name of the job and keep all things as defaultWe will choose the table we crawled as the data sourceIt actually show the table the s3 bucket associated with the tables as well So for more complex design you will have the information about the s3 buckets each table is associated withWe will choose the target data to be produced as parquet format and the parquet file will be stored to S3 In the Target path choose the file path to respective bucketThere you go this is the page where you see what this ETL will do It will transform the source data to parquet format data which is the target data This is a simple design and so it is a straight forward direct mapping of the source data to the target data(Usually you can set your own business logic in the ETL To do so you can edit the python script this job is going to generate now and then run it in the Glue ConsoleIn this step you will see a python script gets auto generatedAfter successful execution of the script you will see a parquet file getting created in your bucket specified At this point create a crawler just using the same configuration as before except change the source bucket to the one where the parquet file resides Run the crawler and see a different table getting generated at the databaseNow check the parquet table and click on View DataUpon clicking the View Data you will see a prompt which will take you to Amazon Athena Athena is a service which allows you to write custom queries against your data A prompt will warn you that Amazon Athena charges for each query But you wont need custom query at this moment and select queries are free of charge Click Preview If everythings good you are supposed to see a result like thisPhew! At this point you can say that the basic data preparation is done Nowall you need is to import this data to Amazon QuicksightAmazon Quicksight is an Amazon Service that lets you import your data and use their tool to build beautiful and interactive dashboard out of them You need to sign up in order to use the dashboard tool On the landing page of Quicksight do the following to import the data : If your data have been imported properly you will see the columns fields and visual types in the dashboard design page You can play around with the tool to come up with your own idea of analysis In this article I wont really discuss about how to build a dashboard because that is a vast discussionIn this way you can play around with Amazon Glue S3 Amazon Athena and Amazon Quicksight to properly design your ETL Crawlers and Dashboard Now that you have designed dashboard like this  each week you can run the python script to scrape the data and crawl it to s3 and hence re-run the ETL to refresh my dashboard You can do all the steps manually or you can create a workflow using Amazon Glue so that all these processes get executed sequentially and then the dashboard gets automatically refreshed at the end of the week and hence showing the updated state of Box Office and how each movies are getting affected due to new releases You can also use lambda function to trigger the scraper This is something you can immediately do if you want to replicate the projectAnother important note about the costing of this project If you run a crawler weekly for a month a cost around $145 would be charged while $367 will be charged for ETL jobs(monthly) S3 will cost about $004I hope this article helps people who want to start learning how services like Amazon Glue and QuickSight and be combined to explore the concepts of Data Engineering and Dashboard designing in AWSIn my next article I will show you how can you automate the weekly refresh of this Dashboard
4qL4vFcpJDrnuRrhphzRdn,In the previous article I showed you how to scrape data load it in AWS S3 and then use Amazon Glue Athena to effectively design crawler & ETL jobs and query the data in order to be presented to reporting tool like Amazon QuickSightIn this short article I will try to explain how to orchestrate the overall flow I described in the previous article in the form of workflow so that all the steps are not required to be executed manually again and again weekly I am saying weekly because I want to update the dataset at the end of the week by running the scraper and eventually run the ETL afterwards It makes sense because the dataset actually gets impacted due to the releases of movies at the end of the week So what I will try to cover in this tutorial are the steps you will need to take to orchestrate the overall flowWhat we already have  (refer to previous article)1We have database and QuickSight but they dont require any manual intervention in the context of this project The aforementioned objects above are the ones which would require orchestration or automation in their execution In order to be automated we need to fix an execution plan which in our case is the sequence described above Amazon Glue have workflow where you can add the steps sequentially and then add a trigger to initiate the eventsTrigger is the most important part in this context For each of the steps to initiate there has to be a trigger We will be concerned with two types of triggers  job event & scheduledJob Event are the kind of triggers which are basically an indicator of the state of executions of the previous steps  FAILED SUCCEEDED etcScheduled are the kind of triggers where you can set time and tell it to initiate the event(crawler/jobs to run) at that particular timeSo lets try to create the workflow nowFrom the Glue Dashboard go to Workflows → Add workflow Give a name to your workflow and click Add workflow button belowYou will see that a workflow has been createdNow once you click on the checkbox as above you will see a graph below where there will be a Add trigger button Click it As you can see below that this is the trigger that we want would initiate our csv-crawling at the friday of each week at 12:09 AMNow that we have created our first trigger we would want it to start the crawler and hence we would add a step of csv crawlerOnce we created the first trigger the graph would automatically look like this and would expect that you add a node followed by this trigger A node would be either Job or a CrawlerThen a window like this would appear with two tabs- Jobs and Crawlers Click Crawlers and add the csv-crawlerNow the graph would look like this: Click on Add Trigger Follow the steps as per the picture The default Event type trigger would check if the crawler executed successfullyThe Graph would now look like this: In this way you can now add another node which in our case would be Job and then add another similar trigger The last step would be another node which in our case would be the parquer-crawler The final graph would be something like thisSo you can either run this workflow right now or wait for the trigger to initiate the first step of the workflow What this would do now is initiate csv-crawling at each Friday and then upon successful execution run the ETL job of converting or shipping the csv data to parquet format and then executing parquet crawler  all automatically In this way you can orchestrate your ETL planning in Amazon GlueSo I hope this simple tutorial helps you getting started with data engineering in AWS In future article I will try to come up with some more interesting tech stack related to effective Data Engineering
9qdhapAN8YZDXjxqTMyPqp,Note: I will be using an EKS cluster on AWS You could use the same steps on other cloud providers tooWell created Kubernetes Operators pack a lot of power and help run and manage stateful applications on kubernetes We had earlier seen how to install airflow on kubernetes using helm charts While helm charts help you get started fast they may not be suitable for day 2 operatios like: Lets find how to install airflow on kubernetes using airflow operatorYou can specify the source of DAGs in the hack/sample/mysql-celery/clusteryaml fileNow its time to deploy the airflow componentsairflow-using-operatorOriginally published at https://prabhatsharmain
ka55UA9wURx3iXyMMcyuBA,StitchData is a company that describes its vision To be the conduit through which businesses take back the control of their dataInterestingly enough where more and more companies are building their business around multi-cloud platforms its becoming evident that we have a channel through which companies can stream their data from one cloud platform to another in an easy and handy wayOur company had a similar problem We needed to find an easy way to sync information from MongoDB Atlas to BigQuery Initially it was ok to have a batch service to load data from MongoDB to Bigquery but soon we realized that we needed to integrate multiple data sources and loading data through ETLs were taking a performance hit With the rise in demand for high availability of data we had to make sure we switch our focus from loading to processing the data Stitch data came in handy but with some caveatsStitch uses the technique called Append-Only in order to sync the information between databases This meant that no data gets deleted but the changes are reflected as a new row in the database It can differentiate the changes in the same record using added columns like `_sdc_sequence` and `_sdc_batched_at` So It is up to us to de-duplicate the data at our endOn their official website: https://wwwstitchdatacom/docs/data-structure/querying-append-only-tables They mention the method for de-duplication as follows: Since every id has a max _sdc_sequence number and a _sdc_batched_at date associated with it therefore if join the original table with max _sdc_sequence and _sdc_batched_at values with the associated ids and then select only those values that are equal to the max of _sdc_sequence and _sdc_batched_at values then we can get the latest record The SQL statement above essentially does the same It groups all the records using id and picks the max values Then it joins it to the main table and picks only those values that have a max _sdc_sequence and _sdc_batched_at valuesThis technique for de-duplication looks correct on a first look but what if there are records that contain the same id _sdc_sequence and _sdc_batched_at values ? We realized while processing the data that some of the records where still duplicated and the reason behind it was duplicated _sdc_sequence and _sdc_batched_at values for the same id This essentially happened because of overlapping sync cycles from stitch to big query We therefore ended up with records that had the same sequence number and batch dateWe found that it is better to de-duplicate the dataset using the partitioning technique provided by BigQuery instead of using the MAX _sdc_sequence and _sdc_batched_at method Here is the approach we usedIn the above approach we partition the table using _id Then order them by sequence number and batch date We then assign a row number to each record over a given partition By selecting only the first entry in that record we ensure that even if there are multiple entries with the same _sdc_sequence number and _sdc_batched_at date we select only one of them This way we ensure that even when we have duplicated records with the same _sdc_sequence and _sdc_batched_at values only one of them gets selectedWe didnt find a lot of resources talking about this issue so I thought it might a useful insight for people setting up a StitchData channel for the first time
bd39FbNyxpBN6VxD8AsW5b,"If youre in any way affiliated with software development it is highly likely youre either directly or indirectly dealing with producing consuming or storing data  be it structured or unstructured This will eventually lead to data abundance and dealing with big-data is almost a standard requirement these days Thanks to the massive adoption of cloud and accelerated innovation in data engineering tools dealing with any volume of data albeit having its own challenges is not as painful as it used to beData engineering is a cornerstone that potentially helps make or break an organizations growth (ie if youre working for one that is data-driven If not please save yourself!) and data pipelines make the cornerstone of data engineering A pipeline is a clichéd word in software engineering which might mean everything or maybe nothing depending on whats involved and at stakeIn computing a pipeline also known as a data pipeline is a set of data processing elements connected in series where the output of one element is the input of the next one The elements of a pipeline are often executed in parallel or in time-sliced fashionWere talking about big data Hadoop and data pipelines  that should and will lead to Spark Spark is a one-stop solution for a variety of big data problems a unified platform for batch real-time(almost) machine learning deep learning and GraphsAWS Glue is a relatively new fully managed serverless Extract Transform and Load (ETL) service that has enormous potential for teams across enterprise organizations from engineering to data to analytics Glue combines the speed and power of Apache Spark with the lightweight data organization of Hive metastores With this combination Glue helps bring together disparate data sources from across the AWS storage eco-system (Glue supports S3 Aurora all other AWS RDS engines Redshift and common database engines running on your VPC (Virtual Private Cloud) in EC2)""While I almost always back using Spark for various big data &/ ML transforms setting up and maintaining a cluster (eg EMR in AWS or DataProc in GCP) is a glorified pain! Glue is not a silver bullet and it might not be as seamless or accessible as you would like it to be not just yet; but the idea of having a serverless spark setup is something Ive longed for  Thank you AWS (not a fanboy but credit where due) Here I'm sharing my learnings of working with AWS Glue  places where it shines and where it breaks youAWS Glue builds on the Apache Spark runtime to offer ETL specific functionalitySpark has Master and Slave Architecture I prefer to call it Master-Worker architecture(Not a big fan of the word slave) Master is the instance that hosts the Driver Program and the Worker is the instance that hosts executors In AWS Glue the driver and executors run on dedicated nodes (Potentially part of a DPU amongst all provisioned for the job) Basically there is a driver a worker an executor and the Cluster ManagerAs shown in Fig 1 above AWS has baked some extra toppings over the spark core  the Glue DynamicFrames and Glue ETL These extra toppings facilitate handshake with spark core in different ways in different versions or releases but the underlying spark engine is almost untouched I explicitly mention this because if you are a spark pro; just go with your existing knowledge and youll be fine On the contrary if you do not know spark despite AWS advertising about how anyone who can write vanilla python can now do ETL that is a bait Nope  PySpark looks like Python but doesnt act like Python & AWS extra toppings will do no magic to make any random analyst become a data engineer overnightGlue divides its main services into the Data Catalog and ETLThe following workflow diagram shows how AWS Glue crawlers interact with data stores and other elements to populate the Data Catalog(Hive Metastore)For data store sources you define a crawler to populate your AWS Glue Data Catalog with metadata table definitions You point your crawler at a data store and the crawler creates table definitions in the Data Catalog For streaming sources you manually (or leverage IaC like Terraform etc) define Data Catalog tables and specify data stream propertiesWhen your job runs a script extracts data from your data source(s) transforms the data and loads it to your data target (sink) The script runs in an Apache Spark environment in AWS GlueNow things that are not so hot plus my recommendations • Glue is not s3 optimized; not out of the box: AWS Glue gives you immediate access to a great deal of parallel processing power You can transform data and push it out to S3 with the generated AWS Glue scripts On the face of it this seems to deliver exactly what it promised but nothing comes for free  and Im not talking about the $1000+ you can easily spend with a poorly constructed job You may see errors like this in your failed job logs: This indicates that your job is attempting to write to S3 at a rate that exceeds its permitted limits Tuning your jobs to optimize for S3 storage is going to be one of the first hurdles you have to overcome Not only because the job might fail If youre not careful S3 costs will rapidly become your greatest expense • Glue jobs have very slow start time (This is solved with their latest v2 release But that has opened another flood gate which Ill mention below)""When working with Glue one of the most expensive problems Ive had to deal with was slow iterations (I'm dealing with a real-time infrastructure) Its tempting to think that the $400 you spent on EC2s and S3 read/writes is the bulk of the cost  it isnt For the first few weeks the biggest cost will be your timeSpark Jobs take about ten minutes to start up meaning theres at least a ten-minute wait between deploying your code and finding out if its broken or not Worse if youre processing large volumes of data you could be waiting hours to find out whether your solution is viable""3 Spark UI does not come out of the box Donno about you but I'm a visual person I like to see the magic as it happens And spark web UI is a real charm It gives a lot of useful information that helps debug &/profile your jobs for the time cost and space Its a bummer that AWS Glue does not facilitate this view out of the box (they instead give you some crappy AWS job metrics which is almost good for nothing) Youve got to put in some man-hours to get the spark UI functional4 Continuous logging is not so continuous! The executor nodes will have to wait for yarn aggregation before you could see your console outputs in cloudwatch This means if youve got any run time issues you need to wait for the whole job to fail and only then inspect the issues While this has changed with Glue V2 (their latest release) that opens a suite of other problems like too many open files etc due to not supporting Apache YARN and HDFS • The error messages arent completely clear I once tried to create a dynamic frame by reading parquet partitioned files from s3 Very randomly say 1/2 times in 10 the job failed with this stack trace: Similarly once added Map operation into one of my jobs This operation would parse data from a given field(s) and set slightly adjusted or enriched data in a different field Upon running it the job failed with the following error: Well as it turns out if there is an error in the Map operation in an AWS Glue job it marks that row as an error and effectively removes it from the set The error doesnt propagate So the first time an issue crops up is when it tries to write nothing to S3 at which point an error occurs • Coalesce/Repartition  a double-edged sword Most S3 slowdowns are due to the number of files youre trying to write It is more efficient to write larger files than lots of small files One mechanism to splits up your files is called spark partitioning A single data frame can be broken up into multiple partitions which are useful because they improve concurrent processing over your nodesBut when youre trying to write out to S3 each of these partitions will be written out as one file If you have a lot of nodes running you could end up writing many more files than you need to So how do you reduce your number of partitions? You have two optionsRepartition  Repartition is one mechanism for changing the number of partitions in your dataset It can be used to increase or decrease the number of partitionsCoalesce  Coalesce can only be used to decrease the number of partitions In this use case it fits perfectlyIt may be tempting to invoke coalesce(1) and reduce your output to a single file This is not likely to work for a large dataset because what youre instructing Spark to do is load all your data into a single node Your node only has so much memory and if your dataset exceeds that limit youre stuck You wont typically want to write out data to a single file because its slow (and will error out if the dataset is big) Youll only want to write out data to a single file when the DataFrame is tinyThe partitioning of DataFrames seems like a low-level implementation detail that should be managed by the framework but its not When filtering large DataFrames into smaller ones you should almost always repartition the dataYoull probably be filtering large DataFrames into smaller ones frequently so get used to repartitioning Grok it! • Reading too many small files  the number of executors needed skyrocket! If youre reading files from a streaming service like Kinesis or Kafka and not grouping files while creating your DynamicFrame these small files get spread across all your executors  too much parallelism which will degrade the overall performanceIf you cant simply reshuffle the data onto a single node you need to tell Spark and AWS Glue how it should go about reading your files If youre reading directly from S3 the following data source code is an example: The groupsize however should be set with the result of a calculation For example 1024 * 1024 = 1048576 • Cannot run Glue locally! This is a total bummer Your development time is going to be exhaustive since youve to bear with checking every change in the cloud As an alternative you may choose to work with Dev EndpointsDev Endpoints are notebooks hosted in SageMaker that have been kitted out with the AWS Glue libraries They also take about ten minutes to start but once theyre running youre free to run whatever you like If youre testing out the logic in one of your jobs a notebook is the best environment in which to do itNOTE: Dev Endpoints can get pricey You can spin them up with lots of CPU and RAM Until you delete them theyre costing you money Make sure to shut them down at the end of the dayDev Endpoints seem great when you begin working with them but be advised: There are a lot of things that jobs do under the hood that you cannot replicate in a Dev Endpoint For example a job will write out files into a temporary location before moving them into the target folder The same code running in a Dev Endpoint does not do this If your S3 permissions are too constrained they may work fine in a Dev Endpoint and not in a JobDont consider the Dev Endpoint your golden ticket Its an indication that your logic is correct but your permissions and infrastructure may be completely wrong • Jobs can fail silently and still indicate as Succeeded in glue console! (This has changed with glue v2 given there are separate error and log streams) Given Glue jobs depends on YARN despite your job failing with one of the run time issues overall status can be succeded in the consoleI strongly recommend setting up alerts\u200a\u200apick your fav amongst AWS SNS or Slack or Pagerduty etc that help alert a jobs success or failureData engineering is not an easy task By easy I mean it is not something that always has a run book or a manual you may refer to and get done with Its a continuous battle and the one you really want to win to help lead your organization get the best out of data insights  be it traditional or MLThe Serverless is great AWS Glue is great Spark is greatExcept its always going to be a bit of a pain and sometimes it frustrates you to the core Nothing worth doing is easy and like anything else its worth perseveringDespite these challenges AWS Glue can be an efficient and useful tool that is great for a number of use cases that require moving or transforming data I applaud how AWS has wrapped all the quirks associated with conventional Hadoop and Spark under the hood  all managed This is a big step forward in helping enterprises get the murky and bizarre world of big data transformation right Its only going to get better As it stands AWS Glue looks like a promising candidate to be a poster figure for data pipelines"
Jz6Rq7tFFN7QWEfLYpNofd,Apache spark is the lightning-fast unified analytic engine with the massive benefits 😮!!! Lets focus about the sparkThe industry needs powerful engine that filled with powerful functionalities with process streaming data as well as batch processing and also requires engine that can response sub-second and perform in-memory processing here follow some drawbacks from technologies used: 👀 Hadoop MapReduce can only perform for the batch processing 👀 Apache Storm and S4 can only perform stream processing  👀 Apache Imapala and Tez can only perform interactive processingBased on requirements of the industry growth need a powerful engine with multiple processing performancesApache spark is Open Source engine which provides real-time stream processing interactive processing graph processing in-memory processing  batch processing with the very fast speed  ease of use and standard interface🔶Languages support to spark: ⚫Scala  Apache Spark framework is built in Scala Language therefore latest and greatest features can experienced that might not be available in other supported programming languages⚫Python  Python programming language has best libraries for data analysis and machine learning like Pandas and Sci-kit learn but they are comparatively slower than Scala⚫R  R programming language is best for machine learning and statistical analysis which helps increase developer productivity SparkR is combination of R along with Spark SparkR uses data processing that cannot handled by a single machine⚫Java  Java programming language good choice for developers who are coming from java and hadoop platform but java does not support RPEL⚫Apache Spark Core  This the foundation for parallel and distributed processing of large data sets Spark core responsible for all basic I/O functionalities scheduling and monitoring the jobs on the spark clusters tasks dispatching networking with different storage systems fault recovery and efficient memory management⚫Spark SQL  Leverage the power of declarative queries and optimized storage by running SQL like queries on Spark data that is present in RDDs and the other external sources⚫Spark Streaming  Allows developers to perform batch processing and streaming of data with ease in the same application⚫Spark MLlib  MLlib eases the development and for the scalable machine learning pipelines⚫Spark GraphX  can work with graph and non-graph sources to achieve flexibility and resilience in graph construction and transformation⚫ SparkR  this is the tool for running R on Spark and very similar to python API except that it follows Rs syntax instead of Python🔶Techyon(An in-memory file system)  in-memory computation has gained traction recently as can perform interactive and fast queries because of it Major drawback of this Techyon is depends on the JVM When JVM crashes in-memory data will be lost and it takes time to load the data again to the memory⚫Standalone Cluster  In this mode each application runs and executor an every node within the cluster⚫Apache Mesos  In this mode fine grained sharing option so Spark shell scales down its CPU allocation during the execution of multiple commands especially when several users are running interactive shells⚫YARN\u200a\u200aThis mode comes along with hadoop distribution and only cluster manager in spark that supports security YARN allows dynamic sharing and central configuration of the same pool of cluster resources between various framework that run on YARNApache Spark gaining best industry-standard engine for big data processing since 2016 because of its interactive performance fault tolerance and the productivity benefits
92teJGtJTh4ft8Es4Hbnrm,There are a million reasons to join Augusts 30 Day Challenge but lets talk about the top fiveIf all your friends started mastering Data Engineering would you? Of course you would! Almost 3000 learners have already joined the challenge and according to a Twitter poll entitled will you join the thirty day challenge? we have an overwhelming enthusiasm for joining the challenge vs not joining • It can improve your career prospectsWhat certification offers that experience doesnt is peace of mind says Niels Buekers Managing Director Fourcast BVBA
eQvFG7rRzi93QFi5uWRQsU,Well admit it  weve been going a little data-crazy this past month Can you blame us? August had a 30 Day Data Engineering Challenge which over 3000 Qwiklabs learners signed up for (and hundreds won!) But were not the only ones who are amazed with what GCP data tools can do Take 10 credits off on us with code 1q-datado-947 and continue on your data engineering journeyThe New York Times has over 100 years of newspaper photos archived Until 2015 these irreplaceable pictures were only stored physically in the basement of the Times building But now the NYT has stepped into the 21st century using Google Kubernetes Engine not only to digitize their photos but organize them GKE runs a cloud database in the form of Cloud SQLDo you have precious pictures that should be protected from the elements of the basement? Theres a lab for that Learn to run a database like a newspaper with the Google Kubernetes Engine Best Practices questLess of a reporter more of a shopper? Data has you covered there too Department store giant Macys uses Google Cloud to sort pack and ship products in a streamlined organized online retail experience To scale your own business to succeed on the world wide web try out the Google Developer Essentials questBest of all scientists are using data engineering to do some real good in the world Dr Mia Gaudet used a Machine Learning pipeline to perform deep analysis of breast cancer tissue samples By processing data through the Google Cloud Tensorflow Dr Gaudet and her colleagues at the American Cancer Society can analyze thousands and thousands of samples in only three months  thats twelve times faster than it wouldve taken without data engineering Dr Gaudet and her colleagues are inspiring but you dont have to be a doctor to master labs like Building an IoT Analytics Pipeline on Google Cloud PlatformWhile the month of Data Engineering is coming to a close theres no need to cry Data Engineering is becoming a more vital part of the way we read shop and live every day
8Ni2ki39c2SQE4avPYeoTR,You read that writeI will not write long sentences and leave that to you to further research on Google as anyway everybody does If this tutorial helps you in anyway please leave me a clapLets beginTo install ubuntu on Windows 10 machine  Make sure you have 64 bit windows 10 install on your systemGo to start -> search  Windows feature and click on Turn windows feature on or off option  Scroll down a bit lower and you will see the Windows subsystem for linux option as shown in below image  Check that option and click ok This will prompt you to restart windows You can restart window to make this changes available immediatelyOnce the windows completes restart process launch the ubuntu by either using search option or with windows icon on desktopWhen you launch the ubuntu for the first time  it will ask for username and password Please note this is nothing related with your windows password You can set the username and password for your completely different ubuntu instances and you will be using this credentials later when required in running any process on your ubuntu appCongrats Ubuntu 2004 is successfully installed on your windows 10 machineNow We will proceed further on to install git and configure the git on linux instanceMost probably git should already be installed on your ubuntu instance To check run below commandNow the git should be configured on your new ubuntu instanceIf you want to make any changes to your configuration above either you can just list the config file in your git folder  type ls -la in the folder and edit the gitconfig fileCongrats your first local git repository is createdIf you want to push your repository to your github account Go to githubcom and login with your username and password create repository over there Useful resources : https://wwwyoutube
A23iLA6cgm8c5tKdgqkiDT,All engineering leaders must spend some time creating a set of guiding principles for their teams Any good engineering team should be rooted in a core set of values These values act as the north-star for a team and enable it to align its activities with meaningful business outcomesData Engineering teams are no different but they have some unique challenges Data Engineering teams often work in silos sandwiched between data producers and data consumers (see https://martinfowlercom/articles/data-monolith-to-meshhtml for more on this topic) It is common for these teams to feel isolated and lose sight of the larger purpose of their functionHaving a list of guiding principles is a first step in creating a sense of identity for the data engineering team in a complex organizational setupFor our data team at Omio we use the following principles to guide us These values are by no means exhaustive They are also highly contextualized to our unique environment and challenges But these principles help us take the right decision in moments of doubt
fdJGBUigyLtqHCLBVPEcfZ,https://developersgoogle
LmVM9sCWCxQGiBPHc66MrD,When running a training job on AI Platform Training you must specify the number and types of machines you need To make the process easier you can pick from a set of predefined cluster specifications called scale tiers Alternatively you can choose a custom tier and specify the machine types yourselfSpecify the scale tier identifier and machine types in the TrainingInput object in your job configurationThe following example shows how to build a Job representation for a job with a custom processing clusterNote: You can only specify workerType parameterServerType evaluatorType workerCount parameterServerCount and evaluatorCount for TensorFlow training jobs and jobs that use custom containers Do not specify these fields for scikit-learn or XGBoost training jobsNote that training_inputs and job_spec are arbitrary identifiers: you can name these dictionaries whatever you wantGoogle may optimize the configuration of the scale tiers for different jobs over time based on customer feedback and the availability of cloud resources Each scale tier is defined in terms of its suitability for certain types of jobs Generally the more advanced the tier the more machines are allocated to the cluster and the more powerful the specifications of each virtual machine As you increase the complexity of the scale tier the hourly cost of training jobs measured in training units also increases See the pricing page to calculate the cost of your jobAI Platform Training does not support distributed training or training with accelerators for scikit-learn or XGBoost code If your training job runs scikit-learn or XGBoost code you must set the scale tier to either BASIC or CUSTOMThe CUSTOM tier is not a set tier but rather enables you to use your own cluster specificationAutoML makes the power of machine learning available to you even if you have limited knowledge of machine learning You can use AutoML to build on Googles machine learning capabilities to create your own custom machine learning models that are tailored to your business needs and then integrate those models into your applications and web sitesYou can use the following AutoML products to create custom machine learning models: AutoML Vision Classification enables you to train your own custom machine learning models to classify your images according to labels that you definehttps://wwwyoutube
3CLJ2vWCTsxwD5v3BarMNY,I sat for the SnowPro Core Certification exam via online proctoring by Kryterion and cleared successfully I want to share my exam preparation and exam experience in order to help interested colleagues in obtaining this certificationSnowflakes mission is to enable every organization to be data-driven Cloud-built data platform makes that possible by delivering instant elasticity secure data sharing and per-second pricing across multiple clouds Snowflake combines the power of data warehousing the flexibility of big data platforms and the elasticity of the cloud at a fraction of the cost of traditional solutionsSnowflake is a SaaS solution and is maintained by Snowflake and not the customer Snowflake manages the services compute and storage layers Data is only accessible via SQL in Snowflake Snowflake can be deployed within AWS Azure or GCP although not in customer controlled accountsSnowflake is a single platform comprised of storage compute and services layers that are logically integrated but scale infinitely and independent from one anotherThe SnowPro Core Certification demonstrates an individuals knowledge to apply specific core expertise implementing and migrating to SnowflakeThe subject areas covered includes: Exam Format: To prepare for the exam I utilized the following resources: I would advise doing all the questions from the Level Up series and the actual Sample Exam (30 questions) Some of the questions are on the real examI estimate that I spent about 8 10 hours on exam preparationSnowflake University contains a number of free courses to help you learn Snowflake and help with certification exam preparationI used the Level Up Series to help fine tune some areas where I needed some improvement The Level Up Series has 8 videos with quizzes dedicated to various topics included first concepts and performance topics The performance topics were really beneficial in learning about caching and query and results historyAdditionally Snowflake University provides a study guide for the exam This has valuable links to the documentation and other exam resources like videos and blogsThe Snowflake Documentation is the source for all content required to pass the exam If there is a single piece of advice I could give is to study the documentation Use the practice exams to determine areas of improvement and use the documentation to study in further depthTackling Snowflake Certification  Practice Questions developed by Hamid Qureshi contains 5 practice exams that really do a good job of helping you prepare for the exam Each exam contains 56 74 questions that you will find on the examThe exam itself is proctored by Kryterion Due to COVID-19 I used online proctoring since onsite testing centers were closed When using online proctoring software will be required to deliver the exam to your computer and enable the webcam and audio for proctoringTo schedule go to WebAssessor and find a time slot that will fit your schedule At the time I scheduled the available time slots were very limitedPrior to your exam be sure to install the Sentinel software that will deliver the exam to your computer This software will block you from accessing other software on your computer during the duration of the exam The software will also require access to your webcam and microphone After installing the Sentinel software a biometric profile will require setup The biometric profile setup will prompt you to enter your full name multiple times and it will perform facial recognitionOverall the exam is straightforward By doing the hand-on lab tutorial reading the documentation in depth and utilizing some existing free resources provided by Snowflake you have a good start to passing the exam
gNkpsJgDfCuMFh2F72Nf3G,Data has been the primary reason why computers & Information Technology evolved In the modern age Data is the key ingredient that is driving most businesses The data storage & processing has come of an ageI started my data journey almost two decades ago working on the traditional BI and ETL tools However over the last few years there has been a major shift from these concepts & technologiesThe Data Warehouse Age : The data warehouse concept has existed for the last few decades They were the really important way to organize your data to meet the BI and reporting needs A few terms like Dimensional Modeling Star Schema SnowFlake Schema ETL were coined and became popular in the communityThere were a plethora of tools that helped to transform and load the data in the data warehouses Most of the tools offered Graphical User Interface (GUI) for how you model your ETLThe Data Lake Age : With the ever increasing 3 Vs of data (Velocity Variety Volume) it started difficult to fulfill business needs with traditional data warehouses Some argue Data Lakes would replace traditional Data Warehouse others argue they complement them However one thing is unanimous that Data Lakes have become an important part of data transformationVariety of Data : Traditionally the data was mostly structured and ETL tools excelled at processing the dataVolume of Data : Data volume is increasing exponentially The traditional ETL tools cannot handle this with the scale up architectureVelocity of Data : The Streaming data coming from the sources like IOT Telemetry ClickStream is more real time It is difficult to process it using traditional ETL tools designed for Batch processingDistributed Computing : Hadoop floated the idea of taking the compute(program) to data instead of bringing data to the compute The traditional ETL tools used the same concept of moving data to compute on scale-up architectureRelational to NoSQL Shift : The Sources and Destinations have become much more varied This does suite the Traditional ETL ToolsEmergence of Cloud & ServerLess: Cloud has been a game changer The early ETL tools were designed for persistent infrastructure They are not best suited for the cloud ecosystemThe traditional ETL players are trying to adopt to newer ecosystemNevertheless in my opinion the structured databases & processing needs are here to stay for the foreseeable future So it would be an exaggeration to say these tools are dead but for sure they are becoming less & less relevant everydayYou can find more similar stuff on my website at : https://wwwrakeshg
Aum7pGVnXELyyFXHvo8vyL,Whenever transactions happen from any channel (web mobile IOT distributed systems etc) get triggered and send a message(transactions info ) to Pub/Sub (topic)What is pubsub Pub/Sub brings the flexibility and reliability of enterprise message-oriented middleware to the cloudRead the message from Pub/Sub topic in realtime and process the data in parallelCheck the fraud deterministic and probabilistic with AIAI PlatformServe trained tensorflow model to detect probability of fraud on transaction Read the fraud list from Bigtable to determine the probability of fraud on transaction and response to apache beams parallel data processingApache beam Writes the real time data to Bigtable when humans need to analyzeRead the data from bigtable when an AI model needs to check fraud listWeb app hosted on (GCP Cloud Run) Automatically scales based on trafficReceive messages from Back-office sends to subscriber (Cloud Run ) to update the Bigtable status to analyzed mean time sends to another subscriber (Cloud Run ) to append a row to BigQuery with latest statusSave the processing data for five years Most of the services which used in the solution are fully managed and highly scalable with horizontal auto scaling or just adding nodes (machines) easily scale horizontallyAllow teams to focus on programming instead of managing infrastructure
3HaVZkBLFYzeh7kFUUhue9,Subscription-based on-demand content and media platforms have risen in demand over the years as they offer more flexibility and better entertainment to users As OTT platforms replace traditional TV sets they are becoming the preferred medium for consuming content be it around entertainment news sports streaming or educationIn India the growth of OTT players is astounding According to a report by the Boston Consulting Group the Indian OTT market will grow ten times to reach $5 Bn by 2023 from $500 Mn in 2018As television programming undergoes a renaissance the promise of more and better data allures advertisers Data and analytics stay as the key to televisions profitability Advertisers now have more access to data due to OTT platforms while content providers and networks leverage data to guide programming decisionsThe growing abundance of data available to marketers advertisers and content providers makes it necessary to have systems in place to analyze process and transform this data into something meaningfulTo offer better personalization content discovery and push the continuous development and transformation of underlying systems it is critical for OTT platforms to use data to its full potentialFor homes that have a subscription service OTT players are using data to guide viewers to the right content The most famous example of data-driven viewership is Netflixs House of Cards The content is designed to be a hit considering platform data such as ratings preferences user viewing habits and so onNetflixs data pipeline is fed with data from millions of set-top boxes and online accounts which the company processes and stores using the Hadoop ecosystem and leverages Amazons AWS platform for cloud computing resources Data from A/B testing has led Netflix to insights that have given tangible results such as up to 30 percent increase in a particular contents viewershipAlluring and retaining the generation of cord-cutters and cord-nevers takes more than great content These viewers expect tailored services in the form of personalization served based on their preferences and choicesPersonalization is not just about the content Everything from subscription plans to metadata frontends and recommendations can be personalized for greater impact and viewership which ultimately drive revenueAll OTT players sit on a huge pile of relevant data about their viewers and their behaviors By putting this data through a cleaning process and leveraging insights for improving the recommendation engine or personalization feeds OTT players are enhancing the user experienceBy knowing what will keep users coming back for more companies are making personalized recommendations and accurate predictions for cross-selling or upselling opportunitiesHaving a more nuanced and sophisticated view of the customer can help companies keep track of this trend in data analytics and OTT media suggestionsCustomer-specific data collected by means of AI and ML and churned with the help of analytics helps OTT players recommend shows series and movies to viewers through personalized push notifications and emailsSince both of these marketing media are proven to have exceptional RoI OTT companies can now use them to make relevant recommendations on the basis of viewer interest and viewing behaviorFurthermore the push notifications and emails can be timed for when the user is more likely to access the OTT media site or mobile app- for greater impact and conversionProgrammatic media promises data harnessing for advertisers using set-top box data to inform OTT players about viewers and their behavior in consuming content Building on the viewers inclination for customized viewing experiences programmatic television can deliver unique advertisements even though viewers are watching the same programmingAccording to eMarketer only 5 10 percent of the TV inventory is available and addressable for programmatic Even then the market looks massive and promising Programmatic advertising is all about the efficient spending of dollars by careful targeting and retargetingThis is why data and analytics look vital for personalized digital advertisingCompanies are now working with entertainment players to measure audience metrics across platforms and channels By aggregating viewers and their behavior companies get a more holistic view of their audience offering advertisers opportunities to execute an omnichannel advertising planBy augmenting data such as viewing history demographics political inclination and so on companies can now offer in-depth insights to advertisers for improved ad delivery and executionOTT ad spending has skyrocketed and providers are maximizing their investments in correlating viewership with consumer purchasing behavior The convergence of OTT with e-commerce driven by analytics and personalized advertising is opening up better and more avenues for profit for providersThe cutting-edge technologies of artificial intelligence and machine learning are also playing a strong role in facilitating content discovery creation and recommendation in the OTT arenaA personalization approach backed by these sophisticated technologies can help companies make leaps of progress in regions such as India where people in the regional market have their own sensibilities and entertainment preferences besides language inclinationsData available to OTT players needs to be churned and utilized for insights so that effective decision-making can be practiced When that happens data utilization will be the driving fuel behind the success of OTT players and their expansionAt RecoSense we use artificial intelligence to build deeper insights across touch points for OTT players and digital media providers Learn more about how we do it here
Wh6jLXRccSdxF76cbKDJxY,Snowflake data warehouse offers many options for importing data into their platform They support several external locations (local AWS S3 Azure Blob Storage and GCS buckets) many file formats (CSV JSON PARQUET XML AVRO ORC) and even different compression methodsFor our current use case we are integrating several legacy systems data dumps into our cloud data warehouse We get several extracts many times a day in our Azure blob storage The files compressed in GZ and have a CSV formatThe recommended way to import data is to use a COPY INTO command which specify several things about the source file You can encapsulate this information in a FILE_FORMAT definition this help you to re-use your already declared metadataWhile exploring the files we found out that besides the delimiters they also were in a fixed width format A flat (or fixed width) file is a plain text file where each field value is the same width and padded with spaces It is much easier to read than CSV files but takes up more space than CSV  It is strange to have both file structure in the same file because you only need one to import them correctlyWe started with the following FILE_FORMAT: TRIM_SPACE is going to remove the unwanted trailing spaces from the fields and EMPTY_FIELD_AS_NULL will convert the empty fields to null This is important because we wanted to cast the columns to their correct type while ingesting into the tablesWhen we loaded several files we faced the following error for one of our loads: After checking the delimiter number for every row in our source file it seems that they are not always equal some of the rows have more delimiters The legacy system sometimes gives an extra delimiter when there is no data present for the field Because of this the width of the rows are still correct but the delimiter structure is notTurns out Snowflake COPY parses the first line of the file using the defined field delimiter and uses that information for every row When it finds a different character it throws an error This makes sense because we expect to have the same number of delimiters for every row It is also possible to get the following error this means that the first line had more delimiter than the nextBecause of the structure violation we must change our approach Changing the legacy system export is out of question We can preprocess and validate the files but it would add another complexity layer to our system We can use the fixed width structure of the files but Snowflake does not support it yetSo we had to create a workaround We decided to load the files as they are into the tables as a single column and parse the information out later To do so we created a new FILE_FORMAT in the system: We can use any field delimiter we decided to use ÿ because it not common in our line of business We are validating our data ingestion further in the pipelineCopying the data into our staging environment: Because we know the width of the data we can use SUBSTR function to parse out the correct fieldsThis workaround let us ingest data as it is and propagate the parsing logic into the SQL layer The cost of it is processing power: The ingestion need ~20% more resources (substring casting etc…) While Snowflake data warehouse is great this additional ingestion option would improve further the system
iSSJc5DD3dvmfVAEJjQApV,This post will go into the details of the tool developed by LinkedIn known as Apache Kafka and how different companies like Netflix LinkedIn Uber Instagram Twitter and Honeywell uses this toolAfter reading this post youll know the answers to the below question: All of the concepts are explained like youre 5Kafka was initially developed by Linkedin to implement or solve the problem of a website activity tracker sending logs to the server and to build custom data pipelinesQuick note: website activity tracker tracks and collects all of the activities that took place on website(like user liking a post searching for something comment spending X amount of time on Y post clicking on a profile)All of these events/activities are sent to serversTechnical definitions: Ill break them down further But before that lets cover key components of KafkaThink of Kafka as an interface a way to connect different sources like a pipeline It is used to transfer the data from one place to anotherKafka is based on Publisher-Subscriber modelThink of it as a newsletter A publisher publishes newsletters(messages) subscribers subscribe to the newsletter(topic) and then they receive new posts(messages) whenever the publisher publishes a postThis is the exact way how Kafka worksMany sources publish the messages(data) and consumers consume themEnough theory lets get into the application of KafkaMessaging system is like an interface that is used to build data pipelinesData pipeline: sending the messages(data) from one source to another All of this comes under Data engineeringSolution: This is where Kafka comes into the play Producers publish the message to a topic Many destination consumers subscribe to the topic and then keep receiving messagesKafka decouples event/message/data producers from data consumersBecause of durability and its fault-tolerance nature Kafka is used to store the stream of records or messagesStreaming data: data thats coming continuously from various sources and that needs to be processed either in real-time or after some timeUsed by both OLAP based systems or OLTP based systemsEx: log data event data lots of events that are happening on any e-com website like click on the product add to cart buy similar itemsKafka is used to moving collect streaming data from one place to anotherNow lets explore how different tech giants use Kafka • LinkedIn use Kafka heavily to build many custom data pipelines and to track activity data on the websiteEach activity has its own topic and those activities are then sent to topics as per its natureThat collected data is then used for data processing and analysis • Netflix use Kafka as a storage system in its Keystone pipelineKeystone is Netflixs data backbone that focuses on data analysis Kafka at Netflix is responsible for producing collecting processing aggregating and moving all microservice events in near real-time and it also stores them on a temporary basisReal-time processing tasks like computing business metrics processing logs data sending alerts to developers whenever something goes wrongBatch-processing tasks like sending data from one source to another Doing ETL(extract-load-transform) tasksActivities like the number of likes comments number of shares are sent to databases like Cassandra using Kafka as an interfaceAggregation queries are executed on the top of those DBSResults(total number of likes) are then returned and published back to another Kafka topicReferences: Originally published at https://rishabhgargdev on August 1 2020
dQTcSDfZzTCBgkMjBWjezj,In this post youll easily understand analytics functions on BigQuery concepts like window over and partition by wont be a pain for you anymoreThere is a point when working with databases where you need some advanced analysis on your data You can achieve complex analysis in many ways using SQL operators and Analytics Functions came to extend the power of SQL making it possible to write in a few lines queries that otherwise would require a lot of self-joins and aggregations This article is for you that have a basic to intermediate experience with SQL and want to understand straight to the point analytics functionsFor our journey lets consider the following scenarios related to a list of students in a school categorized by classes and their sales on a local eventScenarios: Before talking about Analytics Functions consider for the first scenario we are calculating the average age of all students it means we are aggregating/combining a group of rows in a single resultFor this scenario the group of rows combined contains all rowsThe result is 20The second scenario is very similar to the first one except that the group of rows combined wont be all rows but same class rows Then well use the GROUP BY clause to aggregate the Students by class before calculating the Sales averageThe result is: For this scenario each group of rows combined (3 aggregations / 3 Classes) results each an average outputConcluding in a universe of n individuals (rows) each combination of grouping will result in a single output value If you split the individuals in 79 groups youll get 79 outputs or if you just put all indivuduals in one group youll get 1 output as shown in our first scenario When aggregating data you can apply several functions like: MAX AVG COUNT SUMYou can read more about aggregate functions in: https://cloudgoogleNow that we understand what is and how works an aggregate function lets think about the third scenario: We need in the same table each individual students Sales and the average sales within its Class This scenario requires both non-aggregated and aggregated data An analytics function returns a single value for each row by computing a function over a group of input rowsUsing them you can achieve the same result as the aggregation in the second scenario but apply it for each individual row instead of aggregating the result in three output rows you can see the result in the left table Having said all of that lets see how to write an analytics function in BigQueryThats the analytics function syntax in Google Big Query and well modify it to get our desired query and resultThe FUNCTION could be an average sum rank or other supported aggregate function (find more here) Well use the AVG function and PARTITION BY for the third scenario we dont need ORDER BY or WINDOW FRAME We just need to partition the rows by Class before calculating the average and output it for every single rowThe query above is enough to get the results we saw on the previous table The PARTITION BY Class is basically saying: If the Students Class is A calculates the average of all As Sales and output itAnalytics functions besides calculating aggregated data as shown in the previous scenario can also do so through a window A window frame that moves towards the rows and process an operation as you can see in the following generic example that sums the current value with its successorNote that as the window moves it sums the current and the successor number Also be aware that the elements order interferes in the result Thats the basic idea of windowing and you can configure multiple sizes of windowNow we can resolve the fourth scenario: The average students sales sum with its predecessor and successor by Class You already know what is an aggregate function analytic function and windowing Lets put it all togheter and make an advanced queryAttention: remember that the order affects the results when dealing with windowing In our hypothetical scenario we need to order the data inside each group based on Sales Thats the ORDER BY clause on SQLLook now we have almost all the pieces filled we just need to define the window_frame_clause Since the third scenario says The average students sales sum with his predecessor and successor we need to create a window that will contain one predecessor and one successor rowAwsome! Now the analytics function is complete Attention on the windowing that is evidenced in the rightThe Janices Sales_avg is the average of her predecessor Sales: Amanda; 100 her own Sales ;124 and her successor; Sabrina 187 Resulting in 137 That window is in dark red in the right of Partition B DataBut what if we need a larger window frame that considers two elements preceding and two elements following instead of one? you can change the windowing configuration as: Analytics functions are very usefull in many cases and can be customized with several options that arent described in this article besides aggregate functions we have also navigation functions and numbering functions for BigQuery (find more here)I hope that this article helped you understand this complex feature of BigQuery I always try to draw and make graphics in my articles to simplify the understandingYou can connect me via Gmail: rodolfomarcos07@gmailcom or Linkedin: https://wwwlinkedinThank you so much! Pardon my mistakes I hope it will be usefull for you
JEwfCFHZTttvfHiBojcydc,"BigQuery is a managed service provided by Google It is ideal for cloud-based data warehousing that is managed software as a service for more information please visit this link: Pricing: Google Data Studio is a data visualization tool that is used for analysis generating reports please visit this link for more details: Pricing: Its currently freeWe use Google BigQuery (BigQuery) as a data warehouse and Google Data Studio (GDS) as a reporting tool in LBB For data analysis we ingest data from our other sources of data in the form of batch or stream based pipelines in BigQueryIn the figure above it shows how we are using BigQuery for analysis The system was durable until the BigQuery tables became large making queries slow and expensiveWe started monitoring the tables and observing those tables as we pronounced them as Dimension tables getting bigger which made queries expensive and slow In our system GDS reports require data collected daily which needs to be processed each time GBs data from dimension tablesWe have a table A which is 33528 GB in sizebig query:select * from `bigquery-project_iddatasetNametableA` This query will process 3353 GB when runWe modified the table A structure and partitioned it based on a date with a clustered column after references this video Data Warehousing With BigQuery: Best Practices: create table `bigquery-project_iddatasetNametableB`partition by partitioned_date -- date columnsclustered by event_name -- columnsoptions (require_partition_filter=true)as (select * from `bigquery-project_iddatasetNameNow run the query to select all columns of last day on both tables for comparison:Query 1:select * from `bigquery-project_iddatasetNametableA`where date(event_time)>=date_add(current_date() interval  1 day)Query complete (177 sec elapsed 335Query 2:select * from `bigquery-project_iddatasetNametableB`where date(partitioned_date)>=date_add(current_date() interval  1 day)Query complete (215 sec elapsed 9595 MB processed)Whaaat! partitioned date column reduces data read 350 times But wait for a sec it also increased the read time""Now run the Query 2 with where clause on event_nameQuery 3:select * from `bigquery-project_iddatasetNametableB`where date(partitioned_date)>=date_add(current_date() interval  1 day) and event_name='App Installed'Query complete (19 sec elapsed 507Cant Believe It Right! But this is true in 2 seconds we got the result with the help of cluster column event_nameNote: We concluded that we would partition all the tables and restrict them with a partition date column filter Therefore if anyone wants to run the query they should provide the dates with where clauseWe were thinking that we have finally found a solution to the problem but there are also some shortcomings in the clustered table If we start streaming data to a cluster table then the cluster order will start to weaken because BigQuery needs to maintain both partition and cluster order as well as provide data availability that has a lot of operation under the hood is neededThe above solution looks good but how to update intermediate tables: We compared the above options and figured out that except for the last option we needed to maintain another service and their cost separately Also the price of scheduled queries is similar to manual BigQuery queries Therefore we choose the last option for updation of intermediate tablesNote: With the help of the scheduling option in BigQuery we can update partition-clustered tables based on time intervals that select data from the parent tables partitioned into the structure I would suggest at least 10 minutes should be set for scheduling queries Again we do not need to manage scheduling queries so write them ones and use them foreverWe ran this process for 3 months (July  September) and monitored BigQuerys billing report Below is a screenshot of the billing reportBigQuery helps us focus to meet business needs If the time interval is the same for more than one scheduled queries we do not face any issuesFor some reason if we need to add historical data scheduling queries have a backfill option like Apache Airflow"
dXjYvwSQUpN49eY5KMWic9,The goal of this article is simple There are a few common patterns that Ive now realized is quite useful Applications need to publish probably large amounts of it and you want to store it in the cloud and for analysis or machine learning purposes Google cloud is quite powerful but definitely takes some getting used to and to get everything configured properly takes some time to learn and there are gotchas everywhere This article will hopefully allow you to copy and paste and get startedHere are what we are going to build: Here are the steps for prepping GCP and creating the proper resources Most should be fairly straight forward: Next we will look at the key elements of each elements You can also find all the codes here: https://githubThat is it! These are all the components you need to build a streaming pipeline to process data from GCS -> Dataflow -> Bigquery
DncdggNRbfjnU6RcdQNTyb,There should no doubts in anyones mind about how Big Data and AI are fueling the next revolution Data is the new oil and AI refines the oil The questions to ask is the following: Data Engineering has become vital for any organization that is serious about harnessing dataThis blog illustrates how Azure Databricks strives to modernize and yet simplify data engineering to reduce the data to insight turnover timeLet us start by understanding the marketAccording to IDC there are three major trends when it comes to Data and AICloud computing is the primary catalyst to make these changes happen Cloud enables innovationForbes predicts that in 2030 the global business value derived from Advanced Analytics & AI will be $15 trillion This bucket is distributed across smart products virtual agents and decision automationThe potential is immenseThe path to innovation is never easy It has its road-blocks Three key challenges prevent organizations from realizing their objectives with Big Data projects are: By mapping these challenges to the study conducted by TDWI it is clear that most companies are not satisfied with their current solutions: New problems strive for new solutions Addressing these considerations requires a modern data engineering strategyAzure Databricks strives to hit the sweet spot Azure Databricks is the jointly-developed Data and AI service from Databricks and Microsoft with a razor-sharp focus on data engineering and data scienceLet us deep-dive into the key five capabilities of Azure DatabricksAzure Databricks provides five key capabilities: Azure Databricks along with Azures ecosystem offers a solution that is innovative scalable and focuses on value at the right costNow that there is a better understanding of Azure Databricks let us deep-dive into the Architectural constructs of a modern data engineering platformThese are the four critical pillars of modern data engineering Ingest Store Prep and Train Model and Serve It will look traditional but the devils are in the detailsLet us drill down into it: The Architecture is scalable on-demand with built-in high availability (HA) and is capable of processing and serving petabytes of data at lightning speedThe architecture illustrated in the previous section is tried and tested with multiple customers across the globe Architecture is such that it gives the right levers for cost and functionality controlThere are a few fundamental principles to note in this Architecture: This Architectural pattern enables data engineers to focus on doing data engineering with a single goal in mind: To reduce data to insight turnover time Focus less on technicalities Focus more on functionality
i6yUg8h8qyLkiRoT2LsSzz,CI/CD aka Continuous Integration/Continuous Deployment in the context of databases is not a very popular theme The reason could be that enterprise databases are still hovering around 40 year old relational database technology (for good reasons most of the time) and the people working there are still schema-bound Old dogs and new tricksThe data warehouse runs on Oracle 18c and hosted on AWS RDS Its dimensionally modelled with no snowflakes Some of the dimensions are built to serve as a reference data source for the enterprise This write-up wont get into the database design part of the DWThe DW and the SDLC servers hosted in docker containers both sit inside an AWS VPC in a private subnet Users connect to the SDLC tools (Airflow DAGs and DBFit test cases) via browser or Windows RDP but only after establishing an SSH tunnel for securely reaching the private subnet A tunnel is required for DW connection as wellEach developer and tester does their work in their own database schema So user John Doe works within his JDOE schema There is NEVER any work outside of their own schema So every user schema has a copy of the entire DW and hence the entire DW is their playground- but their own version of itEach developer/tester has their own EC2 virtual machine This machine has a docker container which hosts Airflow Liquibase and DBFit Every user has the same container image on their EC2 When an ETL developer has built PL/SQL code in their schema on DW they test in the same schema They can test it directly in their schema or test it by triggering the Airflow DAG on their EC2 server To trigger from Airflow the new code has to be copied from their schema to a new file in the local git repo on their EC2 serverOnce code has been tested git is used to merge the changed files to the master branch The master branch points to a schema dedicated for integration where everyones code gets integrated and runCode deployment is done through Liquibase which is triggered by one of the tasks on Airflow that runs before the ETL task Liquibase deploys code sequentially by default in alphabetical order of the file names It keeps track of migrations to each schema through two changelog tables available in those schemas Liquibase migrates only the changes since last migrationThe DW refreshes every day with new data The Airflow on integration server (inside the third yellowish docker in the pic) runs every day One of the tasks in Airflow DAG will pull latest PL/SQL ETL code from git master branch and save it on the local repository The following task will trigger Liquibase to deploy code on DWs integration schema The next task will run ETL which also includes the latest codeThe Airflow on each users machine (inside the first two yellowish docker in the pic) simply deploys code using Liquibase and runs ETL It doesnt pull from git master Users get the updated code from master branch only when they are ready to merge others changes with their own usually after they have built and tested their own changes This git process is explained later below in Step 4DBFit tests run right after ETL load on the DAG The same steps for development applies for testing as well You can create test cases on DBFit through the browser A test case is a query or a few queries or a query and the expected output value These get saved as a file on the EC2 instance From this point the tester follows the same process as a developer to integrate test cases to master branchWe used a git service called BitBucket Our tasks originate in Jira The Jira assigned task identifier is used to identify each feature branch on git by explicitly naming it so For example a task on Jira to create ETL for FACT_CLAIM could be assigned an identifier CLAIM-DW-150 where CLAIM-DW is the name of the Jira project and 150 is the sequential number of this taskTrigger Airflow DAG to test the local copy of your code on EC2 file system This will run your ETL changes test cases etc After youre satisfied with it start with git When you join a project there is a one-time step to clone the git repository of your project to your local machine (in our case to the EC2 instance)A larger task/feature can be split into multiple feature branches on git instead of just one We use the format CLAIM-DW-150-a CLAIM-DW-150-b and so onPeer review is built into the git process Nobody throws code over the\xa0fenceThere are separate branches for prod-fix (pre-prod) and production environments which essentially point to their own schemas on a production DB server These git branches have gate keepers from DevOps\xa0who\xa0reviews\xa0and\xa0merges\xa0code\xa0migrationsLiquibases default ordering of files while deployment could create some headaches We prefixed the filenames with a sequential number to control this behavior For example 00021_disable_fact_fksqlWe spent a few weeks trying to set up SDLC containers on Kubernetes cluster Despite of having an existing Rancher setup and help from our web services team this proved to be an overkillGetting database developers and testers to be familiarized with automated CI/CD process was a bit of a challenge myself included But avoiding manual changes on higher environments (integration prod) was a big process improvement
ZpaCyhcKigqr3ACpJBQSfW,Steps: : 1) Go to azure key vault and click on the Access Policy You can find access policy under Settings of AKV • Click on Add access policy and provide below details: - Configure from template:  Key Secret & Certificate Management Key Per:  select all Secret Per: -select all Cert Per:  select all • Click on select principal and give the data factory name in search bar and click on add and then save • Go to secrets and click on generate/import and provide the data bricks token in it • Go to data factory and create a linked service for azure key vault In order to create it just type key vault under Data Store (you may see 2 options:  data store/compute) and continue • Give name to linked service for KV and select subscription and existed Key Vault name After successful connection create it • Now we need to create linked service for Databricks using AKV linked service You can find data bricks under compute (Data Store/ Compute) services while creating linked services • Give name for ADB linked service use AutoResolveIntegrationRunTime choose your subscription give name of subscription give ADB resource name select cluster in it click on Azure key vault and select AKV linked service which we had created in step 6 And give the secret name which we had created in step 4 You should be able to see you cluster name in dropdown for cluster id Check if connection is successful and create it
aHZhKSFYSrpdbssY3frLyc,When writing tests it can be useful to take data files from external systems and use those in your tests These files can be quite large so it makes sense to trim them down to a single record before checking them into git This is simple to do by hand for human readable file formats like csv and json but not for binary file formats like Parquet To my knowledge there are no pre-built CLIs that do thisIn this post well install a command line tool and register it to your shell This way all you have to do is open a terminal and run the following commandThis will generate a new file fileparquet_trimmed containing the same schema and just the first record of the input fileRun this command to download and install trimparquet: The output of this command will contain something like this: Copy the line starting with alias from the output and add it to your shell rc which is ~/bashrc if you use Bash and ~/zshrc if you use Z shellThats it! When you open a new terminal you can now trim parquet files like this: If you already have pandas and fastparquet installed in some Python environment you can of course use that environment to save some disk space In that case after installation remove the trim_parquet_env directory and change the alias to point to that environment
HmWMYgen7wLKbDAMgWFYxz,In the modern big data ecosystem data exists in structured and unstructured formats across various sources such as relational non-relational and other types of data storesIt is important to have a tool in this ecosystem to organize these large volumes of raw data into actionable business insightsAzure Data Factory is a managed cloud service that is built for these complex hybrid extract-transform-load (ETL) extract-load-transform (ELT) and data integration projectsIt is Microsofts Azure implementation for Data Integration and Orchestration; allowing you to move your Data from one place to another or make transformations to itADF can be considered as a platform on Big Data Analytics According to Microsofts ADF introduction  the key components of the ADF as follows
NCwDcXSEtWTEP8aksudffk,There are three ways to perform the Copy Data taskIn this tutorial we only focus on ADF user interface (UI) in order to create a data factory pipeline that copies data from an on-premise SQL server source to Azure Blob storage destination(If you want help to create ADF follow this article https://mediumGo to Azure Data Factory And select the Author & Monitor tile to launch the Data Factory UI in a separate tabEven though ADF contains a Self-Hosted-Integration-Runtime it can not be used when it is accessing an on-premise data source/destinationAn Integration-Runtime needs to be created in ADF and installed in the on-premise machineNote : Due to security issues normally it is not recommended to install anything in on-premise data sources In that case you can configure up the Data Management Gateway and install Integrated-Runtime thereFor this tutorial the Integration-Runtime is installed on the same machine where the SQL server instance is installed • 1 Select Author tab on left pane go to Connection and go to Integration Runtimes Click on New • 2 In the Integration-Runtime Setup window Select Perform data movement and dispatch activities to external computes and then select next • 3 In the Integration-Runtime Setup window select Private Network and then click on Next • 4 Enter Name for the integration runtime and select Next • 5 Under Option 1: Express setup select Click here to launch the express setup for this computer • 6 In the Integration Runtime (Self-hosted) Express Setup window select Close • 1 In the author tab click on the Connections go to Linked Services tab and then click on New • 2 In the New Linked Service search and select SQL Server and then click on Continue • 3 Fill the blade as follows • 1 In the Author tab click Create Resource and then select Dataset • 2 In New Dataset search and select SQL Server and then click on Finish • 3 You should be back in the window with the source dataset opened On the Connection tab of the Properties window take the following steps: 51 In the author tab click on the Connections go to Linked Services tab and then click on New • 2 In the New Linked Service search and select Azure Blob and then click on Continue • 3 Fill the blade as follows • 1 In the author tab click Create Resource and then select Dataset • 2 In New Dataset search and select Azure Blob and then click on Finish • 3 You should be back in the window with the source dataset opened On the Connection tab of the Properties window take the following steps: 71 In the author tab click on Create Resource and then select Pipeline • 2 Go to General tab change Name as SQLServerToBlobPipeline • 3 Drag an drop Copy Data Activity to the pane on the right and then rename activity as CopySqlServerToAzureBlobActivity • 4 In the Source tab select the Source Dataset as SqlServerDataset • 5 In the Sink tab select the Sink Dataset as AzureBlobDatasetTo validate the pipeline settings select Validate on the toolbar for the pipeline To close the Pipe Validation Report select CloseTo publish entities you created to Data Factory select Publish AllSelect Trigger on the toolbar for the pipeline and then select Trigger Now • 1 Go to the Monitor tab You see the pipeline that you manually triggered in the previous step • 2 To view activity runs associated with the pipeline run select the View Activity Runs link in the Actionscolumn You see only activity runs because there is only one activity in the pipeline To see details about the copy operation select the Details link (eyeglasses icon) in the Actions column To go back to the Pipeline Runs view select Pipelines at the topThe pipeline automatically creates the output folder named fromonprem in the adftutorial blob container
ZNSMkk4HCR3sjHKvUj4Mqr,Yes it is really a nightmare when the data quality goes wrong for some of the tables in your data warehouse  And it is one of the most important aspects to keep the data quality in checkBut here is some good news that we have implemented in our data pipelines to make sure the data that we keep adding to the data warehouse is maintaining all the constraints that are defined and agreed uponToday I will describe here how we have implemented AWS Deequ for data quality checksSome Words about AWS deequ : The skeleton is apache sparkHere I will explain about the Column profiler and how to add more analysers to the column profiler In the other two components you can refer to the official documentThe repo of AWS deequ can be found hereAnalysis Runners: Here you can mention which analysis you want to run on the column The result will be given in terms of a data frameHow it looks : Use the Analysers on a dataset that I used from the public dataset section in the Databricks community section You can use your own dataset I would recommend using the community cluster of databricks as this gives you an on-demand cluster for freeI have used a public dataset based in databrics dbfs and implemented the modules  Here I have ingested the dataset to a dataframe and then created a dataframe only of 1000 lines to make the processing faster The results can be stored as a data frame : The result looks like this: I have implemented Analyzers Completeness ApproxCountDistinct size Correlation There are multiple analyzers available and some are datatype specificI will implement a custom column profiler based on datatype This is useful for a production scenarioColumn profiler : Here you pass the data frame and the profiler will send you some stats for each column : How can I use the column profiler : The results look like : The results look very promising but in case of an actual scenario we need to customize the profiler so we get more metrics out of the columns The implementation is customizedYou can specify which analyzers to use based on the datatype and also with nameConverting the metrics to the data frame if you want : This is how the updated data frame looks You can add as many as the Analyzers neededIn the next section I will mention how to use constraint checkers Constraint checkers are very essential if you want to bring unit testing to your pipelineI hope you have found the blog useful please let me know if you have any doubts with implementing this in your data pipelineThank you 
dPofLRbxeXQMG3XqSZjqqb,This blog is supposed to be a simple introduction to Java 8 javatime library From personal experience I can proudly say that timestamps are hard…Yes proudly this is because Ive finally gotten somewhat of a hold on them  but by no means am I an expert In this post Ill briefly describe the different types of timestamps that are available to you through Java 8 The next post Java 8 DateTime: Part 2 The Parsing will jump into use cases and considerations of timestamps from the standpoint of a data producer and a data consumer as well as a few code samples that I personally thought the internet was missing  maybe they are not anymore ¯\\_(ツ)_/¯Timestamps are usually made up of a few reoccurring components This will sound really dumb but stay with me Timestamps include the following attributes: Most of these aspects of a timestamp may seem pretty straight forward with the exception of a few that you may not have expected eg Year being optional  this is a very frustrating aspect of working with timestamps but Ive seen it countless times Zone and offset information are two more useful components of timestamps that many may not know about These last two fields will be the key difference when deciding which Timestamp class to useThe 3 formats: The 3 main formats are ZonedDateTime OffsetDateTime and Instant The difference between each of the timestamps is primarily just the amount of information provided shown in the diagram belowLets start at the bottom since this is the starting point for these 3 timestamp formats Among the 3 formats Instant provides the least information Instant provides just the specific moment/instant in time and is stored as just the seconds and nanoseconds (to allow for greater granularity) from epoch Just in case the realm of timestamps is completely new to you: The epoch then serves as a reference point from which time is measured - https://enwikipediaNow using Instant as our reference point we can explore the other two timestamps Going up to OffsetDateTime we add information related to the where the timestamp is from It does not provide an exact zone but we get information regarding how many hours and mins away from a time the timestamp represents Using the second timestamp from the examples above this essentially means the time provided is 1 hour away from UTC Adding the offset back will provide the local time Finally instead of just providing the number of hours away from UTC you can also provide more in-depth information by providing the Zone ZonedDateTime adds all the rules with regards to time zones The full rules take into account things like which zones follow daylight savings time and whether it is currently daylights saving time or not In summary if you know where to look  the documentation does a good job at describing this more succinctlyInstant is the simplest simply representing the instant OffsetDateTime adds to the instant the offset from UTC/Greenwich which allows the local date-time to be obtained ZonedDateTime adds full time-zone rules
Kfzvu83zfcLQ5cDGsNwHMt,"Perfect you are just in time for the code samples I will assume that you came here from part 1 Java 8 DateTime and that you know all the information written in the previous post 😛Parsing Timestamps: In order to parse timestamps you will need to create an instance of a DateTimeFormatter object and specify the pattern of your timestamp Details in the docs""Lets go through this pattern  yyyy-MM-dd'T'HH:mm:ssSSSVV  as we will use it in the following code sample I will break down each segment of the pattern and what that segment represents in a timestamp  once again some segments may be straight forward""-: all represent the characters themselves within the timestamp 'T' The T is surrounded by single quotes because it is just a character and does not mean anything to the timestamp Regular strings/characters need to be surrounded by single quotes to successfully ignore them  yyyy represents the 4-digit year  beware YYYY represents something slightly different and may only cause a problem in rare circumstances  MM represents the month  this can represent a number or text version of the month depending on the number of Ms you use eg 7 vs 07 vs Jul vs July dd represents 2-digit days  days can also be single digits refer to the code below to see how to handle both single digit and double digit days HH represents happy hour  jk this represents just the regular hours in timestamp mm represents the minutes in the timestamp ss represents the seconds in the timestamp SSS in the documentation this is referred to as fractions of seconds  but also microseconds Depending on your granularity add or delete Ss VV represents the zone-id  This can be as simple as the Z at the end of timestamps to represent zulu timezone or can also include offsets and timezone informationStraight forward parsing  you know the format of your timestamp and the zone is provided: As you can see in the above code blocks there are 3 potential methods to parse the timestamp with The last one being the simplest requires you to already have a valid Instant formatted timestamp The second method can be used with a variety of existing formats which can be found in the DateTimeFormatter documentation above The first is the most flexible  allowing you to define your own pattern Documentation on what each character in the pattern means is also available in the link aboveIn the case you are missing Zone information in your timestamps you must provide a default zone into the datetime formatter as done below: In the case you are missing a year: Quite often timestamps may not contain a year  at which point one option is to default to the current year at the time of parsingIt is not uncommon to have to handle varying styles of timestamps The difference in styles may be as simple as sending both single and double digit days vs just double digit days Varying styles will require you to use optionals in your formatterNotice the square brackets in the appendPattern section? It essentially allows the format inside the brackets to be optional Therefore here you will successfully match a date with just one digit days (and an extra space) as well as two digit days This can be expanded to more complex formats Often times being able to support multiple formats using just one formatterIm sure youve realized that in the previous example if that code was to run on the day of writing this (Jan X 2018) the date would be October 2018…which probably is not right considering that date is in the future If your data or system allows you to determine the right year then it may be useful to change the year of the timestamp after parsing This can be expanded to other aspects of the timestamp as wellThe above code changes the year of timestamp from current year to 2017 Other fields can also be modified similar to the example shown above The fields that are supported by the with() method can vary among the 3 timestamp classes The following documentation should clarify what isSupported(): ZoneDateTime OffsetDateTime and Instant But timestamps can easily be converted to Instant from either of the other two formatsBest format to produce is usually the Instant format above  having the timestamp pre-converted to Zulu time This provides a standard way for consumers to use the data Providing local time is also usually sufficient but zone information must be made clear to consumers Quite often it is not provided in the timestamp format nor verbally leading many consumers to just guess the zone  this can lead to some interesting results depending on the data Lastly it is always recommended to provide the year in all timestamps in any type of log message etc This allows developers to not have to guess the year which would be correct 9/10 times (or more) excluding the edge of time (aka the new year  12/31 1/1) It also prevents problems inherent in parsing multi-year data Overall the best data sources we have had do not require us to assume anything in the data and this allows us to preserve the raw logs as they are for future consumption"
9FfBS5XHm35QugHrUcwXSG,We have created and currently manage a platform that provides analysts with 90 days of data for approximately 102 and growing data sources The volume currently averages out to ~7TB per day Due to the size of the data we have obviously learned quite a few lessons from building such an enormous system and making the system stable but Ill save those for another post  so keep an eye out 😄 Even though this system provides an impressive look into the data for the analysts  it still does not cover our customers requirements of having 13 months of data available and being able to perform SQL-like queries against that data Cue the work on SQL Query engine In order to achieve this work weve looked at a few tools  in this series of blog posts I will talk about one of those tools Spark Thrift However there is so much more than just the querying system  there is also the question of managing and using the dataThe following series of blog posts will be broken up as follows: The hardest part of any system overtime is the data Data comes in all shapes and forms; it can imply that you need to convert your data into a format reasonable for your goal This series of blog post should walk you through setting up a simple system to query data via thrift However creating the parquet data might be the hardest aspect since it depends on your dataThe rest of this blog will talk about our approach to generating parquet data  providing a code sample for you to do it yourselfRemember that requirement to have 13 months of queryable data Breaking that down the first part of that is having the data We decided on two potential formats for our data  parquet or orc Looking into it further both formats were fairly similar  a stackoverflow discussion on the differences here  but parquet has existed longer and we have sources that are excessively nested therefore we went with parquet The code was also essentially the same to be able to generate either  which meant we could change in the future if we needed to Other important decisions  S3 was decided to be our datastore and we were going to start with spark-streaming to start getting live data in a usable format then apply it to historical dataIn order to have columnar data  you must know the schema of the data ahead of time Lucky for us we started enforcing a schema on our data not too long ago This was vital for us to even think about generating parquet data for over 100 sources We used POJOs to represent and enforce the schema on top of the dataLets give ourselves a sample to work with  Assume you have an rdd of message like the following: The record contains four fields each of which corresponds to a different datatype Lets also assume that dest_ip can be nullable  aka optional field We will now walk through a code sample to create a StructType that represents this record  the docs can be pretty useful Documentation provides a way to do it in Scala  the code samples will be in JavaSimple  you have started to save your data in S3 For this demo you might want to change that S3 location to something local for example  /${home_path}/parquetTesting/${data_name}/ You will have some additional parameters to set in your spark job in order to get the code to be fully functional Note: The code skips over steps such as setting up your spark session and s3a configurationsIll touch briefly on the different parts of the code  some of the decisions made in the code may not be important or required for your use case so feel free to adjustThe StructType is generated first so that we can feed it in as a schema in order to create a Dataset A StructType can be thought of as a list of StructFields(aka the fields in the schema) There are certain supported datatypes that a struct field can be  as you can see in the example above IP is not one of those therefore that field is saved as a string insteadMoving further down the code where we work with the dataset  there are a few aspects to coverIn this example we had assumed that our data would all be in json format  therefore it is easy to convert by using the json method but other types are also supported We enable option(modeFAILFAST) which is very useful during development  and could potentially be useful for production logging It will inform you immediately if the data and the schema do not match  this usually means that the schema must be changed to fit the data or that data is coming in incorrectlyOnce you have a dataset it can essentially be treated as a table with columns  so if you want to add more information this is the perfect opportunity For our short-term use case we wanted to have an ability to partition the data based on when the data was ingested Therefore we add the current timestamp but also extract yearmonthday as columnsFinally saving the dataset has two key things to know partitionBy allows you to choose the columns within your dataset to partition your data This is key for when you query your data  you can use these partitions to narrow down the data you actually need to search parquet specifies the format you are saving your data in parquet can just as easily be replaced by orc  therefore making it very easy to swap formats further down the line If you chose to implement all the additional steps (eg partitions) you should expect the following outputJust wanted to bring up a few specific limitations that we encountered in our journeyTimestamps are very vital for systems like this Since querying on a timeframe is very common Which is why it is important to be aware of a bug that we discovered during the testing process Timestamps with granularity greater than 3 digit milliseconds do not work as expectedLets take the timestamp 2017 07 30T14:41:09068184Z in the process of converting this timestamp to a dataset it will record the timestamp as 2017 07 30T14:42:17184Z As you can tell most of the timestamp is perfectly fine theres a weird discrepancy between the min value and the seconds value What is happening is that the first 3 digits of the milliseconds get treated as seconds and are added to the timestamp therefore the timestamp is progressed by 68 seconds The last 3 digits are treated as millisecondsThe limitation stems from the class DataFrameReader and its use of the javatextSimpleDateFormat Here is a stack overflow post that helped us uncover itThe second limitation is that if you stop the job in the middle of writing parquet data your data could be corrupted We have not had a chance to look into this to clearly However when you start querying via thrift and corrupted data is part of your query range the query can fail
hdqx54PFzFsrRAtynmntGe,This blog post will be the final installment of Setting up a Big Data SQL Querying System This post will walk you through setting up a thrift server so that you are able to query the parquet data you had previously generated Whether or not you are following the series of blog post this will still walk you through setting up a Spark/Hive Thrift serverBefore we get any further you might be asking what exactly is thrift? This blog helped me tremendously when I was playing with the thrift server to improve its performance It also has an excellent description of the thrift server and its history Here is just a piece of that information: Spark (SQL) Thrift Server is an excellent tool built on the HiveServer2 for allowing multiple remote clients to access Spark It provides a generic JDBC endpoint that lets any client including BI tools connect and access the power of Spark Lets talk about how it came to be and why you should use itTwo notes before starting: Starting the thrift server is fairly simple: Your thriftserver should now have started However just running the thrift server like this gets you a pretty bare bone and weak system to run big data queries but it can work Expanding on this you have a few options when configuring thrift  but we will cover just the following two: This section is not mandatory but has useful configuration informationThe following configurations can be added to /start-thriftserversh commandSpecify a default port: --hiveconf hiveserver2thriftport=9999 Limit the executor memory: --executor-memory 1g  This gives the thrift server 1GB of memory per executor that it runs on Specify the Core Count: --conf sparkcoresSo you just witnessed 3 different ways to set configs for the thrift server This might be confusing if this is your first time with thrift If you visited the blog post above you know that thrift is basically a combination of spark and hive which leads to these configs Typically --hiveconf refers to configurations in hive while in this context --conf leads to configurations in spark Everything else is prebuilt configs for the server to take Prebuilt also includes specifying the spark master to connect toConnecting to masters:--master 10111111111:707710111111When starting up the thrift server if you specify a master (or multiple) for the server to connect to then the server is able to use the resources available on that cluster If you navigate to your spark cluster you should see an application as such: Clicking on that application will take you into the application page and you should have the following information available to youThe UI can definitely be useful but also has a few quirks that would be worth knowing The best way to learn is to start experimenting with queries and seeing what happens (Ill show you one way to start experimenting in the next section) However one useful thing to know is that a SQL query does not necessarily correlate to just one job  a query can sometimes create multiple jobsBeeline should already be available with the package of spark you downloaded Using the following commands you can connect to your thrift server using beelineIf you are following the series of blogposts you should already have parquet data in this folder structure: with the data having a format similar to this: A few new fields were added to show some special steps that are required in the creation of tables on top of this data Adjust your table creation according to your data Specifically the fields@difficultField1 difficult-field2 & nested_field require slightly different syntaxOne more note prior to creating your table  it is entirely fine to have your data saved locally and not in S3  update the LOCATION value in the table below to point to the local locationCreating Your Table: Lets go over the 3 commands above and what they are doing The result of the Create External Table command will be an external table with the name parquetData with the specified schema Two important notes in the schema:1 Fields with special characters are surrounded by ` 2Moving on  You can also specifying the what partitioning exists on top of the data If you pass partitioning information at the time of query your queries should get a performance boost There are a few other pieces of the command that specify the different formats regarding the data Such as the SERDE as well as the format of the input and output of the data essentially specifying that the underlying data is parquet and how to handle thatThe last two commands go together If you were to just create your external table then try querying it you would get 0 results The reason this happens is that when your data is in S3  thrift doesnt automatically pick up all the available partitions in S3 You have to repair the table and recover the partitions If you perform this then your external table will pick up all of your latest partitions available in S3 However any new partitions will not be found until you run the commands againWhen you create tables in Hive where partitioned data already exists in S3 or HDFS you need to run a command to update the Hive Metastore with the tables partition structureDepending on your scale you may run into issues with performance The blog I referred to at the start of the post should be pretty useful This blog mentions a lot of options for performance improvements
9yuHypwgrFu6mXxA3TWUFx,This post will be fairly short covering some must-know information that we encountered while trying to query our data via thrift Using S3 as a datastore is great in terms of costs and ease to use; however S3 can also make certain aspects more complicated Some of the concerns we had when approaching this was being able to effectively use the partitions and metadata of the data to improve querying In order for that to work  the filesystem must allow something called Pushdown Predicate There were two concerns with this aspect that well discuss below Finally knowing how S3 works as a data store in terms of request capacity is also very useful when working with a high volume of data Some of the aspects discussed in the blog and other documentation is still experimental so be warned that things may changeThe Concerns: S3 is an object store not a filesystem and therefore can lack some aspects that you would expect The first being partial reads which allows the filesystem to figure out the information of the file without having to read/download the whole thing This is one of the requirements for pushdown predicate which we will get to shortly This doc is pretty useful in figuring out the settings to apply The docs suggest setting fss3aexperimentalinputfadviseto random in order to allow the random IO  aka partial readsThe documentation also suggests setting filter pushdown  which is the same as pushdown predicate Pushdown Predicate essentially refers to pushing the process of filtering your data down to the filesystem For example you query a table with a months worth of data but want to look at a specific time range If pushdown filtering does not exist your query would be downloading the entire months worth of data prior to filtering the data it needed to return By enabling this setting the filesystem does the filtering and you download just the data required by the query (based on partitioned fields)Finally another aspect to pay attention to is how your data is distributed when saved to S3 This is usually only a concern when your requests exceed certain thresholds  which can be found in the official documentation The documentation explains ways to optimize your s3 object keys for improved performance(object key is equivalent to the filepath of the s3 object)
CgPNTS5KGqjbCPshHjPZCK,Tweets stock trading traffic weather  the world around us is replete with streaming real-time dataStream processing is a paradigm for computing continuous streams of data from a shorter-term snapshot-like perspective As petabytes of granular event data are generated stream processing frameworks enables straightforward aggregation and filtering of this data in real-time (ie Twitters top 5 hashtags in the past 10 minutes)Stream processing frameworks are best paired with a message broker or queuing system such as Apache Kafka Amazon Kinesis or RabbitMQ As a stream processing program consumes data from a queue they can write results in other message brokers/queues databases data warehouses or cachesWriting your custom stream processor is fairly straightforward but achieving the throughput fault-tolerance and production-readiness offered by frameworks such as Storm is no easy feat Open source stream processing frameworks are bountiful  eg Storm Flink Samza and Spark Streaming (all Apache projects) Stream processing is advancing so fast these days that its likely this article will be outdated within a couple yearsApache Storm is a free and open source distributed stream processing framework Here are its strengths: Storm is fast & highly scalable: it can process over a million tuples per second per node Bolts and Spouts in Storm can scale independently and linearly to match the needs of your datas sources transformations or destinationsStorm is fault-tolerant: Because the Nimbus and Supervisor nodes store state in ZooKeeper their nodes or processes can die without catastrophic failure of the Storm Cluster Any new daemons can simply pick up where the last one left offAny programmer can use Storm: Storm contains a Thrift definition for designing and submitting topologies allowing programmers of any language to get started with stream processing at scaleStorm guarantees data processing: even if a node dies or messages are lost On top of that Trident an abstraction over Storm provides exactly-once processing semanticsStorm UI: Storm UI is an incredibly valuable asset for understanding monitoring and debugging topologies Storm UI provides dynamic logging levels distributed log search and dynamic worker profiling via JStack or heap dumps Storm UI even exposes an API for you to retrieve metrics by topology worker supervisor etcA Storm topology is simply a directed acyclic graph (DAG) of processing to be done on a stream of data Each vertex in the graph represents either a data source (Spout) or a unit of processing (Bolt) Each edge represents streams of data between the verticesYou can parallelize your topologys components independently by providing parallelism_hints in your bolts and spout configurations This involves: Its worth noting that in this model of parallel components its possible for a single topology to be running on multiple supervisor nodes in a storm clusterData sources in Storm are interfaced as spouts Spouts can pull data from message brokers queues databases or even distributed file systems Spouts have the ability to replay events sent via stream to bolts in a topology in order to guarantee processing in the case of failureA bolt represents a single unit of processing in Storm Bolts accomplish the following goals: A bolts processing is as simple or complex as you wish to define I strongly recommend breaking down expensive long-running chunks of computation down to sequential boltsAs with spouts bolts can indeed can replay inbound tuples in order to guarantee processingAlthough primary node may be interpreted as a single point of failure Storm has included a Highly-Available design for the Nimbus node utilizing a leader election process in the case that a Nimbus host fails This ensures that as hosts join and leave the cluster at any time active topologies are never lost and stream processing is never ceasedSupervisors are secondary nodes in the Storm ecosystem  they are responsible for running worker processes that execute components of topologiesIts recommended to use a process supervisor (such as supervisord) in order to manage Nimbus and Supervisor daemonsApache ZooKeeper provides easy synchronization of distributed systems with a hierarchical key-value store High availability and high performance are at the center of ZooKeepers design; ZooKeepers job is best done running as a cluster The Nimbus and Supervisors keep their state in ZooKeeper helping them coordinate interactions between each other Check out how Twitter and Elastic utilize and manage Zookeeper
GF7YxsPoRvaW3Lr9BLXRPB,Before you start reading let me just tell you one thing it is really possible to be GCP certified in under one week if you study the right materials I am a GCP Certified Data Engineer but it took me many hours of studying (more than I like to admit) but if I had to put a number on it it would be in the order of 20+ hours study time before I attempted the examThats the thing you need to study the Right Materials… So you can ask me what is the right materials? Well let me tell you a story when I wanted to get GCP Certified (as it the new cool thing to do if you are in the big data/cloud game) I had a dilemma there was actually very few legit studying sources available to get your hands on at a decent priceWhen I was looking around the only sources of preparation were on Cloudera Which were provided by Google themselves which were good right? Well yes and no The studying material were actually pretty good but they have split up the courses and made a specialisation stream with 5 courses (at the time) which they recommended you to take The total cost of this specialisation were in the region of $500+ which is a lot of money just for studying without counting the cost of the exam itself (which is another $250-$300)The only course I took from the specialisation stream was the GCP Fundamentals course because you can do the audit version which is free: But this was 6 hours and it was very basic without taking the other courses in the specialisation stream it was basically impossible to pass the exam So I thought to myself after I pass the GCP Data Engineers exam I must create a course that is easy for people to get started on their GCP Data Engineers journey and become confident to take on the exam without spending half their life searching for resources that is not too expensiveWhen I sat the exam I was a bit surprised many of the questions that were asked revolved around theory and how you can apply GCPs products instead of diving into a specific use case (like how many partition table updates can you do in a second) which was different from AWSs exams which will dive deep into some productsSo I have developed a course which discusses mainly theory and some really useful practicals which I am sure you will enjoy as they are the fundamental building blocks of the modern day Data Engineer Furthermore I tried to cut out information that is not relevant to the exam and many practicals which you can just search up on GCPs web page and tried to develop my own tutorialsThe course I have created is only less than 5 hours which should fit into most working professionals or students scheduleFor anyone that wants to learn about google cloud or become Google Cloud certified check out the awesome free trial below: If you dont like giving emails you can checkout my course website instead: https://wwwgetgcpcertifiedIf you like what you read be sure to 👏 and follow me on Medium
RRReZFD6rfosWf7VLveUDv,In the previous post I talked about data modeling with Postgres and how to create an ETL pipeline that migrates data from a directory of CSV files to a Postgres databaseIn this post I will dive into data modeling with Apache Cassandra a NoSQL database management systemThere are some drawbacks of the relational database management systems when it comes to big data because you need: Apache Cassandra is built for high availability scalability and fault-tolerant systemsA startup called Sparkify wants to analyze the data theyve been collecting on songs and user activity on their new music streaming app The analysis team is particularly interested in understanding what songs users are listening to Currently there is no easy way to query the data to generate the results since the data reside in a directory of CSV files on user activity on the appTheyd like a data engineer to create an Apache Cassandra database which can create queries on song play data to answer the questions Our role is to create a database for this analysis Well be able to test the database by running queries given by the analytics team at SparkifyTo follow along with the upcoming instruction please view its GitHub repoBefore we perform ETL and create an Apache Cassandra database we need to set it up for running on our machine This is a three-step process: This will make a connection to a Cassandra instance on our local machine To establish a connection and begin executing queries we create a session This is similar to creating a database in Postgres where we specify the host and user privilegesThis is very similar to what we used when we connected to the Postgres database in the previous project and got a cursor to it like this: In Apache Cassandra we model our data based on the queries we will perform Aggregation like GROUP BY JOIN are highly discouraged in Cassandra This is because we shouldnt scan the entire data because it is distributed on multiple nodes It will slow down our system because sending all of that data from multiple nodes to a single machine will crash itNow we will create tables for the following queries: This creates the first table and we set the partition key and clustering column based on the last part of the question These two columns together form the primary key Here is an introduction on primary key partition key and clustering columnsHere we read every line of the CSV file and extract appropriate fields to INSERT data into the table discographyLets see if the ETL works correctlyIn the same way we create tables to optimize for the other two queriesThis creates the second table and we set the Partition Key as userId sessionIdand clustering column from the condition set in the questionHere we read every line of the CSV file and extract appropriate fields to INSERT into the table user_artist_relationLets see if the ETL works correctlyThis creates the third table and we set the Primary Key as the two columns: songand userIdHere we read every line of the CSV file and extract appropriate fields to INSERT into the table user_infoLets see if the ETL works correctlyWe pre-processed the entire events_data directory and created three tables in the Apache Cassandra database As mentioned earlier we model our tables based on the queries we will perform on them So to ensure maximum availability and fast throughputs we make separate tables for different types of queriesIf you want to learn more about the Postgres data modeling prior to this you can read more about it here Next we will discuss on Cloud Data Warehousing
HdXNkqfET4wNURBMzy6fAw,This is the first project in Data Engineering Nanodegree For this project we have a music streaming startup called Sparkify who wants to analyze the data theyve been collecting on their app regarding: This data currently resides in two directories: Our task is to model user activity data to create a Postgres database optimized for song play analysis We will do this by creating a database schema and ETL pipelineTo follow along in upcoming sections please follow its GitHub repo for instructionsData modeling is a high level abstraction that organizes data and how they relate to each otherEver planned a trip or recorded payments in an Excel sheet? That is data modeling Think about the process you went through when creating the first few rows You decided what columns to keep Maybe the number of columns changed when you recorded more observations and thought of another important feature But you follow certain rules when creating the Excel sheetSame is the case with databases In database design what you record will eventually end up as a database for an information system So data modeling is also called database modelingHere we will be performing data modeling in Postgres for Sparkify We start with creating facts and dimension tables for a star schema We will use the files in song_data and log_data directories for itThe data used in this project will be used for upcoming projects also So it is better to understand what it representsThe first dataset is a subset of real data from the Million Song Dataset Each file is in JSON format and contains metadata about a song and the artist of that song The files are partitioned by the first three letters of each songs track ID For example here are file paths to two files in this datasetAnd below is an example of what a single song file TRAABJL12903CDCF1Ajson looks likeThe second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above These simulate app activity logs from Sparkify app based on specified configurationsThe log files in the dataset well be working with are partitioned by year and month For example here are file paths to two files in this datasetAnd below is an example of what the data in a log file 2018 11 12-eventsjson looks likeIf you would like to preview the JSON data within log_data directory you will need to create a Pandas df to read the dataNote: Make sure to set the argument lines = True in pdread_json()  This will read every line of the JSON as a new rowUsing the song and log datasets well create a star schema optimized for queries on song play analysis But first let discuss in brief about star schemaA star schema is the simplest style of data mart schema The star schema consists of one or more fact tables referencing to any number of dimension tables It has some advantages like fast aggregation for analytics simple queries for JOINs etcA dimension table is a structure that categorizes facts and measures in order to enable users to answer business questions Commonly used dimensions are people products place and time  Wikipedia • songs  Following info about songs: 3 artists  Artists information: 4 time  Timestamp broken down into specific units: In order to create these tables all we need to do is perform some transformation in the data which are already in song_data and log_data directoryWe will perform ETL on the files in song_data directory to create two dimensional tables: songs table and artists tableThis is what a songs file looks like: For songs table well extract data for songs table by using only the columns corresponding to the songs table suggested in the star schema above Similarly well select the appropriate columns for artists tableNow insert the extract data into their respective tablesVariables song_table_insert and artist_table_insert are SQL queries These are given in sql_queriespy fileWe will perform ETL on the files in log_data directory to create the remaining two dimensional tables: time and users as well as the songplays fact tableThis is what a single log file looks like: For time table we have ts column in log files We will parse it as a time stamp and use pythons datetime functions to create the remaining columns required for the table mentioned in the above schemaFor users table well extract the appropriate columns from log files as mentioned in the star schema above for users tableFor songplays table we will require information from songs table artists table and the original log files Since the log files do not have song_id and artist_id we need to use songs table and artists table for that The song_select query finds the song_id and artist_id based on the title artist_name and duration of a song For the remaining columns we can select them from the log filesNow insert the data into their respective tablesThats it We created a Postgres database with the facts and dimension table for song_play analysis We populated it with the entries from songs and events directory Now our data is useful for some basic aggregation and analyticsIn the next blog I will be discussing data modeling in Apache Cassandra  A NoSQL DBMS
8M24LDtXcAsyB3dfofnfYp,Heres the learning journey which I found useful in preparing for google cloud professional data engineer examLearn about the data engineering capabilities available on google cloud Straight to hands on learning with minimal theoryIt is good to have an exploratory mindset in the first step There are a lot of labs and quests which are available in QwiklabsSome of the quests which I found extremely useful are: One monthIncrease your knowledge with proper formal learning Instructor led course with theory and hands on labs in equal proportionThere is a nice specialisation course on coursera by google cloud It covers all the key areas like GCP fundamentals data warehouse modernisation building batch and stream pipelines ML and AI offerings etcOne monthFamiliarize yourself with the exam pattern nowAttempt the practice exam and see how many questions you can get right Mark the areas which you are still not aware of or feel less confident aboutOne dayShift your focus to learning againThere will be some areas which will require detailed study and better understanding In my case I felt the need to stress more on streaming conceptsIn the practise exam result with each question there are links to the appropriate blogs on google cloud Read through these blogsThere is also useful content in the links in this blog: Step 4 is the an important step as there is a lot of detailed information available in these docs on https://cloudgooglecom/docs And after the knowledge base founded during step 1 and 2 you will be able to get hang of things quicklyTwo weeksExam? Yes focus on the exam nowRevise well before taking the plunge This link provides a very nice summary for revisionOne day
9EHG5GqPET5YsbjAkDViQR,A significant chunk of my work is on the command line For a data scientist and/or engineer command line mastery is a mustI want to share with you one of the most underrated *NIX commands that is a life saver It feels like it has been forgotten and I am guilty of that as well Once in a while it springs back to memory at the time of need and feels like a breath of fresh air So without further ado (cue drumroll)  its screenWhen working on your local machine you can just open up a bunch of tabs in your terminal app but what if you have sshd into your EC2 instance or another remote box? What if its not something quick and dirty but you are about to kick off a long running process? What if youve got a bunch of those? Sure theres nohup and that would suffice if the process can just run in the background without any further interaction However that might not always be the case And thats where screen comes inHeres one example of how I might use it Im traveling and my connection is not particularly stable and/or my data allowance is pitiful but I need to download the OpenStreetMap planet file which weighs around 50GB I log into my cheapo EC2 instance (or better yet my home computer) start a new screen session kick off the download and disconnect Come back later to check on it when Im back onlineScreen is a terminal multiplexer meaning it manages one or more virtual consoles Think of it as a terminal app with tabs but away from homeThere are only two concepts to master Sessions and regions Each session is like a new terminal window Each region is like a new tabHeres a few things you can do: Start a session This would show you a blank terminal screenOr if youve got a bunch of things to work on you could start a session for each one and give each one a nameOnce you are inside a screen terminal you can either terminate that particular session with Ctrl+d or detach from it with Ctrl+a d if you mean to come back to it laterSupposed you have detached from a session youve created aboveyou should see that session listed You will notice that all session names are prefixed with their respective PIDsNow that your screen session is detached you can log out and close your SSH connection Whatever youve left behind running in your screen session is alive and well and doing its thingand voila! You are back in businessIf youve got a single screen session running then theres no need to reattach to it by name It will get picked up by defaultIn my practice I have found screen useful when I needed to kick something off remotely and check up on it once in a while I dont do a lot of work inside a screen sessionIm not including regions here since in my 20 years of using screen I barely used them if at all Your experience might be different Heres a link that covers them well and recaps the concepts above  https://linuxizecom/post/how-to-use-linux-screen/PS Check out tmux as well Screen however has been around for almost as long as I have and is pretty ubiquitous
DssmdEs7FoFJ3P6qXYot7L,The main objective of this post is to help people who are facing issues connecting Kafka Connect with MapR-FS due to lack of proper documentation availableKafka Connect an open source component of Apache Kafka is a framework for connecting Kafka with external systems such as databases key-value stores search indexes and file systemsUsing Kafka Connect you can use existing connector implementations for common data sources and sinks to move data into and out of KafkaKafka Connect is focused on streaming data to and from Kafka making it simpler for you to write high quality reliable and high performance connector plugins It also enables the framework to make guarantees that are difficult to achieve using other frameworks Kafka Connect is an integral component of an ETL pipeline when combined with Kafka and a stream processing frameworkKafka Connect can run either as a standalone process for running jobs on a single machine (eg log collection) or as a distributed scalable fault tolerant service supporting an entire organization This allows it to scale down to development testing and small production deployments with a low barrier to entry and low operational overhead and to scale up to support a large organizations data pipelineTo migrate data from Kafka to MapR-FS we need a reliable service which can save data to MapR-FS as and when it arrives in KafkaConfluents Kafka connect provides HDFS connector which can read from Kafka and persist data to HDFSThe same connector can be used for MapR-FS after some modifications since underlying architectures of HDFS and MapR-FS are differentThis setup will work only when source and sink both store data as JSON For reading/storing in different formats like Avro/Parquet appropriate serializer needs to be specified in connect-standalone
F5CZ9B8qkq6zX4D8g7XtAM,With February release of Apache Spark 220 the engine adds an additional experimental feature to Sparks Structured Streaming  low latency Continuous Processing StreamingContinuous Processing(CP) allows achieving near-real-time 1ms processing latency The feature is useful for an application that latency more critical than exactly once guarantee However the exactly once processing guarantee is possible to achieve making source re-playable and sink idempotentAs the name implies in micro-batch streaming the engine periodically fetches records from sinks in batches It needs to determine offsets to process and schedule periodic a task Thus the latency is bound to the time it takes to launch a job plus batch time Overall it takes 10 100 ms latency To achieve better sub-milliseconds latency CP fetches records continuously from data source reducing latency to milliseconds and satisfying low-level latency requirementsCP is designed to be compatible with existing API and introduce minimum braking changes To enable CP special type of trigger needs to be introduced  TriggerContinuous This trigger continuously processes the data and asynchronously checkpoints at a specified intervalThe execution mode is determined by the trigger specified For example: trigger(continuous = 5 seconds) is a continuous query where the checkpoint interval (the time interval between Spark asking for checkpoints) is 5 secondstrigger(processingTime = 1 second) is a microbatch query where the batch interval (the time interval between batch starts) is 1 secondtrigger(processingTime = 0 seconds) is a microbatch query where the batch interval is 0; that is Spark starts batches as soon as it canRestrictionsAs the feature is relatively new and experimental not all operations are supported There is support for basic SQL functions- projections select columns and selections- filter where Theres no support for multi streams joins and group aggregations For further explanation see official docResources:1 Databricks article introducing the feature • Apache Spark Streaming Documentation
P35UDs9RjhN8ZzpAWEihuU,At Culture Amp weve been customers of Treasure Data for about a year Weve been able to get some great momentum around product analytics with their stack of tools which includes solutions for data ingestion data querying workflow management and data exportTreasure Data are very good open-source citizens and many of the tools that power their stack are open-sourced Their most popular is probably fluentd but their bulk data movement tool Embulk is pretty good And as Ill describe below so is their workflow management tool DigdagAs we looked to move some of this infrastructure internally using mostly AWS tools (Spark on EMR Presto via Athena S3 for storage) Amazon didnt offer a compelling solution to data pipeline orchestration Step Functions and Simple Workflow Service were briefly considered but we decided to opt for a cloud-agnostic solution with a more familiar programming paradigmThe solution de jure for workflow orchestration is Airflow My line has repeatedly been that when starting a new project Airflow is the sensible default To the extent data engineer is mainstream so is Airflow Google Cloud even offers a hosted Airflow: Google Cloud ComposerAs we began executing on this project we spun up Airflow and got to work Another team had already set up Airflow on Fargate so we jumped in and started writing code We noticed a few things: Having worked with Treasure Datas Digdag for a nearly a year something didnt seem right with Airflow Digdag addresses the three key flaws of Airflow: We do give up quite a lot with Digdag compared to Airflow Among them: Nevertheless after making the decision to use drop Airflow and use Digdag I think the trade-offs have been worthwhile The bottom line for us is that these DAGs should be as dumb as possible offloading the vast majority of interesting work to systems designed to handle them The tool should be optimised for that task and for us Digdag makes it all much easier
fGypyJjRqzE5snCZSPMF5U,It always feels good to follow the rhythm of the tweetosphere to use the latest tool or framework (still in alpha version) to be hype to talk about it to blog about it: you need to use this revolutionary bleeding-edge thingyYour ops team will love youBeyond that in the real-world when you have a serious business systems running in production when you cant have any downtime when you need to monitor everything and be responsive: you cant play with your technology stack you cant be hype You could burn yourself and the whole company at the same time Then go bankrupt the next day (OK maybe not)Over time Ive accumulated tons of questions to ask myself and my team when we want to install a new tool/software/framework Its not because Airbnb uses it that we should right? We dont have the same needs Lets put the Cargo cult out of the waySome questions are not relevant according to what is this new thing to install: something front-end oriented does not have the same concepts as something backend-oriented Some pieces are common but not all of them You get the ideaYouve found a new cool thing you want to use lets think about it firstOften underestimated we know a bit what we (the boss/the customer) want but without all the details and we go blindly into the wall thinking well be able to twist the need to adapt to the new tool if needed Think twice about it If you dont have all the details refuse to work and go get themI want a blog! Lets do some reactive functional programming use some monads get the UI to 90fps put the content into HDFS and query it through Impala No? Really do estimate what are the capabilities of the tool if it was created for your use-case or for something different or larger Using a tool because it does more is not necessary an advantage A tool has an history a raison dêtreThe smallest the API surface the easiest it is to think aboutThe hypeness is the enemy of the reflection Try to think outside of the box to see if you really need it Escape hypeness Take a step backOften we dont realize we already work with something similar because the use-case is different (but our tool can already handle it) or because we didnt know all its featuresKnow what your existing technology stack is capable ofYes Github projects are cool Yes tons of new apps are created every day and have nice websites No they are often not production ready for youThey generally answer to some use-cases a company or person had and they built a tool for it Does it match yours? Is it stable? Is it bug free (or identified)? What about the documentation? Can you monitor it? Does it have a proper logging strategy? Does it expose metrics? Is it configurable? This is the definition of production-ready All this post is mostly related to this questionFree is always better But free is not always the bestYou wouldnt want a surprise on this side
e4W9xBb23LV2RE3NNav8G6,Creating a stream of individual Protobuf messagesProtobuf is a great encoding format Its become wildly popular as an encoding protocol not only for gRPC messages but for all kinds of information exchangeWhen using Protobuf in a streaming situation  ie when writing to a file or over a socket  the problem of delimiting one message from the next becomes apparent Encoded Protobuf messages do not mark the end of each message so it is not be possible to reliably distinguish one message from another when reading the stream on the other endWhile Protobuf has no official guidelines regarding this problem there is a commonly used solution to the problem called length-delimiting (sometimes called prefixing / framing) messagesThis article will go through some basic properties of Protobuf why large messages are difficult to work with the length-delimited (length-prefixed) implementation and finally some considerations when using this streaming formatConsider a very simple Protobuf definition of a User: I put this in a file called simpleThe process for writing a Protobuf message to a file is dead-simple Create the message serialize it to binary and write it to the file To read the message from file open and parse (decode) the entire contents of the fileWriting multiple Protobuf messages to a file is no harder than writing one message Simply repeat the write for each message: In order to parse one message from the file however we need to know where the Bob message ends so that the encoded message can be read from the file and parsed individuallyThere are typically two ways to predictably parse blocks from a stream  either each block is of fixed size or blocks are delimited in some wayFor this reason Avro has formed a specification called the Object Container File (OCF) format In this format a special sync-marker is placed in the file to delimit messagesFor Protobuf however well have to find our own solution to the problemA quick-n-dirty solution to the lack of built-in message delimiters is to wrap one message in another: There is however one big drawback to this: in order for Protobuf messages to be reliably decoded the entire message must be decoded at once ie the entire message must fit in memory In some contexts this is not a problem in others it may be catastrophic While the wrapper message grows the memory requirements can quickly become unwieldyThe official recommendation is to keep Protobuf messages around 1MB in size 64MB is the limit from Google and the hard-limit is 2GB in total These limits and recommendations are there for a reason Having a message that is 64MB in size can require several GB of RAM to decode the messageDelimited streaming formats are used everywhere In the case of textual formats such as CSV the delimiter is simply a newline character For JSON streaming can be achieved by using a specific line-delimited JSON format: Since Protobuf is a binary format and due to the encoding specification itself it is not possible to insert some kind of character or byte sequence that delimits each messageIt is however possible to instruct the reader how far it should read before decoding each message This is called length-delimiting or length-prefixing and is similar to how the header of an IP packet contains a header that contains the length of the data payloadThe process for writing length-delimited messages is is simple: serialize each message calculate its length in bytes and store the length in binary format In this example Ive picked a big-endian unsigned 32-bit integer (>L ) Finally write both the length and the message to the stream: The file content layout will look like this: In order to read a message start by reading the first four bytes Then parse those bytes to get the message length Finally read the length of bytes from the stream before decoding the message: Voilá youve just written and read files one-by-one from a file For most cloud providers it is possible to perform this operation over the wire ie each message can be streamed to and from the remote file in real-time This is an extremely powerful property Since we write both the length and message at the same time as well we do not need to worry about incomplete writesThe advantages of using a streaming format are plenty: However its not all flowers and sunshine in Protobuf streaming landThis format is design The lack of clear guidance has led to a lot of tension in the Protobuf communityIn Java and C# there are two functions that can be used ( parseDelimitedFrom and writeDelimitedTo ) but this this compatibility issue has been open since 2018In 2015 a PR was opened that aimed to merge the Java implementation into the core C++ library but it was ultimately closed in 2017For Go there is a delimited reader /writer implementation found in the Gogo Protobuf package called Uint32DelimitedReader and Uint32DelimitedWriter Ive used this implementation quite a bit and its worked well with the Python implementation this articleIn the Rust protobuf package there is a delimited reader / writer but it has been marked as deprecated Possibly for the same reasons the C++ PR was closedI wrote a small library for Python available on Github https://githubcom/sebnyberg/ldproto-py that can be used as a reference for an implementation The package is also available on Pip as ldproto in case someone wants to try it outAll in all the format has been a contentious issue throughout the years but luckily it is not all that hard to just implement it yourself
Q8ntXSaYdtabSSBKx8L9NM,The project is the generic implementation of Mongo-to-Redshift ETL in our big data pipeline Inferring automatically from NoSQL format to SQL format and leveraging the power of PySpark whole databases can be moved to Redshift in minutesA JSON object can be saved in MongoDB just like as it is MongoDB dynamically set the BSON types of every fieldMongo-Spark connector uses a mapping from these BSON types to Spark SQL types It does do the conversion in all worker nodes in the clusterHowever the problem here occurs during loading to Redshift since Redshift does not support some field types such as NullType or ArrayType in Spark SQL The pain does not end here as Redshift do not support nested structureSo to achieve converting the data from NoSQL to Redshifts SQL format we first converted the data to Spark SQL format Then the data flows through a generic and recursive flattening algorithm before going into loading phaseThe algorithm iterates through SparkSQL schema and finds the leaf nodes If an ArrayType is confronted do nothing and store that field{field1while not losing the array fields by keeping them What happens to array fields is now that they go through another generic and recursive function in order to create a new whole table for each of themWhat the algorithm does is to convert array typed fields into a new whole table with arrays elements are represented as rows We keep positions in the array as another column in order not to lose any semantics from original data
9PV7MmFCgBQD7sDrfvDmxe,Getir is an on-demand delivery app that can get thousands of market items and restaurant foods in minutes We are growing every day naturally so our data To keep up with the data size we needed our scalable data platform We needed to start building our this big data platform at some point and this is how we are doing itFor those who are asking What is this big data thing anyways? big data platform enables analytics team to access data in shorter time by simplifying querying the data It enables our system to store much more data for analytics purposes It will present a code layer for data engineers to clean the data before processing it It will present a cluster computing engine for us to use in an easy way on all of our data It will collect all the data from different places into one placeAs size of data increases day by day it gets harder to maintain speed of querying databases which is not meant for analytical purposes We face this exact problem at Getir We use MongoDB for backend data storage Our analytics team often complain about its slow speed as they see their scheduled tasks getting slower Another downside is that MongoDB is NoSQL yet most data analysts get used to tinker with SQL oriented databasesSo yes we are talking about querying MongoDB for analytical purposes and writing custom ETL pipelines as we goGetir did not have the dire need for data engineers as you can see our data pipeline is primal and the data analysts are building their our ETL pipelines for the data they require This may quite likely arise problems as we scale potentially endangering whole system I approach this problem and find a solution by creating a new whole data platform that any person can use for analyticsWhile designing the platform I worked closely with our DevOps team kudos to Ufuk Dag who helped me dearly envisioning the future use of this platform As an R&D engineer it is my job to evaluate varying technologies and select a tech stack for our need Hence I constantly try different technologies and weigh their pros and cons in the architecture By building demo-like prototypes which gives the desired outcome we can see how a technology such as libraries frameworks services providers techniques are performing For example I tried to get stream data directly from Kinesis into Spark which I did not choose over doing it with mini-batches as it had been resulting in many bugs feeling the libraries quite underdevelopedWhile choosing a technology we try to select an AWS-alternative service when available since we as Getir are quite fond of AWS overallOne of main criteria while choosing a technology is to see if it can scale both horizontally and vertically inside our stack Where we cannot use an AWS service I especially focus on this since most AWS services are designed in this wayThe big data platform consists of two main and two supporting projects: Our end-game goal is to keep all the data in AWS Redshift knowing that it is a PostgreSQL structured data warehouse with AWS backing up and maintaining It can scale up return answers to analytical queries in much shorter time for much bigger dataNext we have our distributed system provider Apache Hadoop as I think this tech is the only tech I could not give up as it is the backbone of all of big data stuff Hadoop lets you work on clusters of computers storing data and computer in all of them for which it gives you a single entranceApache Spark is another powerful tech here that I chose early as I knew it it is a modern cutting edge in-memory cluster computing tool that supports working top of Hadoop Spark SQL is an SQL engine that Spark offers and we use it to transform dataWhen using Spark and Hadoop at the same time Spark needs a resource management system that will allocate resources from cluster for Spark or submit its jobs on Hadoop or gives you UI to access all Spark jobs you run on the cluster I go with Hadoops default resource negotiator: YARNThe good thing is AWS of course offers a service called AWS EMR that contains Hadoop Spark any many other big data toolsAWS S3 is used to keep metadata and put some output files It is also used while sinking data into Redshift as a temporary storageMongoDB stores the data we want to move fromAWS Kinesis is the heart of our streaming pipeline It enables us to ingest the change data from MongoDB to our big data platform in real time as well as it can support multiple consumers which any programmer can build to read from change eventsTo trigger creation of EMR clusters and jobs to run in it we use AWS LambdaWhile selecting programming language I had Scala first in mind since I knew it is a popular and common language in big data world However due to my extensive knowledge and its developer-friendly and general purpose structure Python 3 was the way to go However we left the option to use Scala whenever we needed it as we also knew Python in some big data frameworks was not supported Though we never used it however it would be a better choice in many placesOther misc tools are: There are different use cases for interacting with the big data platformNow that we have the big data platform that can give fast data access to data team or BI tools this work and Getir have a lot more future works on its way By streaming the live data into Redshift in near real-time I think someday we might serve our all internal and many external services using the data Big Data Platform offering
AW7WuGGN8QavJ3CFefjzCS,A step-by-step guide for setting Elasticsearch as a remote logging destination for AirflowAirflow supports Elasticsearch as a remote logging destination but this feature is slightly different compared to other remote logging options such as S3 or GCS Airflow expects you to have your own setup to send logs to Elasticsearch in a specific format In this article I will share my learnings and setup for sending Airflow logs to ElasticsearchAs of the current Airflow version (v11010 April 2020) this is what Airflow expects in Elasticsearch logs: With all this in mind you need to have a document in Elasticsearch in this format: First part is getting Airflow to read logs from Elasticsearch Remote log configuration for Elasticsearch is slightly different than other remote log destinations such as S3 or GCS
YPBa38FheV2mSFrz9NpisA,Configure Hadoop on Ubuntu 14I am going to create a hadoop cluster with 4 nodes One node in the cluster will work as namenode (master) and others will work as slave (worker node)Step 1: Open hosts file in the master node Add above 4 nodes IP and nameStep 2: Open hosts file in each data-node Add master IP and nameStep 3: Install Java in all nodesStep 4: Create a group named hadoop and user hduser in all nodes Finally give the user sudo privilegeStep 5: Login to all nodes Change user to hduser create ssh public key and copy that key to the same pcs authorized key file So hduser can now access this PCs localhost without passwordssh-keygen -t rsa -P  -f ~/cat ~/ssh/id_rsapub >> ~/Step 6: This step is for name-node / master-node Now we need to assure that hduser of name-node can access all data-nodes through ssh While ssh login we need to specify user and ip address Hduser of name-node will access data-nodes via hduser of data-nodes Later on we will give all hadoop facilities to hduser account So its makes sense to login to data-nodes hduser account from name-node hduser account First logged in as hduser in the master-nodessh-copy-id -i /home/hduser/ssh/id_rsassh-copy-id -i /home/hduser/ssh/id_rsassh-copy-id -i /home/hduser/ssh/id_rsa** Try to login each machine using ssh EgStep 7: Ensure that in each node we are logged in as hduser (from now on we will do everything as hduser) Command is simple Just su  username Then Download hadoop 273wget http://apachelauf-forumat/hadoop/common/stable/hadoop-273tartar -xzvf hadoop-273tarsudo mv hadoop-27sudo vi Step 8: Load the bashrc file by source commandsource Step 9: Open hadoop-envsh file and change java home value in all nodesvi $HADOOP_CONF_DIR/hadoop-envStep 11: This step is only for name-node (master node)A Lets create name-node and log directoriesThis name-node directory is the local directory for name node And logs directory will be working as name-node log holderB Open hdfs-sitexml and insert following code into configuration tag Change the replication value as your demand but should be less than or equal to number of data-nodevi $HADOOP_CONF_DIR/hdfs-site<name>dfsnamenodename<description>NameNode directory for namespace and transaction logs storage<name>dfs<name>dfs<name>dfsdatanodeusedatanode<name>dfsnamenodedatanoderegistration<name>dfsnamenode<description>Your NameNode hostname for http access<name>dfsnamenodesecondary<description>Your Secondary NameNode hostname for http accessC Open slaves file and insert your slaves nameStep 12: This step is only for data-nodesAB Open hdfs-sitexml and insert following code into configuration tagvi $HADOOP_CONF_DIR/hdfs-site<name>dfsdatanodedata<name>dfs<name>dfs<name>dfsdatanodeusedatanode<name>dfsnamenode<description>Your NameNode hostname for http access<name>dfsnamenodesecondary<description>Your Secondary NameNode hostname for http accessStep 13: This step applies to all nodesA Open core-sitexml and insert following code into configuration tagvi $HADOOP_CONF_DIR/core-site<name>fsB Open yarn-sitexml and insert following code into configuration tagvi $HADOOP_CONF_DIR/yarn-site<name>yarnresourcemanager<description>The hostname of the RM<name>yarnnodemanagerresource<name>yarnscheduler<name>yarnnodemanagerresource<description>Number of CPU cores that can be allocated for containersExplanation: We have 60 GB RAM I gave 40 GB to yarn and kept 20 GB for OS Here minimum allocation MB is 1 GB which refers the memory slot size So if map or reduce requires more memory 1 GB will be added per memory addition request Core represents number of core of the machine will be utilized by yarn It should be around 80 percent of actual coreC Open mapred-sitexml and insert following code into configuration tag Replace my name-node IP with your name-node IP First create a mapred file from templatecp $HADOOP_CONF_DIR/mapred-sitexmltemplate $HADOOP_CONF_DIR/mapred-sitevi $HADOOP_CONF_DIR/mapred-site<name>mapreducemapmemory<name>mapreducereducememory<name>mapreducemapjava<name>mapreducereducejava<name>mapreducemapcpu<description>The number of virtual cores required for each map task<name>mapreducereducecpu<description>The number of virtual cores required for each reduce taskExplanation: Here we specify how much memory the map task requires and how much reduce requires We gave map 4 GB and reduce 8 GB here Also we specified heap size for map and reduce Heap size should be 75 percent of available memory remaining parts are left for code Here for map 3 GB and reduce 6 GB (around) Xmx used to indicate heap size Map and reduce will work in separate container Container is application execution placeStep 14: IP table persistent: A Name-node: Install iptables-persistent and save the configuration to accept at port 9000 and 9001sudo /etc/initsudo /etc/initBsudo /etc/initsudo /etc/initStep 15: This step is needed to be done in name-nodeABstart-dfsstart-yarnRunning an example: This step needs to be done in name-node onlyABwget http://wwwgutenbergorg/cache/epub/1661/pg1661CD Copy text file from local file system to hadoop files systemhdfs dfs -copyFromLocal pg1661Ehadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-273FTroubleshooting: hdfs dfsadmin -report (in name-node) • Name node is in safe modehdfs dfsadmin -safemode leave (in name-node) • There are 0 data node runningstop-dfsstop-yarnstart-dfsstart-yarnsh (in name-node) • Resource manager GUI does not show any nodessudo /etc/initsudo /etc/initSolution: Open port 8031 in the master-node from firewall blockage • Disable firewall
VYtdfkF4ouMmfyDsfNkaof,When it comes to big data and modern warehousing technology you must have heard about Apache hiveOfficial Definition- The Apache Hive data warehouse software facilitates reading writing and managing large datasets residing in distributed storage using SQL The structure can be projected onto data already in storage A command-line tool and JDBC driver are provided to connect users to HiveHive is created at Facebook but later Facebook donated hive to the Apache communityHive provides SQL like language called HiveQL with schema on read and transparently converts queries to MapReduce Apache Tez and Spark jobsOne thing which makes the hive different from other databases/warehouses is it can digest all types of data format (Structured and semi Structured) and it uses Tez/map reduces in the background which reduces time to be executed by Hive QueryIn this blog we will cover converting different data types(semi-structured) residing in a data file into hive structured tables We will take input files as comma-separated values(CSV) TSV Avro parquet JSON and XMLOur input file has Setup as followsFor CSV file we need to create a table in the location(hdfs) where CSV file is presentcreate external table data (Name stringemp_id bigintsalary decimal(126)address stringgender stringis_new boolean)ROW FORMAT DELIMITEDFIELDS TERMINATED BY STORED AS TEXTFILELOCATION /bank/csvtblproperties(skipheaderlineSince we have a header in our text file so we have used skipheader property in tblpropertiesTSV files are just like CSV files just their separator is different Instead of  are going to use \\tcreate external table data_tsv (Name stringemp_id bigintsalary decimal(126)address stringgender stringis_new boolean)ROW FORMAT DELIMITEDFIELDS TERMINATED BY \\tSTORED AS TEXTFILELOCATION /bank/tsv/tblproperties(skipheaderlineThis method can be applied for any character separated file including |(pipe) \\001 etcBefore Jumping into Avro ingestion we must know what Serde means in the Apache hiveSerde- SerDe is short for Serializer/Deserializer Hive uses the SerDe interface for IO The interface handles both serialization and deserialization and also interpreting the results of serialization as individual fields for processing A SerDe allows Hive to read in data from a table and write it back out to HDFS in any custom format Anyone can write their own SerDe for their own data formatsFor Avro we need to have one Avro schema file This ingestion can be achieved by using Avro Serde Since we are using Avro schema for inferring the data type and column names hence we do not need to specify the same at the time of table creation The schema is kept at in HDFS locationCREATE TABLE data_avroROW FORMAT SERDE orgapachehadoophiveserde2avroAvroSerDeSTORED AS INPUTFORMAT orgapachehadoophiveqlioavroAvroContainerInputFormatOUTPUTFORMAT orgapachehadoophiveqlioavroAvroContainerOutputFormatLOCATION /data/avro/tblproperties(avroschemaurl=/data/schema/schematblproperties(avroschemaliteral={type:recordname:emp_datanamespace:orgapachehive
fwo3VsPWzCiaegMAx4iCWB,As part of my Udacity nano degree program I learned the latest open-source data engineering technologies to transform large and complex datasets In this article I am going to describe the project I completed by using these technologies I am using US immigration data along with global temperature US airports dataset and US city demographic data We will use AWS EMR Apache airflow Apache Livy and PysparkWhen we are dealing with multiple large datasets we come across questions like where is the data coming from? How often data need to be updated? Which requires a deep understanding of the data Once we decide on the data model We have to clean up raw data filter the data combine the data and perform aggregation of data Data might also need some transformations to fit our needs Finally we have to check for data quality These tasks need to be managed and triggered based on the dependencies On top of these steps we also need to address the scalability issues In this article lets find out how we can design a scalable system that can address all the above-listed issuesFirstly AWS EMR and Apache Spark are best suited to build large scale data pipelines Apache Spark is well known for fast data processing using in-memory computations In this project I am using Pyspark API I will be showing you how to build data lakes using Apache Spark with Pyspark APITo orchestrate the data pipeline in this project I am using Apache Airflow Apache airflow is a very easy tool to set up with few commands you can have a workflow management tool up and running in no time It has a powerful user interface and can be effectively extended and modeled using python programmingWe will be using Apache Livy to submit the spark jobs to EMR using a REST interfaceWe use an AWS CloudFormation script to launch the AWS services required to create this workflow CloudFormation is a powerful service that allows you to describe and provision all the infrastructure and resources required for your cloud environment in simple JSON or YAML templates You can refer to the Amazon CloudFormation article which goes into details of setting up the cloud The cloud formation script will create an EC2 instance for Apache Airflow It will also create IAM roles S3 and set up a Postgres DB instance for airflow See the picture belowThe cloud formation script will set up the apache airflow We will be using the LocalExecutor for the scheduler hence using Postgres DB The default scheduler for airflow is SequentialExecutorBefore we talk about setting up the stack and other steps Lets talk about Data itselfThe following data set is combined and transformed to the fact and dimensional data and stored on S3Now lets focus on setting up the cloud infrastructureSteps for creating the stack: On AWS go-to cloud formation serviceCreate stack->upload the template in my case its cloudformationyaml Provide a DB password select the key-value pair If you do not have a key-value pair please create one Give the name for the S3 bucket See the steps belowClick next -> next-> then select the I acknowledge that AWS CloudFormation might create IAM resources with custom namesIt will take some time to create the stackMaking raw data availableThe above listed raw data sets are moved to S3 by a python script using boto3EMR setup: The Airflow is up and running in the newly created ec2 instance by our cloud formation script The script takes care of installing the airflow on the instance and copying the project from the git repository Start the airflow scheduler by logging into the ec2 instance If you log into the EC2 instance you can check the /var/logs/user_datalog in case you need to debug the cloud formation scriptUsing ssh connect to the EC2 machine$ssh -i ~/amazon_credentials/spark-EMR-spark-cluster-keypair/spark-clusterpem ec2-user@ec2 52 37 128 107us-west-2computeamazonawsStart the airflow scheduler: You can do this by su and then execute the ~/Now copy the public DNS name from ec2 instance Open the browser and connect to the airflow on port 8080We have our dag immigration_dag scheduled to run every month It backfills the data from 01/01/2016 See the dag configuration: Here is the workflow that creates EMR instances performs data transformations checks for data quality and finally terminates the clustersSince create clusters might take some time We need to wait for the cluster to be available to start with the data transformations Hence we have created a Sensor operator That pokes and waits till the Cluster status become WaitingOnce the cluster is available We start with the transformations This is done by submitting the spark jobs to the EMR using Apache Livy which allows us to submit the job as a REST web service call Once all transformations are done we will check for data quality You can do this by checking for the data and also by checking the S3 where data is storedFinally we need to terminate the cluster This has to be done irrespective of whether the transformation was successful or not To do this I am setting the trigger rule as all done by default it is all successYou can refer to my Github to look at the source code and project documentationSummary: In this article we went through the steps for orchestrating a spark data pipeline using the AWS EMR Apache Livy and Apache AirflowWe can do other improvements for this project like here we are performing all the steps in a single dag But practically the airport codes demographics data may not change as often so can be separated into a different dag Also the transformation jobs wait for the cluster availability using a Base sensor We can change it to use triggersThe weather data we pulled in had data only till 2013 Hence I did not join immigration with weather data It would be interesting to see the travel and the effect of weatherYou could refer to my Github repository to see source code or view project documentation You can connect to me on my LinkedIn profile
mC47SwbaRSiUL9Y3as7MPS,Lirio uses Airflow extensively to define and automate data engineering data science and machine learning jobs across 4 environments We run tens of thousands of tasks through AirflowWe use a hybrid managed + self service approachWhile data engineers are responsible for building and managing many core jobs we provide a self service model where engineers can define their jobs in JSON configuration files that are automatically synced from our private Git repositoryWe run Airflow in a Docker container on EC2We deploy Airflow from a Docker image and it currently runs on an EC2 instance writing metadata to a Postgres RDS instance The EC2 can automatically respin if it becomes unhealthy and restarts Airflow as part of its bootstrap processWe run tasks using the KubernetesPodOperatorAll compute is farmed out to Kubernetes with auto-scaling enabled to add and subtract compute resources as needed This was a huge win for us and we feel that Airflow on Kubernetes is a strong best practiceWe mount a network volume to the Airflow EC2 and to each pod giving a common filesystem for resources that need to be shared across tasks and jobs This approach is more elegant that Airflows XCOM mechanismWe use factories and generators for creating jobsMost examples for Airflow demonstrate cobbling together various operators in hand-written DAG scripts This will quickly produce an unmanageable mess if you expect to run more than a handful of jobsWe feel that jobs should be defined using a standard convention so that everyone creates jobs in a consistent predictable way For us that meant standardizing on the Kubernetes operator and a simple JSON job config with parameters like job-name schedule environment sla image-name command and retriesBehind the scenes Airflow runs these configs through a generator script that calls out to a factory process to build jobs and tasks mapping various parameters to the Airflow invocationThis means we have a single dag_generatorpy script in the Airflow dags directory instead of a script for each job This script automatically generates and registers jobs that match the Airflow environmentExample config file: We automatically deploy job configs from GitAirflow automatically updates the config files that are fed to the DAG Generator every 5 minutes from our private Git repositoryInitially we bundled the configs within the Airflow image but found that created a lot of friction around adding and updating jobs because it required the image to be rebuilt and redeployed every time a job config was changedWe create generic components that can be chained togetherSome examples are our kafka-consumer file-loader and sql-runner components Each component is generic and flexible enough to be chained together to satisfy a variety of requirementsFor example the kafka-consumer might be able to read from any Kafka topic and output the messages to a stream or delimited fileEach of these components lives in a Docker image
eXUUYd8cw5m6yzp3PV8vHU,I had an interesting conversation at lunch today about data privacy and the responsible stewardship of consumer data These are important conversations to have We talk about ethical handling and use of data a lot at LirioI think its important to acknowledge that the perception of consumer data being exploited has been facilitated by less than ethical handling and use of data ranging from poor security policies resulting in massive data breaches to questionable sharing practices to data brokers collecting data without specific deliberate consent from consumersBecause of this consumers are becoming increasingly informed and concerned about privacy and data sharing potentially even confounding legitimate data collection endeavors because they dont know how that data will be used stored or shared Yes there are contractual and regulatory requirements but they are relatively loose in the United StatesIn other words even if I have the best of intentions top notch security and will only use your data responsibly you have no way of knowing that if you dont know me personally or work closely with meAs data professionals we tend to handle a lot of potentially sensitive information for millions of individuals and organizations Our day to day conduct and philosophy mattersWe should take every opportunity to push for good security and respectful data usage policies We should be having internal conversations about how we handle store and use data and always look for ways to do better
bJrgwNvDFTnwTNHt6k77Qa,All of us know what to do right? Front end knows how to write java script data engineering knows everything about big data ecosystem we know what executives would like to know but it still feels that something is missingSo far the biggest obstacle I noticed while working at analytics positions in big companies is the scope of work and number of people involved when it comes to answering seemingly easy questions asked by executives It is not rare for teams dedicated to analytics data engineering and front-end to work separately in scrum framework and thus consequently it can take even weeks until the job is doneThe second problem about such questions is that they are often too general sometimes as data analyst I have impression that somebody up there doesnt really know what he wants Hence delivering proper information is even more challenging because I have to invent metrics and event schema on my ownThat is why I am writing these guidelines for you: first of all to help you find yourself being a bridge between IT engineers and executives and secondly to give you ideas for advanced insights in how people use your product The first two chapters are about general data-engineering related topics You will find there fundamental events or basics about big data storage optimization and more detailed explanation about cooperation and workflowI will use navigation as an example of interaction type and to explain basic information which should be present in events table It is important to distinguish interaction types like navigation search or ads based on what reports are needed by executives If we partition data in table by such event type it will have huge impact for report queries performanceThat is in my opinion the best way to store information so query and storage optimization ability to change and simplicity can meet and we shouldnt have not enough or too much data At every next interaction type I will skip columns like ids properties and partitions so it will look much cleanerThe best situation is when we have event we need but as it often is when it comes to analyzing things we do not gave enough data so the whole process starts to be quite complicated The most important step is the ticket description at the beginning  bad one leads to confusion on every next step and increases time and effort needed from other people to do the jobThat is a place where data analyst should be confident about his needs as he is the most informed person in this chain He should bear in mind and respect a fact that front end and data engineering teams always have something to do and by helping them their time needed for us can be minimized as much as possible That is why table schema should fit any new event and manual testing should be done by usFor each interaction type you will find questions that may be asked by executives proposed metrics to describe it and event schema so the metrics can be calculatedThe user experience: I activate search bar and top searches appears While typing recommendations appear accordingly to the things I type I can still ignore those things and search my typed query and then click on the resultThe user experience: App can start playing video after user interaction or without it as first_autoplay Before video or in the middle of it there could be an ad so there is always type of a content (content ad_before ad_middle …) When the first video ends next related videos can also play when next_autoplay is set to true Also since there could be many players on a page each one with many videos there should be also player_id related to the player boxIt can be an article wiki page or tutorial Users can mark fragment with the type as the most interesting funny or surprising depending on the kind of content It will become highlighted just like in this article  and it will help build and broaden user experience as the user sees others engaging in it like him Also the point event should be sent every few seconds to be able to calculate scrolling speed until some time will pass to exclude not active cardsThat varies a lot between apps I will assume that there could be few rails (columns/blocks) for content at home page and each content has main image and a visible titleYou can define product as anything you would like to sell that could be a real thing or service I assume that there is a short description long one comments main picture other pictures and some specification If your app does not have all of them ignore them and use what you haveAds are pretty simple there are slots for ads and bids from real time bidding service Things starts to be complicated when you thing about extensions blocking java script code which is not possible to trackThose questions usually require querying by all events and do not require more tracking These are just some thoughts which comes to my mind when I think what I would like to know about my app usersJust imagine Facebook and its comment logicI guess there are many things I havent included  you are welcome to write about them in comments - I will also come back to this article after few months to update it with what I have learned
NgCJkMiGiZSdyrz5QSyhp5,Joining a startup early in your career is adventurous and rewarding when you believe in the vision and product of the company But even more sweet is having the leading tech analyst firm Gartner quantifying and qualifying your belief in the product the industries it will impact and its its far reaching potential for decades to comeEach year Gartner publishes a well researched report on the top analytics vendors in the market The most prominent part of the report is the Magic Quadrant graphic which depicts vendors using a two-dimensional matrix based on their Completeness of Vision and Ability to ExecuteI work as a data engineer at ThoughtSpot There has not been a single hands-on / product roadmap meeting where the customer obsession was not right at the center of the table As a part of the product team I have seen the dedication to move forward only when the quantitative as well as qualitative data insights have been consulted regardless of the designation title or organizationWe at ThoughtSpot truly believe data is our guiding star When coupled with experience and intuitiveness youve got a recipe for successThis blog post is not one of the promotional blogs which emphasizes on few clichès when it comes to importance of a product or data driven decisions My full time job day-in and day-out depends on using ThoughtSpot as my go-to analytics tool I wont lie sitting next to the engineering team of your primary work tool and being able to raise feature requests is a huge benefit but I am truly and genuinely amazed by the power of ThoughtSpot every single dayIts not only because ThoughtSpot can handle multi-join paths chasm traps & fan traps and provide an amazing search experience with simple data data loading but because it is a seasoned tool that a citizen data scientist can use Its power to integrate with R DataRobot and Google Cloud Machine Learning Engine blows my mindI am not only proud of the product my company has built but excited to see how ThoughtSpot grows in future
BgtJiciXNgSciHEdzwDNRp,I am teaching an applied data science and engineering course in line with the flavor of research in my lab ICAN The course is titled: Machine Learning & High-Performance Computing for Digital Ag & Biological Engineering; Part 1: Algorithms resilient data lakes & analytics at the edge This course will start in March and will be offered as a one-credit stackable course  a part of a Purdue initiative that aims to deliver stackable courses to create a custom data science curriculumCourse webpage: schaterjiio/teachinghtml; Flyer: http://bitly/chaterji-data-science; Video plug for the course: http://bitABE 591 will have a top-down flavor leveraging real-world data problems and then discussing data science and data engineering algorithms and concepts that can solve them The course will feature data analytics and foundational computer science (CS) concepts that power artificial intelligence/machine learning (AI/ML) tools ubiquitous in todays world The course will unwrap with some application domains in mind focusing on a subset of the following (custom-picked based on the student cohort): digital agriculture internet-of-things (IoT) metagenomics synthetic biology machine systemsCourse Highlights: AI/ML tools power search engines and recommendation engines and even your cellphones and Raspberry Pi-class devices; the right application of these tools can fuel AI-for-the-greater-good ABE 591 is being developed with the curious citizen scientist in mind Thus it is expected that the student will have a willingness to delve in data science and engineering principles with the need to go beyond the tools available through various software bundles and packages focusing on the algorithmic intuition and applications to select research domains inspired by the (food-water-energy) nexus-sensitive global challenges A lot of these use cases will be inspired by some of the projects in my innovatory that straddle Computer Science & Engineering on the one hand and Digital Agriculture and Health on the otherICAN uses a variety of ML algorithms to identify patterns in data for applications such as: cluster data (unsupervised learning) into meaningful classes and then label these clusters or to find patterns in data generated from Internet-of-Things (IoT) sensing devices (eg ground sensors in farms) or data that is captured by high-throughput genomics sequencers (eg Oxford Nanopore sequencers)In particular the course will focus on imparting CS-y concepts and tools that will better equip students to pick the right kind of algorithm or platform to learn the patterns embedded in the data set and at scale Here is a snapshot of topics: Supercomputing clusters and multi-grained parallelization: This will translate to the ability to use supercomputing clusters to scale out the computation and to analyze the data faster Examples of how distributed computation and multi-grained parallelization to perform faster computation using samples (real and synthetic) data sets will be discussed Data stories will depict how computing at the edge (also known as fog computing) can be used for some data processing tasks that demand sub-second response timesHardware: Further some hardware ranging from microcontrollers to graphics processing units (GPU) and domain-specific accelerators such as FPGAs will be coveredOn-device computation: Finally how TensorFlow and Raspberry Pi work together and how to approximate algorithms to run on embedded devices and real world applications of these deployments will also be discussed
YXPbBGcwfFe8U3HJM3BPgv,This blog is second in docker series for data engineers and this blog will allow you to getting started with dockerDockerfile is the blueprint or config file from which docker builds an Image It consists of a set of instructions which is executed one after another and expose the dynamically changing contentWe can use the docker build command to create Docker Imageall the content of the directory where dockerFile is present and will execute the instructions on after another and build layer after layer Then it will be executing the executable commands one after another and build layers one after another As docker builds layers one after another we must create a docker file intuitively by stacking non -changeable layers first and changeable layers atlast meaning say for example we are having a python app we will pull the base image and install python  required modules as first 2 steps as it wont change and the last step will be adding the application code the more is the size of the Image the more is the GC and memory issues while deploying images in prodStep4:Since httpdocker build -t pythonapp 
iGFihSbSES2kBkcE2EMPhQ,A few weeks ago I have passed the new Databricks Certification 30 and wanted to share some tips and clarifications about it as it is a little hard to find This article will provide some helpful information from registering preparation exam tips and experience that would help people to pass the examThough the certification versions match the Spark release versions I was confused on how different will the two exams be? I compared the two exam descriptions to have an ideaThese are the 4 additions to 30 exam on top of 24 are the following: My Recommendation: Its always a good idea to finish the latest version of the certification to learn and cover the new concepts So go for 3Other Additions Reads : Once you submit the exam there is a overall score and result displayed After completing the exam there is a message Your score at this time is unofficial The exam and proctor records will be sent to Databricks If awarded you will receive instructions to access your certificate within one weekNothing to worry after 5 days in my case there was an Databricks official certification issued via email Thats it for now Good Luck for the exam and feel free to ask for any clarifications
NwqCZ3iVvExmcv7mBc55bb,SQL or Structured Query Language is a standardized programming language used for managing relational databases and performing various operations on the data in them While its capabilities are a subject for another time it is known that not many people use it for advanced learning or machine learning techniquesFor a project where I had to do a test-control analysis for investigating the impact of the various assortments of laptops to be placed in a retail store of electronic devices I had written a series of SQL queries on the Oracle DB to create the test-control groups of laptops from different brandsThe problem was to map control laptop device groups to a particular test laptop Now what is a test laptop (or a test device) in this context? Since the end goal is to check the impact of multiple laptop brands in a store with respect to a specific brand (the brand this project was done for) that should act as the differentiating factor of these test and control devices Hence our test devices were several laptops belonging to a particular brand The corresponding control devices were so chosen that they were functionally the same as the test devices had similar applications and other fundamental traits but belonged to different brandsFor instance a particular laptop belonging to XYZ with configuration 4GB RAM 1TB memory and 156-inch screen size (device A) will see a mapped control device in another laptop with same/similar configurations from a different brand (say ABC)(device B) We had to write SQL queries for quantifying this similarity and identifying at least three control devices for the given test deviceBefore we get into the queries and the logic let us first understand a part of the dataset that we had for this exercisePlease note that this dataset has been tweaked and generalized This process described here is a scaled-down demo of what I had done in the actual scenario If you wish to replicate it you can use the logic on your dataset tooThe dataset was made at device-level with the following columns among others The relevant ones are: Having described the data there are a few assumptions and observations: A Both test and control devices will be selected from the same datasetB The test and control devices belong to the same usage category and OS configuration (ieC The similarity of devices will be determined based on the Euclidean distance of numerical metrics like current retail price RAM specifications age device storage size and the screen sizeD To validate the test-control groups we can check the DEVICE_DESC column They should either match or should be similarWe shall first start at selecting the required columns from the base datasetThe above snippet selects all the test devices (BRAND_FLAG =1) from the base table I have also renamed all columns to indicate that these values are all test values To create all possible test-control pairs we will perform a join Since the basic rule is that both test and control devices in a pair should belong to the same creativity description and product line So we will put that condition on the joinSince we ultimately have to compute the Euclidean Distance of these metric values we will start computing those from this step itself The basic formula of the Euclidean Distance is: Here p and q are the two points between which the distance has to be computedHence we will compute the difference between the column values in this step of inner join itself: Now the metric value differences have been computed I wanted to create test-control groups on a combination of metrics I have thus calculated the Euclidean distance based on all 5 metrics selected 3 metrics and 1 metric The selection of these metrics has been done on the basis of the problem statement and business-domain expertiseAfter this all we have to do is select three control devices for the given test device we have to use the Euclidean distances to get these clusters We will rank the Euclidean distances within a test device in an ascending order and then select the top three laptops as the mapped control devices: We have finally selected the top three control devices (using the WHERE condition) for a given test laptop on the basis of three different metric-combinationsAlthough this exercise could be done simply in R or Python I had to do this in SQL owing to some infrastructure limitations This concludes that you can use SQL to do a few intelligent and sophisticated analytical computations However you cannot use it for advanced modelling or AI algorithms Furthermore it is advised to use tools like R Python and SAS for these advanced analytical operations This was just a demo to showcase what kind of operations can be done using SQLAlthough this process could be more mathematically sound using proper cluster analysis this helped us form reliable test-control devices for the consecutive analysis I reiterate there are better tools to do these activities You can use SQL for some primitive operations if need be
SBpnVytLFmjxpbaQTVuY2H,As I explained in previous articles I am a Software Engineer working at HelloFresh I have been with the company for five years so far Before HelloFresh and in this company I had different rolesI started my career as a web developer doing everything something that nowadays is called Full Stack Developer (well I have a firm opinion about that position and it is not good but that would be another article)Then I switch for one year to mobile development working for a Dutch company and finally I become Backend Software Engineer for many years At HelloFresh I had the chance to switch to Data Engineering like a year and a half agoIn this piece I want to explain my experience and which knowledge you need to learn to become a Data Engineer Let me describe you some of these topicsThis is a new data modeling that can result confusing coming from the Entity-Relationship Model With Dimensional Modeling we model the data differently because we have different needsThe basic of Dimensional Modeling is that we have two big components: Fact Tables and DimensionsIn the Fact Table as the name states we are storing Facts for instance we sold one item That order will be a fact something that happened at some point in timeThen we have the Dimensions A dimension gives some extra information related to that fact For instance we can have a product dimension a customer dimension date dimension etcThe Fact Table links to the Dimension with a Surrogate Key This key points to that dimension item at some specific point in timeWorking with data you will need to build your infrastructure to store and process the data In this case you have two different options: use some Hadoop flavor or go to some cloud tools like AWS Redshift plus some other servicesChoosing one or the other it is your decision if you do not want to take care of infrastructure that much then I will recommend going for AWS solution But if you want to have more control of the whole process then I will choose some Hadoop flavorAbout Hadoop flavors I can talk about Cloudera With this Hadoop distribution it is pretty easy to start handling a cluster with some nodes and start using the HDFS to store in a distributed way your dataWith Cloudera Director + Manager it is straightforward to maintain compared with other distributions like MapRIf you decide to go for the cloud solution then you will probably use S3 as your data storage and AWS Redshift as the solution to run some processes and query your dataFor your Hadoop distribution to query your data using SQL syntax you can choose between Impala or Drill I can recommend Impala as an excellent tool to query your data using SQL in combination with the Hive metastoreAs you can see I just mention only a few tools to do some basic stuff So switching to data engineering means to learn a lot of new technologiesWhen you have the data stored in your distributed system the next step is to start analyzing the data Building some tables transforming and aggregating dataIn this case I can just talk about Spark This is the only tool I used so far for that purpose To be more specific PySparkIf you choose a Hadoop distribution you can start doing Map/Reduce tasks out of the box But this is an old fashion way to accomplish your goalNowadays Spark is a powerful tool to run tasks in a distributed way and easy to manipulate and transform your input Usually the more natural tasks are batches processes But if you need some real time analysis you can use Spark Streaming in combination with some message broker system like KafkaOther similar technology would be Flink but to be honest I do not have any experience with itSome advantage of using Spark is that it supports different languages like Java Scala and PythonWith Spark you can push the code to your data nodes and execute the transformations in a distributed way trying to us the memory of the nodes instead of doing I/O operations on the file system This is an advantage compared with Map/ReduceWith the data in your distributed solution your technology to create tasks the next step is to orchestrate when to run each taskIn my opinion the two biggest tools to achieve that are Airflow and Luigi Luigi its dying and losing support and Airflow its an Apache project with Airbnb as the main supporterWith Airflow you can create a Directed Acyclic Graph to set up the order in which the DAG should execute your tasks A DAG is a collection of tasks running in a particular orderThis is also known as ETL (Extract Transform and Load) because all the tasks are going to achieve those three necessary steps Extract the data from an API transform the data in some way and at the end load the data into your data solutionIt is possible to switch your career from Backend/Frontend engineering to Data Engineering but you should be prepared to learn a new set of technologies and concepts very different than the ones you are used toSo my biggest advice is to be open-minded humble and eager to read practice fail and start over In few weeks you will start feeling more confident about all these conceptsBy the way if you are already a Data Engineer in my company we are hiring! It does not matter if you are not a Data Engineer we have other positions open for Machine Learning Engineer Backend Engineer Frontend Engineer Android/iOS Engineer and many others
HMrGf8D7fieVyVJUsK36GW,The most valuable resource driving your company today is its data But usually there are many barriers between your data teams and apps that prevent people from realising the value of it for digital transformationYoure in a position to make using data easier so that its everyones business in your company You can champion an organisational shift stemming from data-driven decisions  if you can manage to spin straw (data-as-a-black-box) into gold (data thats actionable by almost anyone even marketing)Im sharing the tactics used by my data-engineering customers to turbocharge their careers  including upping their gigs to CDO and CTO titles Theyve done this through pure merit by enabling data-driven decisions; not via political strong-armingAnd for those of you who dont care about titles but want to increase your earning potential what Im sharing here will still help to get you placed on the projects you choose and give you more favourable visibility within your organisationWhether youre looking for a promotion or seeking to steer your own projects more autonomously you must seek out and champion inclusive technologies that help democratise dataYou dont have to look hard to find internal customers (marketing sales finance manufacturing etc) who are desperate to operate their group in real-timeHere are some examples: If youve built a data platform youre in an incredibly powerful position to turn your business into a real-time operationIf it were only this easy Simply building a data platform isnt going to get you a promotionThis is where every engineer needs to make a tough but valuable decision Rather than focusing on what to build its time to seek out how you can onboard new users and business use cases onto your platformThis is challenging Engineering and business teams live in different worlds They speak different languages and think differently Theres a natural suspicion because neither one really knows what the other does Not to mention both have had your fair share of failed projects when it comes to dataThis is not exclusively an engineering problem Business teams are notorious for changing their minds and guilty of being sold game-changing technology on the golf courseData is a difference maker It silences the hype and helps to remove ego out of decision making When leveraged fully it brings people and teams together for collaboration and to create apps to drive business outcomes When decisions are made collaboratively backed by data evidence trust ensues and the realm of possibilities expands But done wrong or if confidence is lost this can result in flamed out platform teams frustrated business units and fired project ownersNo one said this would be easy Youre going to need to give business teams confidence that you can reduce their pain costs and risk increase revenue and/or make them successful in meeting their objectivesSo here are some quick tips to be the champion with your data platformDont get too hung up on the technology Just make sure you understand the use cases and find off the shelf technology to support it  rather than adapting use cases to support the technologyAnd dont feel you need to address the sexiest use-case The most unglamorous problems can provide the biggest return to the business and be quick to implementTheres an expression in sales never have lunch alone Everyone needs to eat so why not eat while learning about the challenges one of your colleagues has in another part of the businessSo get out from your desk show-and-tell what youve achieved and ask them about their business and data headachesDemonstrate value very quickly Dont spend months building something to then find out its not what they wanted or their requirements have changedIn some cases give them 40% of what they need enough to give them some value and for you to get the feedback you need Then iterateNo one understands their data like they do Let your internal customers see what data exists and explore it using tools and practices that they are familiar with Seeing their data will give them ideas of what they can do with itA business user doesnt need or want to know whether underneath the hood it is really RabbitMQ or Kinesis or Kafka Cassandra InfluxDB and so onTalk to them about outcomes and how they can use the technology to achieve themIf your data platform is successful youre going to get lots of demand so prepare yourself for how you might scale up This will require finding a method of deploying real-time applications that is efficient and consistentThis is where so many fail  so dont make compliance a show-stopperBusiness teams have strict compliance requirements that they cannot compromise on (lack of compliance leads to dismissal PR disasters fines and even criminal punishment) Understand them right from the beginning and be prepared to have to integrate with their tools and processes They will need to know their data is secure whos accessing it who approved itOffering solutions that are stuck together with glue and custom development isnt going to give them confidenceMight not help you eat lunch but you can otherwise be the hero
QYDmz9ySiN2pZfSdPrwwPn,Tweaking Spark configuration for beginner might be a daunting task as the required knowledge ranges from understanding what does spark driver and executor do to determining the resource needs of themThe easiest way to improve performance is to identify places to cacheThis result in evaluating the DataFrame twice if the DataFrame is not empty A beginner to Spark may not notice this happened to us It flow into EMR until we notice it in spark stagesEMR runs on Yarn quoted below from AWS documentation: Yarn supports 2 mode cluster or client mode If you are running yarn in cluster mode with --deploy-mode cluster  both the spark driver and instance node will reside in the core node Thus there is no need to have a huge master instance taskHowever if you are running in client mode and performing spark-submit from the master node spark driver will be in the master node In this case depending on your spark jobs need a higher end master instance type might be neededBy default EMR adjust spark related configuration such as sparkexecutormemory and sparkexecutorcore based on the core and task instance type in cluster It is usually set to a very low number in order to accommodate for more executors This result in a possibility of OOM for your spark jobThere are many articles in the internet on tweaking executor and driver memory Finding the correct value is never easy and will generally take a considerable amount of effort in understanding and debugging the needs of your spark jobA proposal would be to enable maximizeResourceAllocation for your spark cluster Enabling this configuration will ramp up the driver and executor related memory and core at the expense of parallelism The side effect might also result in under utilised cluster Eg your executor might be bumped to 14 GB in memory each where your spark job might only require 1GB at mostAs a side note: If you are combining tip 2 with this you may need to manually tweak driver memory as EMR will set tweak driver memory based on the lower instance type of (master core) nodes
oMhMXUbT4PkrvkQiCpe2hZ,As important as it is to open-source our work I thought it would be important to share an internal note at Astronomer As a startup it can be super easy to get stuck in the weeds and lose sight of what were doing I wanted to show some love offer encouragement and hopefully inspire some action We have an amazing team with amazing talents but this message weighed heavy on my heart (Not edited): TL;DR: lets get itSlap me in the wrist if this is absurd for the general @channel but I spent a little bit of time tonight thinking about the pivotal moments that happen within startups and how much it takes to build to get thereReal talk: As someone that has only worked in startups literally credits his entire career to Cincinnati startups and has limited/no experience outside of startups thank you It is rare that companies in this stage region industry focus etc do anything as tremendously incredible as we do Anytime I wake up thinking fuck I have to go do this super hard thing I remind myself that what we are doing is so amazing and groundbreaking that there is not a single thing on earth I would rather spend time on At times I want to scream At times I want to toss my hands in the air There are times where I think about what ifsThe good news: As a company theres no towel to throw in We make shit happen ship product and do amazing work And thats why were all here Thats what we signed up forTake this as a bit of encouragement to take 2 minutes to reflect on the mission we are executing on This may sound lame AF or weird but we are building an incredible business I am thankful of the people I get to work with everyday and the problem were solving…Take the challenging/taxing/unpredictable days in stride and know that were BUILDING Lets fucking get it
MCsCPQvdYJ7fLXW8NzvGhB,Data Product is similar to any product which has specific utility of serving the data for analytics reporting predictive modeling optimisation modeling A/B testing and many more avenues of usage Product thinking is imperative to utilise BDD frameworkThe article would touch on how popular testing frameworks BDD and TDD can be used while building data pipeline for data productsThe article will be of special interest to data engineering teamData Product in this article would mean any consumable data asset which has been built to aggregate one or more business processes for analytics needs and which has a name and a location The data can be served by various tools and programming language In essence a Hive table can be classified as a data productJust like a product has components which can be called features for the product development team very similarly it would not be far fetched to break the data product into its components As features of a product would have behaviours features of a data product would also have behaviours The behaviours of the data product would be incorporated in the data pipeline code BDD gives a framework to engineers to specify the behaviours in a test executable mannerBDD is a popular framework which involves multiple stakeholders not limited to business stakeholder product owner and engineering The business stakeholders lists out the features and behaviours of the features which the application or product should have The engineering team builds the functional tests out of the features and behaviours BDD may sound like a waterfall process but it is very much agile The features are incrementally added to the product enabling test test specifications to be incrementally added The test specifications are primarily functional tests BDD test specifications would fall as black box testingBDD enables TDD The functional test specifications as build to specify the behaviour of the feature are picked up one by one to be broken down into unit test cases These unit test cases then drive the code development The cycle of TDD follows in which the unit test cases fail the first time then the code is added to pass the unit test cases the code is refactored if necessary to complete the codeOnce the unit tests pass the functional test for the scenario is run If the functional test passes then the code for the scenario is considered done Next scenario is picked up for development The cycle thus continuesEvery time the complete suite of functional test is run to make sure that additional code has not broken a previously coded behaviourUnit tests are white box testing The test is for the module which is coded to satisfy the unit testsThe previous paragraphs attempted to lay out the concepts of data product BDD and TDD This section would put the concepts together through a hypothetical business scenario Magenta Corporation sells its merchandise through the a digital market place of a giant online retailer Magentas business is picking up However the expense of shipping the package to the customer through a carrier is higher than it had anticipated Magenta wants to optimise the process and also wants negotiate with carrier on the expense to transport the packages Magenta rightly feels that it should first build an aggregated data product of shipping and expense to understand the expense betterMagentas business analyst discusses with the data engineering team to build the data productThe vision is created to build a table which aggregates customer order item quantity shipping information and charges from carrierThe analytics dataset creation is broken down into multiple featuresThe highest priority feature is : Every package number has an expenseFrom the feature specification engineering team builds the testing specifications as can be seen in the picture below The team uses ScalaTest to build the tests Scala is the language which the team uses to build the pipelineAs can be seen in Figure 1 the functional tests are in pending state The GivenThenWhen of Gherkin has been used These tests could be implemented once code has been put together These tests will check the output against the input message or records Hence these tests are called white box testing The internal function of the code is not testedThe next step is to take the first scenario and build unit tests out of it Writing unit tests before the code also encourages design thinking before starting to write the codeLets work out the first scenario  which is Expense of the package is a positive float number and the package expense can not be more than $200The steps of the code would be: As can be noted above each of the steps will involve the code to be written Activity in each of the steps is written as unit tests Below is a sample unit test for reading the file The unit tests are in pending state Once all the unit tests for the scenario of the feature are written then it would be the right time to start the developmentEach of the unit tests are coded As the code is being built for the unit test the tests would be implemented and run The tests would then move from pending state to implementation stateOnce all the unit tests have passed the functional test of the scenario would be implemented Once the functional test passes the next scenario would be taken up to be broken into unit tests and coding
gjQY6G9kHeXecokHjefH3v,"Apache Airflow is an open-source data pipeline task scheduling and coordinating tool This tool was developed by Airbnb back in late 2014 The project was made open-source later in 2016 by donating it to the Apache FoundationAirflow uses DAGs (Directed Acyclic Graphs) for execution and is based on the python programming languageAlternatively we can also make use of upstream and downstream definition methods These methods were used historically and since recent updates to Airflow bitwise operators (>> and <<) are usedaaOperators represent a single task in workflow and usually execute independently Generally operators do not share information There are several types of operators based on the nature of the task to be performedEach operator in the Airflow must have a unique task_id and the dag name to which the operator belongs toThis operator does not perform any job and is usually used for debugging purposesIf you want to run any bash commands or bash scripts during the execution of data pipelines then BachOperator is what you need! These operators use the underlying OS for the execution of bash commands The results can be observed in the log filesHere we provide the bash command to be executed along with the operator identifies and the dag name to which it belongsApache airflow also provides flexibility to define python functions and call them during the runtime execution This is achieved using PythonOperatorConsider we are scraping the contents from a URL We can write a function to fetch data and later pass this function as an argument to the PythonOperatorIn the above code block we have defined the python user-defined function pull_file This function fetches data from the URL and saves it to the provided save path Later this function is invoked by the PythonOperator by passing the function name and argument valuesSensors are the special operators that wait for a certain condition to be trueEg  Creation of file upload of a database record a certain response from a web request etcWhile defining sensors we can define how often the sensor needs to check the condition for its truthnessSensors are derived from airflowsensorsbase_sensor_operatorIn addition to default operator arguments task_id & dag sensors also include the following arguments: a mode = poke: The default mode that runs repeatedly""b mode = reschedule: Gives up task slot and try again later (Other tasks don't wait until the completion of the sensor)2 poke_interval: How often to wait between checksSensors are used instead of operators when the task involves uncertainty of its execution If failure is not immediately desired and we need to add task repetitions without the usage of loops then sensors are usedAdditional details about other operators can be found at https://airflowapacheorg/docs/stable/_api/airflow/operators/indexExecutors define how tasks are executed Various executors available in Apache Airflow are: We can check the type of executors used by each dag using either airflowcfg file or by executing the command airflow list-dagsThis is the default Airflow executor that runs one task at a time It is useful for debugging Although it is simple to understand it is not really recommended for production data pipelines due to long execution timeThis executor runs on a single system and treats each task as a process Parallelism is defined by the user and they can utilize all resources of a given host systemCeleryExecutor uses a Celery backend as a task manager We can define multiple worker systems which makes it significantly more difficult to set up and configure However these types of executors are extremely powerful for organizations with extensive workflowsSLAs are defined within the Airflow DAG to finish the execution of workflow within expected timelines They dictate the amount of time a task or a DAG should require to run An SLA miss is triggered any time when the task/DAG does not meet the expected timing requirementsIf an SLA is missed an email is sent out and the log is stored We can view SLA misses in the web UISLA can be defined in two ways: 2https://learndatacamphttps://airflowapachehttps://enwikipedia"
eZbtmFfYjbuugSHbnR6Nng,A Data Lake is a new form of data-warehouse that supports a variety of data formats & structuring data exploration activities machine learning graph analytics and recommendation systems Traditional data-warehouses do not provide many facilities for unstructured data hence data lakes are usedA data lake is a repository where data is stored in the raw format especially object blob files A data lake can be established either on-premises which requires a lot of hardware costs or can be deployed on the cloud (Amazon AWS Google Cloud Microsoft Azure etc) Most of the organizations prefer cloud technology for data lakes because of many readily available tools less maintenance and pay-as-you-goHowever improper management of data lakes can be disastrous This can lead to the creation of Data Swamps A data swamp is a deteriorated and unmanaged data lake that is either inaccessible to its intended users or is providing little valueThere are several ways to implement a data lake on cloud-based on cloud vendor and its internal services Here I would be providing some of the implementation design on Amazon AWSThis the simplest design where data is transferred from source to the Amazon EMR and later this transformed data is accessed by the BI & Analytics app The source of this design can include but not limited to Amazon S3 AWS RDS Cassandra DynamoDB and EC2 The intermediate stage which includes Amazon EMR (Elastic MapReduce) consists of HDFS and SparkEMR cluster is created once but it can grow and is not supposed to be shutdown The inclusion of HDFS makes this design a little complex and difficult to manageIn this design approach no HDFS is involved and all data is stored on S3 buckets Data is loaded to EMR for processing and later query/processing results are stored back into S3 The EMR cluster is spun on-demand and remains shut down otherwise This approach has several advantages over previously mentioned design such as less costly easier to manage and high performanceFunction as a service (FaaS) is a category of cloud computing services that provides a platform allowing customers to develop run and manage application functionalities without the complexity of building and maintaining the infrastructure typically associated with developing and launching an appIn the above design all data is stored on S3 Athena is a service that can load & process data on Serverless Lambda resources (pay by execution time not by machine up-time)
RYq3gEgfVcW9sehcB9MDjR,TL; DR: This post presents a simple almost trivial mental-model to help think about data-intensive systemsLarge-scale data processing serves multiple purposes At a 30000-feet view every purpose can be bucketed into two broad categories: This categorization is a high high level one I use to reason about data system design and its utility fades fast as we delve deeper into system nitty-gritty Silos appear within & around each of these buckets as we descend into implementation of systems but it is still a useful one to reason about data-intensive applicationsThe basis of this categorization is captured in the following statement: Every data system has two variables: data & query The defining feature of the system is in the temporal nature of these variables In every data system either data or query is transient and the other is persistentIn a data system maintaining materialized views data (or more precisely the view of data) is persistent and query is a transient entity flowing into & out of the systemIn a data system processing events query is persistent and transient data flows through the systemEvery database system can be looked at as a system maintaining materialized views Data is persistent by the very definition of a database It provides a DSL (such as SQL) to query against this persistent data These queries are transient; once an output is generated against the query no record is kept of it (except logs of it perhaps) Some queries mutate data but that is all right It still fits the model; we defined data to be persistent not immutableDatabase triggers are systems processing events A pattern is stored against a trigger and every time a new data point satisfies this pattern a trigger event is generatedA class of systems which belong to the bucket of systems processing events are CEP (Complex Event Processing) systems In fact every system which belongs to the bucket of systems processing events can be called a CEP systemAn analytics system performing batch computations or stream processing or implementing some form of lambda architecture is an example of a system capable of being modeled as either The model depends on the vantage point from where you observe the systemEvery statistic metric aggregation and machine-learning model that the system computes is a materialized view into the source data Thus if we view the analytics system in conjunction with the system-component storing the materialized views ie from the vantage point of a consumer of the materialized views the system exhibits the property of persistent data & transient queryOn the other hand when viewed in disjunction with the component storing the materialized views it exhibits the property of permanent query and transient dataData in a system exists either as state or a stream
g2ZFRatWME8nBwEdXp7VWf,Make everyone part of the journey; avoid entanglementData is considered the new oil\u200a a new opportunity to identify and claim prosperity It doesnt come easy is loaded with impurities in its crude form The process of extracting intelligence from this crude form is where the money isOrganizational awareness on how to treat data is still maturing\u200a\u200asome are lost in the ownership battle of who owns data The new terminology that we all hear is data democratization It means providing all stakeholders access to data Does it stop there? If analyzed critically it holds the key to the question of ownership as wellWho else would know better than the owners of data! Leave it to the data ownersTo understand the data value chain we have to realize that the value of data is at the end of create capture curate cycle Creation is by mostly transactional systems Capture is by engineering data pipelines Curation is by organizing and tagging data elementsCuration is the most important step before extracting value\u200a\u200ait gives data its organizational identity by tagging key data elements Identity of the individual data element is transformed as it is commingled with organizational dataData democratization is not just giving individual departments access to the extracted value of data but also making them an integral part of the value chainOrganizational data is generated by its individual departments In small to medium scale well organized organizations with an Enterprise Architecture in place tagging of data at the point of creation or capture is well established If this tagging is not present democratization should also emphasize the need for individual system owners to perform taggingA payments system analyst will know the best about payments data! Data stewards should work closely with system analysts to gain organizational perspective of that payments dataWhen the individual system data is well documented at a lower most grain\u200ameta data is in place then organizing it at a higher level becomes easierStart with the source system owners make them part of your data journey and may be with their participation the end goal of value realization will be simplifiedData entanglement will provide a sneak peek to maturity of enterprise architecture
CvWyE9nYd9d5zFifJMd7f3,A data warehouse is a central repository where raw data is transformed and stored in query-able forms It is an information system that contains historical and commutative data from single or multiple sources It simplifies reporting and analysis process of the organization It is also a single version of truth for any company for decision making and forecasting Without a data warehouse data scientists and data analyst have to pull data straight from the production database and may wind up reporting different results to the same question or cause delays and even outagesAccording to Bill Inmon recognized by many as the father of the data warehouse and famous author for several data warehouse books A data warehouse is a subject-oriented integrated time-variant non-volatile collection of data in support of managements decision-making processTechnically a data warehouse is a relational database optimized for reading aggregating and querying large volumes of data The DWH simplifies a data analysts job allowing for manipulating all data from a single interface and deriving analytics visualizations and statisticsData warehouses are generally updated less frequently and are designed to give a historical long-range view of data The analytics that is run in a data warehouse then is usually more of a comprehensive view of your companys history rather than a snapshot of the current state of your business Since the data in a data warehouse is already integrated and transformed it allows you to easily compare older historical data and track marketing and sales trends These historical comparisons can be used to track successes and failures and predict how to best proceed with your business ventures to increase profit and long-term ROIA data warehouse has the following characteristics: A data warehouse is subject-oriented as it offers information regarding a theme instead of companies ongoing operations These subjects can be sales marketing distributions etcIntegrated means the establishment of a common unit of measure for all similar data from the dissimilar database A data warehouse is developed by integrating data from varied sources like mainframe relational databases flat files etc Moreover it must keep consistent naming conventions format and codingThe data collected in a data warehouse is recognized with a particular period and offers information from the historical point of view It contains an element of time explicitly or implicitlyThe data warehouse is also non-volatile means the previous data is not erased when new data is entered in itOLTP (Online Transactional Processing) is a category of data processing that is focused on transaction-oriented tasks OLTP typically involves inserting updating and/or deleting small amounts of data in a databaseOLTP mainly deals with large numbers of transactions by a large number of usersOLTP databases are generally the main source for the data warehouseETL is short for extract transform load three database functions that are combined into one tool/pipeline to pull data out of one database and place it into another databasePipeline infrastructure varies depending on the use case and scale However it always implements a set of ETL operations: 1ETL tools perform many functionalities some are: Some tools for writing ETL pipelines: A data mart is a subset of a data warehouse that is designed for a particular line of business such as sales marketing or finance In a data mart data can be derived from an enterprise-wide data warehouse or data can also be collected directly from sourcesA data mart is an access layer which is used to get data out to the users It is presented as an option for a large size data warehouse as it takes less time and money to build However there is no standard definition of a data mart is differing from person to personIn a simple word data mart is a subsidiary of a data warehouse The data mart is used for a partition of data which is created for the specific group of users Data marts could be created in the same database as the Datawarehouse or a physically separate DatabaseWhen data warehouse and data marts are built The base work is done Data prepared and ingested modelled and cleaned Now we can provide accurate reports and analytics over our data This is where the reporting layer comes inReporting is the process of organizing data into informational summaries to monitor how different areas of a business are performing Measuring core metrics and presenting them  whether in an email a slide deck or online dashboard  falls under this categoryAnalytics is the process of exploring data and reports to extract meaningful insights which can be used to better understand and improve business performanceSetting up secure and reliable data flow is a challenging task There are so many things that can go wrong during data transportation: Data can be corrupted hit bottlenecks causing latency or data sources may conflict generating duplicate or incorrect data Getting data into one place requires careful planning and testing to filter out junk data eliminating duplicates and incompatible data types to obfuscate sensitive information while not missing critical dataThe first decision that one has to make when choosing a data warehouse is between cloud and on-premise data warehouse systems and there are certainly advantages and disadvantages to bothWhile on-premise solutions offer Speed as data is on local servers speed and latency can be better managed at least for businesses based in one geographical location Security Availability/Reliability can also be achieved If you have good staff and exceptional hardwareOnce you choose between cloud and on-premise systems there are various vendors to choose from For on-premise systems some options include: For cloud systems top solutions include: While talking about DWH it is important to talk about Data lake too A Data lake is a vast pool for saving data in its native unprocessed form A data lake is not limited to relational data We can store structured semi-structured and unstructured data A data lake stands out for its high agility as it isnt limited to a warehouses fixed configurationA data lake uses the ELT approach swapping transform and load operations in the standard ETL operations sequence Supporting large storage and scalable computing a data lake starts data loading immediately after extracting it This allows for increasing volumes of data to be processed A data lake is very convenient for instance when the purpose of data hasnt been determined yet  since a data lake stores it and later processes it on-demandYou can read more about Data Lake vs
8VYfbBKG3HvoCZxWpdskH3,Another week another geeky update! As I continue to share my knowledge and experience from the world of data engineering I bring to you an interesting insight into Kubernetes If you have been following my blog in my last tutorial I elaborated upon what makes Kubernetes the ultimate container orchestration system and is the platform of choice for scaling and deployment In todays article I will touch upon some important case studies of major companies using Kubernetes to solve their container orchestration problemsOne of the biggest music streaming platforms with 130 million premium subscribers Spotify is an early adopter of microservices and Docker However it was by late 2017 when the platform decided to move from the homegrown Helios to the feature-rich Kubernetes- Jai Chakrabarti Director of Engineering Infrastructure and OperationsIn 2018 the team addressed technology issues required for the migration from the existing content orchestration platform to Kubernetes The team used a lot of Kubernetes APIs and extensibility features to support and interface with their legacy infrastructure for an easy integration Spotify began the migration journey to Kubernetes later that year with a small percentage of its fleet containing over 150 servicesHowever Spotifys Kubernetes migration was not a smooth one as the team ended up accidentally deleting all its Kube clusters not once but twice all with little to no user impact Since the incident the team learned to operate many clusters automatically and safely The team also cut down on its downtime and human error by declaratively defining clusters in code with Terraform backing up and restoring clusters with Ark and increasing scalability and availability by running many more clustersNow many of you may wonder how Kubernetes exactly helped Spotify? Well the time consumed by the team to create a new service and get an operational host to run it in production reduced to a matter of seconds and minutes with Kubernetes In addition to the ease of scale and time efficiency migrating to Kubernetes also helped improve CPU utilisation up to threefold The biggest service running on the platform is capable of taking over 10 million requests per second In the early days of Kubernetes Spotifys team also built a tool called Slingshot that creates a temporary staging environment that auto destructs after 24 hoursBookingcom adopted Kubernetes to achieve sustainable scalability In 2016 the online travel agency migrated to an OpenShift platform to give product developers faster access to infrastructure A year later when OpenShift built its own vanilla Kubernetes platform Bookingcom decided to adopt the new container orchestration systemThe association of Bookingcom and Kubernetes dates back to 2015 when a team at the travel platform prototyped a container platform based on Mesos and Marathon However in order to cater to their need for enterprise features at its scale the team adopted the OpenShift platform Even as the platform offered high-level CLI interface developers faced a knowledge bottleneck that stopped them from being able to support themselves as most of them did not know it was Kubernetes beneathThe team decided on a new solution of building a vanilla Kubernetes platform of their own and customise it In doing so their existing experience of working on OpenShift proved helpfulWe have a tutorial You follow the tutorial Your code is running Then its business-logic time The time to gain access to resources is decreased enormously- Ben Tyler Principal Developer B Platform Track at BookingSo what changed with Kubernetes for Bookingcom Earlier creating a new service could take a couple of days or weeks depending on whether the developers understood Puppet On the new platform it can take as few as 10 minutes The team was able to build almost 500 new services on the platform in the first 8 months of adoption with hundreds of releases per dayHelping Bookingcom with its agenda of sustainable scaling are other CNCF (Cloud Native Computing Foundation) technologies including Envoy Helm and Prometheus The team also developed Shipper which is an extension for Kubernetes to add more complex rollout strategies and multi-cluster orchestrationAnother interesting case study of Kubernetes solving big brand problems is that of Pinterest In 2016 the social media service decided to move to a new compute platform that could both be agile and seamless It started by moving the services to Docker containers and when these services went into production the team began looking at orchestration and Kubernetes became the obvious choiceHow did Kubernetes provide Pinterest the ease of scale and the benefit of simple deployment? Being a service with more than 200 million active monthly users and a thousand microservices running under the hood the pain point for Pinterest was lack of velocity in taking an idea to production due to an inconsistent and complex end-to-end developer experience With Kubernetes the team at Pinterest was able to build on-demand scaling new failover policies and simplify overall deployment and management of JenkinsWe not only saw reduced build times but also huge efficiency wins For instance the team reclaimed over 80 percent of capacity during non-peak hours As a result the Jenkins Kubernetes cluster now uses 30 percent less instance-hours per-day when compared to the previous static cluster-Micheal Benedict Product Manager for the Cloud and the Data Infrastructure Group at PinterestIt was in July 2017 that Pinterest decided to address the issues of running on virtual machines by choosing Kubernetes over other orchestration platforms The team began onboarding its first use case of Jenkins workloads into the Kubernetes system in the beginning of 2018 By the end of Q1 the team successfully migrated Jenkins Master to run on KubernetesSpotify Bookingcom and Pinterest are only some of the many key users on the global scale running Kubernetes in production Given the ease of access customizability and scalability Kubernetes is poised to be the new cloud platform
L3JEyCKtyyeCD6uzaYMmr9,An overview of the Dao Fork scam when the cryptocurrency community came together to reverse an immutable $70 million in transaction valueBeing a fairly new technology cryptocurrency is riddled with its complexities One of the key milestones in the history of the digital currency was in 2016 when a smart contract concept fell prey to malicious hackers compelling its creators community to step in and restore balanceLike any traditional currency cryptocurrency lets you pay for goods and services However it is a digital currency which means theres no printing or minting happening and it can only be used onlineFor cryptocurrency to work a decentralized technology called blockchain is used Consider it as a public ledger that is distributed With the immense security on offer comes a few shortcomings For one anything written on this public ledger cant be reversed Essentially theres no coming back from a transaction unless the receiving party wants toThe Decentralized Autonomous Organization (The DAO) as a cryptocurrency management concept had been designed with an aim to eliminate the need for hierarchical management and operate as a venture capital fund in the cryptocurrency businessMembers of the Ethereum community which is an open-source public blockchain-based distributed computing platform and operating system announced the inception of The DAO at the beginning of 2016 as a smart contract on the Ethereum blockchain While the creation period was successful and managed to raise 127 Ether worth around $150 million at that time several bugs were overlookedIt was on 18th June 2016 an anonymous hacker lent a big blow to Ethereums reputation and The DAO concept Not only did the hacker manage to extract funds from The DAO in the first few hours alone a total of 36m Ether worth around $70 million at the time were drained The security loophole was found related to the splitting function known as recursive call exploitAs described by the Ethereum Foundation using recursive calling vulnerability the attacker called the split function and then called the split function recursively inside of the split thereby collecting ether many times over in a single transaction To put it simply the hacker managed to extract funds from The DAO into a child DAO which copied the originals structure Given the irreversible nature of the ledger the Ethereum community was split into two: to intervene or not to intervene and let malicious hackers take control of the loopholes for the years to comeThe Ethereum community presented two proposals to stop the attacker from draining The DAO First a soft-fork was proposed to prevent the hacker from withdrawing funds from the child DAO after the 27-day window However a few hours before its implementation a bug was discovered which could have allowed attackers to launch a DoS attackNow the community had to make a tough call of releasing the hard fork which would return all the Ether taken from The DAO to a refund smart contract using its `withdraw` function The DAO token holders could request to be sent 1 Ether for every 100 DAO the investors who paid more than that could request the difference from the original address That would mean bending the rules and the non-supporters in the community were not ready for itThose against the hard fork listed down arguments including how bending the rules for one contract would mean doing it for others and more importantly how using hard fork would reduce the value of EtherEventually the hard fork proposal got the upvote and the community members stepped in with the 1920000th block about a month later on 20th July 2016 to reverse the transactionsThe DAO hack may have divided the Ethereum community over Code is law but it did throw light on the weaknesses of the smart contract concepts and how it may have been averted with thorough testing of the code before implementationAs a data engineer I work on cryptocurrencies these days and the DAO Fork Hack is one of the biggest reminders for our community about the sheer vulnerability of nascent concepts such as the DAO along with platforms like Ethereum which are equally prone to unethical activities; pretty much like the internet Securing and safeguarding the technology lies in the hands of the humans even as novel concepts like DAO aim at eliminating human-interference
JnNTSyXa8NLEbak2rkhSVj,David changed his career: he retrained as a Backend Software Developer met team talentio Amsterdam in person and subsequently found his first tech job in the Amsterdam startup of his dreams on the talentio platformHi David! Congrats on your first job in the tech sector! Can you tell us a little about your route to working with tech professionally? Sure! My degree wasnt initially related to software At first I studied Telecommunications Engineering in Barcelona but after a year I realised it was not what I wanted to be doingI came to Ironhack Amsterdam to start my coding careerWhat motivated you to sign up on talentio?I heard about talentio when some of the Amsterdam team came to our class at Ironhack and gave a talk about the platform and starting out in work as a Junior DeveloperGraduating from the course at Ironhack I wanted to give myself the best chance to find a job and decided that I needed more optionsI expected something like LinkedIn but with a more casual environment and a platform where I could have more control over receiving job offersUnlike LinkedIn on talentio you specify the positions you are interested in progressing to and even the particular technologies you want to work with You are contacted only when you are open to finding a new jobWhat was your experience of the platform?talentio is a really nice platform to use its really simple intuitive and convenient Learning to use it took only a few minutesChatting directly to the companies was great and having the chance to discuss your plans and possibilities with your Talent Advocate is really helpfulAll the offers that I got were interesting and most had the proposals I was interested in (in terms of salary and technologies used on the job)How are you now finding your new job?My new job is amazing truly I love my company its environment the people and the offer was just what I was looking for Im starting out now working as a Junior Data EngineerOne last question whats your single favourite point about talentio?Having worked with Sandi is my favourite part ❤ I guess I should say the best part is the Talent Advocate Sandi has helped me a lot not only to find something which fits to my experience and the tech stack I am using but also with really nice advice She even reviewed the way my CV is written Id like to think Sandi is more of a friend than an advocate she was really efficient and I really appreciate her helpThe other important thing is how talentios platform is very easy and intuitive to use that makes it amazing
PyyWR6xtmp6ESfJxWNQREH,Data Engineering is a hot topic in the industry these days Clear patterns are emerging on how to architect data pipelines and best practices around them In this article I will share how we made our data pipelines more reliable and dev-friendly using GoCD About me: I am Azhagu and I work as a Principal Engineer for the Platform Engineering team at EyeEm in BerlinEyeEm as some of you may know is one of the largest photography communities around the world We have around 25 million photographers who have shared more than 130 million photos on our platform We have a website and mobile appsIts quite a goal but we are confidently getting thereIn most of the companies out there the data pipelines do not get the engineering attention they deserve It almost always starts with a grumpy engineer writing a bunch of ad-hoc SQL or shell scripts dumping all the production data into a data warehouse and leaving the data stakeholders to scrape out whatever meaningful data they can out of it In the beginning things were not different here at EyeEm We have evolved a long way from having a read-only MySQL slave specific for data requests to using Sqoop to import parts of production databases into Hive and Redshift nowadays On a very high level this is how our data architecture looks like: We have a lot of input sources With EyeEm favoring a Microservices based architecture in general we have a zoo of services and: They all end up in Hive at first from where we process and store them into data warehouse schema tables (DWH tables henceforth) As mentioned we use Sqoop for copying data directly from databases Flume as our event transport layer and Spark jobs to process the raw data into Redshift tables We also use Spark jobs to generate certain business reports pre-processing data for Tableau dashboards and for other ad-hoc batch processing (eg generating sitemaps for EyeEm) Our Spark jobs are written in Scala and we use Amazon EMR for running long-running Spark clusters We also have some smaller-scale Python/Pandas based data jobs that were coordinated from our Jenkins server For data pipelines and scheduling we installed Oozie and Hue in our Spark clusters and used it to define our data pipelinesThis setup described above had a lot of problems: Oozie is an excellent tool for defining data pipelines but it relies on XML (ugh)! We are big fans of version controlling our data pipeline definitions but it is harder to do it on Oozie on top of EMR The EMR interfaces (including the API) make it hard to automate anything We ended up using Hue to configure our Oozie pipelines but the Hue UI is not at all pleasant to work with We could not configure individual user accounts so a group of us had to share a single Hue account with frequently stepping on each others toes while trying to configure and reconfigure pipelines There was zero information about who changed what and whenOn top of all this we also had pipeline design issues The upstream/downstream dependencies have not been well-configured and if something fails in between the whole pipeline needs one person full-time to babysit it to completion and jobs could run out of order All of these factors were severely limiting our ability to have decent data reliability for our analysts and business teams We wanted to move to a much cleaner data pipeline scheduler/executor which can offer the following solutions: The first choice we considered was Apache Airflow Apache Airflow is an excellent tool for this trade It was built from the ground up for running complex data pipelines It was highly configurable it can run any type of job You can write the airflow DAGs in pure Python code and the dependencies can be embedded in the definition itself I even wrote a blog post about how to integrate Airflow with EMR here But it comes with a price: Airflow with its distributed executors is a beast to set up involving Celery RabbitMQ and other moving parts  having to maintain and update one more distributed system didnt look very wise at that pointSo we decided to go with the second choice: GoCD GoCD has been coming up now and then in EyeEm as our choice to replace Jenkins and it comes with a lot of goodies: These pipelines can be defined using JSON/XML/YAML and stored in a git repository and GoCD can be configured to poll the repo to update itselfGranted GoCD is primarily a CI/CD tool But the most appealing aspects to us are: Here is a small section of our dependency graph visualized in the GoCD UI: It is immensely clear what job feeds to what and also easy to monitor the progress on any day It also easier to plug in new jobs to this workflow Although this migration wasnt without its challenges There have been many and I would like to go in detail about a couple of important fixes and quirks we had to makeA material for a pipeline is a cause for that pipeline to run We shouldnt confuse this with the trigger (that decides if to run the pipeline) Now a cause can be anything such as a git repo (any VCS repo for that matter) another pipeline a package published to the repository such as S3 or similar GoCD considers a pipeline to be unique only in the case when it is triggered by a changed material If you run the same pipeline twice with the same material (eg: the same git SHA) then it considers it as just a re-runThis in itself is not problematic but when you start to chain pipelines as we do the downstream pipeline does not trigger when the upstream pipeline is run again with the same changeset For the downstream pipeline all it assumes is the upstream pipeline hasnt changed as it has run with the same changeset so it neednt run againThe Daily Trigger is supplied a fake material that never changes (a dummy git repo) plus it has a timer trigger that runs it every day at midnight On Day 1 the pipelines run fine as everyone is happy On Day 2 the Import Photo Database will never run This is because even though Daily Trigger runs on a timer the Import Photo Database assumes its a re-run because the material has no changes for Daily Trigger! This is the same when you want to re-run a pipeline because you fixed something: the dependent downstream will never run without the changed material! We fixed this by having a dummy git repository as a material for Daily Trigger and inside Daily Trigger we made a commit with todays timestamp into a file and pushed it to the remote This way we made sure tomorrows run will be triggered as we expectedWe still couldnt make GoCD re-trigger downstream pipelines automatically when upstream is re-run using the same material (like when we want to re-run within the same day) This could be solved partially by: So we decided to live with that problem for now and keep re-triggering the downstream manually in case we re-run anything within a dayIf you have read so far then you already know I am not a fan of AWS EMR Yet we didnt want to be bothered with running our spark cluster I have already written in detail about why submitting jobs to EMR from outside is annoyingly hard and how we can overcome it hereThat guide is very manual and its nice when you are setting up things once But we had to update EMR and spark versions at some point and then we had to do a lot of plumbing to make them work againWith help from the amazing people on the internet we were able to put together a basic shell script which will create a docker image with GoCD Agent image as the base and install the required spark libraries copy the EMR specific ones from the server Once the image is built it can be deployed to a GoCD server and you can invoke spark-submit commands from your pipelines It can also be used to run spark-jobs against the cluster from your laptopWe know this script is not entirely re-usable but it gives a base for anyone to work with who wants to interact with the EMR spark without having to use the badly designed EMR steps API (Like why do you even need an extra layer when spark-submit is already a good interface!)I know GoCD is a very unconventional choice for running data pipelines but as I have explained above it can still work and in the end it works very well for us We were able to make our developers lives much easier and also increase the reliability of our data pipelines We would very much like to know what are your choices for running complex data pipelines and what do you think about our setup? Please get in touch in the comments
dgnhSdibDW5gPHB4jHYBvu,Integrating Airflow running on Docker + Datadog took way longer than I expected so I decided to simplify itCouple of requirements before you get into this guide: You can do this a few ways: editing airflowcfg or by setting configuration options through ENV Im going to outline how to configure statsd on airflow through ENV vars: 3 Congratulations you are doneRun docker-compose and wait to see: Hopefully your dashboard starts populating  this was my (shitty) finished product: Neither me nor Datadog support recommend this approach  but heres their response: Good luck in your Airflow adventures
HbDMhZKjYEDUBJg7Weufvy,Im a really big fan of databases I think its because the work is low-level enough that I feel like I have control over all data storage all logic and all concurrency issues but high-level enough that I dont have to deal with registers I also love that its work that is applicable to every possible other project Want to build anything that isnt a static webpage? Get you some databaseThats actually what drove me towards this project to begin with Last fall I worked with two other Caltech students to build a live-updating data visualization dashboard for tweets about the election (RIP) I designed the architecture of the project and set everything up on AWS We used a couple of EC2 instances to connect to the Twitter Public Streaming API analyzed the tweets that came through with a couple of machine learning models trained to classify them by sentiment and ideology of the author (liberal or conservative) performed some aggregation and updated a DynamoDB instance with the aggregates We used Elastic Beanstalk to host our Node app which read data out of DynamoDB and used D3js to update our dashboard We had a couple of visualizations illustrating differences in tweet volume sentiment and vocabulary between twitter users with liberal and conservative ideologies (classified using our ML models) on four different topics concerning the election and election dayThat project was especially fun because of all of the new technologies I worked with in order to build it I had the chance to learn some web development a little bit of DevOps and a little bit of data visualization One of my biggest takeaways from this project was how many options there were for setting up streaming data pipelines and how complicated they were to set upNote that our AWS architecture involved three different services and Id hacked it together in about two weeks I was positive that there would have been an easier way to set up a data pipeline and not have to worry about learning how to use NoSQL databases or having to use a zillion different Amazon products In my research for pipeline solutions I ran across a couple of options like Kinesis Streaming and PipelineDB I was inspired by these solutions (particularly PipelineDB) since their ability to stream data into relational databases made me significantly more comfortable What I was really looking for was a filter between the firehose of a data stream and the calm organized environment of a relational database I know I can handle databases and I know I can create apps that read data from databases so I wanted a tool that would put me back in my comfort zoneI mean that I was inspired by other streaming SQL solutions to build my own method of turning streaming JSON data (the current standard for most web APIs) into organized records in a relational database I built my streaming solution on top of NanoDB a relational database built by Donnie Pinkston a lecturer at Caltech Donnie built NanoDB for instructional purposes and during winter 2017 I took his Relational Database Implementation class where he deleted large chunks of the relational database system and we had to reimplement them We worked on join planning plan optimization table statistics and plan costing and B+ tree indexes among other things After taking this class I was very familiar with the NanoDB codebase (written in Java) and felt it would save time to write my streaming solution on top of this database system instead of say PostgreSQLI call my database TrickleDB (because its a Nano stream heh) TrickleDB can be used to connect to any source of JSON data and stream that data into a relational database It can also be used to create tables of streaming aggregate functions which can track the data stream over time In theory you could add TrickleDB on top of any relational database not just NanoDB It is functional and I am proud of it and I wanted to share its architecture with the world because designing this software was the most fun Ive ever had in front of a computer (and yes I do watch Silicon Valley on HBONow)The TrickleDB system is modeled after a tree hierarchy There is one global StreamManager object instantiated on startup which can have zero or more Stream objects Each Stream object can have zero or more StreamView objects and must have one Source objectTo set up the system a user must start a NanoDB server which instantiates the StreamManager and then use the following SQL to create a Stream (for this example I made up a Twitter API endpoint that would return JSON data): CREATE STREAM tweets SOURCE https://wwwtwitterThe Stream tweets creates a new Source object which operates a connection to the made-up Twitter API endpoint The Stream also creates a buffer table a relational database table with a schema based off of the column names and data types passed in with the CREATE STREAM command (plus two extra attributes: a timestamp for the new data when it arrives over the connection and a boolean that tracks if the record has been processed)Once I tweet something new the JSON event describing the new tweet will be available over the Sources connection to the Twitter API The Source will parse the JSON and generate a SQL INSERT statement for it and store this statement in a circular queueEvery so often the StreamManager will run a process that executes all of the INSERT statements in the Sources queue adding the raw data parsed from the JSON events into the Streams buffer tableLets say the user wants to track the total number of likes I get on all of my tweets The user will create a StreamView to track this aggregate function on the raw dataThis command creates a new StreamView object called tweets_aggregate that belongs to the Stream tweets This StreamView creates a new table with a schema based on the passed-in aggregate functions on attributes of the Streams schema (in this case SUM(number_of_likes))The StreamView registers all of the aggregate functions that its schema needs to keep track of and passes this information to the Stream For example tweets_aggregate will pass SUM(number_of_likes) to the tweets Stream but if tweets_aggregate were tracking an average instead of a sum it would pass both SUM and COUNT to tweets instead of AVERAGE Well get back to complex aggregates laterThe Stream uses the names of the aggregate functions from all of its StreamViews to create a temporary table (whose schema contains all of these aggregate functions)Periodically when triggered by the StreamManager the Stream will perform these aggregate functions on its buffer table and insert all of the aggregate values into the temp tableAfter the Stream is finished aggregating all of the data in its buffer table it lets its StreamViews know that they can coalesce the data in their tables with the data in the Streams temp tablesLets say I tweet againThis is inserted into the Streams buffer table: Note that the StreamManager handles garbage collection in a separate process periodically deleting processed data from each Streams buffer tableNext the Stream populates its temp table with aggregates of all non-processed data in the buffer table: Then the StreamView coalesces its table with the temp table: Here is another way to visualize data moving through the TrickleDB system if you prefer seeing everything in one placeIt is at the coalescing stage that more complex aggregate functions are handled For example if we wanted to track a streaming average of the number of likes on all of my tweets the StreamView would collect SUM and COUNT functions in addition to the AVERAGE so its schema would look like this: The StreamView would ask the Stream to track the SUM and COUNT functions on the number_of_likes attributes in its buffer table and then coalesce only the SUM(number_of_likes) and COUNT(number_of_likes) columns in its temp table After the StreamView updates the SUM and COUNT columns in its table with the aggregated batch from the Stream the StreamView performs the arithmetic required to update its AVG(number_of_likes) columnNote also that one could have many Streams each with many StreamViews running at the same time into your relational database If you wanted to track averages in one StreamView table and MIN and MAX in another you could just create another StreamView on the same Stream to track different values on the same streaming dataI would like to implement a sliding window functionality to StreamView tracking This would allow a user to track only the last hour last day last six months etc of a data stream in a StreamView and would allow users to have multiple StreamViews on the same Stream tracking different-length sliding windows
fGRDmyUjuSdCNZgtD6VgDW,"Recently I had a chance to get a nano-degree (yep you heard right nano!) on data-engineering (DE) by Udacity Udacity is an online educational platform issuing as a form of proof of received knowledge nano-degree (ND) diplomasUdacity provides a range of free courses (no certificate no code-review) of great quality (I personally recommend Java course by almighty Mr Cay Horstmann and a course Version Control with Git) and nano-degrees NDs are different that you have code-reviews 1 1 online mentor sessions some career support (pimp your CV/github profiles) and a quite high price to pay (400$/month or another higher price it took other ppl to finish the whole ND)DE course was focused on theoretical aspects of designing data models building data warehouses data lakes automating pipelines and working with large datasets I had to pass 5 in-person reviewed projects to become a certified specialistIt was mainly about relational and NoSQL data models using PostgreSQL and Apache CassandraProject: Database and ETL pipeline in both Postgres and Apache Cassandradesigned to optimize queries for a music startup Sparkify • Cloud Data Warehouses: It focused on cloud-based data warehouses using Amazon Web Services (AWS)Project: building an ELT pipeline that extracts data from S3 stages in Redshift and transforms into a set of dimensional tables for Sparkify startup analytics team • Data lakes with Spark: It was about how to store big data in a data lake and query it with SparkProject: an ETL pipeline for a data lake The data resides in S3 in adirectory with JSON files The task is to load data from S3 process the data into analytics tables using Spark and load them back into S3 deploying this Spark process on a cluster using AWS • Automate data pipelinesThis was all about scheduling automating and monitoring data pipelines using Apache AirflowProject: to configure and schedule data pipelines with Airflow and monitor and debug production pipelinesThe capstone project was a sum-up of all topics covered You had to do a small part from each of them building your own ETL system (with at least 2 sources of data having 1 mil of records)I have come to this ND with the following background: 1 year of internships and 2 years of commercial development in software engineering (full-stack) I am fluent in Python and pretty good with SQL It took me 2 months spending about 1 2 hours daily and 4 6 hours on weekends (I was going over all videos/tasks)The quality of the chapters is really different The first chapter IMHO was the lowest quality compared to others It was quoting Wikipedia most of the time and having some errors (I posted them on the Udacity students hub) In general most of the time I spent dealing with AWS and deploying DBs cause sometimes these are quite a riddle I found the most interesting chapters about airflow and spark""Finding errors On some online e-platforms you can leave a comment right behind the video (Stepik) and it is really helpful saves a lot of time searching for solution/hint/explanation/error However here as an alternative exists a students hub where questions in a forum-like way get replied by the students itself It takes a lot of time and not many of them are left answered Also it is hard to focus on the course and understand the material deeper asking and structuring questions when you have a 400$ bounty on your head for each month studying 1 1 mentor sessions are OK Though dont expect any optimization rocket science answers My mentor was a former GSOC scholar and currently a student I didn't use this opportunity much because one call per week is a long time to wait and it is faster to just search on the internet for a solutionProjects have contribution guidelines and code-review feedback mainly consists of whether you met them Projects are interesting to do most time-consuming overall The degree consists of videos/tasks/projects It is possible to skip all the videos/tasks and just do projects in order to graduate""Career Support I had an ordinary ND (there is a plus option with guaranteed employment) Career support for me was to check my github and linkedin profiles I didn't get any feedback on the latter and on the former I had to keep my activity on github each day for two weeks in order to be reviewed and again having $ bounty on the horizon I just skipped that oneMy goal was to finish this course in 1 month However it was impossible having a full-time job and other activities I also had some paying issues and created a ticket but it was not resolved Udacity support could not help me So my strong advice is not to switch the auto-renewal button onOverall there is not much connection between chapters other than the common ideas for the resulting projects So if I am asked whether Id like to go back and pass this ND again Ill better go to coursera/udemy/edx buy some separate courses needed on my day-to-day job and pass em with my own speed (and much cheaper)Having finished the ND my toolset is broader now I have widened my theoretical experience working with Databases (normalization denormalization Snowflake Star schemas OLAP OLTP etc) and practical skills on Spark Cassandra Airflow Postgresql"
cYFGPKQqrcBHw59UMv7LYv,As an engineer and technologist I believe its important to write/blog for several reasons We work in teams and create some solid solutions for clients and organizationsThe Jamf Migrator which could be considered middleware was created to migrate data between Jamf Pro servers While I did not develop this specific piece of software I did gather requirements and consult the client to give us (Jamf the company) money to develop it along with 92000 iOS licenses This project was also evangelized by Apple in the media and there was a product release based on it
i9ZHpzP2YKxcWPwqYHo3GF,A brief guide on how to set up a development environment with Spark Airflow and Jupyter NotebookAs a Data Engineer it is common to use in our daily routine the Apache Spark and Apache Airflow (if you do not yet use them you should try) to overcome typical Data Engineering challenges like build pipelines to get data from someplace do a lot of transformations and deliver it in another placeIn this article I will share a guide on how to create a Data Engineering development environment containing a Spark Standalone Cluster an Airflow server and a Jupyter Notebook instanceIn the Data Engineering context Spark acts as the tool to process data (whatever you can think as data processing) Airflow as the orchestration tool to build pipelines and Jupyter Notebook to interactively develop Spark applicationsThink how amazing it would be if you could develop and test Spark applications integrated with Airflow pipelines using your machine without the necessity to wait someone give you access to a development environment or having to share server resources with others using the same development environment or even to wait this environment be created if it does not exist yetThinking about this I started to search how I could create this environment without these dependencies but unfortunately I did not find a decent article explaining how to put these things to work together (or maybe I did not have lucky googling)Below a step by step process to get your environment running The complete project on Git can be found here$ docker pull postgres:9$ git clone https://githubAt this moment you will have an output like below and your stack will be running :)Not its time to test if everything is working correctly Lets first run a Spark application interactively in Jupyter notebookInside Jupyter go to work/notebooks folder and start a new Python 3 notebookPaste the code below in the notebook and rename it to hello-world-notebook This Spark code will count the lines with A and lines with B inside the airflowcfg fileIn Airflow UI you can find a DAG called spark-test which resides in dags folder inside your projectThis is a simple DAG that triggers the same Spark application which we ran in Jupyter notebook with two little differences: Before running the DAG change the spark_default connection inside Airflow UI to point to spark://spark (Spark Master)  port 7077: Now you can turn the DAG on and trigger it from Airflow UIAfter running the DAG you can see the result printed in the spark_job task log in Airflow UI: And you can see the application in the Spark Master UI: You can increase the number of Spark workers just adding new services based on bitnami/spark:latest image to the docker-composeyml file like following: Environment variables meaning can be found hereWhen you no longer want to play with this stack you can stop it to save local resources: Finally I hope this guide can help you with your work or study and make it easier to provide a complete Data Engineer environment
NR3x5B9nm9wVRGrWVFmUTu,Few years back probably these terms are not as well known and popular career choices as they are nowUsually once we get on to a job we spend a lot of time improving our technical skills but what about the non technical stuff The reason cultivating these non technical skills is tricky because they are not taught online nor there is a book which you can read and say yes I am ready to take on everything these are something that takes genuine effort and growth mindset to build onThis seems like a regular advise but you need to read between the lines hereYou need to connect theory to practiceThese days every one wants to do AI and have data lakes in their architectureThe number one question that gets asked around for this is do what exactly? You need to understand just because you can doesnt mean you shouldThe worst is creating something that nobody is going to usePut on the figurative hat of someone else in your organization and ask yourself what they would care about and where the value add isTo get better insights try engaging with other teams in informal settingslearn about what they do ask them about the data they use and generate that will give a different perspectiveJust because data can be collected doesnt mean it shouldSometimes the problems arent immediately obviousAlways make sure you are building models or if you are a data engineer pipelines under ethical guidelines when in doubt ask around who might be able to help with such sort of questionsThis seems like one more run of the mill advise but it is not how you actually exercise this critical thinking goes a long way in defining how bad#@! of a Data Scientist or a Data Engineer you will beSpend a little extra time getting to know and understand your data setdont just accept it If youre not sure about a particular column or value do some research see if theres a reason for it to be there what was the background Dont jump straight into the waterlook around get your feet wet gauge the temperature and then deep diveI hope you got the driftThis is an obvious one and here it is all about covering the whole 9 yardsYou need to be good at both written and spoken you should be able to explain your results or objective to an absolute non technical person and a Data Scientist guru with equal clarity and insights to back upUltimately in a business context the best algorithm is not the one with the highest AUC score or a huge pipeline that runs in 10 seconds it is the one that the stakeholders understand and trust enough to use it effectivelyTransfer learning is not only applicable in Machine Learning it is very valuable in real life as wellAs inflexible people create a less-than-ideal work environment people who lack a positive approach to themselves and their work can have a similar effect You need to understand that things might not work out the way you expect them to and this can happen quite often as welldont take it personally or think about all the hours lost but think about all the learning gained which you can build on and improve Letting things go when appropriate is a valuable skill to developSo those were some of my top picks of skills that you need to work on to become a good Data Scientist or Data EngineerI am sure there are many more but that is it for now
QYg9piKd6AzLPLMGDNyhcf,A super early-stage startup founder offers me the job as a business advisor About more than 6 years ago I took the job title as business analyst at a corporate A title is just a title I will take the job if it aligns with what I have been doing or planning to do I told him that tech is still my comfort zone Currently my expertise or thought process is still about 80% in tech and 20% in bizI learned from a friend working in Private Equity (PE) firm that pitch deck term in startup domain is classified as either teaser for short form deck with a few pages Information Memorandom (IM) for long form deck with 50 100 pages PE firms typically only invest in sizable deals ranging from $20 mil to a few billions Last but not least they only invest in business with track records of profitableI am a heavy FB user I communicate to other people mostly on FB In fact FB is my new Skype Investors are not alike to me FB is mainly for socializing they dont biz via FB They prefer emailOne of my mentors is kind sought after by entrepreneurs for mentorship Somehow his style is so casual that others who dont know him well might look down on him So dont just a book by its cover saying comes back to meI did not read a deck sent by an entrepreneur carefully before I gave feedbacks to him Id better read thoroughly once before giving any feedbacks Dont trade off the quality of your service due to the lack of timeThere are those kinds of people who might have a lot of knowledge about certain field but they are short sighted about social skillsI got a training curriculum about data engineering run by a group of experienced engineers in this field I vetted it through with another friend that is really closed to what he does day to day in his jobThankfully! I know what I dont know and I have friends out there who can help fill in my knowledge gaps
MCtBgaaxpHmpQyZ5s4Ttda,Using Change Data Capture (CDC) is a must in any application these days No one wants to hear that the changes they made did not reflect in the analytics because the nightly or hourly sync job has not pulled or pushed the data The common problem is that there are a raft amount of web applications which are OLTP and are often backed by a relational database such as Oracle PostgreSQL MySQL etcPerforming real-time data analytics on these database systems requires usage of big joins and aggregations which results in locks as these database systems are ACID complaint and provide good isolation levels These locks may be held for a long duration which could affect the performance of the application for the live usersThus it makes sense to stream data into other teams of your organisation which could perform analytics on it using spark jobs hive queries or whatever is your preferred framework for big data madnessThe following technologies will be used to accomplish capturing data changeDebezium uses logical decoding feature available in PostgreSQL to extract all persistent changes to the database in an easy to understand format which can be interpreted without detailed knowledge of the databases internal state More on logical decoding could be found hereOnce the changed data is available to Debezium in an easy to understand format it uses Kafka Connect API to register itself as one of the connectors of a data source Debezium performs checkpointing and only reads committed data from the transaction logTo run this example you will require dockerWe started PostgreSQL database and bound its port to 5000 for our system We also started zookeeper and which is used by Apache Kafka to store consumer offsets At last we started a debezium instance in which we linked our existing containers ie postgres kafka and zookeeper The linking will help in communicating across the containersOur setup is ready we just now need to register a connector to Kafka ConnectThis console consumer is used for the purpose of an example You have to write a Kafka Consumer for your production systemsNow issue some SQL inserts updates and deletes from PSQL CLI You will see some JSON like output in the console consumerThis JSON like output is in fact an Apache Avro format which is a RPC and data serialisation framework developed within Apaches Hadoop project It uses JSON for defining data types and protocols The best feature of Avro format is schema evolution More about Avro hereIf you use a JSON pretty tool on the output you will find two main keys in the JSON which are schema and payloadThe schema key contains the schema for the row txn record source etc The payload key contains the data change The payload key also contains the before and after for the row in case the row was updated or deleted It also contains the operation type which is useful for knowing the event type that is inserted updated or deletedAlso note that if you change the structure of the table and then perform some inserts updates or deletes the schema printed in the console consumer would also change and evolve to accommodate the dataI hope this small example would have helped you to understand the tools available for performing change data capture from old relational database systems Thank You
LxrEAtpfW8KnCTcW4MVHPc,Youve probably heard that a lot: Our data is a messA lot of analysts and engineers out there are spending their days fighting against their data Here are the main points of struggle: Some data is in PostgreSQL some is in MySQL some is in Google Analytics … thats already 3 different data stores with 3 different query languages Once youve managed to extract from the 3 sources you need to work on unifying the 3 datasets into 1 to be able to analyze itCompared to having a single store youre multiplying your work by at least 3This is often tied to the different stores but it can also be found within the same storage You have a UTC date in a column a local timestamp in another and youre up for some sweet timezone conversions before being able to do anything with your data This is before you even get to any string vs integer cases or the different JSON formats you authorizeIts probably the most tricky thing in the list Youve been tracking things for months to detect a pattern but suddenly the product changes Maybe a tracking error was inserted or a button got moved on the webpage or maybe one of the pages is not accessible as it used to beLater you want to analyze what happened with that event you setup 4 months ago Wow! Why are the page views decreasing starting from a certain date why the button stopped sending events? You start investigating rather than focusing on the pattern you wanted to detectUppercases lowercases mixed cases integers or strings?You dont have a convention dictating how things need to be tracked and collected so you end up with your website sending user-signed-in while your app sends UserSignedIn Now you need to consider both naming conventions so you add a condition in your query… Repeat that by the number of events and youll probably turn crazy soon enough Not forgetting that it also adds complexity in queries and codeAlthough not owning your data is not a problem per-se it can become oneFirst youre depending on someone or some service: if theyre down youre down Then imagine that whoever owns your data decides to make a change in how you access it or how it will get collected Now you need to update all your processes to match the changeAll these obstacles arent an inevitable characteristic of data There are things you can do in your data pipeline to avoid them and simplify a lot the analysis workI find it weird that what I consider to be the most crucial step in the pipeline is often the one we talk and read the less aboutData collection is the very first stone of any data pipeline Its the first touch youll have with your data Everything thats done wrong in that very first step will get carried over to the rest of the pipeline On the other hand if you do things well youll get the benefits later onWhenever you want to track something think about how youre going to query itI cant stress enough how important data collection is for an analytics pipeline You can solve pretty much all of the struggle points mentioned with a little extra thought at the beginningThere are countless tracking services out there Id advise anyone to first look into services that emphasize owning your own data This means that the service could run against a database that you own or that it gives you access to full exports of the raw data (at the very least)If youre not happy with what these services offer you can then move on to other services that will usually trade your control over the data against simplicity (the most famous example being Google Analytics)For data ownership Id advise looking into Snowplow Piwik or any other open-source events tracking solutionIn my opinion the first thing a new data team should do is take some time to agree on data collection guidelines and write them down Any new event should match the guidelines before being shipped to productionThis will solve all the data formatting and event naming issues in the rest of the pipeline Its a very simple thing to do and offers insane benefits in the future both in terms of consistency simplicity and workloadGuidelines can enforce things like: They can also expose some preferences (ex: prefer viewed prefix to seen )When picking one or more tracking services you want to look at where the data will be storedYou ideally want to have everything in the same spot in some kind of warehouse that you ownTrust me Ive been pulling and merging data from third party services like Google Analytics Mixpanel and Keen and I dont ever want to do it again These services have special features that make them great but dont ever think merging their information is easy Youre going to meet the king of all painsIf you can find a way to insert a product version with every single event you send that might help you a lot in the futureYoull be able to tell immediately if an unexpected variation in your event volume or stats is tied to a product change or not Itll also help you with debugging when and why something stopped workingI dont think tracking product versions is as beneficial as the other points but its a really simple thing to have that can save you time occasionally in very specific scenariosLike I said data collection is the most important stone in the data pipeline because it conditions everything that follows (data processing analysis visualization etc …)It is imperative that data collection gets the attention it needs It will significantly impact whatever you do in the future with the data you start collecting todaySometimes youve already collected a ton of data and you think you cant afford going back and updating your data collection because you would break data continuity I think its worth it I think its work breaking continuity once and for all to start fresh You dont even need to do it abruptlyYou can start sending events to a new well thought data collection system in parallel to your current system One day your new system will have collected enough data that you wont have to look at the previous one and youll get all the benefits then Its often an investment worth makingFinal words for all the analysts engineers and entrepreneurs out there:The time and effort you invest in data collection today will be returned to you a thousand times tomorrow whenever you want to process analyze and visualize your data
8sDbYbHukMTfnM7LRw3tou,Note: Before anything this is anti-pattern for software developmentWeve explained how to dynamically generate partitions in PostgreSQL 11 This helps a ton the reading performance but writing to Postgres partitions is another challengeOne of the most common solutions to write to partitions is to write to the master table and associate a trigger to that write operation The trigger would say : Whenever a row is inserted check its ID then insert it in the correct partition Depending on how complex the trigger definition is we can manage to avoid updating it for a while but ultimately youre likely to have to either modify the trigger to match more partitions or to reattach the trigger to another partitionThis is a rather simple solution as we dont really care about all that in our code we just have to write to the master tableIm not going to write about this solution because its widely explained on the internet alreadyLets get a bit more into NodeJS streamsBefore continuing Ill repeat that this approach can be considered as anti-pattern as it ties code and database state According to the pattern your code shouldnt care whether your table is partitioned or notThe usual way of using streams in NodeJS is to read a stream from a source transform that stream and store the result in a table or a file Streams are very handy for high volume ETL processes since they allow us to transform the whole dataset without having to load it entirely on diskNow if our destination is a partitioned table you might want to distribute the data to its corresponding partition This is not something we can do with a single stream because the stream would only write in a single destinationWhat we want is to read from a single source then distribute to multiple destinations (partitions) depending on the content read We can actually achieve this by generating 1 stream for each partition we have reading the content from the source stream and redistributing the data to the correct stream that will deliver the data to the correct partitionTable naming convention: To pull the list of partitions we can either pull the table names from the PG table or maintain a manifest table in which we store the range of each partitionLets consider that we pulled the list of available partitions We need to create a stream for each partition The objective of the stream being to write the rows it receives to the corresponding partitionEach branching stream we create is tied to one partition the one that has its range limits matching the min and max of the stream container objectPassThrough is a special kind of stream that just transfers data and does nothing Its ideal to declare our destination streams as well define how they store the data into the correct partition laterWe use pg-copy-streams to stream to Postgres partition with the COPY statement (faster than INSERT on high volumes) In this case the data is transformed to CSV before copying and the COPY includes a FORMAT CSV optionWith this were now able to pull rows from the source stream read them one by one and distribute each one to a different stream Each stream then stores the data into an attached partition allowing us to dispatch our source data to the correct partitions in parallelThe benefit here is spreading the insertion load to the different partitions and managing a growing partitioning through code rather than manually maintaining a trigger outside of the codebase
YiRXQoaj4rVBYU7pFeyyUv,PostgreSQL 11 comes with a lot of improvements on partitioning like index keys now propagating from master to partitions table It makes it easier to dynamically generate partitions which will be read transparently for the clientIf we focus on range partitioning for this time its clear that each partition should contain the data that belongs to a specific range of IDs IDs not belonging to any partition are to be stored in a default partitionOne of the property of IDs is that theyre usually always growing and increasing When we create partitions were able to cover the whole ID spectrum (from min(t) to max(t)) at the moment of partitioning t One month later (t+1) its likely that the maximum id has increased by a lot and that max(t+1) >> max(t) All the ids between max(t) and max(t+1) are stored in the default partition This means that if we dont re-generate new partitions our default partition will accumulate more and more data and it will ultimately defeat the purpose of having partitions at allOnce in a while we need to shrink our default partition by creating new partitions and moving the data out of the default partition and into the corresponding partitionsWe can achieve this either by pulling partitions from the information_schematables table or by maintaining a manifest table For example the manifest would contain the IDs delimiting all the partitions When a new ID meets the criteria to form a new partition (ex: ID is 20k IDs away from the maximum partitioned id max(t)) we create a new partition and insert the ID in the manifest Note that you can use a single manifest to organize multiple partitioned tables by simply adding a field that identifies the tableHeres the balancing process Note that this is a sample with pseudo code but the process can be fully automated through code • Figure out if new partitions need to be created:We query the manifest and the users table to understand how many partitions are already created and what are the partitions that would need to be created • Copy the default partition up to max(t+1) to a temporary table:Every ID in the default table up to max(t+1) will be stored in a newly created partition We copy them to a temporary table • Delete from the default partition where id < max(t+1):To prevent duplicates and partition collisions we delete the data that we just copied from the default partition Were not allowed to have data that should belong to a partition into another partition If we dont delete the data in the default partition we wont be able to create the new partitions as they should hold data that belong to the default partition • Create the new partitions:We create the partitions that need to be createdNote that since PostgreSQL 11 partitions automatically inherit indexes and primary keys from the master table • Insert the delimiting ids into the manifest table:We update the manifest with the delimiting IDs of the new partition If youre confident that your partitioning strategy is stable and incompressible you could also only store the max ID • Copy the data from the temp table into the corresponding partitions:We can now copy the rows from the temporary table to their corresponding partitions • Drop the temporary table:To go back to a clean state we drop the temporary tableThats it rows from our default partition that should be stored in partitions have been moved The default partition drastically decreases in volume and queries searching for these rows should now be fasterFrom a reading client perspective nothing changes at all The client will hit the same query on the master table but the performance will be increased because the default partition is now balanced into newly created ranged partitionsThe query planner is smart enough to only hit the corresponding partitions
BDsJsYY6edVN2Wg9KGiM5b,I was previously interviewed as a part of a group of technology executives for an article in InformationWeek regarding IoT devices and enterprises The final article can be found here but below Im providing my full responses to the interview questions as I believe there is relevance to our customers and readers • IoT devices are making their ways into enterprises whether they want them or notLike with most tech-trends companies are falling into the usual trap of not planning before they act Specifically organizations are not planning for the infrastructure they will need to store and process all of the data that will be generated by IoT devices and they are not properly securing information as it is being transmitted from these devices or even as the data sits at rest Not planning for the infrastructure needs can result in all types of system failures while not having a strong security plan around the data can expose an organization or its customers to malicious actorsCompanies also continue to keep their IT organizations separate from the business organization Good technology and algorithms cannot be created in a vacuum IT organizations must work hand in hand with the business organization to derive the most value from data When it comes to data context not content is king • IoT devices can produce a lot of dataDetermining the actual value of any data always boils down to the business case As a general rule of thumb enterprises should keep all data they are collecting until they understand what actionable intelligence can be derived from raw data If you are generating the data and you think there is no value that can come from it you are wrong All data has business value it is all in how you look at that informationDepending on the business some data may appear to only value for a short time period (for instance a temperature increase resulting in a temporary slow down of manufacturing devices) but that data can often be used for longer term projects (eg predictive analytics  determining time to next failure) You dont have to keep all of the data around sometimes aggregate information over a time period is good enoughA software engineer a data scientist and business analyst walk into a bar with the same piece of data… When they walk out the software engineer has determined how to decrease the time it takes for the business analyst to extract value from that data by 40% the data scientist has figured out how to make relevant and accurate predictions about how that data will change over time and the business analyst has figured out how to increase revenue by 30% over the next 6 months using that data Throw individuals across multiple disciplines into a room  thats how youll find the actual value of IoT dataThere are a number of infrastructure considerations given the velocity and volume at which IoT devices generate data While storage is often a major concern I would argue that it should be the least of an organizations worries  these days storage is cheap More important are the processing capabilities of the infrastructure you select Are you leveraging a cloud provider like AWS or managing it all in house? Can data being generated be processed individually or is it only valuable in aggregate? The answers to questions like this will alter the strategyIf the data can processed as it comes in how intense are the algorithms it is being run through? You may need a significant amount of CPU If its being done in aggregate you may need more RAM to handle all of the data in memoryOf course at the end of the day the biggest factor will be cost and return on investment for the types of processing you do with that data • IoT data analysis is often bimodalA general recommendation that organizations push as much of the workload to the device as they can so long as it does not impact performance This is a great way to save costs at the end of the day That said  it is impossible to do some analytics on the device as those analytics require other reference datasets or even data generated from other devices to be valuableIoT should be treated just like all other data in the enterprise Whether data is generated from devices comes from APIs or is user generated content you will always derive more value when that data analyzed along side other dataFurther more organizations should also follow the approach of democratization of data Put data into more hands and you will be amazed at the ingenuity of your employeesIn most cases IoT data will have minimal impact on a companys data management practices but it is certainly possible that an organization after deriving value through some analytic or joining of some set of specific data will decide that other pieces of data should be kept longer or exposed through other tools to further enable analyticsWhen collecting data from IoT devices there are a number of security risks that should be considered While you should democratize data as much as possible in some cases that democratization could expose your organization If youre dealing with consumer data generated from consumers leaks of that data could lead to loss in trust of your organizationAll data poses some risk the question that must be asked is what could a malicious individual do with the data youre collecting Part of that requires knowing yourself what you could do with that dataAs a side: I once had a case where the raw data itself was not necessarily a security risk but the analytics we used on that data which resulted in a derived data set was extremely sensitiveIf you have good data security and management policies in place IoT data should not have a major affect The larger problem is when you have no policies in place Due to the risks mentioned above it is important to have security policies for transmission of data data at rest and access to data • I may think the value of my IoT data is X Tomorrow I may discover it is actually Y or X and YFuture potential value of data has a major effect on the collection of data A number of organizations throw data on the floor that they deem useless at the time This is typically the wrong approach As long as you can afford to keep it store all data until you can definitely prove there is no value (in almost all cases you will not be able to prove this) At some point you may find a use for that data and when you do youll be glad you kept it around • Beyond infrastructure  a major question that comes up is how you can process and analyze all of this data  what software tools and how much should you budget for software In most cases you can leverage a variety of open source technologies (Hadoop ElasticSearch Spark Apache Flink etc…) to build a custom solution that makes sense for your organizationWant to hear more? Just reach out
iusaEzqhqpqPwi5VemhJsL,A natural leader Ryan has spent more than 9 years leading a data engineering team at DotDash His teams have been responsible for designing developing and enhancing the companys platform through strategic data acquisition and multi-layered data architectureData is a continually expanding discipline Growth in the sector has resulted in the creation of many new roles Trying to keep track of them can get confusingRyan cuts through the jargon and outlines in plain English exactly what data professionals are doing to benefit your company
RiiYhHyAs35k3gGjvXUX7N,From last two and half years working on Big Data using a few tools from the Hadoop ecosystem and spark For me Big Data comprises of three vs (volume velocity and variety) When I about to start the journey of Data Engineering struggling with many buzzwords like CAP theorem Data skewness shuffling etc But what I discovered is some common sense with the understanding of the above-mentioned tools Here I am taking you on my journey with common sense as my oarWe are working on a BI project for one of credit card provider company The client wants to generate a report of the performance metric for a given merchant says Wallmart for a given period Here performance metrics include total spend of transactions and a total count of customersIt is a simple metrics to generate we can use sum() and distinct() function of Spark over the columns amount and customer_id and combine the result A great job is done but waits it needs to process millions of transactions and response need to be in secondsWhile solving the problem thinking of why not apply the sum and distinct count evaluation into a single scan of the dataset Then come up with a solution of we can sum the spend of customers transaction and take one for count so the transformed tuple is (customer_idsum_spend1 as count some constant says ABC) and in next iteration we can sum the individual customer spend and sum the count and group by it on ABC This solution works and performative over the initial implementationWe can argue that a customer data can be distributed across partitions so we need to do partition by on customer_id that means shuffling but we can use the features of hive bucketing while we create transaction table to avoid the initial partitioningPart of same BI project a new requirement of analyzing intersection and subtraction(A∩B A-B B-A) of transactions of different merchants for a given periodNot an easy metric few more lines of codes compare than Scenario 1 Create two different datasets based on merchants criteria do a left outer join of datasets and evaluate A∩B A-B B-AOhh no its a joint operation between millions of transactions and we are facing a performance issue While solving the problem thinking of both the datasets has the same underlying dataset and come up with a solution for tagging the data with A or B or both based on different merchant criteria Now its become a simple single scan to check for A∩B A-B B-A Just apply a common sense and it solves the performance bottleneckThis metric is a little complicated here for a given merchant wants to know about their customers interaction with competitors and segments of industries like food shopping automobile etc for a given period This metric includes the percentage of customer counts percentage of customer spends in different industry segments of competitorsThis is really a complicated metric initial implementations have the following steps: This looks do-able but we are not able to execute this metric for a period of one two three years even on in-house yarn cluster I took this performance issue until this time I am more confident due to the fixing of the earlier performance bottleneck For the issue deep-diving into GCs log strategies and stumble on the mark-sweep term It gives me a solution and it has the following steps: This solution works well and becomes more performativeThe star schema consists of one or more fact tables referencing any number of dimension tables For Client requirement we are building a star schema and the initial implementation is just joined all the tables This approach has serious problems because it deals with many joins(in some cases outer-join) and the number of dimensions can be range from 5 to 10 dimensions We can also say it aggregated structureHere my assumption is that we can use a NoSQL key-value database and add insert column families for different dimension against same row key Not able to implement due to client restrictionCome up with another approach of a star schema transform each dimension into the same schema and dump each dimension into the same folder as parquet With spark read all the parquet files and partition by on join keyWorking on items similarity for a product catalogs and ends with the cross-join of items to generate every possible tuple for comparisonOhh no! cross joins for 100000 items will create a very large number of records and also not very scalable in terms of prediction of resources required While solving the problem thinking of why generate all possible tuples once then come up with a solution and its something like: So here common sense helps me to solve the problems and of course knowledge of tools helps me to understand whats happening on a system level
bWs4WutciKo7reUeRJisdN,Limit is an usual SQL operator In SparkSQL a limit operation is constituted as a LocalLimit and a GlobalLimit The reason to split a limit operation to local and global ones is to better use the data partition to reduce the amount of data for processing For example image you have 10 data partitions which are 10 million rows/10 GB in each partition If we want to have just 100 rows in the end we dont need to count the 100 rows after we have all the data We just count 100 rows in each partition and then shuffle all rows to one single partition It reduces the data for shufflingHowever if we want to have a big number of rows in a vast table eg 1 billion rows you will have 1 billion rows from each partition for shuffling into the one single partition It will be a huge burdenCurrently I have a pull request to Spark which is not merged yet It tries to perform the GlobalLimit without shuffling the data from all partitions to one single partition The test shows it can boost 3x performance for the big limit number caseWhat the pull request did is to just perform the map stage of the shuffling We dont change the data partition so it looks like we conduct a local shuffling We can collect map output statistics of a map stage By inspecting the statistics we know how many rows in each partitionIn the GlobalLimit operator we ask the necessary rows in each partition until satisfying the number of limit
GdVHkZorr5rtkvY3zwTNPs,Kafka is a distributed messaging system that was developed for collecting and delivering high volumes of log data with low latency Kafka incorporates ideas from existing log aggregators and messaging systems and is suitable for both offline and online message consumption The authors have made quite a few unconventional yet practical design choices in Kafka to make our system efficient and scalable Their experimental results show that Kafka has superior performance when compared to two popular messaging systems They have been using Kafka in production for some time and it is processing hundreds of gigabytes of new data each dayThe paper mentions the emergence of lots of log data generated every day including user activities like login page views clicks likes and other queries In addition to that machine metrics like CPU memory usage also is an important feature This is not only for offline analytics but also very useful in online services Usage may include search relevance recommendation performance ad targeting and reporting and security thingsThe traditional way is to dump the log file on each machine But its time-consuming and not efficient And it only works for offline analytics There are other distributing log aggregators including Facebooks Scribe Yahoos Data Highway but they are primarily designed for data warehouse and Hadoop usage We have the needs for online usage with the delays of no more than a few secondsIt is Distributed and scalable system which offers high throughput The API is similar to the messaging system Applications running can also consume it in real timeWhy traditional messaging systems dont work? • Mismatch in features: they are focusing on offering delivery guarantees which is overkill for collecting log data • Cannot meet the throughput requirement: very high cost when sending a message • Weak in distributed support • Assuming near-immediate consumption of messages: the queue of the unconsumed message is always small If it increases their performance downgrades· A stream of messages of a particular type is defined by a topic· A producer can publish a message to a topic· Published message is stored at a set of services call brokers· A consumer can subscribe to one or more topics from the brokers and consume the subscribed messages by pulling the data from brokers· Topic is divided into multiple partitions and each broker stores one or more of those partitionsWhy is Kafka Efficient? • Simple Storage: Each partition of a topic corresponds to a logical log and each log is implemented as a set of segment files of the approximately the same size When there is a new message from the producer the broker just appends it to the last segment file The logs get flushed after a configurable number of messages set by the user A message is only exposed to consumers after its flushed No message ID: each message is addressed by its logical offset in the logThe consumer always consumes messages from a particular partition sequentially: a) If a particular message offset is acknowledged it means all messages before this offset is consumedb) Pull request contains the offset of the message and an acceptable number of bytes to fetchc) Broker has a sorted list of offsets including the offset of the first message in every segment file • Efficient transfer: Producer can submit a set of messages in a single send request The consumer will receive several messages in a request even when they are processing the message one by one No caching: Relying on the file system page cache Both producer and consumer access the segment files sequentially and the consumer often lagging the producer by a small amount An API sendfile is used to reduce unnecessary 2 copies and 1 system call • Stateless Broker: How much each consumer has consumed is not maintained by the broker but by the consumer itself But how to delete the message? Time-based SLA for retention policy: A message is automatically deleted if it has been retained in the broker longer than a certain period typically 7 days Side benefit: A consumer can deliberately rewind back to an old offset and re-consume the dataDetecting the addition and removal of brokers consumersTriggering the rebalancing process in each consumerMaintaining the consumption relationship and keeping track of the consumed offset of each partitionWorking of Zookeeper: 1) When broker or consumer starts up it stores its information in a broker or consumer registry in Zookeeper Broker registry contains hostname and port and the set of topics and partitions stored on it The consumer registry includes the consumer group to which a consumer belongs and the set of topics that it subscribes to • Each consumer group is associated with an ownership registry and an offset registry Ownership registry has one path for every subscribed partition and the path value is the id of the consumer currently consuming from this partition The offset registry stores for each subscribed partition the offset of the last consumed message in the partition • Paths are created in Zookeeper are ephemeral for the broker registry the consumer registry and the ownership registry and persistent for the offset registry • Rebalancing happens when the initial startup of a consumer or when the consumer is notified about a broker/consumer change through the watcherRebalancing algorithm: a) Calculate the set of available partitions each subscribed topic Tb) Calculate the set of consumers subscribe to Tc) Calculate N = (number of available partitions)/(number of available consumers)d) Each consumer will be assigned N partitions: Writes to the owner and start consuming data from the offset registry • Delivery guarantees: Only guarantee at-least-once delivery A consumer can have their own de-duplication logic if they care about duplicates Messages from a single partition are delivered to a consumer in order But no guarantee on the ordering of messages coming from different partitionsCRC for each message is used to avoid log corruption If a broker goes down any messages stored on it not yet consumed becomes unavailable If the storage system is permanently damaged the message is lost foreverUse cases of Kafka at LinkedIn: · Online consuming and offline jobs· Tracking: Monitoring event is used to validate data loss· Avro is used as a serialization protocol· Producer doesnt wait for acknowledgment from the broker· More efficient storage formatThe paper mentions the following areas for improvements: 1 Adding built-in replication of messages across brokers for data recovery in case of machine failure • Adding stream processing capabilities like support for windowing or join functions • Reduces Performance: the brokers and consumers start compressing these messages as the size increases Due to this when decompressed the node memory gets slowly used Also compress happens when the data flow in the pipeline It affects throughput and also performance • Issues with Message Tweaking: The broker uses certain system calls to deliver messages to the consumer However Kafkas performance reduces significantly if the message needs some tweaking So it can perform quite well if the message is unchanged because it uses the capabilities of the systemReferences: Kafka http://sna-projectsKafka Paper: http://notesstephenholidaycom/KafkaKafka Architecture: http://cloudurablecom/blog/kafka-architecture/index
BNRekacmj4uqHyTZPyJtBY,Step 2 : Create a data consumption script from source application file system etcStep 3: Create scheduled Databricks job to process data as per business requirement and then push the proceed data back to Azure SQL which is our again targeted systemNote: If required you can create and store intermediate results and table for the backup data quality test and other common data usage into you data lake or delta lakeStep 5: Mapped target data source with your tableau desktop and design the business insightStep 6: Publish the dashboard and give Live connection or Schedule data refer as per your business requirementThis is the best way to achieve the ad-hoc data analytics requirement with scalability
g6xVuHVAnPhEfnY4RyMafb,"Hello Readers I work for an airline and i am part of Data Solutions Development team We use Snowflake as a data warehouse and AWS analytic services in combination to build and deploy data pipelines into productionRequirement: Origin and destination forecast booking table is refreshed with new data once a day The fleet assigner view should select latest data from booking table which needs to be executed and output should be written as fixed width format to a CSV The view should run also once a day after the booking table refresh This CSV file should be unloaded to AWS S3 bucket from snowflake as s3://bucket_name/yyyy=4_digit_year/mm=2_digit_month/dd=2_digit_date/fleet_assignercsvSo there are couple of things to do in the above requirementIn order to achieve above i have opted for Snowflake Streams Stored Procedure and Task features to automate file unloadI can not use the original data here due to GDPR reasons so i am going to use some random column names and values with less rows and columns in a sample table Also a simple query created as view to generate the output as fixed widthSo assume the booking table look as follows initially with no rowsIn order to track the changes on booking table create a snowflake stream object on top of this table Below is the statement for the same""Initially when you queried the stream it will be empty because i haven't inserted any rows after the stream creation So let me insert some rows to booking table to see how the stream looks likeNow if you query the stream with following statement you will see only the new changesCreate fleet assigner query based on the above stream object as view to simplify thingsCreate IAM user with policy which has access to the above bucketCreate a java script stored procedure which unloads fleet assigner view output as CSV to S3 bucket Please read the comments in the stored procedure to understand the logicIn the above stored procedure As part of copy statement i am selecting all rows from the fleet assigner view which internally selects latest data from booking streamIf you are executing any DML statement on any rows using the stream once that execution is successful the offset is advancedMultiple queries can independently consume the same change data from a stream without changing the offsetA stream advances the offset only when it is used in a DML transaction including COMMIT transactionsTo test the above stored procedure use call keyword After executing it you will see the file unloaded to S3 bucket in the following way as shown in the below screen shotCreate a snowflake task to automate this process and schedule the task on every day at 6 PM UTC The task should be executed if stream has any new data in itIf you verify the S3 bucket it will download a file per day as per below screen shotthis will be exported to CSV and will be unloaded to S3 in the next run as per task schedulerI hope this information helps you Thanks for reading"
gnfnFZCeWUaFkKRMhAnooN,"Hello Folks I work for an airline and i am part of Data Solutions Development team We use Snowflake as a data warehouse and AWS analytic tools in combination to build and deploy data pipelinesI wanted to share this one particular incident with you all After a long day at work I monitored the snowflake account Billing & Usage to see Credits Used and Average Storage UsedThis particular day the credit consumption for a LOAD_REV warehouse was sky rocketed it consumed more than 2000 credits Five data pipelines share this warehouse and this high consumption is very unusual because our daily loads would consume around 50 credits with a Large warehouseWhen I checked behind the scenes to know what went wrong I found as this warehouse was scaled up from Large to 2X-Large to load huge history data into snowflakeSo for a moment I thought this is convincing because its one year of history data though i still doubt why it would cost so high to load into bunch of data vault tablesLets look into each reason in more detail""Reason-1 These querys are initially ran on a Large warehouse with one year history data and failed to execute because it couldn't take more load at one go Which means failed querys duration on the warehouse also consumed credits To run these again warehouse was scaled up to 2X-Large to load one year history data at a single goReason-2 After scaling up from Large to 2X-Large warehouse the history load was successful with long running querys but the clustering order was messed up due to one big bulk load to data vault tables Generally the daily load is based on DV_LOAD_DT column so the data in tables clustered naturally by snowflake based on date column but when it was loaded entire history at one go and the load order was incorrect so it caused clustering order to failReason-3 Based on the DV tables publish/data mart tables are loaded because the clustering order messed up in DV layer hence data mart layer querys took long time to finish which also the reason to the costImpose limits on the number of credits that are consumedI wish we had this in place from the beginning when we started using snowflake but not too late to have it so we configured it to get the notifications when the credit consumption reaches 60% of estimation the estimation is based on the average consumption from the previous daysInserting data as we get it automatically results in data clustered by load date resulting in fast queries when filtering by load date or values that correlate with itCase Study:We loaded one batch a day to DV and following each load a publish table (DM) from DVPublish loading used to take slightly short of two hours on a large warehousewhen we have loaded history data to data vault at one go without a proper order hence publish loading started taking as many as twelve hoursThe issue was not with the data but the order it was loaded in; all at once instead of day-by-day or batch-by-batch based on the day orderWe can drop the clustering key on a table at any time which prevents all future re clustering on the tableAfter the clustering order messed up on DV tables we created an explicit clustering key on DV_LOAD_DT column for all the effected DV tables to auto cluster by snowflake when this is finished Publish loading was fastFormatting the query string in a consistent way makes it possible to embed different levels of information in itFor example source layer load name target etcThe tags are displayed in the output of the QUERY_HISTORY table or view It is also visible in the query history UITagging your querys makes it possible to easily identify situations where for example long running queriesAfter setting up this parameter we now can filter query history based on tags and see the metrics of it this is really a cool feature to useAfter this incident We become cost conscious and started looking into all workloads to get a deeper understanding Before i tell you about the findings i wanted to tell you little bit about our data feeds and corresponding warehousesFor example Look at the below table I have listed few data feeds here and in real scenario there are manywe have used LOAD_CUST warehouse for customer data feeds LOAD_REV for revenue data feeds and LOAD_FL for fleet data feeds and LOAD_NET for network data feeds Warehouses are divided as per the types""Most expensive warehouse: the reason is that in revenue feeds like Forecast Inventory and Availability need large warehouse because of huge data sets yield and Origin & Destination are smaller feeds compare to others though they still run on large warehouse because these are part of revenue Overall from all the feeds in revenue some query's can be executed on a Small or X-Small warehouse but still those are being executed in Large warehouseUnder utilized warehouses: the reason is that Fleet Network and Customer data feeds are using Medium warehouse though these are not used in full extent When we looked at AVG_RUNNING it is less than 1 and AVG_QUEUED_LOAD value is zero from warehouse load history  also some querys can be executed on a X-small or Small warehouse So these kind of warehouses can be merged into one and if multiple querys are submitted at the same time then multi cluster feature in snowflake would helpRefer below link to know more about how multi cluster warehouses are useful for the continuous workloadsSo we re designed the way we use warehouses to overcome above problems i would like to share some of the best practices hereAll these are multi cluster warehouses with auto resume and suspend option enabledWe started running everything with X-Small warehouse and monitored the query load By default we know some querys cant run on X-Small so we moved those to either Small or Medium or Large warehouse when monitoring these loads we looked into the query performance and execution time based on these factors we changed the warehouse for some querys from X-Small to Small or Medium so on so forthThis way you will have less warehouses to manage and these will be effectively utilized Also when you keep warehouse alive for longer time by directing query loads from multiple warehouses to one or few warehouses then you will get benefited from cache and auto suspend and resume costLastly I would like to mention my team members Kalle Viiri Jarkko Venna Ella Potka who were part of this process and without whom this analysis could not have been possible I hope this information helps you Thanks for reading"
D4vWLZZWq32n5CknqSPT8L,Output for above query: from the above screen shot if you look at the START_TIME_HOUR and AVG_RUNNING columns almost 24 hours the warehouse is running and executing statements of course there might be gaps if you look at the 5 min interval the concern here is the AVG_QUEUED_LOAD at some 1 hour intervals like hour 2 7 10 14 16 18 19 22 23 the querys are queued which means the warehouse could not scale out though minimal queuing is accepted and no need to worry so to overcome this i have utilized snowflake multi cluster warehouse featureOn 2020 03 17 output at hour 2 AVG_QUEUED_LOAD is 1574 you wont see that big number in 2020 03 26 output as i mentioned earlier minimal queue is totally fine hence the scale out workedThree options to mitigate • Increasing the cluster count (if using a multi-cluster warehouse) will allow more concurrency which should also help reduce queuing and blocking • Increasing the warehouse size will provide more throughput in processing the queries and thereby can help reduce the queuing time • Identify querys which are causing queuing and move the workload to different warehouseOutput for above query: Note: If you have snowflake ACCOUNTADMIN role assigned to you then you will be able to use above querysPlease replace the warehouse name and date according to your needHope this information helps you in optimizing your warehouse Thank you for reading this article
kgfaHuDk9f5asr8Nju649J,Being a decidedly passive person when the general election was announced I had already come to terms with the fact that I would not be out door-knocking and leaflet dropping However much like Liam Neeson in Taken I did have a particular set of skills that I might be able to put to work to reach prospective voters in a different wayAs a bit of a politics nerd I hold a long-term ambition to create a tool that allows you to compare different MPs voting records There are a few reasons for this the main one being that there sometimes exists a cavernous gulf between an MPs rhetoric and their actual voting record when inside parliament This tool could serve as a way of keeping MPs honest and of keeping voters better informedWhile assessing the feasibility of such a tool I discovered that Parliament has an API that allows you to scrape the House of Commons division voting data A division in the house occurs when there is enough opposition to a question posed by the speaker that they are unable to establish a clear answer This then results in an arcane and extremely British process whereby MPs are sent off to two different rooms: the yes room and the no room The amount of people in each room is then recorded and the highest total in their respective rooms wins the debateI decided that Id use this data to measure MP similarity Once I scraped all of this data I should be able to answer the question: When there is a disagreement in the house  how likely is a chosen MP to agree with another chosen MP?This is not a perfect method of measuring MP similarity For example if someone votes against the government often on relatively unimportant legislation but always votes with the government on the bigger issues then the result returned may overstate the level of their opposition However given the simplicity of this method and that these issues fade into the background when looking at higher profile MPs (given that they vote on most things) I decided to press on and conduct a review once I had some numbersI set to work writing a basic scraper in Python that used urllib and Pandas that would pull the data I needed from Parliaments Data API First Id grab the top level divisions data which looked like this: This would all be loaded into a Pandas dataframe in one line of code Id then begin looping through the uri column and using its row value to scrape the more detailed individual MP voting data However due to a strange looking implementation of REST architecture this uri value is actually an invalid link Unfortunately this means youre required to extract the numerical value on the end of the uri and use that as a key to append to the following link: http://ldadataparliamentOnce Id figured that out I then scraped the individual voting data Which looked like this: I now had all the information I needed to construct my data set! Its worth noting here that the choice of file I made to download these sources in was CSV I had a number of API related issues when trying other formats However this did end up working in my favour as the CSV files were denormalised and therefore a lot quicker to process There was no need to worry about how fields linked up with one another The obvious trade-off were making here is for disk space as well as RAM but fortunately the data set were dealing with is around 15GB in its entiretyNow that I had the data I wanted to do some investigations and needed a test case Given Jo Swinsons recent election to the leadership of the Liberal Democrats she seemed to be a great fitMy intention here in agile terms was to create a spike I wanted something quick and dirty to establish whether the methodology Id come up with was feasible or not I also needed to decide who else I was going to use as a comparison point I decided that given Swinson was a party leader Id compare her to the other major party leaders and see who she most aligned with  beginning with Theresa May Given their three years of diametric opposition on Brexit I had assumed that this would highlight a substantial level of disagreementI wrote an algorithm that loaded all of the division voting data into one dataframe I took a slice of this dataframe where the voting MPs name was Jo Swinson to represent her voting record I then took a separate slice for Theresa May and joined the two together where they had both voted on the same topicI could then calculate the two metrics I needed: the amount of times they agreed and the amount of times they disagreed I also opted to add another metric here called Weighted Voting Score This was a score that summed the agreement/disagreement metrics but also assigned extra weight to represent how full the house was at the time of each vote This was a way of giving greater credence to votes on important issuesFor example if Theresa May voted for a park bench to be named after Stanley Kubrick but Jo Swinson disagreed and nobody else in the commons cared enough to turn up for the vote then the metrics look like this: If they disagreed on a piece of legislation in a full house of commons (with 646 voting members) then the scores would look like this: This gives us a fairer representation and a better insight into their voting record than the raw figuresAfter running these calculations I was astonished to find that Jo Swinson and Theresa May had far more in common than I had first imagined Despite being completely opposed to one another for three years on Brexit I got the following scores: I then compared her to another significant party leader Jeremy Corbyn: This is when I paused to establish what exactly was going on here I was unaware of Jo Swinson prior to starting this project but after some reading I established that she played an enthusiastic role in the coalition government of 2010 Strange then that she should be coming out as anti-austerity now Overcome with righteous indignation it was at this point that my project took a diversion My investigative spike had now morphed into a campaign against the re-election of Jo SwinsonStay tuned for part 2 where I begin my surprisingly successful campaign step into the murky world of Twitter and cobble together my first website
4Z6fJcr9DGPb6mVAzpCQXH,I am a web application developer as well as data engineer Until now i have worked with 3 different companies and 2 different areas including health care advertising and bankingSo interview is always the first step when you have intention to enter into technology area Presently the popular fields is still focus on AI machine learning Web Development Mobile Development Cloud and Big Data But join tech should based on your enthusiastic and professional view to the market and technology skills Lots of person said you must be smart if you are a technical person whereas not actually The latest trend of tech is the combination of tech and any other areas like finance travel local business fulfillment and delivery So tech is not only coding not only production not only RabbitMQ This is also said what is tech in 2020So as a developer you need to learn………abcdefgThen the most importantly is your career path plan a career path is not a easy work Evaluation on the different skills you had the time you spend on personal improvement a good model is do a reflect on the work you finished on one day and one week Then wrote the work note to remember the contribution you have made to company and the achievement you made for yourself That is a big step to help you build confidence
7SwzFAGSGvTAnmnrYueCrd,In this post I want to show you how to export Salesforce data using Scala️The motivation to write this article was that writing my first Python script to interact with Salesforce APIs wasnt that easy The reasons could be: But leaving the past aside happens that recently I was asked to build another integration Now instead of upinserting data on Salesforce we needed to retrieve from there to our Data WarehouseAs I am also trying to learn more about Scala and practicing is everything I thought 🤔 why not?Going\xa0forward what well cover on this article\xa0in\xa0summary: In order to proceed well need: We know that one of the most problematic things when building an integration is the early steps when you need to see the data and understand it So I think that we can agree that handle access to the data is one of the hardest things to deal when building an ETLThats why I learned to keep it simple while I am just getting started So lets use curl to check if we are able to access the API Here we are assuming that you already have created the Connected App as described here and already have in hands the CLIENT_ID and CLIENT_SECRETInteresting thing about the login at Salesforce: Salesforce provides you a password of 32 chars length and you use the first 8 to login using the Web UI But when authenticating through the API you need to use the full password (32 length) I dont remember very well but I think that they inform you about that when the user is created Either way Its is a tricky thing to keep in mind and hard to remember if you are not working for the platform on your day-by-dayAll that said run the following code in your command lineIt should print out a Salesforce access tokenAs a programmer coming from Python one of the first things Ive quickly got was that with Scala everything starts with sbt Its everywhere So lets create a fresh new project using the scala-seed template as mentioned in the oficial sbt documentationsbt new sbt/scala-seedThis template comes with a Hello World example But here we are one step above so well ignore the content from the template and just preserve the folder structure In the next section well get deeper in the code and logicAs this is a very technical article get prepared to read code Below Ive embed the main files but you can find them all also at the github projectAt first take a look at the folder/files structure so you can have a better idea on how well proceedI recommend to start from here (the salesforceconf file) so we can already put a check on the Authorization stepsalesforceCopy /src/main/resources/salesforcesampleconf as /src/main/resources/salesforceconf and replace the sample values with real onesObjectExporterIts serves only as the entry point to our codeSObjectThis is the main file here It has all the logic regarding the API requests responses parsing and file outputUtilHere the code is almost the same as provide at the documentation example Its a class that has only one method called getAccessToken which request a token to do the authorized requests to the APIResponseTypesThis file has the case classes needed to handle the API responses using the Gson libraryTo build I usually use the sbt-assembly plugin that way we can have packaged all the dependencies into one single jar fileFollow the plugin setup instructions reload/restart sbt shell as needed then inside the sbt shell run assemblyAfter the build inside the SBT shell you can run: Or in the command line: It will print some debug messages while in the sbt shell and also because we have some println calls on the retrieveRecords method in the SObjectscala fileThe script may hang for a while depending on the internet connection and the size of the data on Salesforce At the end it will print a success message and well have the output file writtenOur approach here is not the standard ETL (Extract Transform and Load) way (but the ELT) We do not have a transform step It is just not needed for the purpose We opted out to manipulate the data from the data warehouseAlso we will not cover the load part in this article because it can vary depending on your choose of database or data lake implementationThe output of our script its a standard New Line Delimited Json file so you should have no big problems with thatBut one thing that can be a daunting is to map the Salesforce types to your database typesIf youve reached this point the following code will help you in the schema creationNow what we need to do is to call the method dumpSchema that we just defined to see the field type mappingIve created a github repository to share this adventure You can open an issue fork start a discussion or just ask about something clicking here Hope that this article somehow helped youand the Forcecom REST API Developer Guide (pdf)
S8YX7GEVByDQdaG9QdBHfM,This guide will tell you what is Google BigQuery how can it leverage your data analyses and what cool features Google BigQuery offers that makes it a perfect solution for data warehousingLets start first talking what is data warehouse: According to AWS: A data warehouse is a central repository of information that can be analyzed to make better informed decisionsSo basically it is to extract all the data you can from different sources and store them in a structured way in a central storage where you will be able to do queries and generate insights and take decisions from dataBigQuery is Googles fully managed petabyte scale low cost analytics data warehouseFully managed: Plug and play user doesnt have to care about nothing in terms of infrastructurePetabyte scale: Quickly analyze gigabytes to petabytes of dataLow cost analytics: Analyze up to 1 TB of data and store 10 GB of data for free each month So you can use it even without paying anything only data storage which is pretty cheapGoogle BigQuery has a lot of cool feature but its more remarkable are: One of the coolest things about Google BigQuery is its constant evolution and progress: In 2016 on GCP Next a query that represented a Petabyte took 2457s (about 4 min)In 2018 the same query over the same dataset took 1 min 53s The difference was not in the performance but in the way data was analyzed (data was clustered and partitioned on the dataset)In 2019 on Google Next only 11s! were needed for the same queryThese facts are explained in this video: The video also explains what is the difference between a traditional data warehousing and what BigQuery offers I will try to summarize that here: Traditional approach for data warehousing: This approach is built using an event handler (eg Kafka)+ data warehouse (DW eg Apache Hive) + transformation jobs pipelines (eg Hadoop Spark Impala) + Any analytic tool for doing queries and create dashboards (eg Hive + Tableau) This architecture can process data in batches (historic data stored) and also in real-time (streaming data coming from sensors) However there is crucial word that characterizes this approach: on-prem What this means? That someone will have to install all these technologies in your data center or in machine and pay for all these resources even if they are not being usedBigQuery modern data warehousing: So one of the coolest thing about BigQuery is the separation of storage and computing someone can scale up the storage infinitely and keep the same computing Also someone can scale up computing without having to scale storage (All this is possible thanks to Googles petabit network)It is also serverless It is ready to useAlso brings the possibility of doing real-time analyses (thanks to combine it with Google Dataflow) instead of the traditional approach where someone have to dump the data for a period of time to see what happened in that period (delayed results)Centralized storage: Data is accessible to all tools which can leverage it (BI ETL tools Hadoop ML tools and so on) There is a bunch of connectors that read in parallel without affecting one anotherSecurity: Detailed logs of what happenedSharing: Grant people access to your data (specially using views of queries and analyses)Predictive: BigQuery MLModels can be built based on the data in your data warehouseBigQuery ML: There are several algorithms already implemented someone can use to make predictionsIn the video they mentioned that they took the Netflix dataset for recommendations they used a Matrix factorization algorithm and obtained results more or less equivalent than the best results Only because they were able to process the whole dataset not because of the fanciness of the algorithmFor neural networks (Alpha) Google ships the data to Cloud ML Engine but all is transparent for the userAuto ML Tables: Someone can point a BigQuery Table and generate a model without having a special degree in ML Only setting what is the outcome that is wanted to be predictAutoML is a black box high accuracies can be obtained easily (in my experience with AutoML for images overfitting is easy to obtain too thats why a good validation and test strategy must be taken into account)BigQuery ML uses simpler models easier to understand Variables that produce the outcome can be also identified However it does require some knowledge in MLFull list of features: Google Warehousing Architecture: Use cases: Before Google BigQuery had in its documentation like four main use cases right now you can find a bunch of them here Lets summarize a little bit two of them which have the common steps involved in data engineering • Internet of things: Google Cloud IoT is a complete set of tools to connect process store and analyze data both at the edge and in the cloudUse cases: More info can be found here: Taken from: 2 Optimizing Large-Scale Ingestion of Analytics Events and Logs: In this architecture data originates from two possible sources: Benefits: Streaming input: Immediate analysis eg an event might indicate undesired client behavior or bad actorsBatch process: Events that need to be tracked and analyzed on an hourly or daily basisStreaming input: Critical logsCold path: Dont require near real-time analysisTaken from: Datasets: Commercial: Commercial data providers are accelerating your time to insight by hosting their data offerings directly in BigQuery Cloud Storage and Cloud Pub/SubExamples: Public: A public dataset is any dataset that is stored in BigQuery and made available to the general public through the Google Cloud Public Dataset ProgramCosts (March 24 2020): Active storage: $0020 per GBLong-term storage: $0010 per GB If a table is not edited for 90 consecutive days the price of storage for that table automatically drops by approximately 50 percentStreaming Inserts: $0010 per 200 MB You are charged for rows that are successfully inserted Individual rows are calculated using a 1 KB minimum sizeQueries (on-demand): First 1 TB per month is free $5Now what you are really interested in: Examples: Before loading a CSV lets review some important concepts: 11 Schemas: BigQuery allows you to specify a tables schema when you load data into a table and when you create an empty tableWhen you specify a table schema you must supply each columns name and data type You may optionally supply a columns description and modeA column name cannot use any of the following prefixes: These are reserved name of BigQuery architecture • 2 Data types: 13 Loading example: Lets download a CSV from any Kaggle challenge for instance: Click on: From that folder get the file: all_sources_metadata_2020 03 13Go to consolecloudgooglecom: Click on projects and create a new project: Set any name and then go to: https://consolecloudgoogleIf you dont a billing account you will have to set one before creating a dataset: Now we can create a dataset: The following overlay will appear: Lets call the dataset coronavirus And the remaining options can be left in their default option They are more related on how is the data is going to be storedOn the left click on: Now we can click on Create Table: Another overlay will appear: So normally we would select Create Table From: Upload However as our file has a size higher than 10MB we have to upload it from Cloud StorageGo to https://consolecloudgoogleCreate a bucket called datasets-any-other-unique-idYou can drag and drop now the file in the bucket Btw if it prompts an error of permissions just reload the pageClicking on the file name we can get the URI of the file we will use it to load the file in BigQuery: We are going to create the dataset in BigQuery using Auto Detect Table Schema which I dont recommend I wont get all the correct data types for your columns but its okay for this example: Click at the bottom in Create Table We will end with a table like this one: Also we will be able to watch a preview of the dataset in Preview: 14 Some queries: Lets check which dates are available for the articles in our datasetLets look for the word success in a abstract: And there are a bunch of utilities we can find there for stringsWe can also check how has the research been evolved in terms of the COVID-19: And which are the journal who have contributed more in the topic: BigQuery ML: Lets go to: And download the data essentialy trainAs we see in the plot we created a model that overfits perfectly of train data but what happens after we dont have more days? It seems pretty weird: First a model cannot be based only on the date more than that probably Date is not being interpreted in the model as sequence with monotonic increment So lets do that again with Date as a TIMESTAMP We will do the same queries but casting Date to TIMESTAMPCreating a model: Metrics: Looks better more credibleAnd for train and test data: We should submit that on Kaggle and maybe become rich! Im kidding we need still a lot of work here
DsK9HHWAYxoSE6twgeU7JP,When we make operations against our Cosmos Databases we use something called Request Units (RUs) which is what you use for throughput If youre writing an item to Cosmos you spend RUs If you read an item in Cosmos youre spending RUs If youre making a query in Cosmos you get the idea This is consistent no matter what API youre using for your Cosmos accountWhen we provision our containers and databases we set the amount of RUs that we want to reserve for capacity This has to be sufficient enough to ensure that our resources in Cosmos are available at all timesWell thats what this post is for! What Im attempting to do here is explain how throughput works in Cosmos DB and how you can optimize the design of your Cosmos DB solution to help you decrease the amount of Request Units your application uses to prevent throttlingFor this article Im going to use NET code samples against a Cosmos DB account that uses the Core API (SQL)Heres a high level overview of what RUs are: Request Units are the currency that we use to make operations against our Cosmos DB databases Its rate based and takes into account memory cpu usage and input/output operations No matter what API you use for your Cosmos account costs are measured in RUsWhen we provision RUs we provision it by increments of 100 RUs per second We can scale this at any time in increments or decrements of 100 RUs This can be done either programmatically (imo the cool way) or by using the portal We can provision throughput either on the Database level or on the Container levelLets say we provision a container with 400 RUs per second We can make 10 queries a second that cost 40 RUs Anything beyond that well start to experience some throttling and we should look to scaleIts a good idea to keep in mind how big your item is going to be When an item increases in size the number of RUs needed to write and read the item will increase as well This is also the case for how many properties an item has As our properties increase the RU cost will also increaseBy default every item is indexed Also by default every property within an item is indexed This allows for quick queries on any property but can be expensive when it comes to RU expenditure If we want to save RU cost for Cosmos Operations we can limit the number of indexed properties by defining our own indexing policyDefining our own indexing policy is fairly straight forward We can set the indexing mode and exclude and include property paths to indexIn Cosmos DB there are two indexing modes: We can implement custom indexing by deciding which property paths we want to include or exclude This can help us lower the amount of storage our container uses and improve our write operationsJust to give you a quick primer on how indexing works in Cosmos DB every item is projected as a JSON document and then converted into a tree like format Every property of an item gets represented as a node in a tree The root node would be created as a parent to all first-level properties of the item and then the leaf nodes would contain the scalar values carried in the itemIts probably a good idea to discuss this using an example: Lets say that Ive got a Cosmos DB with a Task collection Items within the Task collection has the following schema: The default indexing policy (where every property is indexed) would look like this: As you can see every property in our item is indexed apart from the etag (this is a default property that gets created with every Cosmos item) By default range indexes are enforced for any string or number property and spatial indexes are enforced for any GeoJSON object This provides us with fast query times but it can get expensive in terms of RU consumption and is overkill for our simple exampleSo for our Task items Im going to apply indexing on just the Task Name Not the best example and in production scenarios youll probably want to index on a few properties and perhaps even build composite indexing policies but for the purposes of this article its enough: So in our new custom indexing policy weve just included the /TaskName/? property of our item and excluded all other pathsIf I was to give you a bit of advice on indexing Id recommend applying indexing on properties that your code queries against In our example say we had a Azure Function that just queried the Task collection for names of tasks we would just index the TaskName property as Ive done in the example aboveI wont go too deep into Cosmos Data Consistency in this post but both strong and bounded staleness consistency levels will consume around two times more request units when performing read operations when we compare it to other consistency levels in CosmosThe more complex our Cosmos DB queries are well spend more request units This depends on a number of factors including how many results are returned how many predicates we use the size of the data etc The good thing about Cosmos DB is that provided the same query is used on the same data well spend the same amount of request units on that query no matter how many times we execute that queryStored Procedures and Triggers also consume RUs which depends on how complex the operations are when we they are executed To help us see how many RUs they consume we can inspect the request charge header to see how much they costLets take the following NET sample Here were executing a Stored Procedure connected to our collection and then using the RequestCharge property on our response to see how many RUs that particular stored procedure has consumed: Throughput in Cosmos DB is charged hourly regardless of whether you use it or not Monitoring your queries to see how many RUs they consume is an effective way to ensure that you have the right level of throughput provisioned on your container or databaseYou can also use the Cosmos DB SDKs to scale throughput as needed depending on your anticipated workload Say if youre doing most of your processing during a Monday at 5am in the morning you can programmatically increase the provisioned throughput via the Cosmos DB REST APIAs I mentioned earlier we can provision throughput either at a Database level or at a Container levelWhen we provision throughput at a database level all containers within that database will share the provisioned throughput This is a cheaper way of provisioning throughput but it comes at the expense of not receiving predictable performance on a specific containerWe cant selectively apply throughput to a specific container or logical partition as its shared unevenly among all our containers All containers within a database that has provisioned throughput must be created with a partition keyOne reason we might want to provision throughput at the database level is that we have a few collections (less than 10) in our database and we want to save on costs For example say if we have a database for errors and have a couple of collections for different types of errors and were rarely reading these items we would provision at the database level to save some moneyWhen we provision throughput on a container its reserved for that container (obviously) That means that container will receive that throughput all the time This throughput will be distributed uniformly across all logical partitions of the container but you cant specify the throughput for a particular logical partition If one of our workloads running on a logical partition consumes more than then allocated throughput well start to experience throttlingWe would specify throughput on a container in situations where we would want guaranteed performance that that particular containerWe can mix and match throughput provisioning If we have a container within a database that has throughput provisioned at the database level that we need to have guaranteed performance on we can scale the throughput provisioned on this container as and when we need itHaving a good partitioning strategy is key for throughput By ensuring that we have a partition key that isnt skewed we can prevent an issue called hot partitioning occurring This is essentially when one partition hogs the throughput when we run Cosmos operations against itFor example say we have a Order collection that is partitioned by name One customer has 10000 order items stored within our collection and all our other customers have 100 order items associated with them in the same collection the customer with 10000 orders would hog the logical partition and therefore hog most of the throughputHaving a partition key that has a wide range of values optimizes our query costs and can help us save on throughput costsFinally We can monitor throughput via the metric pane in Cosmos DB Through the Throughput tab were able to measure such metrics as: We can also set up alerts against these metrics using Azure alerts that can fire off emails to account administrators use a webhook to connect to an Azure Function that will automatically increase provisioned throughput on our container or database or even fire an alert that integrates with any IT Operations service that we may be using via Azure MonitorHopefully this article helps you in optimizing and provisioning throughput on your Azure Cosmos DB databases and containers Ive gone into more detail for the strategies Ive implemented in my day-to-day work and Ive noticed some significant savings in RU expenditure If you want to read more about optimizing throughput the Azure documentation is the best place for a detailed explanation into a variety of different strategies
NKV8YnqdsN227mtnimdR7s,Heres your guide to understand the working of data engineer and how you can jump on the data engineering bandwagonIn simple terms Data engineering is someone who builds an infrastructure for consuming data processing it and gaining insights on the data But in reality the role is so much more complexIn the current day the work of a data engineer in overshadowed by the emergence of Data Science In most cases the work of data scientists is reliant on the structured aggregated data generated from the data engineering workflow Any experienced Data Scientist will also have the skillset of a data engineer and know how to work alongside data engineers to get best results for data analysisSince the role of data engineers is evolving every day they needed to  learn multiple programming languages work on integrating variety of systems and learn emerging technologies and tools It is not easy to box the skillset for the role Ok that may sound like there a lot lets simply it and take small steps to unravel the path to Data Engineering We will start with the basic requirements to enter the fieldStructured Query Language has made it possible for us to interact and make sense of our data Every part of the infrastructure uses SQL in some capacityIn the data ingestion process if your source systems are relational database like SQL server MySQL etc or NoSQL database like HBase MongoDB or Cassandra you will need SQL to query the source system and make sense of the dataIf you are designing a Data Warehouse they need to use SQL to build transformations in order to modify the data and also build queries on the data stored in the warehouse to analyze itEven in the sense of using distributed systems tools like Hive and Spark use SQL to query the data on distributed clusters Different products may use different syntax of SQL (T SQL or PL/SQL or Spark SQL) but the underlying functionality is still the sameYou can start working on basics of SQL on this free codeacademy course or you can install MySQL and follow along their tutorialsIf you walked down the data engineering path a decade ago it was all about building efficient relational data warehouses ETL pipelines and generating reports for analyzing metricsThe principles of traditional data modeling are still reliant even though the process to structure of a Data Warehousing has changed Without going into the nitty gritty of design changes I want to talk about the principles that are followed in todays Data Warehouse development processData warehouse are central repositories of integrated data from one or more disparate sources They are considered a core component of data analysisThe fundamental principles for designing dimension and fact tables are still a basic requirement for anyone aspiring to be a Data Engineer This is the most strategic part of the job and designing the 100% efficient Data Warehouse comes only with experience I say strategic because you need a good understanding of the business requirements and how the end-users (could be data scientists business executives) would like to see their dataData Warehouse Tool Kit is a great book to learn about traditional dimensional modeling principlesFor a practical course to follow along you can start with this coursera courseWith the data engineer role evolving so quickly programming languages has become a necessity in order to build robust infrastructureI worked on building a pipeline to ingest log files and processed the data on a Spark cluster to extract credential attributes from the file and identify if they mapped to a registered user in the system The script for processing was written in PySpark which is basically a Python API for Spark I did not rely on additional services to do the validationAnother instance was when I was working on AWS cloud platform and wanted to process files as soon as they were dropped into S3 buckets So I wrote a custom code on Lambda service to trigger a function that would process the files when they appear in the specific bucket Since Lambda is serverless I skip the whole part of setting up infrastructure to run my codeThese are just a few instances but with programming languages you can hack your way through multiple big data systems and customize it to save time and costDistributed systems have significantly changed the landscape of data engineering pipelines Be it data processing or storage it has given way to make these processes work faster and scale up to hold large datasets If you dont have a basic understanding of the concepts of DS then using the tools will only get more complexBig data solutions typically involve a large amount of mostly non-relational data such as key-value data JSON files or time series data that cannot be processed by the traditional relational database systemAll of the Big data tools use the distributed architecture to process and store data Here is a list of some of the most commonly used and popular systemsHadoop Distributed File System or HDFS is a distributed file system that was created as a way of being able to horizontally scale out storage by adding commodity hardware Now there are numerous services available that provide the same functionality as MapR FS AWS S3 Azure Blob GCP Cloud StorageApache Spark is a good option for distributed processing framework It has the potential to replace the traditional ETL process while processing structured and unstructured dataApache Hive is an important part of the Hadoop ecosystem provides a way to project structure onto distributed data It uses SQL to generate results and can be helpful to generate insightsApache Kafka is a distributed stream-processing platform useful if you are building a real-time analytics pipelines or dashboardsThere are few other skills that will come in handy but is not a requirement to get started Knowing how to write UNIX scripts or use command line will be useful when working on big data infrastructure All cloud platforms have command line tools available that give you the ability to setup and work on their services if you dont want to use their GUIIts becoming increasingly prominent to have a shared environment to store infrastructure code or code related any part of development process Now data teams also rely on version control system like GIT or Microsoft TFS (team foundation server) Although this was not a requirement in the past when ETL tools took care of the version control with teams building their data infrastructure that spans across multiple systems and clouds this is now a requirementData engineering a vast field that takes care of the data when it leaves the software system until it goes into the hands of Data Scientists
8a7cxYAgZzbExm2eQYMVes,Liping Peng is a Senior Data Engineer at Wealthfront She grew up in China and attended the University of Massachusetts Amherst for her PhD degree in Computer Science After graduation she stayed in the US and now lives in Palo Alto In her free time Liping loves to travel  making sure she tries all of the local delicacies along the way and even makes sure to scoop up the latest pieces of fashion in each new place she exploresMy friend referred me to work at Wealthfront! While doing my research on the company I realized that there were many attractive things about joining the company and today I am still constantly impressed First I was (and still am!) amazed by the products Wealthfront delivers and I felt a strong connection to our mission and the way we deliver advice solely through technology Additionally I saw early on that working at a fintech company could force me to expand my knowledge about my own finances while giving me the opportunity to work and learn as a software engineer After seeing these things come through in my research I went onsite to interview I enjoyed the talks with all the interviewers I met They were friendly and sharp at the same time and all seemed to enjoy working here And last but not least I especially like the size of the company Its large enough to have clear functional divisions and small enough for employees to make a visible impact The growth opportunity for both the company and myself is promisingYou joined Wealthfront right after finishing your PhD in Computer ScienceThe transition was very smooth My manager even mentioned it in my first review cycle Wealthfront provides a series of onboarding sessions to warm you up for work that are extremely helpful Each new hire has a dedicated onboarding mentor too My mentor was always supportive and shared knowledge and skills with me without any reservation I am very lucky to report to a manager who sees the potential of individuals and stretches my comfort zone to set me up for career success without making me feel too uncomfortable I really appreciate what he has done for meIts definitely the near-real-time (NRT) platform project Before we built the NRT platform analytics jobs that required a low latency only ran on our in-house clusters To keep up with the growth rate of our business we needed to be able to support low-latency applications in a more scalable way without sacrificing the productivity and operability I am honored to have led this project which was staffed with ten people at the peak and spanned well over three months This was a great chance for me to grow both my technical and soft skills like leadership and communication Now we have streaming applications running on the cloud service giving us horizontal scalabilityI seldom wear sunscreen in CaliforniaWealthfront prepared this article for informational purposes and not as an offer recommendation or solicitation to buy or sell any security Wealthfront and its affiliates may rely on information from various sources we believe to be reliable (including clients and other third parties) but cannot guarantee its accuracy or completeness See our Full Disclosure for more important informationWealthfront and its affiliates do not provide tax advice and investors are encouraged to consult with their personal tax advisor Financial advisory and planning services are only provided to investors who become clients by way of a written agreement All investing involves risk including the possible loss of money you invest Past performance does not guarantee future performance
DS78aj8sPcHaq7rTGoxQi2,Megan grew up in southern New Jersey before receiving her Bachelor of Arts in Economics from Princeton University Prior to Wealthfront Megan worked at JRI America Inc supporting an international bank as an engineer on the Middleware team Currently she is a software engineer on the Data Platform teamYou previously worked at a large bank in New YorkAt Wealthfront I am able to own my projects from the design phase all the way to deployment in production At my former company we had scheduled software releases only a few times per year but now it is not uncommon for me to deploy to our production environment several times per week! This faster-paced environment has also created a ton of learning opportunities for me which is something I really valueWhile the work itself is fun being surrounded by really smart and motivated people has definitely been the best part of my experience at Wealthfront so farPrioritization  you cant do everything so how do you determine what the best way to allocate your time is given considerations such as business goals code maintenance and requests from other teams? At Wealthfront I have the opportunity to be much more involved in the process of evaluating trade-offs of working on one task or project versus another and I have learned a lot about the importance of being able to prioritize effectivelyI have always loved working with data which is one of the reasons I decided to major in economics during college  I love the feeling of having a huge lump of data which at first feels useless until you know the right way to gain insights! Currently Im most interested in the challenges that come with developing big data applications ie creating scalable reliable and maintainable systemsYou mentioned your major in college was EconomicsIt definitely wasnt easy but the decision to go into software engineering after graduation was a great one for me I took one programming class as an undergrad and really enjoyed it but had never considered engineering as a career When the time came to job hunt after graduation I realized that software engineering made the most sense for me because it would give me the opportunity both to continuously learn and to build something that has a positive impact on other peoples lives I was able to learn the ropes of software development at my first job after graduation and supplemented my knowledge with a few online courses to round out my skill set It was a very gradual transition that continuously challenged me (and still does!)One of the biggest challenges our team has is deciding which projects to prioritize over the next several quartersProgramming languages arent the only languages I love: Im absolutely obsessed with learning foreign languages and cultures So far Ive studied Mandarin Chinese Turkish and Spanish formally and most recently I started learning Russian and Japanese in my free timeThis blog has been prepared solely for informational purposes only Nothing in this material should be construed as tax advice a solicitation or offer or recommendation to buy or sell any security or financial product Wealthfront Software LLC (Wealthfront) offers a software-based financial advice engine that delivers automated financial planning tools to help users achieve better outcomes Investment management services are provided by Wealthfronts affiliate Wealthfront Advisers LLC an SEC registered investment adviser and brokerage related products are provided by Wealthfront Brokerage LLC a member of FINRA/SIPC Wealthfront Wealthfront Advisers LLC and Wealthfront Brokerage LLC are wholly owned subsidiaries of Wealthfront Corporation© 2019 Wealthfront Corporation All rights reserved
P2Qo3AFaFywkriUqoSVDEM,This article is based on the presentation I gave at the Data Council Conference in Barcelona in October 2019 titled A Federated Information Infrastructure that Works I went through the slides typing what I spoke over them edited the text and added some of the most relevant slides and code snippets in between paragraphsIn this article we describe how we provided easy access to data to local and central teams of Data Analysts and Data Scientists in AdevintaAdevinta is a leading online marketplaces specialist in 16 countries Our marketplaces help everyone and everything find new purpose: we help people find jobs; buy sell or rent apartments; buy and sell second hand items… In Spain we own very well know brands like Fotocasa Habitaclia Milanuncios Cochesnet or InfojobsWe also have a global services department split between Barcelona and Paris and this part of the organization is where I work at I manage a team of Data Engineers working on building and governing curated datasets (Business Intelligence at scale) for analytics purposesGiven this multi-tenancy set-up and the fact that both local and central teams need to make data-informed decisions we identified the following problems to solve: The journey of building the information architecture that could solve all the above problems started a couple of years ago and today we can proudly say that the majority of the problems have been solved During this journey weve identified three main challenges: Lets go into detail on how we solved each one of themIf we classify companies using the authority dimension we can distinguish between centralised and decentralised organizations In a decentralised organization all the authority is delegated to the different operations whereas in a centralised organization the authority resides in a central bodyDuring the last years in Adevinta we have switched our data strategy between both ends of the spectrum When I joined in 2013 the company (named Schibsted Classified Media back then) was a portfolio of completely decentralised operations Each brand was executing fast thanks to its autonomy but granular data sets for analytics or to build data products at a global scale were non-existentEach marketplace had their own storage system and some of them depending on their level of maturity had a data warehouse too On the other side the global source of truth was a simple corporate KPI database with very aggregated data that the different brands were sending via an APIA couple of years later the strategy of the company switched and we became a more centralised organization and so was our data strategy A central data platform was built with the goal of storing all companys datasets for consumers to use The problem with this approach was that data was too raw too dirty and often incomplete; which reduced its accessibilityThis made our data strategy to pivot again to a federated architecture in the middle of the authority spectrum We kept some things from the monolithic data platform like its physical storage AWS S3 and we used new approaches to become a federationIn this architecture which is the one we stabilised into each operation keeps their autonomy and local storage systems even data warehouses Data generated by common event tracking systems and shared components is cleansed globally and used to calculate metrics and to segment users This work is done once by a central team (ours) and provides the ability to compare and benchmark all the different tenants or operationsOperations can use these global data sets thanks to the downwards federation Each regional data warehouse is a different Redshift instance and thanks to Redshift Spectrum we can make available data from corporate data sources in the data lake which is in S3 very easily Central teams can query this same data using a global Athena instanceThis downwards federation works well because data is not physically duplicated: Redshift Spectrum and Athena are just views on top of S3 This has reduced enormously data quality discussionsIn order to scale our federated Business Intelligence architecture we embraced the concept of data sets as products as described in this article: How to Move Beyond a Monolithic Data Lake to a Distributed Data Mesh Quoting Zhamak Dehghani: For a distributed data platform to be successful domain data teams must apply product thinking with similar rigor to the datasets that they provide; considering their data assets as their products and the rest of the organizations data scientists ML and data engineers as their customersThe main characteristics a dataset as product needs to have are: Lets go into detail in each one of these characteristics and explain how we have solved each one of themIn order for datasets to be discoverable weve built a search engine of datasets on top of S3 The first version of the search engine just indexed all buckets and paths which was very good but it didnt have any proper governanceAfter several iterations we ended up with a dataset registry where you have to actively register your dataset so that it appears in the search This allowed to also request mandatory metadata fields at the moment of registration; which made addressability and search easier and powerfulHaving addressable datasets has made our teams that work with data more productive On one side Data Analysts and Data Scientists are autonomous in finding and using the data they need On the other side Data Engineers have far less interruptions from people asking where they can find data about XChecking data quality regularly and automatically is a must to fulfill the trustworthy characteristic of datasets as products And owners of the datasets need to react accordingly to the results of these checksAlso we knew that some people was not using the dashboards or the data because they didnt know if the data was accurate One success story we had in this aspect is the fact that we provided contextual data quality information to consumers of the data; like for example in Tableau dashboardsHaving this contextual data quality information has increased trust in the data and again made Analysts and Product Managers more autonomous in using the dashboards to make data-informed decisionsOnly creating and registering a new dataset is not enough for people to start using it As we commented before metadata is very important for adoption The characteristics that describe each dataset are: Some of this metadata is generated automatically but other points are requested to the creator and maintainer of the datasetCommon nomenclature is mandatory to make datasets inter operable Some basic rules we implemented are: Registered data sets should not be automatically available to everyone Employees need to request access to each one of them and data controllers need to grant or deny access individuallyWhen requesting access it is mandatory to specify until when the access is needed and for what purpose This has worked well for us because again it provides autonomy to our data usersAs stated earlier our team computes core business metrics for reporting and analytics While doing so weve realised that all metric calculations follow a simple pattern: So it made sense to abstract our code and provide Engineers with a simple framework to reuse and extend our core set of business metrics: With reusing this simple code we can calculate as many metrics as we want This configuration is then passed to the cube() function in Spark that calculates subtotals and a grand total for every permutation of the dimensions specified This way we have OLAP cubes already pre aggregatedAnother example of these reusable framework and libraries is the one to run user segmentation models By default we provide a standard user segmentation using a RFM model But we also provide the functions to run any type of segmentation This example is a custom example taken from a Jupyter notebook: This enables local and central teams to calculate their own user segmentation and share it with others again providing a lot of autonomy to every Data Analyst and Data Scientist in the organizationAs weve mentioned several times during the article one of the main achievements in building a federated information architecture is the autonomy weve given to data consumersThe non-invasive governance program we deployed and the development of datasets as products have been key successes in scaling our journey into being more data driven in all parts of the organization
J4Z5Cxng37PBd5ZWbxCUnk,How do you implement and test data pipelines with BigQuery to create intermediate tables and manage metadata and data discovery? I used to use Apache Airflows operators with BigQuery However I basically need to implement code in python and manage the dependencies between BigQuery tables manually As well as actually apache airflow enables us to test BigQuery tables with the CheckOperator  But we need to implement BigQuery queries even to test not-null or unique column It can be useful but not productive to me And then apache airfow doesnt support metadata management and data discoverydbt(data build tool) which is not only a cloud service but also an open source project enables analytics engineers to transform data in their warehouses by simply writing select statements dbt handles turning these select statements into tables and views The open sourced dbt is a CLI to run jobs to create tables and test the created tables It supports not only BigQuery but also postgres redshift and snowfrake at the time of writing the articleI would like to highlight the useful functions of dbt rather than explaining the detailed features and how to use it step by step If you would like to get started with dbt  Get started with BigQuery and dbt the easy way should guide youWhen we build multiple steps data pipelines with BigQuery it is bothersome to manage the dependencies between jobs For instance if we implement a DAG with apache airflow we have to manually implement steps Moreover if an intermediate table is used by multiple DAGs it is difficult to manage the triggersWhereas dbt offers ref() and source() macros that enables data analysts to refer to dependent tables Even if dependent tables like customer_orders depend on other tables we dont have to manually manage the order to run jobs dbt automatically analyze the dependencies and run jobs in the right order We dont need to manually manage dependencies So data analysts can build even complicated multiple steps jobs using only BigQuery queriesAs the image below show dbt execute models/jobs in the right order of the dependencies when running dbt run with jaffle_shop example locallyThere are some approaches to validate BigQuery tables and views such as not null column and uniqueness of a column For instance apache airflow provides the check operator for BigQuery As long as we express tests with BigQuery queries the operator enables data analysts to check the data quality However we have to implement the DAG in pythonMeanwhile dbt enables data analysts to validate not only intermediate tables and views but also even existing tables and views using only YAML files For instance the column customer_id should be unique and not null all we have to do is put unique and not_null in schemayml for the tableAs the image below show dbt execute models/jobs in the right order of the dependencies when running dbt testwith jaffle_shop example locallyAs I mentioned in Reason 1 dbt automatically analyze dependencies between tables (including existing tables) and view The web UI of dbt enables us to see data lineage from the dependencies We dont need to manage metadata of data lineageConsider if a table is used to build some intermediate tables and the table is something wrong We probably want to understand affected tables by using the origin table As we can see the image below we can analyze the impacted tables on the web UIdbt enables us to annotate and tag models (intermediate tables/views) and sources (existing tables/views) using only YAML files That would be helpful to select models to be partially run and search data on the web UI We can put documents and information about foreign keys to related tables with YAML files toodbt is based on Jinja which is a modern and designer-friendly templating language for Python So we can custom macros As well as dbt packages enables us to easily reuse external libraries in our dbt projectsdbt is a powerful tool to build data pipelines with BigQuery using only BigQuery queries and YAML files We no longer consider dependencies between tables and views manually As well as we can annotate and tag tables and views with dbt Moreover the web UI enables us to see and anlyze data lineage and to do data discovery We can locate metadata information with jobs to build data pipelines
iDw7xMeXWEEhUcNVfxnQQY,Someone on a chatroom Im in was asking about using Azure Databricks so I thought Id take a look at what it was all about for a project I was thinking of doingSecondly while I mentioned previously that Azure Notebooks was only good for prototyping I soon discovered the ability to connect an Azure Data Science VM to a Notebook which probably merits giving the Azure Notebook platform a second lookAs you know I have been quite interested in the volumes of passenger data generated by the Land Transport Authority of Singapore with regard to the usage of public buses and trainsA question came up through the analysis of that data  can I use it in order to find the most congested section of the MRT in terms of passengers? Theres two parts to this: We start with the path generator since its needed for the second bit (obviously) but this is the more boring bit Of course I could simply avoid writing my own path generator and just use Google Maps API calls or something but Im doing something on the order of 180² path calculations and I dont want to get banned from Google Maps for excessive API usage Besides while a self-made path generator would be computationally intensive I should only be doing this once or twice a year when new stations openThe idea is to build a representation of the MRT map in code for a Dijkstras pathfinding algorithm to walk while representing actual travel times in order to influence the route chosenThe actual MRT system is a lot more complex so lets explain the concept with an example Take this sample system with two lines ABC and DBEF Assuming a path is taken from A to E what we need is to determine the path that is taken  in this case A-B-EWe then combine them with the passengers travelling A-B and B-E in order to get the real loading on those two segments  but for B-E its not so easy Since passengers travelling B-F also have to pass through E well have to throw them in somehowConfusing? This is where the pathfinding algorithm comes in On my computer Ive generated the route data for getting between every pair of MRT stations The paths I have follow the fastest route between a pair of stations similarly to what Google or MyTransport app would tell you  although since using unpaid interchanges count as two trips the pathfinder will go the long way to keep you within the system You can ignore those or decide that people are weirdWith the question of routes settled now we talk about walking the paths and processing the dataI did it myself on my own computer but it took hours given that I run on a toaster approaching its fifth birthday That in mind lets look at what Azure offers us: The most user friendly one at first glance is Azure Databricks The setup wizard even deploys all the necessary compute resources for you so all you do inside Databricks is to write your code and run it One point for DatabricksHere though the devil is in the details Databricks uses Apache Spark as its main compute platform While it supports Python which Im more used to the use of Apache Spark means theres a learning curve to things Ive also found that Spark appears a lot slower for my workloads compared to running locally or in Azure NotebooksAzure Notebooks as mentioned previously provides a more visual platform for prototyping code compared to the Python shell with Jupyter notebooks a familiar environment for those who are trained on Anaconda (like me) The free compute tier can be quite slow and its useful for small prototyping work The big boys can arguably use something better  like how you can connect an Azure data science VM to Notebooks One point to Notebooks for familiarity another for extensibilityHowever Ive found that when doing personal work this can be quite troublesome All the DSVM provides is a JupyterHub server and you have to pull the credentials and other such connection information from the Azure portal for Azure Notebooks to connect to Its not as easy as Databricks where theres a big button in the Portal which you can just click on to get into your data workspace which then automatically enables all the compute resources it needsOf course Notebooks does have its convenience features to connect to a DSVM but you have to be using Azure Active Directory (another can of worms) to do so Thats obviously out of my league and no point setting that up So Ill just have to deal with thisIn short Notebooks 2 Databricks 1Now lets get back to this project If you came here from FTRL this is likely what youre interested inAfter letting the notebooks hum away in a datacenter somewhere what we get back is a bunch of Excel files These Excel files tell us how many people are predicted to use a train line between a pair of stations However since we dont have a surefire way to know who took what route for sure the data should ideally be moderated with those data collected by the folks that are hired to count people on platforms Without that data for now we can take what we have here with a pinch of salt but were doing all this for science anywayLike all other datasets created Ive also sectioned the data by hour From this we can see what are the MRT lines that are really in need of help because theyre reaching the point of overcrowding As a reminder an acceptable loading of passengers is 1600 for a 6-car train which we can scale down to 800 for a 3-car trainSo heres a quick summary as of December 2019: Maybe we can talk more about this on FTRL sometime later since theres a variety of operational issues we should look at Though for now remember that the numbers I quote arent distributed evenly across the entire hour it could be such that the travel patterns are heavily weighted in a certain direction But this is what LTA gives us so we have to deal with itAs usual source code and generated files are available here Dont worry all my prototyping was done in the Azure Notebooks copy of the repoI didnt really write this as a full guide on how to setup a Notebook with a VM or how to use Databricks but just as a personal reflection on what among the dizzying array of offerings on Azure that Ive found suits my projects betterPersonally as a hobbyist Id recommend using Notebooks Databricks costs a pretty penny and the language barrier means that your code may not make it through fully intact While the free tier of Notebooks can be pretty gimped the ability to attach a Data Science VM should you be able to pay for one also helps to make Notebooks far more extensible for future production workloadsOf course with a DSVM you should remember to turn it off when youre not using itFull disclosure: As a Microsoft Student Partner I receive $150 of monthly credit for Microsoft Azure That credit pays for the Azure resources Im using to work on this projectBut you can do (almost) everything I did with $100 of free Azure credit (and more) if youre a student with a valid school email address More details here
5L8ujyqK4w6XJNPV9hYfWz,Today Id like to talk about building serverless data lake on AWSThe reason of writing this post is to share my thinking with the world to get feedback about my prototype vision and at the same time to share experiences that may be of interest to data engineer practitioners and other peopleWhat Id like to do is to start with what a modern data lake pipeline looks like on AWSThe first thing is generation generating data sources The typical ways to generate data sources in traditional application is done by transaction legacy system ERP system web logs more and more like capturing information about consumers actually hitting the website sensor networks feeding data into data pipelineThe next part is collection side and you might see services like polling services running on EC2 going out to enterprise system to poll data from file systems or databases Modern system might use technology like AWS Firehose to poll real time data And you might also see AWS snowball which is used to transfer petabyte-scale dataOn storage side this is a very traditional piece S3 figures a large storage of data and as you see later in this story it figures large ETL processes as well You would see AWS RDS and customers write their own database on EC2AWS Glue is a fully managed extract transform and load (ETL) service that makes it easy for customers to prepare and load their data for analytics You might use services like Lambda S3 Cloudwatch to assist Glue crawler and job execution There will be discussion about ETL in this articleYou might see service like Athena which acts as a query engine against your data You might see AWS EMR a lot of users run these services like spark processes on top of EMR Some people might run Redshift & Redshift Spectrum while some people might run Kinesis to deal with real time datalastly we get the end consumer of data pipeline You might see business users to use data in their dashboard Business analysts who are doing things like tableau to do SQL queries as well So there are variety applications variety virtualisation tools end users can useWe have setup the data lake cloud pipeline and they are serverless and they looks very modern and matureThe challenges associate using these tools with ETL stage is the volume of data they are dealing with Some users have seen the situations where their data growth or data requirement or storage requirement have grown from a linear function to an exponential function They are talking about how many years it takes their data to double That has been driven by things like increasing time resolution of data increasing sources of dataAnother things is about disparate data You might get data coming from a general ecosystem not just from agency internal applications So you get a community of systems trying to fit data into a single data lakeYou may see a large volume of existing data from legacy systems or from any other different applications all generating data with different format within different times The challenge of ETL job is to simply them to a unified wayAs a consequence ETL stage will become a very large part in your data lake pipeline Many organisations create many complicated scripts in spark sql or event bash shell to help proceed ETL process And what we really want to be from the data is the analyst & consumer rewarding partWhat does AWS Glue try to solve is the downsize the ETL stage for organisations and systems It is probably the tool you want to use in terms of most of ETL requirementsWhat makes Glue so powerful The first thing is Glue can automatically discover your data and build catalog of data so it scans your data and put its catalog in a compatible meta data schema which can be used by Athena Redshift etcOnce it generates the catalog Glue makes it searchable by Athena EMR Redshift They can leverage the catalog and query the data directly from S3 There is no need to load the data into a database You can run the query against the data in place and the data catalog is the theory behindThe other thing Glue does is to generate the ETL code for you It generates clean logic enrich logic mapping logic for you The code is adaptable which means it uses Python or Scala extension to query/transform the data You can pull the code and run them in your choice of environmentMore importantly Glue runs all the jobs serverless which is a very powerful feature on deploying data lake on AWS When you run them in serverless the jobs like OS compatible installation setup infrastructure disappear which means these jobs are running on AWS cloud platformLastly Glue has the ability to schedule and trigger your jobs on your choice You can define a schedule on a regular basis or on demand You can also setup complex dependency chainLets go back to our theory how do I interact with my data? The typical is four steps In practice many organisations have multiple iterations goes on For now we just describe it as four steps to complete this sequentlyCrawling is actually a process to discover what schema is in your data It is a simply a matter of pointing Glue at the data in s3 bucket or database running on RDS for example telling you to find out what this schema is behind this Glue uses crawl to discover that schema and publish the meta data on its catalogMapping is a step to take your source/discovered schema to your target schema As a ETL designer you might have some schema in your mind to map to So you are going from source schema to a target schema And the map step can be as simple as renaming a few fields and doing type conversion or much complex task like restructuring your data doing things such as relationalising your data which isnt relationalised by its natureOnce youve actually done the mapping you can run query tools directly against it to discover more structures about your data And you can iterate this process build a chains of ETL processes to reach your target stateLastly you can schedule these jobs Once youve done with the on demand experimentation type of the process You are ready to deploy your things on production then you can schedule these on a regular basisLet me go through the steps to discover your data one by one I will highlight some theory behind so you prepare when we get into the demoAs we mentioned crawler automatically discover your data One of the key element that it has a whole of libraries to build up classifiers For example if you have decade old data format that was invented by someone long since retired and it was designed by human readability and not so much for automatic processing you can actually write customised classifier to parse that by using tools like regular expression And you can build them with your pipeline automaticallyClassifier is the logic crawler uses to explore your data store in order to define metadata tables in AWS Glue data catalog If the classifier recognises your data format it generates a schema for you Each time a crawler is executing it may use more than one classifiers to support its work each of which returns a value known as certainty indicating its percent certain about the schema it generates There are a few build-in classifiers you can use but you can also define your customised classifier if all build-in classifiers dont work for your dataRefer to https://docsawsamazoncom/glue/latest/dg/add-classifierhtml for more information about using classifiers in your crawlerYour data may be organised in a hierarchical directory structure based on the distinct values of one or more columns For example you might decide to partition your application logs in Amazon S3 by date  broken down by year month and day AWS Glue can use these partitions to filter data by value without making unnecessary calls to Amazon S3 This can significantly improve the performance of applications that need to read only a few partitionsThe ETL jobs is defined as a business logic that is performed by Glue job It links a target source table and transform the data into a target schema in a target catalog table After completing a Glue job you will probably have a new catalog table on Glue database as well as a target storage source like s3 bucket for storing the transformed dataA script is generated by Glue when you start a Glue job which is to transform the data and loads it to its target storage You can define a mapping table from source schema to your target schema in Glue job as blow The script is generated by the column mapping table which can be in Python or Scala language with Spark extension You can leverage all spark features to do your data transforming and reformatting and you can also setup a local environment to experiment your ETL process in a testing or dev environmentPlease refer to AWS doc https://docsawsamazoncom/glue/latest/dg/author-jobhtml for more information about Glue jobOnce the job is completed you can start querying the data through AthenaThe last thing you need to do is to keep your data in sync with data lake pipeline Your data source will receive data all the time The way how it works is basically to trigger Glue data pipeline whenever there is new data landing to your original data source There are many different ways to achieve this such as defining a cron job scheduling a cloudwatch event etcYou may want to have a look what the infrastructure looks like in order to build the whole data lake pipeline I have published a data lake project including IoC Glue related services You can find it from https://githubcom/zhaoyi0113/datalake-severless-pipelineWorking on data lake serverless architecture is quite different than other traditional application Sometimes you will find it is quite hard to follow the pipeline and it is even impossible to virtualise the whole process AWS offers us some kind of support on this part like using cloudwatch to monitor the service execution stats set up alert message or using SNS for notifying any failureshttps://github
QWgUVmTYENUSmrtCwvM45A,"A key component of our Kraken Public Data Infrastructure to automate ETL workflows for public water and street data is a cloud hosted instance of Apache AirflowTo understand the significance of Airflow to build a data infrastructure I recommend first reading this post authored by Maxime Beauchemin and his description of Data EngineeringA Data infrastructure is curiously analogous to water infrastructureConsider that untreated water from ground wells or snowpack require serious physical infrastructure in the form of pipes aqueducts and treatment facilities to guide its flow into the taps of our homes and businessesSimilarly water usage data that comes in different shapes and sizes from the various water retailers need to be refined towards powering a shared analytics platformAirflow is a platform to programmatically author schedule and monitor data pipelinesFor us this means automating a series of steps to securely Extract water data from the source Transforming this data by relying on a trusted community of data parsers and then Loading this refined data into the SCUBA Database that powers our core suite of analytics that are made available to the CaDCs subscribing utilitiesAirflow was originally built by Airbnbs data engineering team and subsequently open sourced into Apache AirflowARGO is one amongst many data organizations that use Airflow for core operationsTo that end we wanted to give back to the community and ensure that installing this fine piece of software is accessible by more purpose-driven and public data organizationsWhat follows is a complete step-by-step installation of Apache Airflow on AWSFull credit to putting these instructions together goes to our Public Data Warrior Xia Wang who was key to researching testing and implementing Airflow in its early stagesWe are not going into detail on how to create an AWS instance Amazons instructions provide thatWe use a Linux Ubuntu Server 1604 LTS  It is also important to create at least a t2medium type AWS instance Anything smaller is not recommendedAssuming we have a clean slate ubuntu server First we need to install python and the python package management tool pipTo install Python 2There 2 commands will give us the bare minimum to kickstart the airflow installationAirflow is shipped with a sqlite database backend But to be able to run the data pipeline on the webUI we need to have a more powerful database backend and configure the database so that airflow has access to it In our case we decided to install the postgresql databaseSo far as we know the most recent versions of postgresql (8 and 9) dont have compatibility issues with airflow""Now that weve installed the postgresql database we need to create a database for airfow and grant access to the EC2 userThen we will receive a psql prompt that looks like postgres=# We can type in sql queries to add a new user (ubuntu in our case) and grant it privileges to the databasewill tell us the connection informationOne last thing we need to configure for the postgresql database is to change the settings in pg_hbaconf""will tell return the location of the pg_hbaconf file (it's likely in /etc/postgresql/9*/main/) Open the file with a text editor (vi emacs or nano) and change the ipv4 address to 0000/0 and the ipv4 connection method from md5 (password) to trust if you don't want to use a password to connect to the database In the meantime we also need to configure the postgresql""listen_addresses = '*'Next step is to install the system and python packages for airflow A general tip: if an error message arises during the installation pay attention to which package failed the process and try to install the dependency for that package and try againFirst install the following dependencies: After installing these dependencies we can install airflow and its packages (You can modify these packages depending on need"
FMaYD5yyDzPivKxtjmxAAw,Originally published by our friends at AstronomerIn 2015 five NYU graduates founded the Advanced Research in Government Operations nonprofit  the worlds first data utility ARGO Labs hopes to bring a change to an Excel for everything mindset in local government and shift the paradigm using data from reactive to proactive operationsARGO is different than typical top-down innovations that look great on a corporations resume but give little ownership to the city itself Their vision as a non-profit is to collaborate not just on a project level but also on a technical level operating more like delivery agents than consultants Thats something that enterprise organizations dealing with proprietary technology simply cant do So when ARGO began exploring the technology required to build operate and maintain data infrastructure in the public sector its no surprise they landed on Apache AirflowFor ARGO Airflow checks multiple boxes One: its open source so ownership can be handed off to the client while ensuring transparency and reusability Open source components arent just open though; theyre also groundbreaking reliable transparent diverse and agile (dig into Astronomers CTOs blog post to find out why) Which means theyre best in class another important criterion Check and checkFor the operation to be truly successful however ARGO cant just build great data infrastructure and let it run Nor can they afford to maintain it indefinitely while they continue to power new initiatives But if they create data pipes that are defined in code  neat lines of Python not drag-and-drop ETL spaghetti  any engineer can maintain port extend and collaborate with it anytime Code-based pipes are a core principle of Apache Airflow Control for the client: checkFurthermore Apache Airflow through its web-based user interface allows clients to actually see the data pipes in action which is crucial to building trust and co-creating data infrastructure with municipal managers Unlike slick websites or apps that do little to change how the underlying data is managed with an organization Airflows reliance on a structured and open-ended approach to data workflow management raises the bar for how stakeholders manage their digital processesAlso worth mentioning: public sector projects cost tax dollars So while cities will sometimes shell out millions for an initiative ARGO is determined to remain affordable Apache Airflow makes that possibleEqually important however are Apache Airflows technical chops ARGO co-founder Varun Adibhatla puts it like this: At the end of day were plumbers We only work when data flows So we sat down with Varun to find out what that looks like for ARGOTo kick off their first initiative powering a coalition of water managers in California Team ARGO headed west in the throes of a drought of historic proportions They quickly realized that a trove of water data existed but when it came to connecting the dots on how to respond to the drought through proactive conservation methods went largely unused Artisanal data projects were happening ad-hoc but little was integrated into a common operating system Worse the inherent fragmentation of how water is managed in California was magnified in its data Such asymmetry has disproportionate effects on millions of CaliforniansSo ARGO organized a group of local water utilities willing to pioneer a bold new approach to use data in a shared manner The work included ingesting standardizing and refining the fragmented water use data which came in all forms This disparate data need to be refined into a stable and well-maintained structure ARGO using Apache Airflow created a data parsing workflow to clean and refine water use data across 14 Californian water utilities and load it into a secure database to power analytics But well let Varun tell us about it in his own words: The ARGO crew believes that public services on a whole are headed for what we call a Digital Tsunami Digital Tsunami is a combination of several waves of change brought about the prolonged culture of Excel for everything a lack of standardized data infrastructure across public services and a fundamental shift in how we as citizens engage with government servicesWe believe that local public agencies cannot on their own manage the transitions needed to successfully change to a digitally native organization They need an independent and unbiased data utility to experiment and deliver in this new world ARGO exists to help municipalities all over surf this Digital Tsunami through a delivery of open and imaginative data and analyticsI spent the better part of a decade orchestrating data in Wall Street during the Financial crisis and became skilled in the efficient movement of financial data What I realized towards the end of my career in Wall Street was that back in the early 90s big banks got together and agreed to speak in a common transactional language This was called the Financial Information Exchange or FIX protocol FIX protocol is the glue that allows major financial institutions share data quickly These data standards I later found out are not unique to Wall Street but exist in almost every sphere of trade and commerce (Ex: air traffic control cell phone number portability etc) More intriguingly many of these protocols are managed by unbiased neutral and non-profit organizations (I write about this in detail in  Bringing Wall Street and the US Army to bear on public service deliveryMy shift to the public sector was motivated by a personal desire to repurpose my data engineering skills towards positive impact but also a realization that vast swathes of public services lack standardized protocols to communicate in the digital realmA California state of mind and the willingness of a few bold public water utilities to try something new was what helped us get off the ground We owe our existence to these public leaders Patrick Atwater part of the core ARGO crew and project manager of the CaDC is a fifth generation Californian and was already deeply invested in the effects of drought and water management before arriving at NYU CUSP He also co-authored a childrens book about how the drought impacts everyday life in CaliforniaAbsolutely not! We were just getting started with Amazon Web Services standing up databases and data parsing infrastructure on the cloud We were very fortunate to get early support from the inspiring data organization Enigma whose Chief Strategy Officer and NYCs first Chief Analytics Officer saw what we wanted to do and supported our mission by offering Engimas data and infrastructure Jedis This was critical to initially scoping the challenge ahead of usWhile evaluating Enigmas ParseKit product for our needs we stumbled upon Apache Airflow via Maxime Beauchemins Medium post The Rise of the Data Engineer It was then I realized the potential of AirflowWhile in Wall Street I spent many years using Autosys an enterprise workload automation product developed by Computer Associates aka CA Technologies that was used by many big banks at the time What was different about Airflow and clear from Maximes excellent description was that it was developed by data engineers for data engineering (ie the frictionless movement of data)The fact that it was open source was the cherry on top I also want to take this opportunity to thank Maxime and the entire deployment team responsible for Airflow 180 that came just in time for usIn addition to creating the necessary technology and delivering results quickly we needed to manufacture and preserve trust across our local utility partners This can be especially challenging when most of the value thats being generated goes unnoticed so the burden was on us to find creative ways to message the heavy data pipingMoreover many of our utility partners were consciously going against political convenience in supporting our effort Preserving goodwill and trust across this complex landscape was challenging to our 45-person strong outfit (05 because we relied heavily on heroic volunteer researchers to help us deliver) In meeting these challenges we ended up creating a special community of purposeful water data warriors who are truly committed to seeing that water systems are better managedWe presented our trials and tribulations at the 2016 Bloomberg Data for Good Exchange conference titled Transforming how Water is managed in the WestWe call it the Kraken because ETL has been this mythical beast of a problem in the public sector as portrayed by Dave Guarino Senior Software Engineer for Code for America His ETL for America post really shed the light on the intractability of implementing cross-platform ETL in the public sectorApache Airflow allowed us to ingest parse and refine water use data from any water utility in any (tabular) format using any language we wanted In addition to using PythonOperators to handle most of our data parsing we use BashOperators and SSHExectuteOperators (to move data between machines) PostgresOperators SlackOperators and CheckOperatorsWe are also conscious of operating in an eTl environment where we are light on e and l as they do not involve time-sensitive ingestion or loading  and instead emphasize the T as our value lies in parsing data from different shapes into a single shape to help power analyticsA capital E and L imples ingesting real-time streaming data and loading it in highly available possibly NoSQL databases We are not there yet and understanding this current need has helped us build consciously and deliver our core suite of analytics These include the State efficiency explorer and Neighborhood efficiency explorer that reflect the diversity of local conditions while evaluating statewide conservation policies and programsOur data infrastructure also powers a rate modeling tool to illustrate how the shifts in water rates impact customers bills and utilities revenue This helps water utility managers navigate an environment of uncertain water futures and implement conservation and planning programs that can adapt to the on-the-ground realityA recent and significant benefit was realized by one of our leading utility partners Moulton Niguel Water District MNWD who were able to save up to $20 million in recycled water investments The forecasting tools and ability to access accurate data in a timely manner was key to realizing thisThis Airflow-powered data infrastructure provides key planning benefits which are mission-critical so California can adapt to changing water conditionsThe SQUID project was conceived in Fall 2015 and involves collecting and integrating street surface imagery and ride quality data applying computer vision and image processing techniques towards rapidly measuring the overall quality of a citys streets and bike lane infrastructureAnswering this question we believe is key to prepare cities for a future that amongst other things includes autonomous vehicles To end Ill give you a sneak peek of how were addressing city streets
RKeLYpHMtLkpXqiHkpL9nu,A few days ago another event from Elastic{on} Tour series took place in Frankfurt welcoming around 200 developers/architects or whoever likes data engineeringIf you are not familiar with Elastic{on} Tour series short intro for you: It is a smaller one-day version of original Elastic{on} which began with the idea to bring Elastic{on} closer to its fans all around the world Understandably  the original Elastic{on} is located in San Francisco and usually takes 3 daysEven though Elastic{on} Tour in Frankfurt took just single day the content was very concentrated and every single minute was worth of visitingThe topics of talks were sorted into three categories  the news from Elasticsearch and its ecosystem the case studies and how-to/best practices Apart from talks the Ask-me-anything booth with a dozen of Elastic tech guys present was the right place to discuss problems or get insider info about new stuffAs the list of new things is quite long here are just the most interesting ones: The most interesting use case was presented by guys from Mayr-Melnhof Karton company MM Karton is worlds major cardboard producer with plants all over the world and as they are using modern machines full of various sensors the idea of gathering data into one single data store with real-time monitoring of production process comes somehow naturally
Z5ck7Muxj74XQSYPVJWBRt,Grafana is an open-source nightly built dashboarding analytics and monitoring platform that is tinkered for connection with a variety of sources like Elasticsearch Influxdb Graphite Prometheus AWS Cloud Watch and many othersOne of the biggest highlights of Grafana is the ability to bring several data sources together in one dashboard with adding rows that will host individual panels (each with visual type)Among the other pros are: However Grafana also has some cons: First add rpm repo: Bash: Then install using yum: Bash: Installing via repo adds system unit for running in daemon modeTo run as daemon do the following: Bash: If you want to enable auto-startup on the boot run: Bash: All defaults for running are configured in environment variables in /etc/sysconfig/grafana-server: As to Grafana configurations everything is listed (including defaults) in /etc/grafana/grafanainiIf you want to access Grafana from outside (not localhost only) set http_addr config to bind to all interfaces explicitly or leave it blank to do the same thing implicitlyIf Grafana is still inaccessible make sure that the firewall does not block traffic on Grafanas portTo add a port to allowed use: Bash: Then check it in: Bash: Installation of plugins may cause several troubles due to incompatibilities of Grafana versions The most common problem is that the plugin is installed but not detected and thus not usable To avoid such situation it is better to follow the next steps to install a pluginFirst make sure that `/var/lib/grafana/` folder is owned by Grafana user and has all permissions If not then run: Bash: Stop Grafana: Bash: Make sure to clear cache in the browser from which you access GrafanaThen install a plugin using the cli utility: Bash: Check that your plugin installed successfully: Bash: After that start Grafana up again: Bash: Check your installation in a browserFirst you will need to log in By default Grafana creates an admin user with admin password on startup (maybe changed in /etc/grafana/grafanaini)As you log in you will be prompted to connect to your first data sourceWe will connect to an instance of the elasticsearch cluster: Just fill all the fields to connect to the cluster Note that in place of the index name you may specify the pattern for lookupThen you may proceed to create the dashboardYou will end up with an empty dashboard with one row In that row you can put any panels you want  each panel is responsible for one visualChoose the type of panel and explore the ways to interact with it Important note: in the upper left corner you must choose the time interval for which the data will be used and displayed (in our case we set up the time field to be the one from data itself; you may use logstash @timestamp or anything you wish)The tabs of the Graph are grouping the settings logically so they are easy to discover The way data is queried for building visual is all mirrored in Metrics tab The way the visual looks is set in Display tab Save the dashboard by clicking diskette sign on the top and giving it concise name That is it Build cool dashboards fastIn this article we highlighted key pros and cons of Grafana We have also given a brief and simple instruction on Grafana installation and starting operation Follow these simple steps and get the benefits of Grafana application Build engagement and appealing dashboards to make your data easy for comprehension in a few clicksOriginally published in activewizards
Jv5noqvckf3epHG3Z8Zqyu,Kafka monitoring is an important and widespread operation which is used for the optimization of the Kafka deployment This process may be smooth and efficient for you by applying one of the existing monitoring solutions instead of building your own Lets say we use solution with Apache Kafka for message transfer and processing on our project cluster and we want to monitor itFortunately Kafka developers give us such an opportunity In this article we will give you some hints related to installation setup and running of such monitoring solutions as Prometheus Telegraf and Grafana as well as their brief descriptions with examples As a result well see the system Kafka Broker Kafka Consumer and Kafka Producer metrics on our dashboard on Grafana sideKafka is an open-source stream-processing software platform written in Scala and Java The general aim is to provide a unified high-throughput low-latency platform for real-time handling of data feeds The storage layer of the software platform makes it extremely beneficial for businesses in terms of processing the streaming data Moreover Kafka is capable to connect to the external systems via Kafka Connect Apache Kafka provides you with opportunities: Prometheus JMX exporter is a collector designed for scraping and exposing mBeans of a JMX target It runs as a Java agent as well as an independent HTTP server The JMX exporter can export from various applications and efficiently work with your matrixWell use Prometheus JMX exporter for scraping Kafka Broker Kafka Consumer and Kafka Producer metrics Java and Zookeeper should be already installed and runningBash: 2 Download Prometheus JMX exporter: Bash: 3 Edit Prometheus JMX exporter config file Well append it with Kafka Consumer and Kafka Producer scraping query: More accessible queries defined by Confluent hereNow we are fully prepared to start Kafkas services with Jolokia JVM agent This material is just an example so here well run the console version of Kafka Consumer and Kafka Producer But you can run Jolokia Agent with own consumer and producer based on JVMBash: 2 Start Kafka Consumer: Bash: 3 Start Kafka Producer: Bash: Prometheus is an open-source time series monitoring solution with pull-model collecting storage flexible query language and high-throughput availability Prometheus has a simple and powerful model allowing to carry out analysis of infrastructure performance Prometheus text format allows the system to focus on core features Thus Prometheus proves to be very performative efficient and easy to runBash: 2 Download and install Prometheus: Bash: 3 Append /etc/prometheus/prometheusyml for needed exporters: 4 Prometheus systemd service (/etc/systemd/system/prometheusservice): 5 Start Prometheus: Bash: Telegraf is a powerful open-source data collecting agent written in Go It collects performance metrics of the system and services Telegraf provides opportunity to monitor process and push data to many different services This agent has some beneficial peculiarities making it a good choice in terms of data collecting and reporting: Bash: 2 Install Telegraf: Bash: 3 Change config: comment out InfluxDB output and then append outputs list with Prometheus exporter • Run Telegraf: Bash: Grafana is a popular fully-featured open-source frontend dashboard solution This is a visualization tool designed to work with a variety of data sources like Graphite InfluxDB Elasticsearch etc This solution allows the fast and easy development of dashboards for users Key functional opportunities provided by Grafana are as follows: Bash: 2 Install Grafana: Bash: 3 Run Grafana: Bash: Now well see metrics on Prometheus side and also prepare Grafana DashboardOpen http://localhost:9090/graph and start to explore with kafka prefixIn this article we attempted to compile short and comprehensive guide on the installation setup and running of such monitoring solutions as Prometheus Telegraf and Grafana These solutions prove to be very efficient in collecting metrics preventing problems and keeping you alert in case of emergenciesOriginally published in activewizards
chvWDKjaYwS3dmQk7tDmJ5,So they told you that NoSQL is the place where to store your de-normalized ex-relational (big) data right? And you believed them didnt you? Well that could not have been your worst decision as long as you have thought about a plan B to apply when the first users  coming right after your data migration has been completed  will ask you: I dont have the key to access the key-value storeIf your data system must provide massive alongside with punctual access to small piece of data in a lake of potentially (b)millions of records sequential flows like file systems (even if distributed) are not the right choice: thats why key-value stores backed by a distributed and scalable architecture gained so much success in the last few years especially when companies facing re-platforms of their relational data systems approached the Big Data worldUnfortunately the lack of efficient access patterns (a metric which can obviously be different use case by use case) different from the key-value one usually brought to light a sad piece of news: therere no secondary indexes! WHAT?! Well Im afraid so At least not usually built-in in most of the broad-adopted technologies like Apache HBaseIn most of the projects we worked on at AgileLab we leveraged Apache HBase to tackle the problem of having a consistent distributed fault tolerant and PetaByte-scalable data base where to store unstructured or relational-denormalized data in key-value fashion
4w7nBJqYoBkzijUu4pivur,In this tutorial We will learn how to configure the Ubuntu 1604 OS on AWS (Amazon Web Service) EC2 (Elastic Cloud Compute) Instance from scratchLaunch the instance by clicking on Launch You will be prompted to create and download a private key This key will allow you to connect to your instances with SSH If you dont download this or delete it somehow you wont be able to connect to your cluster If you lose it dont panic but youll have to shut down the instances and start up new ones Name it whatever you like I just used MyLab_Machine Click View Instances to see your instance booting up in the EC2 dashboard Youll want to write down the public hostname called Public DNS on the instance panelYou are logged in nowNote: If you forget to terminate the instance when not in use you could be hit with real sticker shock when you get your next invoice
6jM5XX7W4T3UsjoJNCCxs8,We will try to create an image from an existing AWS EC2 instance after installing Java and Hadoop on it If there is no instance created yet create one and login to the instance using this articleNow the Java and Hadoop are installed We will declare the environmental variables in the instance which helps applications locate hadoopFor Java: For Hadoop: For Hadoop Configuration directory: For reflecting to current session with out restartingCheck whether the environmental variables are available or notCluster-wide configuration: NameNode specific configuration: DataNode specific configura: You can access the NameNode WebUI
7XYUUhJGqcnZgftLHzZFHy,Labor day weekend is just around the corner! Karim is amped for a well deserved vacation He logs in to Airbnb to start planning a trip to San Francisco and stumbles upon a great listing hosted by Dany He books itA moment later Dany receives a notification that his home has been booked He checks his listing calendar and sure enough those dates are reserved He also notices the recommended daily price has increased for that time period Hmm must be a lot of folks looking to visit the city over that time he mumbles Dany marks his listing as available for the rest of that weekAll the way on the east coast Sara is sipping tea in her cozy Chelsea apartment in New York preparing for a business trip to her companys HQ in San Francisco Shes been out of luck for a while and about to take a break when Danys listing pops up on her search mapAdapting to data evolution has presented itself as a recurrent need for many emerging applications at Airbnb over the last few years The above scenario depicts examples of that where dynamic pricing availability and reservation workflows need to react to changes from different components in our system in near real-time From an infrastructure perspective designing our architecture to scale is a necessity as we continually grow both in terms of data and number of services Yet as part of striving towards a service-oriented architecture an efficient manner of propagating meaningful data model mutations between microservices while maintaining a decoupled architecture that preserved data ownership boundaries was just as importantIn response we created SpinalTap; a scalable performant reliable lossless Change Data Capture service capable of detecting data mutations with low latency across different data source types and propagating them as standardized events to consumers downstream SpinalTap has become an integral component in Airbnbs infrastructure and derived data processing platform on which several critical pipelines rely In this blog we will present an overview of the system architecture use cases guarantees and how it was designed to scaleChange Data Capture (CDC) is a design pattern that enables capturing changes to data and notifying actors so they can react accordingly This follows a publish-subscribe model where change to a data set is the topic of interestCertain high-level requirements for the system were desirable to accommodate for our use cases: There are several solutions promoted in literature for building a CDC system the most referenced of which are: There are several desirable features of employing the database changelog for detecting changes: reading from the logs allows for an asynchronous non-intrusive approach to capturing changes as compared to triggers and polling strategies It also supports strong consistency and ordering guarantees on commit time and retains transaction boundary information both of which are not achievable with dual writes This allows to replay events from a certain point-in-time With this in mind SpinalTap was designed based on this approachAt a high-level SpinalTap was designed to be a general purpose solution that abstracts the change capture workflow enough to be easily adaptable with different infrastructure dependencies (data stores event bus consumer services) The architecture is comprised of 3 main components that aid in providing sufficient abstraction to achieve these qualities: The source represents the origin of the change event stream from a specific data store The source abstraction can be easily extended with different data source types as long as there is an accessible changelog to stream events from Events parsed from the changelog are filtered processed and transformed to corresponding mutations A mutation is an application layer construct that represents a single change (insert update or delete) to a data entity It includes the entity values before & after the change a globally unique identifier transaction information and metadata derived from the originating source event The source is also responsible for detecting data schema evolution and propagating the schema information accordingly with the corresponding mutations This is important to ensure consistency when deserializing the entity values on the client side or replaying events from an earlier stateThe destination represents a sink for mutations after being processed and converted to standardized event The destination also keeps track of the last successfully published mutation which is employed to derive the source state position to checkpoint on The component abstracts away the transport medium and format used At Airbnb we employ Apache Kafka as event bus given its wide usage within our infrastructure Apache Thrift is used as the data format to offer a standardized mutation schema definition and cross-language support (Ruby & Java) A major performance bottleneck identified through benchmarking on the system was mutation publishing The situation was aggravated given our system settings were chosen to favor strong consistency over latency To relieve the situation we incorporated a few optimizations: Buffered Destination: To avoid the source being blocked while waiting for mutations to be published we employ an in-memory bounded queue to buffer events emitted from the source (consumer-producer pattern) The source would add events to the buffer while the destination is publishing the mutations Once available the destination would drain the buffer and process the next batch of mutationsDestination Pool: For sources that display erratic spiky behavior in incoming event rate the in-memory buffer gets saturated occasionally causing intermittent degradation in performance To relieve the system from irregular load patterns we employed application-level partitioning of the source events to a configurable set of buffered destinations managed by a thread pool Events are multiplexed to thread destinations while retaining the ordering schema This enabled us to achieve high throughput while not compromising latency or consistencyThe pipe coordinates the workflow between a given source and destination It represents the basic unit of parallelism Its also responsible for periodically checkpointing source state and managing the lifecycle of event streaming In case of erroneous behavior the pipe performs graceful shutdown and initiates the failure recovery process A keep-alive mechanism is employed to ensure source streaming is restarted in event of failure according to last state checkpoint This allows to auto-remediate from intermittent failures while maintaining data integrity The pipe manager is responsible for creating updating and removing pipes as well as the pipe lifecycle (start/stop) on a given cluster node It also ensures any changes to pipe configuration are propagated accordingly in run-timeTo achieve certain desirable architectural aspects  such as scalability fault-tolerance and isolation  we adopted a cluster management framework (Apache Helix) to coordinate distribution of stream processing across compute resources This helped us achieve deterministic load balancing and horizontal scaling with automatic redistribution of source processors across the cluster To promote high availability with configurable fault tolerance each source is appointed a certain subset of cluster nodes to process event streaming We use a Leader-Standby state model where only one node streams events from a source at any given point while the remaining nodes in the sub cluster are on standby If the leader is down then one of the standby nodes will assume leadershipTo support isolation between source type processing each node in the cluster is tagged with the source type(s) that can be delegated to it Stream processing is distributed across cluster nodes while maintaining this isolation criteriaFor resolving inconsistencies from network partition in particular the case where more than one node assume leadership over streaming from a specific source (split brain) we maintain a global leader epoch per source that is atomically incremented on leader transition The leader epoch is propagated with each mutation and inconsistencies are consequently mitigated with client-side filtering by disregarding events that have a smaller epoch than the latest observedCertain guarantees were essential for the system to uphold to accommodate for all downstream uses casesData Integrity: The system maintains an at-least-once delivery guarantee where any change to the underlying data store is eventually propagated to clients This dictates that no event present in the changelog is permanently lost and is delivered within the time window specified by our SLA We also ensure there is no data corruption incurred and mutation content maintains parity that of the source event Event Ordering: Ordering is enforced according to the defined partitioning scheme We maintain ordering per data record (row) ie all changes to a specific row in a given database table will be received in commit orderTimeline Consistency: Being consistent across a timeline demands that changes are received chronologically within a given time frame ie two sequences of a given mutation set are not sent interleaved A split brain scenario can potentially compromise this guarantee but is mitigated with epoch fencing as explained earlierJustifying there is no breach in SpinalTaps guarantees by virtue of design was not sufficient and we wanted a more pragmatic data-driven approach to validate our assumptions To address this we developed a continuous online end-to-end validation pipeline responsible for validating the mutations received on the consumer side against the source of truth and asserting no erroneous behavior is detected in both pre-production and production environmentsTo achieve a reliable validation workflow consumed mutations are partitioned and stored on local disk with the same partitioning scheme applied to source events Once all mutations corresponding to events of a partition are received the partition file is validated against with the originating source partition through a list of tests that asserted the guarantees described earlier For MySQL specifically the binlog file was considered a clean partition boundary  We set up offline integration testing in a sandbox environment to prevent any regression from being deployed to production The validator is also employed online in production by consuming live events for each source stream This aids as a safeguard to detect any breaches that are not caught within our testing pipeline and automatically remediate by rolling back source state to a previous checkpoint This enforces that streaming does not proceed until any issues are resolved and eventually guarantee consistency and data integrityA shortcomings of consumer services tapping directly into SpinalTap events for a given services database is that the data schema is leaked creating unnecessary coupling Furthermore domain logic for processing data mutations encapsulated in the owning service needs to be replicated to consumer services as well To mitigate the situation we built a model streaming library on top of SpinalTap which allowed services to listen to events from a services data store transform them to domain model mutations and re-inject them in the message bus This effectively allowed data model mutations to become part of the services interface and segregation of the request/response cycle from asynchronous data ingestion and event propagation It also helped decouple domain dependencies facilitate event-driven communication and provide performance & fault tolerance improvements to services by isolating synchronous & asynchronous application workflowsSpinalTap is employed for numerous use cases within our infrastructure the most prominent of which are: Cache Invalidation: A common application for CDC systems is cache invalidation where changes to the backing data store are detected by a cache invalidator service or process that consequently evicts (or updates) the corresponding cache entries Preferring an asynchronous approach allowed us to decouple our caching mechanism from the request path and application code that serves production traffic This pattern is widely used amongst services to maintain consistency between the source of truth data stores and our distributed cache clusters (eg Memcached Redis)Search Indexing: There are multiple search products at Airbnb that use real-time indexing (eg review search inbox search support ticket search) SpinalTap proved to be a good fit for building the indexing pipeline from data stores to the search backends (eg ElasticSearch) particularly due to its in-order and at least once delivery semantics Services can easily consume events for the corresponding topics and convert the mutations to update the indices which helps ensure search freshness with low latencyOffline Processing: SpinalTap is also employed to export the online datastores to our offline big data processing systems (eg Hive Airstream) in a streaming manner which requires high throughput low latency and proper scalability The system was also used historically for our database snapshot pipeline to continuously construct backups of our online database and store them in HBase This dramatically reduced the time to land our daily backups and allowed for taking snapshots at a finer time granularity (ex: hourly)Signaling: Another recurrent use cases for propagating data changes in a distributed architecture is as a signaling mechanism where depending services can subscribe and react to data changes from another service in near real time For example the Availability service would block a listings dates by subscribing to changes from the Reservation service to be notified when a booking was made Risk security payments search and pricing workflows are a few examples of where this pattern is employed within our ecosystemSpinalTap has become an integral part of our infrastructure over the last few years and a system fueling many of our core workflows It can be particularly useful for platforms looking for a reliable general purpose framework that can be easily integrated with your infrastructure At Airbnb SpinalTap is used to propagate data mutations from MySQL DynamoDB and our in-house storage solution Kafka is currently the event bus of choice but the systems extensibility has allowed us to consider other mediums as well (ex: Kinesis) Lastly we have open-sourced several of our library components and are in the process of reviewing the remaining modules for general release as well
f7qYBK8MdFhzWayPXR89SD,At Airbnb our offline data processing ecosystem contains many mission-critical time-sensitive jobs  it is essential for us to maximize the stability and efficiency of our data pipeline infrastructureSo when a few months back we encountered a recurring issue that caused significant outages of our data warehouse it quickly became imperative that we understand and solve the root cause We traced the outage back to a single job and how it unintentionally and unexpectedly wrote millions of files to HDFSThus we began to investigate the various strategies that can be used to manage our Spark file count in order to maximize the stability and efficiency of our Data Engineering ecosystemThroughout this post I will need to use the term partitions quite a lot and both Hive and Spark use this to mean different things For this reason I will use the term sPartition to refer to a Spark Partition and hPartition to refer to a Hive partition I will use the term partition key to refer to the set of values that make up the partition identifier of any given hPartitionAs the term ETL implies most Spark jobs can be described by 3 operations: Read input data process with Spark save output data This means that while your actual data transformations are occurring largely in-memory your jobs generally begin and end with a large amount of IOA common stack for Spark one we use at Airbnb is to use Hive tables stored on HDFS as your input and output datastore Hive partitions are represented effectively as directories of files on a distributed file system In theory it might make sense to try to write as many files as possible However there is a costHDFS does not support large amounts of small files well Each file has a 150 byte cost in NameNode memory and HDFS has a limited number of overall IOPS Spikes in file writes can absolutely take down or otherwise render unusably slow pieces of your HDFS infrastructureIt may seem as though it would be hard to accidentally write a huge amount of files but it really isnt If your use cases only involve writing a single hPartition at a time there are a number of solutions to this issue But in a large data engineering organization these cases are not the only ones youll encounterAt Airbnb we have a number of cases where we write to multiple hPartitions most commonly backfills A backfill is a recomputation of a table from some historical date to the current date often to fix a bug or data quality issueWhen handling a large dataset say 500GB-1TB that contains 365 days worth of data you may break your data into a few thousand sPartitions for processing perhaps 2000 3000 While on the surface this naive approach may seem reasonable using dynamic partitioning and writing your results to a Hive table partitioned by date will result in up to 11M filesLets assume you have a job with 3 sPartitions and you want to write to 3 hPartitionsWhat you want to have happen in this situation is 3 files written to HDFS with all records present in a single file per partition keyWhat will actually happen is you will generate 9 files each with 1 record When writing to a Hive table with dynamic partitioning each sPartition is processed in parallel by your executors When that sPartition is processed each time an executor encounters a new partition key in a given sPartition it opens a new fileBy default Spark uses either a Hash or Round Robin partitioner on your data Both of these when applied to an arbitrary dataframe can be assumed to distribute your rows relatively evenly but randomly throughout your sPartitions This means without taking any specific action you can generally expect to write approximately 1 file per sPartition per unique partition key hence our 11M result aboveBefore we dig into the various ways to convince Spark to distribute our data in a way thats amenable to efficient IO we have to discuss what were even aiming forIdeally your target file size should be approximately a multiple of your HDFS block size 128MB by defaultIn pure Hive pipelines there are configurations provided to automatically collect results into reasonably sized files nearly transparently from the perspective of the developer such as hivemergesmallfilesavgsize or hivemergesizepertaskHowever no such functionality exists in Spark Instead we must use our own heuristics to try to determine given a dataset how many files should be writtenIn theory this is the most straightforward approach  set a target size estimate the size of your dataframe and then divideHowever files are written to disk in many cases with compression and in a format that is significantly different than the format of your records stored in the Java heap This means its far from trivial to estimate how large your records in memory will be when written to diskWhile you may be able to estimate via the size of your data in memory using the SizeEstimator utility then apply some sort of estimated compression/file format factor the SizeEstimator considers internal overhead of dataframes/datasets in addition to the size of your data Overall this heuristic is unlikely to be accurate for this purposeA second method is to set a target row count count the size of your dataset and then perform division to estimate your targetYour target row count can be determined in a number of ways either by picking a static number for all datasets or by determining the size of a single record on disk and performing the necessary calculations Which way is best will depend on your number of datasets and their complexityCounting is fairly cheap but requires a cache before the count to avoid recomputing your dataset Well discuss the cost of caching later so while this is viable it is not necessarily freeThe simplest solution is to just require engineers to on a per-insert basis tell Spark how many files in total it should be writing This heuristic will not work on its own as we need to give developers some other heuristic to get this number in the first place but could be an optimization we can apply to skip an expensive calculationA hybrid is your best option here Unknown datasets should be with a count-based heuristic to determine file count but enable developers to take the result determined by the count heuristic and encode it staticallyEven if we know how we want our files written to disk we still have to get Spark to get our sPartitions structured in a way that is amenable to actually generating those filesSpark provides you a number of tools to determine how data is distributed throughout your sPartitions However there is a lot of hidden complexity in the various functions and in some cases they have implications that are not immediately obviousWe will go through a number of these options that Spark provides and various other techniques that we have leveraged at Airbnb to control Spark output file countCoalesce is a special version of repartition that only allows you to decrease the total sPartitions but does not require a full shuffle and is thus significantly faster than a repartition It does this by effectively merging sPartitionsCoalesce sounds useful in some cases but has some problemsFirst coalesce has a behavior that makes it difficult for us to use Take a pretty basic Spark application: Lets say you had a parallelism of 1000 but you only wanted to write 10 files at the end You might think you could do: However Sparks will effectively push down the coalesce operation to as early a point as possible so this will execute as: The only workaround is to force an action between your transformations and your coalesce like: The cache is required because otherwise youll have to recompute your data which can be very costly However caching is not free; if your dataset cannot fit into memory or if you cannot spare the memory to store your data in memory effectively twice then you must use disk caching which has its own limitations and a significant performance penaltyIn addition as you will see later on performing a shuffle is often necessary to achieve the results we want for more complicated datasetsCoalesce only works for a specific subset of cases: 1 You can guarantee you are only writing to 1 hPartition2 The target number of files is less than the number of sPartitions youre using to process your data3A simple repartition is a repartition whos only parameter is target sPartition count  IE: dfrepartition(100) In this case a round-robin partitioner is used meaning the only guarantee is that the output data has roughly equally sized sPartitionsA simple repartition can fix skewed data where the sPartitions are wildly different sizesIt is only useful for file count problems where: Repartition by columns takes in a target sPartition count as well as a sequence of columns to repartition on  eg dfrepartition(100 $date) This is useful for forcing Spark to distribute records with the same key to the same partition In general this is useful for a number of Spark operations such as joins but in theory it could allow us to solve our problem as wellRepartitioning by columns uses a HashPartitioner which will assign records with the same value for the hash of their key to the same partitionHowever this approach only works if each partition key can safely be written to one file This is because no matter how many values have a certain hash value theyll end up in the same partitionRepartitioning by columns only works when you are writing to one or more small hPartitions In any other case it is not useful because you will always end up with 1 file per hPartition which only works for the smallest of datasetsWe can modify repartition by columns by adding a constrained random factor: In theory this approach should lead to well sorted records and files of fairly even size as long as you meet the following conditions: As we discussed earlier determining the correct files-per-partition value is far from easy However the first condition is also far from trivial to meet: In a backfill context say computing a years worth of data day-to-day data volume changes are low whereas month-to-month and year-to-year changes are high Assuming a 5% month-over-month growth rate of a data source we expect the data volume to increase 80% over the course of the year With a 10% month-over-month growth rate 313%Given these factors it seems we will suffer performance problems and skew over the course of any period larger than a month or so and cannot meaningfully claim that all hPartitions will require roughly the same file countThat said even if we can guarantee all those conditions are met there is one other problem: Hashing CollisionsLets say you are processing 1 years worth of data (365 unique dates) with date as your only partition keyIf you need 5 files per partition you might do something like: Under the hood Scala will construct a key that contains both your date and your random factor something like (<date> <0 4>) Then if we look at the HashPartitioner code it will do: (See orgapachesparkPartitionerEffectively all thats being done is taking the hash of your key tuple and then taking the (nonNegative) mod of it using the target number of sPartitionsLets analyze how our records will actually be distributed in this case I have written some code to perform the analysis over here also available as a gist hereThe above script calculates 3 quantities: Collisions are significant because they mean our sPartitions contain multiple unique partition keys whereas we only expected 1 per sPartitionThe results are pretty bad: We are using 63% of the executors we could be and there is likely to be severe skew; close to half of our executors are processing 2 3 or in some cases up to 8 times more data than we expectNow there is a workaround  partition scalingIn our previous examples our number of output sPartitions is equal to our intended total file count This causes hash collisions because of the same principles that surround the Birthday Problem that is if youre randomly assigning n objects to n slots you can expect that there will be several slots with more than one object and several empty slots Thus to fix this you must decrease the ratio of objects to slotsWe do this by scaling our output partition count by multiplying our output sPartition count by a large factor something akin to: See here for updated analysis code however to summarize: As our scale factor approaches infinity collisions fairly quickly approach 0 and efficiency gets closer to 100%However this creates another problem where a huge amount of the output sPartitions will be empty While these empty sPartitions arent necessarily a deal breaker they do carry some overhead increase driver memory requirements and make us more vulnerable to issues where due to bugs or unexpected complexity our partition key space is unexpectedly large and we end up writing millions of files againA common approach here is to not set the partition count explicitly when using this approach and rely on the fact that Spark defaults to your sparkdefaultparallelism value (and similar configurations) if you do not provide a partition countWhile often parallelism is naturally higher than total output file count (thus implicitly providing a scaling factor greater than 1) this is not always true  I have observed many cases where developers do not tune parallelism correctly and result in cases where the desired output file count is actually greater than their default parallelismThis is an efficient approach if you can meet a few guarantees: In the examples we assumed many of these things could easily be known; primarily total number of output hPartitions and number of files desired per hPartition However I think its rare we can ask developers in broad to be able to provide these numbers and keep them up to dateThis approach is not a bad one by any means and will likely work for many use cases That said if you are not aware of its pitfalls you can encounter difficult-to-diagnose performance problems Because of this and because of the requirements to maintain file count related constants I feel it is not a suitable defaultFor a true default we need an approach that requires minimal information from developers and works with any sort of inputRepartition by range is a special case of repartition Rather than applying RoundRobin or Hash Partitioners it uses a special one called a Range PartitionerA range partitioner splits rows across sPartitions based on the ordering of some given key however it is not performing a global sort The guarantees it makes are: To summarize a range partitioning will cause Spark to create a number of buckets equal to the number of requested sPartitions It will then map these buckets to ranges of the specified partition key For example if your partition key is date a range could could be (Min: 2018-01-01 Max: 2019 01 01) Then for each record compare the records partition key value to the bucket min/max values and distribute them accordinglyWhile this is overall fairly efficient the sampling required to determine bounds is not free To sample Spark has to compute your whole dataset so caching your dataset may be necessary or at least beneficial In addition sample results are stored on the driver so driver memory must be increased  roughly 4 6G at most in our tests  but this will depend on your record and dataset sizeRepartition by range seems to deliver what we need in theory However the first guarantee  all records with the same hash will end up in the same partition  is a sticking point For our purposes this makes it the same as a simple repartition but more expensive and thus unusable as it standsHowever we should not give up on repartition by range without a fightWe will make two improvements: First we will hash the columns that make up our partition key We dont actually care about the relative sorting of our keys or the raw values of our partition keys at all only that partition keys are differentiated and a hash guarantees that This reduces the cost of sampling (as all samples are collected in driver memory) as well as the cost of comparing partition keys to to min/max partition boundsSecond we will add a random key in addition to our hash This will mean that due to the hierarchal key-based sort we will effectively have all records for a given partition key present in multiple sequential sPartitionsAn implementation of this would look something like: On the surface this looks similar to the repartition by columns + rand approach earlier and you might suspect it has the same collision problemsHowever the hash calculation in the previous approach amounts to: Which ends up having a total number of unique hashes equal to your sPartition countHere the hash calculation is simply: Which has effectively infinite possible hashesThe reason for this is the hash done here is only done to determine uniqueness of our keys whereas the hash function used in the previous example is a two-tier system designed to assign records to specific limited buckets of recordsThis solution works for all cases where we have multiple hPartition outputs regardless of the count of output hPartitions/sPartitions or the relative sizeEarlier we discussed that determining file count at the dataset level rather than the key level is the cheapest and easiest approach to perform generically and thus this approach requires no information to be provided by developersAs long as the total file count provided to the function is reasonable we can expect at most fileCount + count(distinct hash) files written to diskI recommend using count-based heuristics and applying them to the entire dataset rather than on a per-key basis Encode these statically if you wish to increase performance by skipping the countBased on my above evaluations I recommend using the following to decide what repartitioning scheme to use: Use coalesce if: Use simple repartition if: Use a simple repartition by columns if: Use a repartition by columns with a random factor if: Use a repartition by range (with the hash/rand columns) in every other caseFor many use cases caching is an acceptable cost so I suspect this will boil down to: Repartition by range works fairly well However it can be improved for this use case by removing some of the guarantees and constraints it provides We are experimenting with custom more efficient versions of that repartition strategy specifically for managing your Spark file count
cFkv5FNZC76kRuXh9bB6Cx,Logging events are emitted from clients (such as mobile apps and web browser) and online services with key information and context about the actions or operations Each event carries a specific piece of information For example when a guest searches for a beach house in Malibu on Airbnbcom a search event containing the location checkin and checkout dates etc would be generated (and anonymized for privacy protection)At Airbnb event logging is crucial for us to understand guests and hosts and then provide them with a better experience It informs decision-making in business and drives product development in engineering functions such as Search Experimentation Payments etc As an example logging events are a major source for training machine learning models for search ranking of listingsLogging events are ingested into the data warehouse in near real-time and serve as a source for many ETL and analytics jobs Events are published to Kafka from clients and services A Spark streaming job (built on top of Airstream Airbnbs streaming processing framework) continuously reads from Kafka and writes the events to HBase for deduplication Finally events are dumped from HBase into a Hive table hourly Since the logging events are input to many pipelines and power numerous dashboards across the entire company it is utterly important to make sure they land in the data warehouse in a timely fashion and meet the SLAsThe volume of events generated is massive and rapidly increasing That poses serious challenges to existing ingestion infrastructure particularly the Spark streaming job that ingests events from Kafka to HBase In this article we discuss the challenges in scaling the infrastructure and a solution that can support higher throughput by an order of magnitude and with better efficiencyIn the current Spark Kafka connector there is a one-to-one correspondence between Kafka partitions and Spark tasks Basically one Spark task is instantiated to read from one Kafka partition to ensure the ordering of events when they are processed in Spark However with this design we cannot simply scale the Spark streaming job by increasing the parallelism and allocating more resourcesTo increase the Spark parallelism and throughput one has to assign more Kafka partitions to topics with large events or high QPS events Unfortunately that is a rather manual process and is not scalable when there are a large number of topics (which keeps increasing)Another problem is that assigning more partitions to a topic in Kafka does not retrospectively apply to events that are already in Kafka The additional partitions are only available to new events It is impractical for us to anticipate spike of events and assign more partitions beforehand to the affected Kafka topic A spike could come any time and could be due to various reasons such as new product features or holidaysWhen event volume reaches a critical level large Kafka topics often could not be ingested fast enough to the data warehouse The problem is exacerbated by data skew in the events which we will discuss nextDifferent event types are being logged with significant variations in their volume and size Some are really sparse and some may have a QPS thats several orders of magnitude higher The sizes of event types could range from hundreds of bytes to hundreds of kilobytes The box plot below shows the large variation of average event size for the Kafka topics (note that Y axis is in log scale) Although we try to assign more partitions for larger events there is still serious skew in the Kafka partitionsSkew is a serious problem in general for data applications In this case some Spark tasks would take much longer to finish than others It leads to idling of many executors and waste of resources since a Spark job moves on to next stage when all tasks in a stage finish Kafka partitions with the largest events would take an unreasonably long time to read if the topic does not have enough partitions This results in lag in the Spark streaming job since batches are processed sequentiallyDue to the challenges above there is little headroom in the throughput of the Spark streaming job Once the job is delayed due to various problems (like bad data nodes or Hive Metastore outage) it takes a really long time to catch upFor example lets assume a job with 2-minute interval processes a batch in 1 minute on average If the job is lagging behind for 4 hours it would take another 4 hours to catch up If we want it to catch up in 1 hour that requires 4X headroom (ie process each batch in 24s) Apart from recovering from incidents large headroom is also necessary to handle seasonal spikes Therefore for near real-time ingestion it is crucial to have extra headroom in throughputIn an ideal system we would like to be able to horizontally scale out the Spark streaming jobs (ie achieving higher throughput by increasing parallelism and allocating more resources) We would also like these jobs to be load balanced so each task takes a roughly equal amount of time to read from KafkaTo achieve these two goals we at the Airbnb Data Platform team developed a balanced Spark Kafka reader that satisfies those two requirementsFor streaming ingestion the ordering of events is not a requirement since ingested events are processed minimally and then stored in HBase individually This allows us to re-think the model and look for novel ways to address the scaling issues As a result we created a new balanced Kafka reader for Spark that 1) allows arbitrary number of splits so the parallelism can be increased to provide higher throughput; 2) calculates the splits based on event volume and sizeAt a high level the balanced Kafka reader works as follows: Below is a simple example with 2 Kafka topics and 3 splits Events in topic A have a higher QPS but smaller size than topic B The balanced Kafka reader would group subsets of these events together so that each split reads 1/3 of the data from Kafka One split (split 2) would include 4 events from topic A and 1 event from topic B so that the total size is 8kb for each splitNote that Step 1 may be improved in the future by computing the mean event size dynamically so new topics and topics with frequent changes in event size are better accounted forThe problem of assigning offset ranges to splits evenly is very similar to the NP-hard bin packing problem Sophisticated algorithms to optimal solutions and fast algorithms to non-optimal solutions do exist with non-linear computational complexity However they cannot be used because our problem is somewhat different in that 1) the number of splits (or bins) is fixed; 2) an offset range (or item) can be split into smaller piecesInstead of adapting a complex existing algorithm we developed a simple yet effective algorithm that is illustrated belowThis algorithm is super fast with O(number of splits) It just goes through the splits and Kafka partitions once sequentially The result is that weights for most splits are extremely balanced except the last split that may have much less weight (which is fine because we are wasting resources of at most one task) In one test the estimated weight-per-split is 489541767 with 20k splits Weights for the smallest and largest splits are 278068116 and 489725277 respectively The second smallest split has a weight of 489541772 Excluding the smallest split the difference between the second smallest and the largest splits is 183505 (only 004% of the largest weight)The balanced partitioning algorithm performed well in both test and production The variance of Spark task running time (as shown in the graph below) is much more evenly distributed than the original Spark Kafka reader Most of the tasks finished within 2 minutes A small portion of them took 2 to 3 minutes Comparing to the wide ranges of event QPS and size the small variance in task running time demonstrates the incredible effectiveness of the balanced partitioning algorithm By taking event size and volume into account it ensures the ingestion workload is evenly distributed across executorsThe balanced Kafka reader is a crucial piece of scaling the streaming ingestion of logging events It is also important to make sure there is no other bottleneck in the upstream and downstream systems In this case we improved Kafka and HBase to enhance their throughput and reliability For Kafka the brokers were migrated to VPC which has 4X throughput A streaming job is set up to monitor the QPS per Kafka partition so that when event volumes increase more partitions can be added in a timely manner For the downstream HBase the number of regions for the HBase table was increased from 200 to 1000 so bulk-loading the events to HBase can have higher parallelism (which is determined by number of regions)For the Spark streaming job speculative execution was enabled to better handle reliability issues in the underlying infrastructure For example one Spark task could be stuck due to reading from a bad data node with faulty disks With speculative execution the job is much less likely to be affected by those kinds of issuesThanks to the balanced Kafka reader Spark applications consuming from Kafka is now horizontally scalable with arbitrary parallelism The balanced partitioning algorithm is simple and has proven to be extremely effective Because of these improvements the Spark streaming job for ingesting logging events can handle an order of magnitude more events than the previous one Stability of the system has improved so much that we have not seen any significant lag since the changes were deployedFor future event traffic growth and spikes the Spark streaming job for logging event ingestion will be able to handle them smoothly and efficiently There is no more worry about skew in the events If the job happens to be lagging due to issues of underlying infrastructure it will be able to catch up rapidlyThe problems we solved here is not uncommon in large-scale Spark applications and data applications in general It is important to carefully understand the data itself and how it is processed in each step which could reveal potential bottlenecks skew in the data and opportunities for optimization For example Spark provides a nice UI showing the DAG for each job From that one can understand how a job is executed and whether it may be tuned for better performance via caching repartition and etcScaling streaming ingestion of logging events involves many upstream and downstream systems The project was a collective effort of four teams across Airbnb Data Platform and Production Infrastructure It would not be possible without the immense contributions from Cong Zhu Pala Muthiah Jinyang Li Ronnie Zhu and Gabe Lyons We are grateful for Xu Zhangs enthusiastic help with Kafka We would like to thank Guang Yang Jonathan Parks Gurer Kiratli Xinyao Hu and Aaron Siegel for their unparalleled support on this effortWe are indebted to Gurer Kiratli and Xiaohan Zeng for their help in proofreading this blog post
YPrtVYNQxP8RCJQhfbkoiV,A difficult challenge of any software development is ensuring the code youve created has its intended effect Over the years the software field has evolved impressive approaches to testing using a combination of tooling practice and automation Modern complex applications that used to require weeks of manual testing can now be tested in a matter of minutes without any human involvementThe data field is far less mature in this area Its not uncommon for modern data pipelines to be subjected to time consuming manual testing I believe the reason for this is twofold Firstly data development has only in the past few years started to adapt practices from software development (eg data/analytics engineering) More importantly though testing data pipelines typically involves a combination of testing not only logic but very complex state and a lot of it ie the current set of all records in the database In my work as a data warehouse developer and engineer it wasnt uncommon for systems that worked flawlessly in DEV SIT and UAT to fail in production This normally happened as testing occurred on smaller subsets of data to ensure a timely response or because a unseen state had occurred in the dataBeing able to test your data pipelines against a current replica of production in an automated and timely way provides near certainty that the changes will succeed in production and brings data development in line with software developmentAt Airtasker weve developed an approach that allows us to do that using a combination of dbt (our data transformation tool) Snowflake (our data warehouse platform) and Githubs PR status checks (how we validate changes to the pipeline) In the remainder of this article Ill detail how Airtasker uses those tools to automatically test any changes to our data pipeline against a production clone prior to merging and deploying those changesThis technique is made feasible by Snowflakes zero copy clone technology which Ill explain briefly Snowflake stores all your data as objects on a cloud file system like S3 It maintains a metadata store to track where the data is stored for each table column and data value Whenever any changes or additions are made to a Snowflake database new objects are written into the data store layer eg S3 and the metadata store is updated There is never any updating of existing objectsWhen you use Snowflakes cloning statement only a copy of the metadata store is created pointing to the pre-existing objects This is the zero copy component as no new data is created or written When you make changes to that clone new objects are written and only the metadata is updatedSnowflakes zero copy cloning allows you to clone large databases in a brief amount of timeOne caveat of Snowflakes cloning is that only the ownership of the cloned object is updated never any of the child objects This is problematic as if you clone an entire database the schemas and tables are still owned by the original owner rather than the cloner so theyre unable to be modified unless you share the same role as the owner which isnt desirable here as you dont want your test environment to have production permission This is a curious design choice by Snowflake but can be addressed by separating the cloning into multiple stepsThis approach details how to clone specific schemas and all linked child tables in a production database and make them available for testing or development in a different database At a high level the approach is: Just to be extra clear there are two clones created during this process prod-clone and test-clone All testing and development is done on a test-clone Prod-clone is periodically refreshed after each successful runLets run through the implementation using dbtFirstly configure some variables in the dbt_projectyml fileThen well need macros to clone schemas and update the permissionsThen prior to each test run the following macro is used to prepare a clone that the pipeline can run againstAs were on an older version of dbt we use 2 pre-model hooks to execute these operations (details below) however dbt 014 makes operations available as first class citizens and Id recommend using those insteadclone_prodclone_buildAfter each production run we use clone_prod model to create the prod clone (ie prod-clone) then prior to PR testing runs Github runs our CI job via the dbt cloud integration which then uses the clone_build model to generate a new clone (ie test-clone) from the prod clone that has the permissions properly configuredThe dbt cloud configuration for our CI environment looks like this: We hope this technique proves as useful to you as it has for us
PE37MfzoR2Mz9Y4iim2Dq2,Understanding how our customers interact with our product is crucial to Airtasker In the data space we call this user behaviour analytics User behaviour data is collected as events that are triggered by user actions Because theyre specific to users were able to understand our customer base in aggregate drop into specific cohorts and even review an individuals journey to understand the challenges faced User behaviour analytics plays a critical role in feature planning (both development & decommissioning!) and experimentationImportantly for Airtasker were also using it to drive an outcomes focused culture for our product and engineering teams User behaviour analytics offers a window into the real world impact of their work a connection thats hard to maintain when residing in the often abstract world of software developmentMaximising the value of your user behaviour data requires that it is consistent both semantically and structurally Semantically its important to ensure events having the same meaning across platforms (eg web iOS Android) For example avoid having a Sign Up Completed event defined as having completed a user profile on iOS but as providing only the username and password on Android A lack of consistency makes it difficult to understand how your product performs overall and to compare performance between platformsIts also imperative to define events with a common structure so you can interrogate them in a consistent and expected way This makes it easy for users to understand and work with the data This is particularly challenging when multiple teams working on common areas of a product generate event data independently This common problem typically results in event data sets that are difficult to use You have multiple different events for the same action casing differences in event and property names (ouch!) and a mixture of similar but only slightly different property values (logged in vs logged_in anyone?)Being aware of these challenges at Airtasker we put thought into how we could solve these when we kicked off a cross team effort to reset our product instrumentationWe started by developing a product instrumentation plan that abstracted away specific platform and implementation details and focused on the product actions the user was taking in the real world For example when a user posted a task the event was Post Task Completed as opposed to Post Task Button Clicked which might need to be different on the mobile apps if the UI for posting a task was different (eg Post Task Button Pressed) This method of defining events ensures the product and UI can evolve at an implementation level without rendering critical events on the product journey obsolete or misnamedOnce we had our instrumentation plan we turned to the challenge of semantic and structural consistency We felt the best approach was strong data governance enforceable via code We created a single Github repository that defines every event we track as well as the associated properties This single source of truth for event definitions provides a company-wide view of the events we collect enforces consistency and reuse across platforms and supports a strong data governance process as new instrumentation can only be added through the use of approved pull requestsTo give you some insight into how we structured the repository there are two key YAML files event_definitionsyml and model_definitionsymlThe event definitions file defines every event to be instrumented along with its structure attributes allowed values and so on For example: The model definitions file defines the models that could be added to an event if that model was in context for the event The models are our common business entities like tasks and offers and the purpose of defining the models was to ensure the model properties were consistently structured across events and platforms For example: To support all this on our client platforms web iOS and Android we implemented an Airtasker Analytics library that sends events to Segment and allows developers to trigger an event in a type-safe manner When developers use the library they dont provide the formal event name or model details instead each event is associated with a method which can be called in order to trigger the event All the developer has to do is provide the required models and additional parameters as method parametersOne of the coolest things about this entire project is code generators which read the event_definitions YAML and automatically generate these methods We currently have code generation for event methods in Swift and Kotlin and aim to have TypeScript definitions available soon Code generation guarantees event property name and property value consistency and makes it simple for engineers to add new events without mistakes On top of all this we implemented a linting library (Cerberus) to ensure that only valid YAML was committed to the repositoryAs an outcome of all this our user behavioural analytics has taken a huge step forward Were gaining new insights into how users interact with our product and were encouraging a culture that values customer impact All of this comes together to contribute to our ultimate goal building a loved product that empowers our community of taskers to realise the full value of their skills
ghsSNYx8n4xxWv33Ess5xH,At AMARO collections are a thing of the past here we have new launches every week In fact we have around 100 new products per week that need to be activated on our website That amount of new pieces every week means lots of photoshoots and description sessions need to be completed in order to make our lookbooksIf you look at this process from the customers point of view youll see it has everything to do with product discovery which is a key point in our customer journey We always need to make sure that they have an excellent preview of what theyre getting and the only surprise ahead of them is how great our product quality isIn this article Ill talk about how we found a tool to help us become more focused and organized in this process This not only allows our team to have a seamless workflow with more focus on creativity but it also creates a better experience for our customersWe can describe our daily work routine as 11 steps that lead us to deliver a great picture for the website That includes styling shooting filming post-production and writing the description Those steps are organized into 4 cycles that depend on each other and need to be executed at the right timing so that no product goes online missing a photo or a great descriptionTo synchronize that weve always relied on several spreadsheets This process was time-consuming and made it hard for us to have metrics So we decided to search for a solutionWhat we needed was a tool that would be easy to use have a straightforward process be simple to create reports from and be connected to our internal database\u2060  where we have all the information the studio team needs throughout the process such as: And another important point: we needed to make sure that nothing went missing along the wayAfter a long search we got to know Airtable the tool that would have all the features we needed Airtable is a friendly database that in their own words lets you organize anything you can imagine In my words its a spreadsheet with the power of a database and terrific UXWith Airtable we are able to: All that was left was the connection to our internal database and heres where I fit in in this whole processWhen I stepped in the tool was already being used and we were gaining a lot of efficiency and clarity But the studio team could only take photos of products that had been manually pre-registered on AirtableThe solution for this connection involved the use of Airtables API as well as Lookers API (where all of our ETLs reside)We developed a solution with Python that extracts everything we need from Looker transforms it and delivers it to Airtable in a specific table Therefore we can skip the manual pre-registration and the information on Airtable is automatically updatedIn the transformation we deal with all of our business rules In our context that can be anything from filtering which products are available for a potential shooting to understanding how our barcode system works It also means understanding how to better structure data so the updates needed are less time-consuming For example we transformed all the numerical data to ranges turning it into categorical data That makes the update process faster and it doesnt make a big difference for the final userAirtable is collaborative and is used by multiple people so something weve worried about was not losing the historical data we have stored there To make sure that doesnt happen we have a copy of everything in our Data Lake Now we can also make further analyses and join data with other metrics on LookerWe built everything in a way that was modular and reusable (in fact we now have four similar solutions being used throughout the company in other contexts) and weve scheduled it in our Less Server solution (from which you can read more on here)By building our own personalized Airtable and Looker functions which serve perfectly for our needs we can re-use them later when working on other projects This makes things easier for our team giving us the opportunity to tackle new challenges instead of just solving the same problem over and over againWith this new process we have 38% more styles going active in 5 business days than before and all product descriptions are able to be completed before the product arrives in the studio\u2060  something we were rarely able to do before We can attribute our improving NPS (which saw a 2 point growth in 2019) to projects like thisAs a plus: Less bureaucracy giving the team more time to focus on creative details that had been previously skipped due to the tight schedule Since we launched this tech upgrade the studio has been able to think about things like new props angles and backgrounds Its safe to say that our lookbook is looking better than everThe new process has also led the AMARO team to consider Rapid Application Development as a solution to become more agile and create value in short cycles using tools such as Airtable among others currently available in our CX Tech Stack
PXjjzx48YD4DyNwjN75egP,Lets talk about trends? Studies about Usability and User Interface are growing in the corporate world Now both small and big companies have the mindset of delivering the best experience for their customers by continually improving their digital platformsAbsolutely every part of the customer journey in the app (or website) has to be continuously considered and reconsidered: every flow icon and message that hits the screen If you think about it it is exactly through this endless improvement cycle that things evolve and new products are launched The telephone for example was invented more than a hundred years ago giving origin to todays smartphones  the incredible devices that most of us cant live without nowadays The same applies to other fields such as architecture the automobilistic industry and of course fashionSince the concepts of UX and UI are so present in our routine why shouldnt we apply them to data visualization? The huge and complex world of big data tables and grey tools (aka Excel) deserve a modern dynamic more intuitive and accessible way of visualization and therefore comprehensionThe human brain is attracted to movement patterns shapes and colours The way we process information when graphics and imagery are used is much more effective than any other format of text or spreadsheets It all goes back to our origins as human beings: it is obviously easier to distinguish a square from a circle or a red line from a yellow one than it is to comprehend large numbers for instanceThe data visualization mindset is now gaining more interest from companies which already know those are crucial to decision making Visual data is easily captured and stays on the mind for longerFurthermore we are living in an era of big data in which we are smashed with more information than we can process That is why we need to make things easier through the use of visual elementsThat being said data visualization tools are also growing in number Big names as Tableau Power BI and Data Studio are very popular even though with limitationsAMARO a company that always aims for sophistication and effectiveness in all areas is now applying these two values to data visualization After all we are a data-driven companyLooker our first data visualization project has been helping teams for months to use data for decision making Mostly because of its friendly layout and easy navigation  not to forget the accuracy of the data of course We also use Data Studio a mainstream tool provided by Google that is connected with other Google productsIts all about that visualization Since our first years at school we learn about it: maps graphs and infographics are good examples of this universeIn short? Data visualization is the graphical representation (visual) of information and dataNowadays almost everything is measurable In other words the capability of accessing and understanding this information is essential for a companys future Sounds simple but requires special attention: graphs must not be created without a good understanding of the context A graphical representation of good data when made in a superficial way can become biasedMoreover a dashboard  the panel where information is gathered  needs an aesthetical balance for the first eye-contact in order to impact the user We must not stylize a graph with random colors simply trying to make it more beautiful: you have to combine functionality with design Keep in mind what is your goal and the information you need to extract and then choose the design accordinglyIt is also important to consider the amount of information: too little information may not be enough to retain the viewers attention or worst it can send an incomplete message On the other hand the overload of visual elements can end up being harmful: if too much information is given at the same time with no clear context the more cognitive effort you will demand from the user and bigger is the chance that he/she will give up the task So sensitivity and common sense are necessary for the building of data visualizationNo there are many other points to be considered In addition to all the visual harmony that weve already talked about other aspects have to be considered for the creation and validation of effective dashboardsLook at the graph belowIf your answer is yes you already have incorporated cultural patterns The ability to identify patterns is one of the main points to pay attention to visual environment creationCulturally we have some interpretation patterns: When we need to transmit information about a given action in a given period of time and we want to see this by months for example the recommendation is a bar or a line graph The closer these visual representations are to our mental models the better will be our understandingBack to the graph  even without context is a good example: could you get the message? Were you able to understand without further explanation? It is very important to know if the goal was achieved This is the main beacon between quantity and the way information is passed Dont limit yourself with users next to you Try to find professionals that dont have direct contact with the data in order to validate and guarantee that this information will be accessible to a major number of peopleAnd last but not least check the accuracy of the data It doesnt matter how beautiful and functional your dashboard is if you dont trust your data Make sure that the information is correct and that the sources are clear to the userWith all these concepts arising together with the even more number-driven market it is natural to observe a movement by professionals towards decision-making processes based on data Matching a data scientist with all the knowledge of data mining and the visual orchestration of a designer is not easy However the intersection of all these skills when well implemented generates value for professionals and guarantees a strategic vision to the company as a whole
6tJNvSbrY7qK6v9Bizueqt,Amobee is a leading independent advertising platform that unifies all advertising channels  including TV programmatic and social The Platform Data Team is building a data lake that can help customers extract insights from data easily One of the challenges we meet is the data volume Each table can vary from TB to PB The table can have tens to hundreds of columns The data source can be first-party/third-partyTo make our data ingestion more scalable and to separate concerns we have built a generalized data pipeline service on top of Spark and an independent data catalog service on top of Hive The data catalog service is a centralized place to manage the table metadata including schema partitions etcThis design works well for many situations But we also found that the catalog service had a hard time dealing with some big and wide tables After some research we found that the limitation comes from HiveWhile building a data lake in a hybrid cloud environment it is important to design the architecture as scalable pluggable recoverable replay-ableData can have different views in each geo partition There can be a case when we want one cluster to be a subset of another one or another case where the two geo partitions are totally in sync That is why we built a centralized data catalog service to manage different status across different clusters Hive was chosen to be the foundation of the catalog serviceHive is an industry standard It offers a centralized place to store the metadata for all the data sources Developers can use well-designed APIs to interact with HiveOur pipeline service performs the ETL to transform the data into efficient formats for querying and we use our catalog service to communicate with Hive and update the schema and partitions accordingly All the tables are created as external tables in Hive But there are some limitations in Hive for our case: Delta Lake is an open-source storage layer that brings ACID transactions to Apache Spark™ and big data workloadsThe metadata in Delta Lake is part of the data It coexists with data and stores the status of data Delta Lake store those status inside the transaction logOn top of this transaction log file Delta Lake has introduced many best practices of building a modern data lake The features listed on the Delta Lake documentation include: ACID Transactions: Data lakes typically have multiple data pipelines reading and writing data concurrently and data engineers have to implement a tedious process to ensure data integrity due to the lack of transactions Delta Lake brings ACID transactions to data lakes It provides serializability the strongest level of isolation levelScalable Metadata Handling: In big data even the metadata itself can be big data Delta Lake treats metadata just like data leveraging Sparks distributed processing power to handle all its metadata As a result Delta Lake can handle petabyte-scale tables with billions of partitions and files at easeTime Travel (data versioning): Delta Lake provides snapshots of data enabling developers to access and revert to earlier versions of data for audits rollbacks or to reproduce experimentsOpen Format: All data in Delta Lake is stored in Apache Parquet format enabling Delta Lake to leverage the efficient compression and encoding schemes that are native to ParquetUnified Batch and Streaming Source and Sink: A table in Delta Lake is both a batch table as well as a streaming source and sink Streaming data ingest batch historic backfill and interactive queries all just work out of the boxSchema Enforcement: Delta Lake provides the ability to specify your schema and enforce it This helps ensure that the data types are correct and required columns are present preventing bad data from causing data corruptionSchema Evolution: Big data is continuously changing Delta Lake enables you to make changes to a table schema that can be applied automatically without the need for cumbersome DDLAudit History: Delta Lake transaction log records details about every change made to data providing a full audit trail of the changesUpdates and Deletes: Delta Lake supports Scala / Java APIs to merge update and delete datasets This allows you to easily comply with GDPR and CCPA and also simplifies use cases like change data capture • 0% Compatible with Apache Spark API: Developers can use Delta Lake with their existing data pipelines with minimal change as it is fully compatible with Spark the commonly used big data processing engineMost importantly: No Vendor Lock-in Delta Lake itself doesnt change the data you stored in Hive You can easily remove the metadata and convert it back into a Hive table so there is minimal riskSome of the good features listed on the website but not in the open source version include statistics and data skippingAll that being said we still need Hive because it brings in many features that Delta Lake doesnt have like getting a table schema from outside of Spark job or storing key-value properties in the Hive tableTo migrate from Hive to Hive+Delta there are some features missing after we performed a comparison between our pipeline and a Delta-based pipelineHive offers JDBC connections that can let developers run DDL and DML easily It also offers a pretty comprehensive API to get the schema of existing tables and make it easy to run DDL when it is required Our catalog service can update the table schema according to the data schema of pipeline output However with Delta Lake we cant get the schema directly because the latest schema is stored in transaction logs Instead you need to use the Spark API to read it It will be expensive to launch a Spark job for fetching the schema from existing tables and run DDLAfter researching some open source frameworks Livy becomes the new option for us We can retrieve schemas or run DDL in Spark by making a HTTP request to Livy server Livy can reuse the open Spark session so we dont need to pay for the cost of starting a new Spark job thus reducing the time it takes to access the schema More importantly the catalog service can run any DDL and DML outside of the Spark environment This reduces the risk of changing our existing framework and enables more flexibility of using the data lakeWe would like to control the output file names so that for one immutable input set we know exactly what the output files will be That way if we need to re-run the job the previous output is overwritten If we directly write out data using the Delta Lake API from our Spark job then Delta Lake will always insert new filesTo make our pipeline service re-playable it will write out to an intermediate location then move into the final destination This can give us more flexibility to manage our pipeline output files In order to register these files when not using the Delta Lake save API we leverage some lower level Delta Lake API to commit files by ourselves Each data pipeline task can report its output files to the catalog service The catalog service is responsible for inserting the files into transaction log and update the schema accordinglyAfter Delta 040 Delta Lake introduced an API to easily convert an existing table into Delta format The performance is acceptable for small tables but it works pretty poorly when dealing with big tables on Google Cloud Platform (GCP)To improve the performance of table conversion we implemented our own tool for converting data into a Delta Lake table The tool similar to Hive will first define the schema of Delta table followed by a call to our implementation of MSCK for Delta LakeMSCK is used to recover the partitions that exist on the file system but not registered in the metadata The idea is to find out the difference by scanning the file system and compare with what exists in the transaction logDelta lake itself has very good API to get all available files inside the transaction logs We have built a scalable way to scan files on any file system Combining these two the catalog service can report the untracked file to Delta lake through MSCK APIMSCK works pretty well especially when we convert Delta format tables on the cloud It only takes 25% of the time compared with the native table conversion API on GCP environment It also performs better than the original API in our on-premise environmentThe original architecture without Delta Lake does a good job in separating the concerns of each component The main changes of migration will go into data catalog service To support better interaction with Spark we introduce Livy server into our architecture: 5Slow to get table properties: Delta allows for table properties but it needs to be accessed through a Spark job With Hive we can directly get the table location and schema information using a Hive client To address this we store the properties information into Hive Metastore for easier fetching The catalog service is responsible to update the transaction log first and store those properties back into the Hive Metastore accordinglyLack of roll back support: In the open source version DELETES/UPDATES/UPSERTS are supported in Delta format but the roll back feature is missing If there is something going wrong we want to have a way to perform a 1-click recovery The VACUUM command is executed lazily We developed a feature to let the catalog service scan the log and decide what files need to be inserted or removed between 2 commits before vacuum happensThe prototype shows that there are no performance degradations in the query After testing on production size data Delta Lake works better Here are the relative performance metrics we obtainedThe original Delta API is easy to use but it doesnt fit into our architecture The performance is especially bad in the cloud environment The API we built on top of Delta API performs well in a hybrid cloud environment With these changes we are able to manage the data lake in different hybrid cloud environments easily
7jNENnCSQENY3ERVcdV2hT,I recently wrote about using clickstream events collectors such as Snowplow or Divolte to power more reliable and deeper analytics It is however possible to create your clickstream event collector in a few clicks using Microsofts CloudBesides the tracking script the Azure stack can handle all the functions of a clickstream collector with three components: Logic App EventHub and DataLake StorageOnce the data is in data lake storage it is possible to query it using Microsofts Data Analytics USQLThere are a few pros and cons related to leveraging this type of serverless solution to capturing clickstream dataThe Clickstream collector can be set up in a three-step process First setting up the storage layer (blog or DataLake) then setting up EventHub with Data Capture and finally create a logic app that will send events back to the event hub • The second step is to create an event hub with data capture turned on This will export to blob/data lake storage the data ingested at specific (configurable) intervals • The third step relates to the creation of the logic app The logic app needs to be composed of only two components An HTTP Request receives and sends event to the event hubThe HTTP request logic app component provides an option for schema validation The basic logic app event schema that we are using in this example is provided below: The following python code can push data to the logic app where the URI variable is the hosted HTTP endpoint provided by the logic app It is worth noting that to be able to push to event-hub the content needs to be encoded in base64A tracking script can be created in Javascript in the same manner; an example javascript implementation is shown belowIf the logic app has been able to push the data to event hub it should show events as succeeded within the UI as below: After the data capture interval has elapsed a new file should appear in the data storageUsing AVRO reader we can read the content of the file and see how the data is being stored: There are a few things that can be done to productionalize this: Using the combination of Logic App/EventHub/DataLake Storage provides a quick way to deploy and gather clickstream data There is still some more work that might be required to having it more production-ready Still by default Logic Apps handle availability scalability the retrying logic as well as the loggingOne of the main factors to consider is the pricing logic which is per execution sites with a low amount of visits would benefit pricing wise from relying on this type of integration but sites with a high volume of visits might want to look at a different approach
LQHSaD87MpzoFBV6koMdbV,Data quality is a journey it doesnt come in one day and the focus should be more about improving data quality than having it right on day one Having a data governance model implementing testing for data quality are all things that help on this journeyA more thorough approach looks at the different areas of planning validation cleansing surfacing and documentation of the various data objectsHaving a clear plan on what information and how it should be collected is the first step to be able to have a good data qualityIt is crucial within a data planning to define what to collect and where to collect it from identifying the different sources of informationData layer: A data-layer is a software component that provides simplified access to data stored It is often used to refer to a front-end component to integrated with GTM and external tags componentsPart of defining the data is to set up the attributes and event definition that we want to collect and map these to what the different tags/systems are expectingNaming Conventions: help to make clear what specific data is for they can be used to define table names where prefix and suffix can give insights to the origin of a table being a table from an operational data store (eg: O_ ) a staging table (eg: STG_) or an aggregate table (eg: A_)  They are also useful in defining field names and can also be used to standardize source input such as utm parametersFormat & Standardization: Data feeding into the systems should abide by a specific format defined beforehand This data should be already standardized to some extent There exists iso standard for quite a few data points such as country code (ISO 3166) gender (ISO 5218) LatLong (ISO 6709) language code (ISO 639) currency (ISO 4217) dates (ISO 8601)Logging and Data Structures: Logging frameworks complement standard event logging with additional data in a standard format as well as allow for the integration of these events into existing data pipelinesThere is also a need to define how the data structure should store the events There are trade-offs between having generic data structures and specialized data-structuresGeneric data structure makes it easy to leverage one data set along with others think for example of a contacts table that would encompass every interaction with a customer be it SMS email direct mail or another source of information Having the data in one generic data structure makes it very easy to consume these sets of information together Generic data-structure however does not make it as easy to consume the more specialized information For example in the case of the contact table we previously mentioned an email open-rate in generic data structures this information would likely sit within a nested objectThere are quite a few checks that can be performed to see if the data receives is matching expectation From data structure checks value checks lifecycle checks or a referential checks all these validation processes help ensure that the data is up to specData Structure checks: Checks that the incoming data conform to the data structure Checks the number of columns present the datatypesValue Checks: Check that the values in the datasets match what would be expected from incoming data Think for example of product prices It is not normally expected to find a product price with a negative valueLifecycle checks: Can show if there might be missing data in the dataset Think about a purchase on an e-commerce site For a purchase to happen a couple of actions need to be performed before it such as browsing the website clicking an add to cart or purchase button and flowing through the checkout A lifecycle check would check that this is indeed happeningReferential integrity checks: provide a validation that the reference to other data objects also called Foreign Keys exist If we go back to the e-commerce purchase example a referential integrity check could be to look at the products presents in the orders and see if they also exist in the product master dataMonitoring and alerting can be set up to warn about multiple issues in DQ such as: There are multiple steps or approach to data cleansing The first decision is whether or not to make the data flow through the system when not passing validation The second decision is if you decide to make it flow through the systems how to treat itReformatting of data from different systems: Reformatting the data can be possible in some cases when data types can be cast when the string can be harmonized through replacing regexp matching or through a lookup tableValues: Specific values can be inferred or capped to help with the interpretation of the data down-streams; product prices can be set up to be higher than 0Missing Lifecycle data: In cases where lifecycle checks failed it might be possible to infer some of the values for this lifecycle Taking the example of the product purchase that we touched based on previously it can be possible to reconstruct the checkout event for customers who made a purchase but where not checkout event is present Some of the data in a checkout event for instance can be directly inferred from a purchase event For example the products and prices some others such as the timing of the event are less certainIt is worth noting that the way the data within the event is inferred will have an impact on the downstream metrics being calculatedReferential validation failing: Reference not existing can happen for multiple reasons it could be that the product data was deleted before an import could capture it but a transaction based on this product already happened In this case it can be useful to insert the information we know about the product into the reference tableMaster Data management: help create a consistent view of your key data entities It unifies and merge the different records that relate to the same entity and promote fields coming from varying degrees of authority and promote specific attributes to the golden recordOne of the challenges in the cleansing process is that it is very much dependent on the cause and applications for the data It might not be great for reporting not to include some records when one of the fields doesnt pass a DQ check but it might be worth to exclude them when attempting to build a predictive modelSurfacing data to multiple stakeholders through dashboards or datasets help improve data quality by having multiple pairs of eyes on the data Data that is surfaced and continuously monitored for business performance tend to be the data of the highest quality as there is a business incentive to have anomalies identified and correctedAnalyses following metrics deviation can help identify data facing some issues For instance the wrongful assignment to a category for a given event imagine the assignment of a video view to a device web ios or android: If the video views are suddenly getting assigned to a different category (web in the example above) chances are is that there is a wrongful assignment most likely coming from a logging bug Surfacing this data in the dashboard makes it very easy to identify this behavior and puts some organizational pressure on fixing the root causes of these anomaliesBesides having clean data in the system it is important to be able to surface the necessary information related to the datasets so that users can find the required information use them effectively and without making the wrong assumption as to their meaning The concept of data dictionary is important to that endeavour a document providing information related to the data structures and describing the datas contentThere are quite a few tools out there that can help in that endeavours: Some companies go a bit beyond these and include an approval workflow to handle the lifecycle of a metric definition Metric GovernanceThere are multiple steps to take to ensure data quality they need to be taken at all stages of the data usage funnel Only by going with an incremental approach it is realistic to achieve the goal of having clean data
KsQ9ZJrY2WBGnq3ivacLGr,I while ago I listened to a podcast with Ian Gorton who talked about principles which rule systems processing vasts amounts of data Even though it has been already 4 yours I find many of his insights still holding true In this post I summarize them and try to put them into a present-day technology contextAccording to Ian the key problems with the Big Data are: They all boil down to a single issue: very large complex data sets are available now and it is not possible anymore to process them with traditional databases and data processing techniquesIn 2014 (when the talk was recorded) it was indeed true These days (late 2018) there is one more caveat to this: Moores law 4 years ago CPUs and memory were not as fast/not as optimized for parallel processing as of today Some size of data which might be considered Big Data back then right now could be handled by a single machine This boundary is only going up This means that for some slower organizations actually moving to Big Data tech doesnt make sense anymore as they can solve the problem by just buying faster hardware for their data server instead of building a cluster for Hadoop/SparkThe processing systems need to be distributed and horizontally scalable (ie you can add capacity by adding more hosts of the same type instead of building a faster CPU)If we define Big Data tech this way we can formulate principles to follow while engineering large data processing systems: In our cloud-enabled world the technical challenges of scalability and resiliency to failure bring in a trade-off Most of those problems can be solved by simply paying more money for cloud services Through outsourcing the system design and following architecture templates outlined by a cloud provider you can remain completely focused on using the data to solve actual problems Unfortunately this wont work forever Once you reach a certain size you will need to rebuild your system with very specialized components because off-the-shelf solutions wont work anymore You need to make a conscientious decision: choose between building own system slower but with certainty it will scale well or start faster with cloud servicesAnother concern which quickly appears when developing a Big Data system is making sure it works properly  testingHence how do you know that your test cases and the actual code that you have built are going to work anymore? The answer is you do not and you never willIf you are working with a Big Data system you can never know how it will behave in production because recreating the real conditions is too costly This means that the only reliable and predictable way to build such a system is to introduce a feedback loop which will tell you if you havent broken anything as early as possible what boils down to: continuous in-depth monitoring of the infrastructure and using CI/CD in connection with techniques like blue/green deploymentSome closing thoughts: Link to the original talk and transcriptOriginally published at pkoperekgithubio on September 27 2018
K8AD2BY4DQF7kTFnoM6oKF,A few years ago being a data engineer meant managing data in and out of a database creating pipelines in SQL or Procedural SQL and doing some form of ETL to load data in a data-warehouse creating data-structures to unify standardize and (de)normalize datasets for analytical purpose in a non-realtime manner Some companies were adding to that a more front facing business components that involved building analytic cubes and dashboard for business usersIn 2018 and beyond the role and scope of data engineers has changed quite drastically The emergence of data products has created a gap to fill which required a mix of skills not traditionally embedded within typical development teams the more Software Development Oriented data engineers and the more data oriented Backend Engineers were in a prime role to fill this gapThis evolution was facilitated by a growing number of technologies that helped to bridge the gap both for those of Data Engineering and those of a more Backend Engineering backgroundBig Data: The emergence of Big Data and the associated technologies that can with it drastically changed the data landscape with Hadoop open-sourced in 2006 it became easier and cheaper to store large amount of data Hadoop unlike traditional RDBMS databases did not require a lot of structuring in order to be able to process the data The complexity to develop on Hadoop was initially quite high requiring the development of Map Reduce jobs in Java The challenges of processing big data forced the emergence of Backend Engineers working on analytical data workflow It was not until Hive was open sourced in 2010 that the more traditional data engineers could get an easy bridge to get on boarded in this era of Big dataData Orchestration Engines: With the development of Big data large internet companies were faced with a challenge to operate complex data flow without any tools such as SSIS used for more traditional RDBMS working in this ecosystem Spotify opened sourced Luigi in 2012 and Airbnb Airflow (inspired by a similar system at facebook) in 2015 Coming from a heavy engineering driven background these orchestration engines were essentially data-flows as codePython being the language most of these orchestration engine were built on helped them gain ground benefiting based on the traction in the PyData ecosystem and from the increase use of python among Production engineers Traditional Data Engineers coming into this ecosystem needed to adapt and up-skill in software engineeringMachine Learning: The trove of data that was now possible to collect from the internet Machine learning quickly gained traction Until the advent of Hadoop Machine Learning models were usually trained on a single machine and usually applied on a very ad-hoc manner For large internet companies in the early days of Hadoop leverage machine learning models required some advanced software development knowledge in order to train and apply models into production with the use of frameworks such as Mahout leveraging upon MapReduceSome Backend engineers started to specialize in this area to become Machine Learning Engineers very production focused Data Scientists For a lot of startup this kind of development was however overkill Improvement in SKLearn a python project open started in 2007 and the popularization of orchestration engine made it fairly easy to go from a proof of concept by a Data Scientist to production ready workflows by Data Engineers for moderately sized datasetsSpark & Real-time: It was the release of Sparks MLlib for python in 2014 that democratized machine learning computation on Big data The API was fairly similar to the one Data-scientists were used to from the PyData ecosystem and further development of Spark further helped bridged the gap Spark further offered a way for data engineers to easily process streaming data offering a window towards real-time processing Spark enabled an increased contribution of data engineers towards data productsCloud development & Serverless: AWS was officially launched in 2006 its storage layer S3 had been built upon Hadoop the traditional big data platform Elastic Map Reduce was launched in 2009 making it easier to dynamically spin up and scale Hadoop clusters for processing purposeThe move to the cloud had multiple implication for data engineers The cloud abstracted physical limitations for most users it meant that storage and compute was essentially infinite provided one can pay for it Optimization previously done to keep business running waiting for new servers to be installed or upgraded needed not to be done anymore So was the work previously done tasks scheduling to allocate the load across time due to resource constraint The cloud by allowing for scaling up and down resources made it much easier to handle high peak batch jobs typical in data engineering This however came at the cost of having to manage infrastructure and the scaling process through codeThe introduction of Lambda function on AWS in late 2014 kicked off the serverless movement From a data perspective data could be easily ingested without managing infrastructure The release Athena launching in late 2016 pushed things further allowing to query directly onto s3 without the need to setup a clusterThe role of the data engineer is no longer to provide support for analytics purposes but to be the owner of data-flows and to be able to serve data both to production and for analytics purposesTo that end Data Engineering has been looking more towards a software engineering perspective Maxime Beauchemins post on functional data engineering advocates for borrowing patterns of functional programming and apply them to data engineering The emerging data ops movement and its manifesto in turn borrows from the DevOps movement in software engineering
DGLy3wbN97MoNVpB6vqWca,Storing data for distributed system can be a complex affair there are multiple approach and nuts and bolts that needs tweaking to get data storage just right for specific applications Choosing a storage approach has impact on performance storage cost redundancy engineering complexity etc and these decisions need to be taken in the context of the application its dataset and its usage patternsThere are two main ways to deal with storage redundancy one approach is through data replication the other is through what is called as Erasure Coding Both methods have their particular advantages and disadvantages and are better suited to handle different needsThe simplest approach to having storage redundancy is through data replication where we would be storing an exact copy of the data should anything happen to the original In computer storage such as desktops and servers this is usually been implemented through something called RAID 1 RAID stands for Redundant Array of Inexpensive DisksHDFS has typically been relying on a replication factor for reliability and redundancy The way this works is by relying on direct copy of HDFS blocks usually stored in different racksWhen a block or here a bit becomes unavailable it can be directly copied directly from a replicaHaving full replicas of the data means that if there is an issue sourcing the data from one copy of the disk it is always possible to retrieve it from one of the copy without interruptionIt also means that we can facilitate multiple concurrent reads on the same datasets This can be useful in cases where there are popular tables in a data-lake for instance if there are pieces of data that are inaccessible only then would there be a need to queuing operations not on the rest of the dataThere are drawbacks of this approach however the software needs to do some work to keep the replica in sync and there is the extra cost of keeping the replicasErasure coding is meant to provide a more space efficient way to create redundancy rather than relying on straight data replication It works by generating parity bits that allows to reconstruct the original data should one of the original bits go missingThe concept of erasure coding has existed for some years in different form as part of RAID 5 and 6 for special kind of memory (ECC) or for file archives such as par and par2 file extensions popular with newsgroup file downloaders It has been introduced in Hadoop 30 as part of HDFS-ECOne of the simplest form of parity check is to implement an even/odd parity bit The parity bit checks if the sum of all the bits is even or odd We can see that implemented in the picture belowWe have four different source bits 2 with 1s 2 with 0s Their sum is 2 which is even therefore there is no (0) remainder which becomes the parity bit Now lets imagine one of the four original bits go missing We can still recover the information contained in that bit using the parity bitIn the example above our third original bit is missing because we know the size of the original bits chunk (4) we know that the sum can only be 1 (if the 3rd bit is 0) or 2 (if the 3rd bit is 1) But because we have a parity bit equal to zero we know that the sum needs to be even The sum needs to be 2 and the third bit needs to be 1As we can see erasure coding offer a much more space efficient way to create redundancy than relying on replication Where with replication the total space taken was number of bits * replication factor  the total space occupied is now number of bits + (numbers of bits / bits per parity bit) If we compare that to the example above we had a multiplier of 2 for replication and of 125 for erasure codingThere are a few drawbacks from using this approach however repairing missing bit is more computationally intensive than a straight copy and it does take some time to reconstruct Another drawback is that based on one parity bit only 1 bit can be reconstructed at the time you would need to increase the number of parity bits if more than one is neededErasure coding provides more a redundancy upon failure where it is possible to reconstruct the missing information It does not offer the same ability to have multiple concurrent reads on the same dataset as it is possible to do with replicationData locality has an impact on performance on applications with distributed systems that can contain data far apart from its processing layer performance can be significantly impacted Approaches such as data-processing collocation coupled with sharding can bring about significant improvement on speedWhen dealing with data transfers there are a couple of factors to take into account the overall transfer speed and the latency The transfer speed is a factor of the overall size of the message that need to be transferred while latency is an overhead to initiate the data transferBoth of these factors can be bottlenecks when dealing with distributed and cloud computing The data transfer speed tend to be a major bottleneck when the message size tends to be fairly large as is usually the case with media content such as pictures or video while the latency issue can lead to bottlenecks when there are a fairly large amount of very short calls made to the datastoreThere are a couple approach to deal with some of these issues one is to use mini-batching to reduce the relative amount of overhead in the transfers the other is to collocate data with the applicationWith collocation since the data is stored closer to the application the latency tend to decrease and since the data is stored locally and does not need to be transferred through the network the overall transfer speed increases Platforms such as Apache ignite allow for this type of collocated computationCollocation particularly plays well a replication factory in which case it is possible to efficiently parallelize computation on the same partition of the datasetThis also applies to the different datasets that might need to be joined together This is something that can be addressed through what is called a database shard A database shard allows to partition the datasets based on a Shard key and group them together in the same serversIn large databases sharding has an impact particularly for join operations Join operations requires the matching and merging datasets based on set conditions usually relying on a specific ID If the data was not sharded and collocated it would imply that the data would need to be constantly transferred between servers resulting in a lot of inefficiencies and network I/OSharding however introduce an extra level of complexity when not handled natively within a database (such as with MongoDB)Beside storage locality compression methods play a big impact on performance and space utilizationBeside the space savings obtained from compressing data it can be beneficial to compress data for other reasons Compression can increase the actual data transfer speed it can also prove more performant to read a compressed file and decompress it on memory than reading a very large file straight out of disk particularly when dealing with highly compressible data such as text filesThe most typical compression algorithm used in Big data are GZIP Snappy LZO BZIP2 and LZMA Each compression algorithm has different performances for CPU utilization memory requirements compression ratio as well as compression and decompression timeThe Snappy and LZO algorithms tend to have really fast compression decompression time but relatively low compression performance overall making them well suited to handle Hot data GZIP has a more balanced overall performance while BZIP2 and LZMA offer a high compression ratio making them suited to store archives and historical dataDatasets can leverage leverage replication to achieve redundancy and higher performance but at a higher storage cost than erasure coding this can be particularly beneficial for frequently accessed data Archive/Historical data on the other hand might be more suited for erasure coding as a way to store large amount of data at a lower cost No matter what both measures have an impact on the total amount of data consumed and it is important to plan for this extra data needCollocated processing with the data is an impactful performance measure which reduces the impact of latency and data transfer Processing/data collocation is supported by some platforms such as Apache Ignite Sharding is a technique used that facilitate this move albeit at a cost of increased complexityCompression often makes sense for data in distributed system; there are multiple compression algorithms that can be used for storing and transferring data each with their own pros and cons and the choice of the algorithm should be situation dependent
A5BWSRa6Sr9iTztetcfywo,I have recently written about the approach to handle master data management matching and merging of records into master records Leveraging SSOT is however a bit more complicated than just being able to create these records It is also about being able to propagate these changesHard merge provides a consolidation of records that do not allow for disassociating the different records that were merged Propagating the changes coming from a hard merge vs a soft merge can be easier in some aspect and harder in other There are quite a few distinctions in the different approaches that can be taken to propagate the unification of records be it from leveraging an API to pushing actual updatesUsing the API for hard merge can quite straight forward most of the work happens under the hood and is abstracted from the other systems provided they are not doing calls to scrape the API incrementallyAn example of how that would be set up is shown above where every get request to the different IDs are pointing to the new updated recordFor API that allows to scrape the different changes on the data contained within it a different strategy needs to be applied The minimum consists in providing an updated set of information that contains all the records that were part of the merge and provide a status as to whether the record is active or not Better practice also provide information as to whether the records have been merged onto a different idThe example above represents a mockup of how an API allowing scraping could look like The API is calling a product catalog where the same product has been set up multiple times for different languages The same product is being represented in with an English French and Dutch name with different ids A merge strategy has been performed to consolidate the records representing the same product onto a master product representing the English version of the productThe French (record 2) and Dutch versions (record 3) have been merged into the English version (record 1) Each record contained initially contained one similar product that have been merged using an aggregation merging rule Records 2 and 3 have been in turn de-activated and provided with the information related to the record (English version/record 1) they have been merged ontoIt is the responsibility of the client system ie: the one calling the API to reflect the merge changes onto their system based on this informationThere are different ways to propagate a hard merge as a data feed integration From an active update that goes back to old records and updates their specific id relates a passive update that pushes the necessary information required to do a local merge to a Snapshot Update which provides a full replicas of the different merged entitiesIn some cases a system can be actively updated the records and their associated data It is the role of the integration pattern to updates the different pieces of related integration with the newly merged idLets have a look at what is happening in the example below There are different customer records present in the database each with separate transactions associated with these ids The IDs 2 and 3 are duplicated records of ID 1 Using an active integration with an external system after the merge has occurred the information will get updated onto the other systems The activity status of the merged records will get updated to inactive and the ids of the transactions associated with these user ids will be updated to the master recordThis type of integration often relies on the ability to perform an update or upsert operation by another id (in this case a transaction_id) than the id you want to modify This usually requires that this other id is set as a primary key in the external system you want to integrate with and that this system allows for this kind of updateIt is often a better pattern to provide in these associated events a master_id field rather than directly updating the records ids This allows us to retain the original data while at the same time offering the merging functionalityThe passive update relies on a similar approach to the approach taken for API scrapes The main difference is that instead of getting the information pulled from an API the information is being pushed to other systems When offering an incremental API and having taken the API scrape strategy to record propagation it a natural extension to provide a passive update feed webhookAnother approach to deal with record deduplication is to periodically provide a full sample of the merged dataset and clear any other records This type of integration pattern however has some limitations Using this approach offers some drawbacks: One of the ways to mitigate these issues with Snapshot updates is to consolidate a list of all the identities previously used for the recordIn the example below chaise (ID2) and stoel (ID3) have been merged on the record chair (ID1) but this maser record still holds a memory of the previously used ids for this master recordOnce the snapshot has been pushed it is the role of the target record to leverage the information contained into the id_list to re-route and re-link the different entitiesTo get a sense of which integration pattern to use for Hard merge propagation the system landscape and what the different systems can support is the first criteria to check If the system is not able to support the update of records for instance through an API call the active update strategy would be somewhat challenging to implementAnother consideration to have is the overall memory needs and transfer size of the different approaches The Snapshot strategy for instance can require a decent amount of memory in the different system it integrates with and usually involves some clearing of previous snapshots being sent Sending a full snapshot would also require a certain level of data transfer which might face limitations for a large volume of data in terms of API throttling issuesSystem responsibility is another factor to take into consideration when looking at which strategy to apply While in most cases it is the original merging system that is responsible for applying the merging and authority strategies it can be the case that some of the data needs to be merged locally In that particular case it is vital to ensure that the merging algorithm used in the target system matches the one used for the original merging systemSoft merging on the contrary to hard merges allows us to keep the original records and to remove the associations between the records as needed Propagating the changes in both the associations and the merged entity records requires a different approach than for hard mergesSimilarly to the hard merge strategy in the case of an API routing call everything works under the hood The client can extract the different values related to a given id but is not able to perform scrape on the full dataset In this context it is the role of the API system to apply the different merging and authority strategies to aggregate the recordsThe diagram above shows how this specific could work When an API call is made to any of the records to retrieve information there is a check on an internal association table to check which master id the record belongs to Then a query is performed to extract all the records having the same master id An authority strategy is then applied to the resultset and the API responds with the merged dataWhile for a hard merge propagation applying an authority strategy is relatively trivial to do and the more difficult issue to tackle resolves around record routing and entities linking For a Soft merge for the routing and entity linking it is usually sufficient to provide an update of the association table either through incremental update(pictured below) or by providing a full snapshotThe more complicated part for soft merge associations is the application of the authority strategy on the merge recordsOne way to deal with this is to provide at the same time the original record and the authoritative master record In this case the record is only fully soft- merged in the source system And changes in the association would result in an update in both the association table and in the concerned master records (master id one in this example)Another approach would be to reconstruct the same authority strategy on each set of records associated with the same master id This is something that might be practical to do when dealing with a reasonably simple authority strategy (eg time-bound strategy applied at record level) and the target system lets you use that kind of business logicAssociation with filtering can add an extra layer of complexity concerning both the calculation of the Master Record and the association of linked entitiesMaster Record needs to be resolved locally: In our example below we are running a small sport apps business and have a user signed up for 3 of our apps frisbee fan football freak and hoops & hoops Hoops & Hoops was our first app and we initially didnt think that we would need to be merging user profile until we came up with our other app Football Freak When Football Freak came out we revised our terms of service to a V2 which allows us to be merging user profilesIn this example only records with agreed tos ≥ 2 can be merged Yet we would like to provide the fully combined (all three identity) record as soon as John logs onto Hoops & Hoops and agree to the terms of serviceTo achieve this we need to be providing the system with: Using this set of information it is possible to re-apply the merging logic in the target systemMaster Record does not need to be resolved locally: In case the master record does not need to be resolved locally we can use the same approach as for the normal association table integration with master records The specific associations with filtering conditions can be collapsed and summarized invalid associations (ie which passed the filtering conditions)The example above shows how both the original records have been kept intact while pre-calculated merged master records have also been provided for John Smith ID4 and John Andrew Smith ID6Linked Entities: In the case of linking entities we need to consider the fact that we are merging another profile onto anotherThe Master record should keep all its associated entities while the entities linked to a record that is being soft merged should pass a filtering condition This can be the case for instance when legislation or the term of service only allows to leverage data points after the association of the profilesBelow is just such an example John our user of Frisbee fan and Football Freak app has agreed to the v2 of terms of service at 12 We have as his master record ID4 the one he uses for Frisbee Fan Because this is the primary identity we can leverage all the event data linked to it (Event A B and C) We are furthermore able to leverage all the linked events data to his associated profile ID 5 (Football Freak) that are passing the filtering condition (Event E and F)An example of how the soft merged linked entities could be computed in SQL is shown below: When looking at propagating there are a few factors to consider from resource utilization processing speed and the advantage and disadvantages of having part of the unification logic deferred to the other systemsResource utilization: Processing time for the calculation of the master entities can be quite intensive when dealing with transient identities (eg session-id) it is important to understand what type of load you would be putting in the systemSpeed: There are two aspects to consider in terms of speed the processing time to calculate the merged profile and the time it takes to re-calculate the combined profile based on disassociationsLogic deferral: Different approaches defer a different amount of logic to the external systems When integrating with them it is worth checking whether the systems can apply the strategies needed to unify the records and whether it would be desirable for the systems to do soPropagating record merge changes is a complicated affair that depends on the chosen merging and authority strategies target systems capabilities and overall vision of how the different systems should be interactingThere is more than one way to approach the problem and identifying which approach is appropriate requires an intimate knowledge of the landscape data and strategies used for unification
nqrB3RNKbKMpqS7MMfrAaE,Clickstream events collectors are applications that let you collect raw clickstream data from the front-end of an application There are multiple reasons why you should rely on these event collectors and setting them up isnt that complexThere is at the same time an overlap and a complimentary value between these solutions Snowplow and Divolte are event collectors collecting the raw data that is needed to do deep analysis Raw data can be used to increase the depth of analysis it can for instance be leveraged for conversion rate optimization (CRO) for instance where it allows for a granular analysis of the different customer touch point using click path analysisExporting clickstream to raw data is a feature that is offered within Google Analytics 360 but not the free version and if you are only looking at acquiring the 360 version for this feature going the snowplow route might turn out to be more cost effectiveThe second complimentary value lies in using them to potentially bypass some of the tracking restriction of ad blockersSetting them up on your own domain allows to bypass domain black-lists and modifying or having a different tracking script lets it avoid checksum detection The ability of further customizing the tracking script name allows it to bypass ad-blockers looking for track or analytics in their nameThese solutions allow the data to be ingested as a data stream and creating an application it is possible to push back this data to Google analytics or other analytics toolsThe clickstream collectors ability to push data to a message broker such as Kafka allows to include them as an integral part of an application The type of applications that could rely on this type of data range from real-time reporting real-time marketing trigger to real-time (prediction) model evaluationTracking Script: The tracking script role is to capture the different actions performed by users browsing the website and pushes these events to the clickstream collector APIClickstream collector API: A clickstream collector API is merely a receiving end point of an API that might perform 1) request authorization 2) schema validation and push the data to a message broker for ingestionMessage broker: A message broker is there to allow for the asynchronous processing of the data One of the most popular message broker for data is Apache Kafka Applications can directly consume the data stream to compute real-time aggregates or filter the streamData Sink: A data sink will take the incoming data from the message broker and push it to the storage layer This is usually a S3 bucket on AWS a DataLake storage on Azure or a plain HDFS fileStorage Layer: The storage layer provides a long term storage for the incoming data Most compute engine on Hadoop such as Presto Spark or their cloud equivalent such as AWS Athena are able to query files on a bucket storageAt their core setting up these solutions can be done through setting up Container applications both snowplow and divolte provide docker images These can be setup on a VM a container service such as Azure Container instance or on Kubernetes or docker-swarmThe containers should interact with a load balancer for autoscaling and a messsage queue The docker-compose file of divolte for example bootstrap a Kafka instance for exampleThere are different methods for the implementation of these clickstream loggers from a front end perspectiveUsing a clickstream event connector can bring a lot of benefits in terms of how your data is being tracked and in the granularity of the data available or making the data available in real-time to the application There are open source solutions for it that can easily be deployed based on docker containers
HdwRhoV5pn26zDF63TrzT8,Analytics India Magazine in collaboration with Google Cloud and Qubole is organising a workshop for developers who work extensively on data analytics platforms This is a huge opportunity for the developers and data engineers who are looking to gain hands-on experience on how to leverage Google Cloud Platform and build end-to-end solutions
MqDgFJqf5CpCz2iBZPL6Ci,Big Data has become a very popular term among the people in IT business its even popular outside of data engineering professionals So why everyone keeps talking about it? Where all this buzz is coming from? It is coming from\xa0any\xa0direction: cryptocurrencies online advertisements social media e-commerce mobile app games\xa0and\xa0so\xa0onIn past twenty years usage of the internet has amazingly increased and it keeps increasing even faster day by day\xa0Revolution of social\xa0media\xa0has been changing our lifes in any\xa0possible\xa0meaningWe have Instagram Facebook Twitter Tinder Uber Linkedin Health Trackers Running Partners and many more others which we have been using depending on our taste So what if we are looking at the pictures of others reading what they posted looking for a job on Linked-in; where is the revolution? Well not only everything is connected all world is connected right now; every single person is literally connected to each other somehow If it is not the revolution then I dont know what is You can check out drastic increase of internet usage in last 15 years through link below: https://wwwpewresearchBelieve or not but you leave your footprints whenever you are\xa0online Every single actions you make when you are online is getting recorded And its possible to follow this footprints by using a piece of code called Cookies I bet everybody remember those pop-ups telling us to accept terms of use There is many other ways to track actions of a website visitor but cookies are still the most common way to do it No need to get excited\xa0or feel concerned\xa0about\xa0that\xa0footprints There is also strict regulations called GDPR which is all the companies have to comply withImagine a huge telecom company and its 30 million 50 million of customers Lets say you are the CEO of that company What would you do if you could have opportunity to get to know your customers more? I bet youd take it So that is exactly what companies are doing they are taking advantage of the knowledge they have about their customers to provide better services and gain more customers from potential audience We can understand that there must be many data sources; each of your likes on Instagram your comments your searches on Google your followers and twits on twitter the last job you viewed on linked-in etc…  those are the actual data you have generated during your scrolling sessionsAnd furthermore most of us use an email account to sign in to social media systems which means we provide very good hint about who we are With using our email now they can know which tweets we are posting what we searched for on Google which GSM operator we are using which banking company we are working with which web-sites we have visited and what pictures we like So please just use your imagination to understand the variety of data you are generating when you are online and imagine how valuable insights that data can deliver about youIts a hard process to combine such an amazing amount of information companies must have amazingly big carefully built systems and computation power to get\xa0this\xa0job\xa0doneI can give you more example of Big Data use cases like scientific studies production bands aviation\xa0AND defence industry media industry streaming platforms pharm companies etc But that is not our goal right know; it supposed to be brief explanationSo I feel like I already got lost enough trying to tell you about the concept of Big Data and I want to say no more about\xa0it the idea is clear: using high volume or/and fast emerging data combining real-time sources and processing making complex calculations and running multiple operations in order to get desired results means Big DataHadoop came into stage almost 15(13 actually) years ago promising integrated data storage and multiprocessing infrastructure With Hadoop you can create clusters consists of multiple computers(servers) called nodes Nodes can form a cluster togetherIn an Hadoop cluster every node communicate with others to share their states and information This communication ensures availability and consistency of the clusterWe are talking about cluster aka group of items How you can manage all the members of a group and get them work together sharing one purpose? There must be a control mechanism right? Hadoop follows Master/Slave paradigm of distributed systems to manage the cluster In this architecture each node has one role within a cluster which may be master or slaveYou have to set one of your machines as master node which is called NameNode and the others will be slave nodes which is known as DataNode The NameNode is the centerpiece of an HDFS file system It keeps the directory tree of all files in the file system and tracks where across the cluster the file data is kept It does not store the data of these files itself
AegMdaX6ZWaW6c7Lby46Nr,Gitlab has a cool internal culture that makes things public by default This includes their entire Data Analytics pipeline I wanted to get familiar with their infrastructure so I decided to take their work and adapt it for my own purposeA while back I wrote some code to pull data from Fitbit Trace (snowboard tracking app) and some snowfall data in order to do some data analysis about last years ski season I decided to automate the ingestion of Fitbit and Trace data and do my transformations using DBT instead of PandasThe (simplified) pipeline consists of: I created a Gitlab group called snowboard-analysis then I forked both the analytics and data-images repos into it Then I created a Kubernetes cluster (see Setting up GKE) named data-ops in us-west1-a with 6 nodes from the Gitlab group Kubernetes page Im using these values for name and zone since theyre hard coded in a few places and didnt feel like changing them Although 5 nodes would suffice 6 nodes lets it scale when theres multiple tasks running at the same time You could also enable auto-scaling in GKE to adapt this I also created a group runner for CI in order to be able to run the CI/CD scripts when I push changes to the repoAfter cloning the repos I had to change the hardcoded repo and docker registry paths with the following command: I also removed all unnecessary DAGs custom extractions DBT models & tests Snowflake roles users and dbs etc I left a few things that seemed helpful like some of the DBT and Snowflake related DAGs as well as Sheetload which can pull data from Google sheetsWith all the unnecessary stuff gone I created my extractions for Fitbit and Trace as well as the DAGs for each I also had to update the requirementstxt file for the data image in order to add the Trace and Fitbit packagesCreate a Google Cloud account if you dont already have one If you create a new one you get $300 of credit which is perfect to try out this project for free Once youve created it enable the GKE API in the consoleThen go and create a service account with the Kubernetes Engine Developer role When asked about a key go ahead and create one and download itOnce the cluster is set up (see above to set it up through Gitlab) its time to set up kubernetes locally to use the cluster we created An easy way to do this is: Now in order to upload the necessary secrets create a secretsyaml file (filling in your own values): I put everything into stringData instead of data just to make it easier to see and edit However for a prod environment youd probably want to use data and do the base 64 encode yourself The cloudsql-credentials is the service-account credentials you downloaded from GoogleTo upload the secrets simply run kubectl apply -f secretsyaml When running airflow locally for testing the namespace is set to testing In order to create the namespace run kubectl create namespace testing and to upload the secrets run kubectl apply --namespace testing -f /secretsyamlGo to https://wwwsnowflakecom/ and start a free 30 day trial When you create an account you will also be creating a main user with SYSADMIN ACCOUNTADMIN and SECURITYADMIN roles Although the Gitlab data team uses a variety of users roles databases and warehouses I simplified most of it to use only one user and a few different roles I still have quite a bit to learn about permissions but I was able to get things to run after playing around with it for a bitThe repo has a rolesyaml file that describes each role user warehouse db and schema along with their permissions They have a DAG that uses Meltano permissions command to generate the SQL needed for setting up the permissions Currently this can only run in dry mode but I imagine the goal is that Meltano should be able to handle setting all the permissions I had to run this and copy-paste the SQL and make a few changes to get things to work smoothly There was a bit of a chicken-and-egg problem since that DAG uses PERMISSION_BOT role to run so I had to create that first and give it the right permissions I could have simplified things a lot more to use a single user and role and it would have been much easier to set up but I wanted to get a better sense for how its done at GitlabMost of the Airflow setup is done in the Airflow image I made some changes to the config mainly to lower the number of active connections and I also made some changes to the images to make them a bit smaller I also had to change the hardcoded project name in the deployment manifest to match my projectFor simplicity I just used a SaaS Postgres db for airflow data I used https://wwwelephantsqlcom/ but any public Postgres db should work One thing to keep in mind is that youll have anywhere between 10 20 active connections at any given point (from the webserver scheduler and workers) After that I just added AIRFLOW__CORE__SQL_ALCHEMY_CONNand AIRFLOW__CORE__FERNET_KEY to my Kubernetes secrets as described aboveTo deploy simply: This creates and starts all the necessary Kubernetes components for deploying Airflow and exposes the webserver To access it locally run kubectl port-forward deployment/airflow-deployment 1234:8080 then you can open it up at localhost:1234DBT is mainly run as a DAG Theres a docker image that bundles all the necessary packages to run DBT commands and this image is used to run the different commands as tasks within the DAG The DBT project structure is a sub-directory of the analytics repo The main thing I did here was create my own Fitbit and Trace dbt models that do the following: One really cool feature of DBT is the ability to test the data coming out of your models This is helpful to make sure you dont have duplicate entries null values etc I still havent played around with this but will get to it laterIn production the DBT profilesyml file is generated using env vars coming from secrets However for testing locally you should create your own ~/dbt/profilesyml file to look something like: The main repo has a docker-composeyml and aMakefile which make it easy to test DAGs and DBT models locally However this does still need the Kubernetes cluster to run the Airflow tasks on and it also needs the secrets in the testing namespace (see above)Whenever you run Airflow DAGs the DAG definition will be picked up from your local repo but the Kubernetes pod will pull the latest image from the registry and the latest code from the hosted repo using the specified GIT_BRANCH so your changes to extraction code will need to be pushed firstTo use the provided docker-compose and Makefile youll need to set a couple of environment variables: You can read a bit about how to test things locally here but it mainly involves setting up the Airflow db on a local postgres container with make init-airflow going into the Airflow container with make airflow and running airflow commands and/or going into the DBT container with make dbt-image and running dbt commandsNow you may be wondering why? Isnt this overkill? Well yes yes it is but the reason I did this was to get familiar with the Gitlab pipeline This was a great way to learn about Kubernetes DBT Snowflake and running tasks using the KubernetesPodOperator on Airflow Although I barely scratched the surface with each of these I now have a much better basis to continue to learn more about these different tools IMO its a lot easier to learn by doing than by just reading
ceci6awrvVBij4upZEhtgo,The goal of this article is to compare the performance of two ways of processing data The first way is based on the Window function The second way is based on Struct These two ways of processing data sometimes can help to achieve the same results but in different waysThe window function performs a calculation over a group of records called windows that are in some relation to the current recordThe struct function is used to append a StructType column to a DataFrameLets look on a data: The parent field is StringType The time field is TimeStampType The child field is StringTypeEach child should belong to several parents In this case we need to find the last parent for each child The last parent is the parent that has the last timestamp In results we will have pairs as parent-childThere are several ways to implement it on Apache Spark From my point of view two the most readable and the most elegant solutions can be usedThe solution looks like: Firstly the string with the child list has been converted to the array using the UDF functionAfter that the data should be filtered in case of an empty array is existing The previous steps are necessary to clean data Now data is ready and we can explode our array Because of this we get pair as parent-child but the goal is to find the last parent for our child Window function helps us with this deal max($utc_timestamp) over byChild returns the max utc_timestamp over a group of records that are related to the current child As soon as max utc_timestamp has been found for each child we just need to compare current utc_timestamp with max utc_timestamp If they are the same it means we found what we were looking for Its the last parentThe solution looks like: Here we made the same steps to clear data The following steps are to prepare a pair of utc_timestamp with parent by using struct function Its similar to the tuple After that we can find a parent with maximum utc_timestanp by a child using a simple max function In the case of using the max function with the struct column this function compares values by the first field in the struct and if these values are the same the function will compare by the second field in the struct In the last step the parent can be gotten from the struct with the following code: select($pparent)Thats it We got the same result as in the previous optionThe story shows two options of transformation data to find the last parent for the child Both options look good and you can choose any option you prefer more From my point of view the second option looks more readable but it requires knowledge about how the struct column works with aggregation functions
GU4Ke2grTjd8JUNsCrjhmt,In this blogpost I will explain what partitioning and clustering features in BigQuery are and how to supercharge your query performance and reduce query costsPartitioning a table can make your queries run faster while spending less Until December 2019 BigQuery supported table partitioning only using date data type Now you can do it on integer ranges too If you want to know more about partitioning your tables this way check out this great blogpost by Guillaume BlaquiereHere I will focus on date type partitioning You can partition your data using 2 main strategies: on the one hand you can use a table column and on the other you can use the data time of ingestionThis approach is particularly useful when you have very large datasets that go back in time for many years In fact if you want to run analytics only for specific time periods partitioning your table by time allows BigQuery to read and process only the rows of that particular time span Thus your queries will run faster and because they are reading less data they will also cost lessCreating a partitioned table is an easy task At the time of table creation you can specify which column is going to be used for partitioning otherwise you can set up the partitioning on ingestion time Since you can query this table in the same exact way of those that are not partitioned you wont have to change a line of your existing queriesAssuming that sampling_date is the partitioning column now BigQuery can use the specified values in the where clause to read only data that belong to the right partitionsIn the cases above both the loaded data and the query results have to belong to the referenced partition otherwise the job will failClustering is another way of organizing data which stores one next to the other all those rows that share similar values in the chosen clustering columns This process increases the query efficiency and performances Note that BigQuery supports this feature only on partitioned tablesBigQuery can leverage clustered tables to read only data relevant to the query so it becomes faster and cheaperAt the table creation time you can provide up to 4 clustering columns in a comma-separated list eg wiki title You should also keep in mind that their order is of paramount importance but we will see this in a momentIn this section we will use wikipedia_v3 form Felipe Hoffas public dataset which contains yearly tables of Wikipedia page views These are partitioned by the datehour column and clustered on wiki and title columns A single row may look like this: The following query counts broken-down per year all the page views for the Italian wiki from 2015 01 01If you write this query in BigQuery UI it will estimate a data scanning of 45 TB However if you actually run it the final scanned data will be of just 160 GBWhen BigQuery reads only read rows belonging to the cluster that contains the data for the Italian wiki while discarding everything elseIt is important because BigQuery will organise the data hierarchically according to the column order that is specified when the table is createdLets use the following example: This query needs to access all the wiki clusters and then it can use the title value to skip the not matching clustersThis results in scanning a lot more data than if the clustering columns were in the opposite order title wikiAt the time of writing the query above estimated a scanning cost of 14 TB but it actually scanned only 8756 GB of dataLets now invert the clustering columns order putting first title and second wiki you can do so using the following command: Running the Pizza query on our new table my_project_id:dataset_uswikipedia_2019 should be much cheaper In fact while the estimation was still of 14 TB the actual data read was just of 263 GB that is 33 times lessAs final test lets try filtering on the wiki column: The data read estimation is always the same but now the actually data read jumped to 14 TB (the entire table) whereas in the first example the actually data read was just of 160 GBNote: Since BigQuery uses a columnar store title is not null ensures that we refer always to the same number of columns in every query Otherwise the data read from the last query is lower because we refer to fewer columnsIt is evident that choosing the right clustering columns and their order makes a great difference You should plan it accordingly to your workloadsRemember always partition and cluster your tables! It is free it does not need to change any of your queries and it will make them cheaper and fasterAuthors Github and Twitter
7LxqNgUba6B3D8xst4crLi,As a Data Engineer on of the common use-case is to adjust date across various sourcesIt is common that the data we get from source system may not be in required format as show below and it is wise to load the data as string as loading it as date results in loss of data df_date=dfselect(fto_date(dtdd-MM-YYYY )Github Link: https://github
S9DPurZvbzULVye97QdjWB,Lets say you want to scrape the top 10 links that show up when you search anything on youtube At the same time you also want to scrape the top 50 comments for each of the top 10 links and finally do sentiment analysis on the scraped data Of course you dont want to do it manuallyHere are the steps you can take to do this • Video Title text • Video URL • Subcription Channel • No of Views • Date link posted • Data Cleanup: This consumes a lot of time because people can comment in any language can use smiley sarcasm etc There are many python libraries which can help you to clean up the data Go ahead and explore further on this • Sentiment Analysis: Once you have the clean data you can do NLP and do sentiment analysis and finally do visualization on top of thatBelow are the steps with the codeStep 4: Invoke webdriver and launch youtube siteStep 7: Launch url for top ten extracted links For each URL -scroll down to required position to load comments section -sort by top Comments -scroll down two times to load at least 50 comments -for each comment(50 or less) extract elements below and put them in try catch block to handle exception in case particular element is not present for comment • Comment text • Author name • Date comment posted • NoStep 8: create dictionary for elements extracted from main link and child link and write to opened csv fileHere is the output consoleAnd here is the sample output extracted in the csv fileOnce you get the data in the csv file you can do further analysis using various python librariesSelenium is a famous library for web scraping using python Go ahead and play around with this library to extract data from various other websites But before that do check if it is legal to scrape data from their website I think you can use web scraping for learning purpose but not for commercial use caseFeel free to reach out to me if you have any questions
JkSgK2VhFy9vdUNszg5Kj4,"Testing is really important for coding It lets the code surely works It is really a way to save time rather than time lost as it is thought But I definitely dont ignore the fact that unit-testing is one of the toughest things among coding phases If you are dealing with data it is even harder to do it Many people give up on that challenging phase If you care about the quality of the work that you deliver and making sure you deliver things that you think are working properly you have to be into it Making mistakes is the nature of this job This is almost nothing about being a good developer A good developer can also do kind of simple mistakes We are not a machine But what makes us bad developer is not using machines to test and make sure our work is really done I mean unit-testing Lets put this aside since this is a topic that should be discussed in another articleAs I said when you are dealing with test needs data it is hard to prepare those kinds of test data which is called provided data or initial data and there are two ways of feeding your unit-tests with the provided that One is what we call file-based which you provide the test data in CSV files or another which is more programmatical way where you insert into the tables in Java We will look at the file-based one in this articleThe library that makes it possible to test Hive scripts is called Hive Runner It is a library that is built and open-sourced by Klarna It is a zero-installation library which means that you just import a java library and write your test and no need for further integration with anything It means no Hadoop HDFS hive installations are neededAs you may guess the test needs to be prepared by executing those scripts to have the tables preliminaryschoolstudent""Let me explain the code; hiveShell is starting automatically by default If you wanna set some variables before it starts you need to setatuoStart as false in annotation and set your variables in setUp and start hiveShell manually I didn't need it in hereIn my test data files there are 10 students 5 students at Cumhuriyet İlköğretim Okulu school 2 students at Atatürk Lisesi school and 3 students at Samsun Anadolu Lisesi schoolYou can check and play with the example in this article at https://github"
7n4v2eMeF9F4s54m5u9Jsv,A data pipeline is nothing but a flow of data from one system to another system As a Data Engineer we often need to build or maintain data pipelines traditionally known as ETL pipelinesHere in this article I will be demonstrating the ETL process and how to automate it using AWS Serverless services like Lambda and storage services like S3I am a beginner and wanted to make use of time at home during this global pandemic to create a data pipeline from scratchhttps://covid-19-coronavirus-statisticsprapidapiAs the data is small most of the services will be free to use in AWS The main agenda of this article is to Extract the data by invoking an External API store the raw JSON response into an S3 bucket and transform and store the refined data into S3 We will also schedule the pipeline to make it completely automatedNote: We need an AWS account (requires a credit card) and a little bit of python knowledge to follow alongLet us now look at our architecture before getting our hands dirtyThis is our Data pipeline which extracts the data from API using AWS Lambda Python function and stores it into the S3 raw folder Then we use another Lambda Function to read information from the raw folder and transform it into CSV which is more organized and store it into the S3 rfnd folder Then we make use of AWS Quicksight to quickly create a Visualization DashboardSteps to create a Lambda function: Go to AWS console and type AWS LambdaWe can either choose a function from scratch or use existing blueprints or browse from a serverless app repository We are going to create a function from scratchSince we are creating a python function we will choose python36 as the environment Give a function nameWhen it comes to role we can create a new role with basic lambda permissions which creates and uploads all the logs to Amazon CloudWatch logs We can also create a new custom role or choose from an existing role For this project we will create a new role with basic lambda permissions and s3 read and write permission because our lambda function will read and write objects into s3Also if you cant create a role from lambda function go to IAM and create a new role with AWSLambdaBasicExecutionRole and AmazonS3FullAccess permissionsNow from the options choose use an existing role and select the role which you have just createdAfter creating a function you can either edit code inline or zip the whole package and upload it here Since we are using requests module in python to send an HTTP request we are going to download the requests module in the folder using pip install requests -t  Now zip the whole package and upload it to AWS lambda All our packages and lambda function must be inside our root folder so copy all the contents and paste it in our root folder and delete that folderThe new folder structure looks like thisNow paste your python code which sends HTTP requests to external API inside your lambda_functionpyCreate a test function by clicking on the select a test event and configure test event You can just give test: test JSONNote: If everything is good you can see the 200 response and your response JSON will be stored inside your s3 bucket raw folderCreate another Lambda function to transform and store as CSV: Follow the above steps to create another Lambda function which reads and stores the data in the form of CSV into the rfnd folder The code to do this is belowAutomate the pipeline using AWS CloudWatch Events and Lambda TriggersNow if you want your lambda function to execute regularly you can use AWS CloudWatch Events to schedule a cron job Click on Trigger and select AWS CloudWatch EventsCreate a new rule Give a Rule name and Rule DescriptionI scheduled my lambda to run everyday at 07:00 PM cron(0 19 1/1 * ? *) You can use Cron Maker to generate your expression and add the triggerThe next part is whenever this lambda function executes and stores raw information into the raw folder I want my second lambda function to triggerAdd the trigger and select the S3 service and provide bucket information and prefix and suffix parametersVisualize using AWS Quicksight: Now you have your CSV file in your rfnd folder its time to visualize the data using Quicksight Go to Quicksight and click on new analysis Add a new dataset Select your data source which will be s3 in this caseEnter a name for your data source and upload a manifest file that contains information about your S3 bucket location and file formatsSelect the Geospatial visual type and give the city your Geospatial field and confirmed(sum) as your sizeCongratulations you have created your own Automated Data Pipeline for free using AWS free tierI know you might be excited to share your visualization with your friends but unfortunately Quicksight wont let you do thatNote: This is my first experience creating a Data Pipeline from scratch using AWS and Serverless model I have learned many things working on this Please do let me know your thoughts on this or any other better approaches
PBtVVa5m2wdfHdAPQqZcsk,"With the exponential growth of data and a lot of Business moving online it has become imperative to Design systems that can act in real-time or near real-time to make any business decisions So after working on multiple backend projects through many years I finally got to do build a real-time streaming platform And while working on the project I did start experimenting on the different tech stacks to deal with this So I am trying to share my learnings in a series of articles Here is the first of themAlso in this series the main focus will be on how-to rather than how-does-it Well spend most of the time learning how to implement our use case However I will cover some theory as and when requiredThis post is aimed at engineers who are already familiar with microservices java programming language and looking to build their first real-time streaming pipeline This POC is divided into 4 articles for the purpose of readability They are as below:: A proof of concept on how to build a real-time streaming(CDC) pipeline There would be efforts needed in terms of optimization on the same to make it production-ready""Imagine that you work for an E-Commerce company selling some fashion products You have the business team that wants to make some decisions based on some real-time updates that are made on the backend systems They want to view some dashboards and views for the same So let's assume that the backend is built on microservice architecture and you have multiple systems interacting with each other during any user operation and every system interacts with different databases""Let's consider three of the systems for our POCSo for this example the business team wants to create an outbound dashboard This contains the number of orders types of items sold cost of each item and cost of shipping the items This would be constantly updated at any point in time in multiple systems based on user and real-world actions as seen belowOnce the customer has selected and the payment is made the order goes to the warehouse and from there the item is sent to the customer Consider that after each action one system would be updated in real-time""We know that this data is present in Order Service Warehouse service and logistics service Assume all of them are using the MySQL database and all of them are updated in real-time So now that we have looked at the use case let's think about what could our solution to this problemPossible Solution: We need to figure out a way to capture all the updates/inserts that happen in the different databases in different services and put it in a single place from where we can work on building some reports and working on some analytics So this is where Kafka Connect and Debezium comes inSo before we jump into the implementation of this system we will need to understand a few concepts They are as below:: Change data capture: Change Data Capture (CDC) tracks data changes (usually close to realtime) CDC can be implemented for various tasks such as auditing copying data to another system or processing (and reacting to) events In MySQL the easiest and probably most efficient way to track data changes is to use binary logsThe binary log is a set of log files that contain information about data modifications made to a MySQL server instance The log is enabled by starting the server with the --log-bin optionThe binary log was introduced in MySQL 32314 It contains all statements that update data It also contains statements that potentially could have updated it (for example a DELETE which matched no rows) unless row-based logging is used Statements are stored in the form of events that describe the modificationsOpLogs: Similar to MySQL Binlogs MongoDB has something called Oplogs which are similar to bin logs and are used for CDCDebezium: Debezium is a set of distributed services to capture changes in your databases so that your applications can see those changes and respond to them Debezium records all row-level changes within each database table in a change event stream and applications simply read these streams to see the change events in the same order in which they occurredBefore we dive into the problem statement one thing that I did want to put forth was that in this POC I am building a Complex Event Processing (CEP) system There are very subtle differences between stream processing real-time processing and complex event processing They are as below:: This depends more on the business use case The same tech stack can be used for stream processing as well as real-time processing""So now that we do have an overall picture in terms of what we want to achieve and what are the concepts that are involved the next step is to understand the overall technical tasks and the tech stack that we would be using to build our system Let's take a look at the sameLets dive into the tech stack that we would be using for this POC They are as below:: So in terms of overall tasks we will be splitting them up as below:: So in this article we looked at the below:: Next steps: In the next article we will look at how to setup MySQL database for BinLogs to be written and docker setup to create the required infrastructure in our local systemIf you do like this article please do read the subsequent articles and share your feedback Find me on LinkedIn at rohan_linkedIn"
RewxJXAw4aVmLN2pk9MbW3,"In article 1 of this POC we looked at the key concepts required to build the real-time streaming system in article 2 we looked at the MySql setup and the local infrastructure setup In this article we are going to look at reading the real-time data ingested using Kafka streams perform some aggregations on them and create indices in elastic search We will as usual start with some basic concepts then look at the technical tasks that we have and then the final implementation of the systemNote: If you need a better understanding of the entire problem please read the first and second part of the article it contains detailed information regarding the problem statement the architecture infrastructure and MySQL setupKafka Streams is a client library for building applications and microservices where the input and output data are stored in Kafka clustersTo interact with streams in Java Kafka provides The Kafka Streams DSL (Domain Specific Language)Two main abstractions that we are using from streams DSL are as below:: I am gonna quote the example provided in apaches official documentation for both of the aboveIn terms of the overall technical tasks we had the below:: We have finished tasks 1 and 2 in the first article in this series along with getting a detailed understanding of the different concepts required to be understood for the problem Now lets jump to the rest of the tasks:: So now that we have our Kafka topics created the next step is to read the streaming data perform some aggregation using Kafka streams as required and store this in elastic search""So let's look at what we want our final index to contain and its sources before we proceed with any code""Let's look at points 1 and 2 via codeWe are listening to logisticsTopicorderTopic and wmsTopic to begin withWe are initializing a new streambuilder and creating an orderKtable logisticsKtable and wmsKtable from the corresponding topics We are as of now not doing anything with the streamed dataSo to construct our final aggregated view we need to create a few intermediate aggregationsWe are first processing the orderKTable and wmsKTable first setting the orderId as keyNow that we have our individual tables ready the next step is to create the intermediate aggregate tables We first join order and wms table on orderId to create an aggregated view with shipmentId as keycreateOrderWmsStream method takes the merged stream and sets shipmentId as key The merging of streams is done in assignValues methodAssignValues takes the two incoming streams it extracts the JSON from them and we use this to create our result DTO objectNow our next step is to merge this table to shipmentKtable to arrive at our final aggregate streamassignValues and apendValue methods are used purely used for setting values to the aggregated viewsSo with this we have our final aggregated table readyWe read the final aggregated stream and we create a map in the controller from it the map s key are the columns of the DTO and values are the DTO values This map is required to store data in ElasticsearchOnce we have created a map we call the service class and also provide the index that we want to create This is done in the below line""Let's look at the service and the DAO class which is used to persist data into elastic search""we check if the index already exists if it does we don't create the documentsElastic search has a java high-level rest API which can be used for operations to work with elastic search Here we are using IndexRequest to create a new index in elastic searchThe above statement is used to create a new request to create an elasticsearch Index We create an instance of RestHighLevelClient as mentioned earlier to interact with elastic search from javaThe above statement basically does the creation of indices in elastic search""Now that we have written code to create aggregate views required for business the next step is to make updates to the created index based on individual event updates in real-time Let's have a look at how we can go about this Let us look at how we do this for one of the events from wms system We can replicate the same for other systems as wellWe first listen to the wmstopic and create a stream from itOnce we have this stream we need to check if the stream is an update or just insert we figure this out by checking if the stream has a before the object Existence of before object means that the current record is an updateWe now traverse through each record and we call the service class to do our updates""The above is the controller for working with updates Let's look at the DAO for the sameSo what I am doing here is I am using UpdateRequest which is the java high-level client class provided to work with updates to Elasticsearch indices We again use RestHighLevelClient to update the index""We have now read from the stream performed some aggregation created an index and performed some updates Now let's picture the entire thingSo now that we have reached the final step lets recap everything that we have done in this exercise:: Our last step is to verify the code in Elasticsearch I also am covering the code set up in our local for those who want to try this outIf you do like this article please do read the subsequent articles and share your feedback Find me on LinkedIn at rohan_linkedIn"
WdfqRsSk7tRnq6UDe8btxJ,As a data engineer it is quite likely that you are using one of the leading big data cloud platforms such as AWS Microsoft Azure Google Cloud for your data processing Also migrating data from one platform to another is something you might have already faced or will face at some point In this post I will show how I imported Google BigQuery tables to AWS Athena If you only need a list of tools to be used with some very high-level guidance you can quickly look at a post that shows how to import a single BigQuery table into Hive metastore In this post I will show one way of importing a full BigQuery project (multiple tables) into both Hive and Athena metastore There are few import limitations for example When you import data from partitioned tables you cannot import individual partitions Please check limitations before starting the processIn order to successfully import Google BigQuery tables to Athena I performed the steps shown below I used AVRO format when dumping data and the schemas from Google BigQuery and loading them into AWS AthenaStep 1Step 2Step 3Step 4Step 5Step 6So why do I have to create Hive tables in the first place although the end goal is to have data in Athena? This is because: So Hive tables can be created directly by pointing to AVRO schema files stored on S3 but to have the same in Athena columns and schema are required in the CREATE TABLE statement One way to overcome this is to first extract schema from AVRO data to be supplied as avroschemaliteral  Second for field names and data types required for CREATE statement create Hive tables based on AVRO schemas stored in S3 and use SHOW CREATE TABLE to dump/export Hive table definitions which contain field names and datatypes Finally create Athena tables by combining the extracted AVRO schema and Hive table definition I will discuss in details in subsequent sectionsFor the demonstration I have the following BigQuery tables that I would like to import to AthenaIt is possible to dump BigQuery data in Google storage with help of Google cloud UI However this can become a tedious task if you have to dump several tables manually To tackle this problem I used Google Cloud Shell In Cloud Shell you can combine regular shell scripting with BigQuery commands and dump multiple tables relatively fast You can activate Cloud Shell as shown in the picture belowFrom Cloud Shell the following operation provides the BigQuery extract commands to dump each table of the backend dataset to Google Cloud StorageIn my case it prints: Please note: --compression SNAPPY this is important as uncompressed and big files can cause the gsutil command (that is used to transfer data to AWS S3) to get stuck The wildcard (*) makes bq extract split bigger tables (>1GB) into multiple output files Running those commands on Cloud Shell copy data to the following Google Storage directoryLets do ls to see the dumped AVRO fileI can also browse from the UI and find the data like shown belowTransferring data from Google Storage to AWS S3 is straightforward First set up your S3 credentials On Cloud Shell create or edit boto file ( vi ~/boto) and add these: Please note: s3us-east-1amazonawscom  has to correspond with the region where the bucket isAfter setting up the credentials execute gsutil to transfer data from Google Storage to AWS S3 For example: Add the -n flag to the command above to display the operations that would be performed using the specified command without actually running themIn this case to transfer the data to S3 I used the following: Lets check if the data got transferred to S3 I verified that from my local machine: To extract schema from AVRO data you can use the Apache avro-tools-<version>jar with the getschema parameter The benefit of using this tool is that it returns schema in the form you can use directly in WITH SERDEPROPERTIES statement when creating Athena tables You noticed I got only one avro file per table when dumping BigQuery tables This was because of small data volume  otherwise I would have gotten several files per table Regardless of single or multiple files per table its enough to run avro-tools against any single file per table to extract that tables schema I downloaded the latest version of avro-tools which is avro-tools-182jar I first copied all avro files from s3 to local disk: Avro-tools command should look likejava -jar avro-tools-182jar getschema your_dataavro > schema_fileavsc This can become tedious if you have several AVRO files (in reality Ive done this for a project with much more tables) Again I used a shell script to generate commands I created extract_schema_avrosh with the following content: Running extract_schema_avrosh provides the following: Executing the above commands copy extracted schema under bq_data/schemas/backend/avro/ : Lets also check whats inside an avsc fileAs you can see the schema is in the form that can be directly used in Athena WITH SERDEPROPERTIES But before Athena I used the AVRO schemas to create Hive tables If you want to avoid Hive table creation you can read the avsc files to extract field names and data types but then you have to map the data types yourself from AVRO format to Athena table creation DDL The complexity of the mapping task depends on how complex data types you have in your tables For simplicity (and to cover most simple to complex data types) I let Hive do the mapping for me So I created the tables first in Hive metastore Then I used SHOW CREATE TABLE to get the field names and data types part of the DDLAs discussed earlier Hive allows creating tables by using avroschemaurl So once you have schema (avsc file) extracted from AVRO data you can create tables as follows: First upload the extracted schemas to S3 so that avroschemaurl can refer to their S3 locations: After having both AVRO data and schema in S3 DDL for Hive table can be created using the template shown at the beginning of this section I used another shell script create_tables_hivesh (shown below) to cover any number of tables: Running the script provides the following: I ran the above on Hive console to actually create the Hive tables: So I have created the Hive tables successfullyAs discussed earlier Athena requires you to explicitly specify field names and their data types in CREATE statement In Step 3 I extracted the AVRO schema which can be used in WITH SERDEPROPERTIES of Athena table DDL but I also have to specify all the fiend names and their (Hive) data types Now that I have the tables in Hive metastore I can easily get those by running SHOW CREATE TABLE First prepare the Hive DDL queries for all tables: Executing the above commands copy Hive table definitions under bq_data/schemas/backend/hql/ Lets see whats inside: By now all the building blocks needed for creating AVRO tables in Athena are there: If you are still with me you have done a great job coming this far I am now going to perform the final step which is creating Athena tables I used the following script to combine avsc and hql files to construct Athena table definitions: Running the above script copies Athena table definitions to bq_data/schemas/backend/all_athena_tables/all_athena_tableshql In my case it contains: And finally I ran the above scripts in Athena to create the tables: There you have itI feel that the process is a bit lengthy However this has worked well for me The other approach would be to use AWS Glue wizard to crawl the data and infer the schema If you have used AWS Glue wizard please share your experience in the comment section below
9ZXuACuXNocUWZcfGeDvvw,This is a small tutorial on how to improve performance of MySQL queries by using partitioningAs a Data Engineer I frequently come across issues where response time of APIs is very poor or some ETL pipeline is taking time So as a part of improving the performance of the systems which uses MySQL as database I use partitioning as one of the basic optimisation stepIn this article I am going to show performance difference between a partitioned table and non-partitioned table by running a simple querySetup: I am using a local MySQL server MySQL workbench and salaries table from the sample employee data You can also download the dumps of salaries tables sample data from hereMySQL is a Relational Database Management System (RDBMS) which stores data in the form of rows and columns in a tableDifferent DB engine stores a table data in file system in such a way if you run simple filter query on a table it will scan whole file in which table data is storedPartitioning a table divide the data into logical chunks based on keys(columns values) and stores the chunks inside file system in such a way if simple filter query is run on the partitioned table it will only scan the file containing chunk of data that you requiredSo In a way partitioning distribute your tables data across the file system so when query is run on a table only a fraction of data is processed which result in better performanceLets take an example: Data in our salaries table looks like this: By default this data is stored in single chunk in side the file systemIf we partition table by taking emp_no columns as key the data will be stored in multiple chunks based on number of partitions: Now we understand how partitioning works lets start testing this feature and find the difference in run time of SQL query with and without partitionsI have ran this query multiple time as shown below and average run time is 17302 sec approxlets us examine how MySQL execute this query using EXPLAIN clauseAs we can see from the result there are no partitions in this table that is why partitions column has NULL valueNow we will create partitions on this table before inserting dataI am selecting emp_no column as key In production you should carefully select this key columns for better performanceOk now we are ready to run our simple filter query on this partitioned tableAgain I ran this query multiple time as shown above and average run time is 0So as we can see there is a significant improvement of query run time from 17302 sec to 023 secOnce again lets check how MySQL execute this query in partitioned table by using EXPLAIN clauseThis time partitions column returns some value in my case its p73(Might be different for you)So Whats actually happening is that MySQL is scanning only one partition (small chunk of data) that is why the query runs significantly faster than non-partitioned tableConclusion: Partitioning is a powerful optimisation technique which will help you in improving your query performance In order to properly utilise this technique it is recommended that first you analyse your data and properly choose the key columns on which partitioned is to be done as well as a suitable number of partition based on volume of your dataI hope you like my article If you want more information on this topic you can follow and message me on Instagram or LinkedIn
h2yZFStRFusLKme3qmqyd3,Whether it be your own database of customers that visited your roadside shop or that of a million dollar company new data is being accumulated every day and sometimes even feeding in this data isnt as straightforward as it seemsIf you dont feel like reading the explanations but just need the brief summary feel free to skip to the summary sectionThe time required for inserting a row of data is determined by a number of factors: Its funny how the fastest way to insert data does not actually make use of the INSERT commandThe fastest and the recommended way of inserting data is actually through the following command MySQL even goes as far as to say that this method is 20x times faster than traditional insertsInstead of inserting data using the INSERT command store the data in tab separated file called datatxt You can also instruct MySQL to read a csv file This is a good exampleMySQL comes with default parameters loaded into a configuration file These parameters are usually good enough for starters but as the workloads become more complex and specific they need to be modifiedThe location of the config file can be found by the command mysql --help It returns the list of files which are used for reading configurations It is usually located in /etc/mycnf or /etc/mysql/mycnf 
Haa2HyeETD249nDebJcdvZ,Apache Kafkas popularity is increasing day by day It developed by LinkedIn in 2010 and Teach industries are using the Hadoop Ecosystem services for the various Streaming and Messaging use cases and its number is growing day by day Features like Scalability Reliability Durability why Kafka is So popularPlease refer to these articles Docker JDK CDH if the requisites are not metEnvironment :1 Java version 82 Cloudera Manager 5163 Cloudera Distribution Hadoop 5Note: Im running Cloudera Docker Container for this blog and here is the memory and storage allocationCopy the Parcel Repository URL which you want to installNow Go back to http://localhost:7180/Log in to the Cloudera Manager Web UINavigate to Hosts →ParcelsAdd the copied Parcel_URL to the Parcel ConfigurationNote: We selected to install Kafka 410 versionSave changesWait for a few seconds added Kafka service will appear in the Parcel listNow Download → Distribute → Active itVerify that Kafka Parcel activated and available to add Kafka service to the Cloudera ClusterNavigate to Cloudera Manager Home PageNow lets add it to Cloudera Cluster Click on the Add ServiceSelect the Kafka option and ContinueAllocate the server instance for Kafka Broker and Gateway and ContinueIn this step keep these configurations as it is and ContinueNow Kafka service is added to the ClusterIf you find this error dont worry It is because of some default configuration we will solve the errors in the following stepsLets go back to the CM home pageError: Service will be in the Stopped mode with the Critical warning messageNow Lets fixed this error with the required configurationSave these changes and restart the Kafka serviceNow verify the Kafka service is in the Good Health stateCongratulation! Now you are ready to use Kafka ServiceLets create a Kafka Application : At first open Terminal and create a Kafka topicCheck the list of available Kafka topicsSee you all in my next article Follow me to get more updates about data engineering
8xGo9yZeCBGE6qhoQbphFa,A very big first food retail chain was migrating their data platform to AWS cloud Currently they had IBM DataStage as structured ETL tool  Teradata & Microsoft SQL Server as OLAP databases Tableau as reporting layer and Hortonworks Hadoop distribution in their on premise data center In the new platform they intended to build a data lake using S3 Redshift as the OLAP database Talend for data integration and rewire tableau to Redshift to serve reporting layer A 10 node r42xlarge long running EMR cluster was commissioned to process semi structured data  like XML & JSON for Clickstream Kitchen Video System etc data processing  this was replacement for on premise HortonWorks Hadoop clusterThe nature of the migration was to be lift-and-shift I will not discuss strategical/management considerations here but would discuss the implementation partWhile there had to be extensive migration plans I am particularly going to discuss second task from above list  and what challenges we faced and how we created a nifty generic solution which saved us significant time/effort and helped accomplishing the task smoothlyPetabyte scale data was locked in Teradata & SQL Server which had to be extracted by partitions zipped and copied to S3 In next step data from S3 is to be loaded to Redshift  all through there should be validations steps to make sure data traveled correctly throughout the process Initially the team started doing this one table at a time and copying data over VPC private/public subnet using internet creating DataStage ETL jobs for each individual tables with specific metadata partition key validation logic etc This required too much time for creating pipeline for each tables slow data transfer and babysitting the whole process from start to end We did not find a good generic solution available in the market to meet our specific need so we needed a bespoke highly configurable solution to do this task seamlessly  which can run on its own with configured values do validation and send automated report after completion of tasks We categorized our need as below: We created a piece of software mainly using unix and python I will discuss below how this was used to accomplish each individual tasks: Major database vendors provide utility to bulk import/export data to/from their databases Teradata & SQL Server too have their utilities to bulk export data  FastExport and bcp respectively Most of our data was in Teradata and a small percentage was in SQL Server so Teradata is picked to explain the process but the same process can be run using bcp configuration or any other databases bulk export utility as neededFastexport exports data from a Teradata table in 64K blocks this is very useful for extracting large volume of data This utility needs specific column names in varchar format to be provided and does not work with select *   Which is not a problem as the column names can be gotten from system table and in the configuration file there can be place to omit or add a derived columns or add filters One more problem is the default delimiter in the exported data is \\t which may not be the one we always want as the data itself can contain \\t making it difficult to distinguish between data and delimiter Fortunately Teradata supports user written OUTMOD routines which can be used to preprocess data before writing to a file so this can be used to replace delimiter as desired I found a routine written in C and compiled as dlmt_vcharso which does the jobThis project source code can be found in the below github link: https://githubMajor components of this process are: snowballini: This is the master configuration file which contains credentials for source databases different paths delimiter zip option columns exclusion custom sql copy to snowball or via internet (for small table as snowball is a physical device which needs to be manually shipped more on this later) etcinput_list: list of tables in each line with comma separated filter conditions if anysnowball_mainksh: This is the master script which is to be called to run the process  This reads table names (and other additional info if given) in the input_list file and iterates over the list At the beginning it creates a master log file and write the summary information for each table like  start time end time time taken raw file size zipped size file path etc In each iteration it finds the tables metadata from dbccolumns form the select query with columns casted in Varchar add filter information if any or take a custom sql query for which data to be extractedsnowball_cs_trfmksh: This script helps in formatting metadata retrieved from dbccolumns system table and creates an intermediate {variable}csdat table which is used in the main scriptsnowball_csfe: This takes the above created sql file and inputs from snowballini and creates an intermediate file snowball_cs_seded{FE_LOG_SUFFIX}fe with all parameter replacement which will be used in next step to run the Fastexport extraction process from a tablesnowball_mainfe: This is the final Fastexport script which uses the output of the previous process and actually produce the data extract filedlmt_vcharso: This is the shared object (compiled C library) which is used in the above script to aid in preprocessing extracted data like changing delimiter This works with environment variable FEXP_DELIMITER which is configured in the ini filezip_move_to_s3ksh: Based on defined configuration value this script zips extracted data file and move to derived s3 path either via public internet or write to snowball deviceWhile this process worked smoothly for over ~95% percentage of data there was a small fraction of data primarily in SQL server which had columns like comment or other free form text columns for which it was difficult to use a reliable delimiter For these cases we needed to export/import the data in a format which comes with schema along with the data We used Sqoop in these cases for the data extraction process and used Avro format Rest of the process goes as usualFor large volume of data transfer using public internet may take days/weeks and be very unreliable For small tables we transferred data using public internet and for large volume we ordered snowball devices as needed While ordering snowball devices disk spaces (like: 50/80 GB) and s3 bucket name needs to be provided The device itself is a rugged box with its own shipping label Once the device shipment is received it needs to be connected in the data center and then snowball client is needed to be configured and used to write data to the device As now the connection is local to the on premise data center write is very fast Once writing is done the device can be detached and shipped back to AWS and they will move the data to same s3 bucket and keys as it is written to the snowball device This is an straight process and well documented in AWS Snowball Developer GuideNow the data is in S3 this needs to be loaded to corresponding Redshift tables There needs to be certain consideration to load to S3: Redshift table creation was a mix of automated script and manual editing to include some extra columns with default values as needed Below are the scripts used in this process: input_list: A list of tables to be loadedredshiftconf: Redshift credentials information in JSON formats3conf: Key & secret access key in JSON format from where s3 data to be readgetcolssh: Read header information from extracted data and prepares column list which to be passed in COPY commandRedshift_Loaderpy: This is the main script which uses psycopg2 library and uses above files to iteratively load data to corresponding tables as in input_list This also generates logs which is used for final validationSome times we needed to use different S3 paths to load a particular redshift table in this case a manifest file was created to mention all S3 paths (supports wildcards) which was used for redshift table load Data to be loaded in Redshift should be distributed across many files instead of having only one gigantic file This will help Redshift to load data from many different files simultaneously and speed up loading process greatly To fully utilize maximum degree of parallelism number of files should be a multiple of number of slices available in Redshift We dont want the files to be too small as this will add overhead or too big because of increased processing time Approximately file sizes should be close to 128 MB and data equally distributed among files as close as possibleAutomated Status Reporting & Validation: There are two major steps where we want status report to be generated saved and emailed Two steps because data extraction and load happened in two different times (or days)For first step some key metrics reported are  table name partition key extraction start ts extraction end ts extracted file size extracted row count S3 file path File size at S3 etc A sample report in this stage looks like below: For second step while loading the same data to S3 we need similar log with basic validation like record count match Some logs to be tracked are  Table name S3/manifest path time start and end duration number of records inserted beginning records count end records count etc A sample report in this stage looks like belowMore specific criteria based on table could be implemented as needed eg generating and matching a R style table summary for both tables But we found that its always better to do a manual validation for individual table/table sets before the cut over time  ie when officially we are done with history data migration and switch on simultaneous run for both cloud and on prem data pipeline Both processes need to be monitored for some time before sunsetting on prem processHow did you do your data platform migration to cloud  did you use proprietary tools or developed in house solution like we did? Would love to hear your experiences
AmxRojgvriGAxToqqGMSMF,Pivot table is one of the most commonly used features in Excel for reporting purposes The user does not need a programming background to perform data analysisAssume a sales report as above (heavily simplified for demo purpose) on the orders sold by all the salespeople across all regions Pivot table enables multi-dimensional questionsAnd the output as shown below will answer the questionSay your company is doing very well and there were 10 million orders for last month Now imagine your analyst running pivot table on an excel with 10 million rows Hopefully he or she will still be sticking around for next months analysisIn this post we will describe how we can achieve this on Athena  a highly scalable manner to query huge amounts of dataAthena is a managed query service provided by AWS The query engine is based on Presto Athena supports most operator from presto and is a popular choice to query data in s3 With the recent federated query announcement Athena can also query other data sources such as Postgres Truly splendidQuery to achieve the above through AthenaThe query above will produce the below output Exactly the same output as the earlier example if we used the pivot table from ExcelFirstly on the inner query Example output of the inner query as shown belowThe column kv1 is populated mainly with the function multimap_agg multimap_agg(region amount) kv1Notice column kv1 is a map with key = region and values = all the price in the regionAfter the inner query the last step would be to sum the value Presto provides a variety of array helper functions In order to sum up the value we will use the reduce operator
T8pHo9uB2E37xCmAedwRY3,Sampling data operation is something you need to feed your other environments than production something like test environment or for unit testing etc Because your data is probably so huge amount as you dont want to use the same scale for other environments you will look for sampling the data and scaling it down without losing the scale as much as possibleGood sampling is actually kind of something hard to achieve Because you need to sample it without breaking the scale of actual data as much as possible I say as much as possible because it depends on what you are doing What I mean is; you need to define the columns which the scaling will be on For example Lets assume you want to sample your data by scaling it on a field something like category field You can sample it by limiting all rows or limiting each category group with a strict size by the help of rank windowing function or as limiting each category group a percent over group size by the help of percent_rank windowing function I think the third one which is percent sampling in some fields is a better way of doing the samplingThis means that we are going to take only n rows of each category Lets assume this n is 2There is some amount of casual products as electronic ones It doesnt seem good scaling at all Becuase There are double times more casual products than electronic productsI think this is the best way of doing it Becuase percentage gives it always on a proper scaleLets assume we want to scale each category group down as %50 percent percent_rank windowing function is what we are looking forThis will result look like: As you see the scale still remains between categories There are still double times casual products than electronic ones
dDJRyTunC9XxJnopexdEt3,How to combine SAP with the Google Cloud Platform Provider of powerful data analytics tools such as BigQuery Data Studio or recently looker  to gain a powerful data analytics platform and valuable insights If you are interested in practical data analytics approaches for SAP data this article could be also interesting for youWith SAP HANA the SAP Data Service is already build in so you can easily export data from a SAP application or its underlying database to BigQuery However the combination of GCP and SAP is also possible for companies who are still living in the ERP worldThese following points show possibilities of loading data from SAP to Google Services: Therefore a possible solution could look like this: Getting the data is the first step Afterwards you will deal with the question of how to realize a delta logic because you probably dont want to load all the data on a regular basis via full load This would only come with disadvantages like heavy loading times and also having no historical data Especially departments like controlling internal audit etc often build up their data analytics based on historical data viewsWhile the SAP Data Service comes already build in with a CDC wizard other data integration approaches might need some extra work To realize this step its important to get to know the SAP tables and table structure very well Realizing the delta load on an own logic can be realized especially with the help of two tables: CDHDR (Changes Header) and CDPOS (Changes Position) These tables are tracking changes in master data or transactional dataWith the new era of column based data warehousing tools such as BigQuery data transformation is often necessary due to performance but also because of different table structures or data types between the source and target systemOne example is that in SAP you have the tables BKPF (Accounting Document Header) and BSEG (Accounting Document Segment) Due to its column based structure it will make sense to denormalize the data within the BigQuery The solution could be to join BKPF and BSEG and if needed other master data and reference tables  for example with the help of the build in BigQuery DTS (Data Transfer Service) As a result you can come up with a denormalized data object architecture as shown in the figure below: Another transformation to gain performance and cost efficiency could be to use nested data structures Newer data warehouse technologies like BigQuery or Amazons Redshift do work better with this data structure Use cases for working with nested data are eg header/segment tables like BKPF and BSEG or if you have cases where a table holds columns which dont change much like: combined with historically often changeable columns like: The last part would be to realize reports and data analytic possibilities With BigQuery the data analyst already has a great tool for analytics Data scientists can use Data Lab or one of the many ML Services within the GCP Data Studio could be the right solution for business users for sharing KPIs dashboards etc To implement a Role/User access model within the BigQuery its a common way to use the BUKRS attribute (Company Code)  so a user would only see data of the company code assigned to him thats also the way SAP handles user rightsConclusion  the combination of SAP and GCP could lead to significant data analytics competencies and possibilities in your company SAP systems are hosting a lot of interesting data that wants to be analyzed while GCP comes with many data analytics/scientist tools and services which will enable to do so
biQhhdf4u8u7XJhyD7UiUb,This document describes how to set up a data engineers development environment in local machine to do local data development and testing without needing any cloud infrastructure • Install and configure IntelliJ IDE: Download IntelliJ Community Edition from https://wwwjetbrainscom/idea/download/  copy this to a folder like ~/Softwares/Install downloaded IntelliJ software using all default settingsFor Mac user the simplest way of installing git is to use homebrew package managerOtherwise git software can be installed from wwwgit-scmcom and manually installedGithub (or Stash/Gitlab etc) can be used as a repository manager for git Git repositories can be viewed from https://githubcom/xxx/ But to clone or pull a repository from github to local a SSH tunnel needs to be set up Git repository link with https can also be used but that will require authentication on every interaction with github which is annoyingBelow are the steps to create an SSH key-pair and add those to ~/ssh folder This will create a ssh key-pair  one private key file (id_rsa) and one public key file (id_rsaCopy the public key from id_rsapub and paste it in github as shown belowDownload Apache Spark version 202 (or a later version) from https://sparkapacheorg/downloadshtml use option Prebuilt for Apache Hadoop for 27 and laterDownload and Install Java SDK 8 from http://wwworaclecom/technetwork/java/javase/downloads/jdk8-downloads-2133151Choose appropriate operating systemDownload Apache Hadoop built version(binary) from https://archiveapacheorg/dist/hadoop/core/hadoop-27Download Apache Hive built version (binary) from https://archiveapacheorg/dist/hive/hive-12Go to the home directory and add below commands to the bash_profile file Change the paths below as neededOnce those commands are added the bash_profile file needs to be sourcedAt this point java hadoop git hive and spark should work fine from any location in local Mac terminal Verify and take corrective measures if there is an errorAs the SSH tunnel is set up in step 3 a git clone from local terminal can be used to clone a repository to a particular branchGit SSH url for the particular project can be found in github (eg git clone  recurse-submodules git@githubcom:xxx/xxxNavigate to the particular directory where the repository will be cloned and issue a git cloneThis will pull down the code from git to local At this point a separate branch needs to be created for change trackingOnce the changes in local is committed the change can be merged to a higher order branchWhile on the local git repository directory issue below command to create and checkout to a new branchThis will first make local git master branch in sync with remote and create a new branch from there This should be done to make sure all new development work is starting on top of the latest master copyA virtual environment for the project needs to be created where project specific python packages can be installed This is particularly helpful as most of the times user might not have root access to add a package to base python installation and this also helps in keeping the base installation clutter free and separatedA virtual environment can be created either from IntelliJ or from Mac terminalFrom IntelliJ go to the project structure → SDK and then click the + sign to create and assign a virtual environment to the projectTo create and activate virtual environment from terminal issue the commands: pip install virtualenvvirtualenv venvsource At this point there might be some additional python packages or dependencies which are needed for the project For source and testing code these may have been listed in the requirementtxt and test_requirementsTo install these : This should install all the project & testing requirements Check for any errors and resolve those if anyIt may be a good idea to install git autocomplete which will save time from issuing full git commandsFor MacOs this can be installed as below remove git if git is already installed: Create aliases of frequently issued commands EMR/EC ssh commands etc in ~/bash_profile These are very handy and spare us from remembering specific ip addresses issuing full commands etc
H3H5DUbLkBEGoFfsqmd8Jb,2019 was a big year for AppsFlyerAnd not just any kind of BIGTrillions of daily HTTP and postback events BIGTerabytes to petabytes of data BIGOn a personal note I started at AppsFlyer in December of 2018 as our 500th employee  and we nearly doubled that number in 2019 alone The engineering group being the largest group at AppsFlyer demonstrates the hypergrowth that we are experiencing on all frontsThis graph is taken from one of our presentationsIn truth it doesnt really matter what you define as the Y axis as it can represent nearly every single thing at AppsFlyer: the number of people clients revenue and finally that which makes it all possible  our engineering backbone that powers all of this growthYou may have heard that AppsFlyer raised $210 million dollars earlier this week to fuel our next phase of growth but Id like to just pause for a minute to actually recognize everything that we accomplished making this milestone possibleOne of the main challenges in coping with rapid growth is understanding how to scale without compromising stability robustness security or SLAs (ie the actual business goals) while also being conscious of utilization and cost effectiveness None of these are simple feats and they become exceedingly more difficult when coupled togetherThis kind of growth requires a forward-thinking approach where you actually find yourself thinking about the technologies and systems engineering of tomorrows challenges todayThis data is just a snapshot of the growth in just one year alone where the last quarter of 2019 actually constitutes in numbers all of our usage in 2015 and 2016 together and this is an ongoing trendWe are growing at 10X scale every single yearAs an organization that prides itself on its engineering one of our biggest challenges is scaling without breaking the bankThis is achieved by making sure our systems are flexible enough to scale up and down as needed and that we can utilize services such as spot instances and still maintain zero downtime for our clientsOne of the metrics that represents this most for us is our spot instance usage vs on-demand hours Check out the infographic and learn about some of the exciting facets of our engineering organizationWere growing were learning through real hands-on experience and being challenged daily were geared up to tackle tomorrows engineering complexities and all this will be fueled by our new funding round
TdTQ7GEY6LfrfuhXQCyVUs,Things become increasingly complex when you give context to the volume of the events and in our case were talking about more than 90B events daily and around 200TB of daily data that is ingested into our system to AWS S3AppsFlyer Engineering is always hiring just like our products always-on 24/7 SLAsIn order to process and compute all these events AppsFlyer maintains around 50 Hadoop clusters (vanilla with an in-house auto-scaling system) that runs over 35k+ daily Spark jobs that slice dice and pivot the data around to give our customers the most precise data they needOne of the core technologies enabling and supporting this scale of data operations is Apache Airflow that schedules and executes all of these jobs across the various clusters while being aware of the different characteristics of each jobWhat is Apache Airflow in a nutshell?From the documentation: Airflow is a platform to programmatically author schedule and monitor workflowsUse Airflow to author workflows as Directed Acyclic Graphs (DAGs) of tasks The Airflow scheduler executes your tasks on an array of workers while following the specified dependencies Rich command line utilities make performing complex surgeries on DAGs a snap The rich user interface makes it easy to visualize pipelines running in production monitor progress and troubleshoot issues when neededWhen workflows are defined as code they become more maintainable versionable testable and collaborativeThink about Airflow like a cron server but with a lot of features like designing workflows that are fully customizable with a great UI alongside centralized loggingAirflow is crucial for AppsFlyers core business as this is where the large majority of our ETL tasks run as well as many others With this in mind we had to take a stability first approach and we made sure that every part of Airflow is highly available: As mentioned previously we support about 50 different Hadoop clusters and all Spark versions since 16 for this we created our own SparkOperator that receives the required data as a parameterBesides the actual operator that runs the Spark job we have many more operators that are used to scale our clusters whether EC2 spot nodes or on-demands to be able to run the actual taskThis is a result of learning that as a byproduct of managing the Hadoop clusters ourselves we often find out that one of the clusters is malfunctioning and need to redirect all the jobs from the malfunctioning one to a different cluster To make this possible we created a way to override the actual hadoop_cluster parameter that the user enters at runtime this is done through a dictionary that we enter as an Airflow variable that looks like this: Docker OperatorWe break our executed tasks into two different workloads: These two workloads are complex when running on a single instance and thats why we understood we needed to create complete isolation Fortunately the fact that we use CeleryExecutor introduced an added advantage We created two sets of Airflow workers where each listens to its own queue One set of machines is a general type and the second has more computing resourcesBecause AppsFlyer was already heavily invested in technical services that are bundled inside a Docker container it was a given to use this ready-made tech stack in Airflow as wellOnce a repository is built in Jenkins the code automatically gets containerized and uploaded into Artifactory From there we use an internally built Docker operator that allows us to get and execute these containers inside of the Airflow workers Another benefit that we gain from this is the fact that all of the required resources & code dependencies such as packages modules and other resources are already packaged inside the Docker image and we dont need to install anything on the workers themselvesAirflow VariablesAirflow variables is a key-value store inside of Airflows metadata database AppsFlyer loves the flexibility this provides a bit too much we use it to store common parameters for multiple jobs specifying the number of nodes in a specific cluster (for the scaling operators) reusability of commonly used jars… We really just use it everywhere Thats why its no real surprise that we had several production issues directly related to variables changes Where the thing that is missing for us is proper auditing validation and proper CI to understand if the variable change breaks the DAG itselfThis was another opportunity to build it ourselves To accomplish this task first we added LDAP authentication to the Airflow web server Doing so allowed us to separate the users based on the LDAP groups we decided that regular users will be prohibited from viewing the Admin page (user_filter in Airflows configuration)  but this also created the unwanted byproduct of eliminating the Variables page view To make the flow more user-friendly we created a new tab in the UI and added a view-only section for the variablesChanges to variables are made from within a Git repo and we invoke a deployment pipeline that updates the metadata DB itself with the latest variables The fact that its from a Git repo allows us to audit revert and see the changes easilyDevelopment WorkflowIn AppsFlyer we aim to give the developer a simpler and transparent experience To do so our operators code is inside a common directory as the DAGs definition themselves to provide full transparency of the infrastructure codeOnce the developers perform changes on their local computer they can spin up an Airflow Docker image that will render their changes in real-time  to see that their dependencies are in place and it even imports the variables from the production Airflow so they can see the rendered commands properlyDisclaimer: In the Docker image we only start the webserver because we dont want stuff running from a local computer and potentially have real production impactAnother capability that we provide to verify that everything is working as it should is to run tests on the local repository Using a script we load the entire local repo to a DagBag object By doing so we have all the dags parsed as a dependency tree just as the scheduler sees them Errors are caught during import and the test will failAfter the local tests pass successfully the developers run a Jenkins job that essentially runs the same tests as a safety mechanism After the build passes properly when needed the variables in the Airflow dashboard are also updated The DAGs themselves are uploaded to an S3 bucket and then they are pulled to the NFS server so that the various Airflow components will see the updated code at the same timeFor major infrastructure changes we have a separate and more comprehensive setup that includes a test Airflow cluster This enables us to test any major infrastructure change or a breaking change made to one of the operators by deploying it from a working branch to the test Airflow cluster before deploying to our production Airflow operationAlerting & MonitoringDefining proper alerting and monitoring is one of the most important things with any AppsFlyer component Besides monitoring the basic host metrics of CPU memory disk usage We have also created dashboards based on Airflows metadata DB using Grafana PostgreSQL data source to have analytics on success/failure of the jobsOn the alerting side we have created a few hacks that make our life a bit easier: In conclusion AppsFlyer really loves Airflow Its simplicity allows us to be as versatile as we want Scaling is easy and the maintenance cost is really lowWe still have a long way to go with proper permission management improving the CI/CD flow and a few more features that we are thinking about but Airflow does make our lives much easier and Id encourage you to check it out if you are looking to have a robust solution to orchestrate scheduling workflows of your large-scale operations
UMMSebGfXfaCCTiaKUdHgR,This week the team at Ascend is launching our Autonomous Dataflow Service which enables data engineers to build scale and operate continuously optimized Apache Spark-based pipelinesThe Ascend product is the culmination of more than three years of research and development Along the way we have learned from our early adopters spanning a range of industries including media digital consumer health care and logistics Through these partnerships we have discovered considerable consensus on the challenges facing Big Data teams and initiatives; challenges that are not adequately addressed by the array of tools that have come to market in recent years As many of you know all too well most data engineering teams must piece together solutions from several vendors or open source projects keep current on the latest announcements of cloud service providers and track advancements and vulnerabilities on multiple frontsInstead we believe that you should be able to focus on the data not configuring multiple cloud environments spinning up Spark clusters diagnosing failed jobs removing obsolete data to reduce storage costs and securing sensitive information With Ascend you define Data Services that communicate with each other and the rest of the enterprise via Data Feeds Data Services are built up out of Dataflows which define the connections to external data locations and the series of transforms required to clean convert join and aggregate data to produce Data FeedsFrom prototype to production data engineers rely on Ascend to manage and maintain the intricacies of data pipelines so they can focus on the data Lets take a look at some of the unique features that make this possibleThere are many advantages to being able to operate across multiple cloud providers From its inception Ascend has been designed to be cloud-agnostic and portable A majority of our customers either run Ascend on more than one cloud provider or have migrated their Ascend Data Services from one cloud provider to another Migration is a simple matter of exporting the Dataflow definitions from one environment and importing them into the other; Ascend automatically brings all Data Feeds in the new environment up to date This means you can enjoy the flexibility of choosing where to run workloads based on the cost of compute resources or proximity to your other data environmentsConcealing the variances between cloud providers under a common API gives Ascend the flexibility to adapt our platform to the underlying infrastructure An example of this is the object storage technology we deploy: MinIO The MinIO gateway presents an Amazon S3-compatible API on top of Google Cloud or Azure With Ascend your data is always accessible via an S3-compatible virtual bucket even when Ascend is not running on AWS This approach also enables a clear path to on-premises deployments which will utilize NAS drives for storage while maintaining the same virtual bucket API as a cloud deploymentWith Ascend you will never need to learn about Amazon EKS vs Google Kubernetes Engine or how to spin up a Spark service on each cloud platform; we handle that for you Most Ascend users are not even aware of which cloud provider the environment is running on With Ascend you have the freedom to choose your target cloud platform based on economic or architectural concerns and the entire environment can later be ported between cloud providers without retraining users rewriting code or adapting APIsThe Ascend platform is built around the model of computation A Dataflow is a directed acyclic graph (DAG) in which every node contains a that is evaluated on its inputs Transforms can be arbitrarily complex but they have no side-effects and only depend on their declared inputs The leaf (and root) nodes of this DAG are connectors  which are the inputs (and outputs) of the Dataflow For every node of the Dataflow we can derive the expression it computes by labeling the connectors and flattening the sub-DAGTo create a new Dataflow in Ascend you construct the DAG and then define the necessary connectors and transforms Connectors can specify the location connection method and credentials for external data or reference another Dataflow using  Transforms are specified in the users choice of programming language The initial release of Ascend supports SQL and PySpark with more options in the works Multiple programming languages can be combined to build the same DataflowTransforms also handle partitioning for their inputs and outputs For SQL transforms Ascend will automatically generate the pattern from the code For other languages you can choose from a number of common patterns including mapping full reduction partial reduction and unionAscend infers the output schema of each transform from the code defined in either SQL or PySpark Our early users have found this feature very helpful when writing downstream transforms Weve also observed that it encourages an incremental approach to building queries much like how you usually work in a SQL console or Python notebookThe Ascend web app includes a graphical editor to design your Dataflow Soon you will also have the choice of defining the Dataflow blueprint as declarative code analogous to the way services are defined in Kubernetes You can switch back and forth between graphical and code descriptions and Ascend will keep everything in syncAnother choice that you have to make when architecting a Big Data platform is which compute engine to use Compute doesnt stand alone  it needs to read and write data somewhere-so it requires co-located storage It needs to be robust so you dont need to worry about failed jobs which are inevitable with any large-scale data processing tasks It must balance performance and cost-intelligently scaling the compute cluster  and automatically choose appropriate instance types And just when you think youve found the ideal compute engine the cloud providers or the open-source community reveal a new technology du jourTo address all of these issues we developed an elastic data fabric Elastic because it automatically grows and shrinks its footprint based on the workload and data fabric because it coordinates compute and persistent storage The current elastic data fabric utilizes a Spark cluster deployed on Kubernetes combined with the cloud providers persistent object store Ascend manages the specifics under the hood so you dont need to worry about managing Kubernetes deployments and Spark clusters The compute cluster is auto-scaled and draws from both permanent (on-demand) and preemptible (spot) instances to optimize performance versus costWeve also made this data fabric serverless centered around the functions being evaluated instead of individual jobs With the Dataflow model the transforms you code are functions so every computation involves a functional expression  the application of a function to specific input values The elastic data fabric accepts requests to evaluate an expression and derives from that expression a token which is returned Multiple requests with the same expression return the same token and are therefore automatically de-duplicated At any later time you can request the value of the expression using this token You never need to track Spark jobs because the data fabric intelligently diagnoses failed and timed out jobs and re-executes any that do not require human intervention All errors reported back to you are actionableFinally the entire elastic data fabric is encrypted both at rest and in transit with access controlled at the level of individual Dataflows Teams can control access to sensitive data and decide which derived Data Feeds to share with other teamsAt Ascend we are dedicated to staying at the forefront of distributed computing technologies Weve deliberately designed the data fabric interface so that in the future you will be able to swap in alternative implementations such as to support interactive queries or real-time streaming Stay tuned for more developments on this frontNow that we have a multi-cloud multi-language elastic data fabric we need a way to effectively control all that power and to observe and confirm that it is operating at peak performance given specific cost constraints Theres been a recent trend toward declarative specifications for cloud computing: declare the state you want to end up in and let the system handle getting it there An Ascend Dataflow is essentially a blueprint that describes data you want; the Dataflow Control Plane decides to make it soEarlier attempts at orchestrating Big Data computation have centered around executing scripts at prescribed times (eg via `crontab`) or defining the dependencies between tasks as a DAG and kicking off runs of the entire DAG at regular intervals (eg Airflow) While the dependency information in an Airflow DAG is a step toward a declarative specification Airflow tasks have side-effects and can read data from anywhere not just from their upstreams There is nothing in Airflow that verifies the correctness of the dependency graph which can lead to subtle errors In contrast Ascend Dataflow transforms are guaranteed to be side-effect free and depend only on their upstreams We already described above how this functional representation enables a serverless elastic data fabric but it also allows the Dataflow Control Plane to optimize execution based on the Dataflow semanticsThis combination of an elastic data fabric and the Dataflow Control Plane is extremely robust: The Control Plane references the Dataflow DAG to schedule computation on the data fabric Each scheduling iteration determines which transforms are ready to be computed and generates requests to the data fabric for those computations but does not wait for the results or attempt to track running jobs Instead it just looks for more work to schedule At any time the value of each expression in the DAG is either computed or pending computation; data is never inconsistentThe Dataflow Control Plane also understands how data partitioning is affected by each operation so it can leverage parallelism to the greatest degree possibleEvery value may be partitioned and every transform defines the partitioning pattern for its output Partitions are hierarchical which allows for efficient handling of data with millions of partitions At any time you can drill down and access any partitions that are available to determine which ones must be recomputed and which ones have errors that require user correctionBecause the Dataflow Control Plane manages data partitioning it can also perform incremental recomputation Frequently when new data arrives only a small subset of the partitions are affected The Dataflow Control Plane detects this and determines exactly which partitions of each transform must be recomputed based on the partitioning patterns associated with the transform You dont have to write additional logic for each transform to handle incremental computation Its also easy to backfill data: when you increase the range of the read connector(s) to include the historical data the Dataflow Control Plane will schedule computation of the relevant partitions for all downstream transformsIn Big Data systems failures are inevitable Computation sets have become so large that even with a very small probability of error per task the probability that there will be at least one error approaches 10 Redundancy doesnt help because all redundant computations are equally likely to fail What does work is being able to recompute just the failing portion of the computation and stitch the correct data into the result When the elastic data fabric encounters a processing error it tells the Dataflow Control Plane which partition(s) failed so it doesnt need to reschedule the entire transform to recover Instead it only schedules the partition(s) that failed If the error requires human intervention (eg a corrupted input file) you dont need to wait until the next nightly run to see if your fix worked Once the correction is made the Dataflow Control Plane will discover it and recompute only the relevant partitionsAll of these powerful capabilities are available today with the Ascend Dataflow Control Plane In addition we are already hard at work on additional automated Dataflow optimizations which will continue to drive down cloud spendIf youre a data engineer who prefers wrangling data over managing cloud infrastructure I encourage you to give Ascend a try We are currently offering a free trial of our hosted service so you can quickly start building no strings attached Check it out here and we can get you up and building quicklyOriginally published at https://wwwascendio on July 17 2019
TFTSqJNh4UiRotCKSy4KAR,Importing click and cost data into Google Analytics allows you to monitor performance across all your marketing channels in one placeWith all your data in Google Analytics you can see how different marketing channels work together to drive growth for your businessBy using Google Analytics you can create a better attribution model to evaluate your marketing performance and address issues like brand and retargeting campaigns taking more credit than they deserveIt also solves duplicate conversion issues where multiple channels take credit for the same conversion For example if a user clicks on your ad through Google Ads and then sees your Facebook Ad before converting each ad platform will attribute a conversion Having all your data in one place allows you to see the complete customer journey and credit each marketing channel accordinglyEvery 24 hours the advertising data from the Non-Google ad platform is uploaded to your Google Analytics account Advertising cost impression and click metrics are imported along with campaign UTM information This data then gets joined with the rest of your Google Analytics dataBy tagging the destination URLs of your campaigns you can pass detailed information about your campaigns to Google AnalyticsWithout UTM tags the information that Google Analytics receives is limited to the source of the user (eg Facebook Microsoft Ads) In other words you wont be able to distinguish between organic and paid activity from those channelsGoogle Analytics doesnt have automatic integrations with Non-Google Ad platforms So the next step is to upload the advertising cost data from the Non-Google ad platforms to Google Analytics We are going to use Attriva to automate the uploadLearn more about Attrivas features hereNow that you have all the data in one place use the links below to learn more about using this data to make better decisions to improve the ROI of your marketing spend
FWjM6wLaK8pF32NLyhXqZW,For the impatient here is the link to our Github repository: https://githubWith todays adoption and success of event-based data processing systems there is a wide range of opportunities to gain insight into a business in real time A message broker such as Apache Kafka is a candidate of choice for these systems Besides implementing a message queue Apache Kafka also offers a light-weight solution for stream processing called Kafka StreamsIn this blog post we introduce how Kafka Streams can be used to process real time data and expose queryable APIs You can find the corresponding code in our GitHub project The examples are based on the real world data set LFM-1bConsider the following example of a music streaming service: Every time a user listens to a song an event is emitted and written to a Kafka topic These listening events may have the following structureIn such a music streaming service we may want to analyze general metrics such as the overall count of events the last time the user listened to a song or the users top ten artists But eventually we want to do more than analyzing metrics It might be interesting to leverage the events to provide recommendations or detect anomalous behavior Hence there is a need for a service that processes the stream of events exposes calculated results through an API and is easily extendable Fortunately Kafka Streams provides all the tools to create such a serviceThe corresponding architecture is illustrated in the diagram below: The topic event-source contains all listening events There is a standalone Kafka Streams application for each use case such as analyzing metrics or providing recommendations As we explain in our blog post Queryable Kafka Topics with Kafka Streams Kafka Streams allows us to expose processed records via a REST API in real-time We use an additional REST API to unify the individual applications APIs and create a combined user profileNext we dive into different use cases note challenges and show how Kafka Streams helps solving themLets revisit the example mentioned above: We want to create a queryable user profile store created in real time in a streaming fashion with the following metrics: Computing the number of plays as well as the first and last listening event is straight forward On an incoming listening event the application increments the current number of plays Additionally it compares the events timestamp with the current first and last events timestamp respectively and updates them accordinglyGenerating the current top tens requires more work We explain our approach using the example of tracks Calculating this metric for albums and artists is analogousFirst we use Kafka Streams DSL to count all tracks for each user We map to a new key that contains the user and track id and group by it Afterwards the built-in count method is called We could try to use these counts to update the users top ten tracks However revisiting how Kafka Streams scales shows why this does not work Each application with the same application id processes a topics messages for certain partitions The count method triggers a repartitioning based on the new key Thus it is not guaranteed that the state store contains the queried user profile and returns nullWith that in mind we map the count again to a new key-value pair consisting of the user id and a composite value based on the track id and the current count This can be safely processed because the stream of counts and the user profile share the same key type and subsequently the partitioningRecommender systems are an integral part of modern web services They aim at personalizing a business product and increase user engagement In our example we can recommend tracks albums and artists to a user Well suited recommendations may increase the time a user spends on our platformWe adopt Twitters GraphJet to create recommendations GraphJet generates them by performing a random walk over a bipartite graph Each GraphJet node holds all current data to efficiently do so Again we consider the example of tracks to explain our implementationThe first step is to create a bipartite graph There are two sets of nodes: users and tracks An edge between a user and a track represents a listening eventThe second step is to compute the recommendations for a given user using the SALSA algorithm The starting point is the users node We sample an interaction edge and walk to the corresponding track From there we again sample an edge and walk back to the associated user We do this a configurable number of times Additionally multiple such random walks are performed During these random walks we count the number of times each track is visited The algorithm recommends those tracks that are visited most often ranked by their countTo go back and forth we need to be able to efficiently find the tracks a specific user listened to and the users that listened to a specific track In Kafka Streams we create two state stores which act as the indices for the respective directions Hence each store maps a user or track id respectively to a list of all adjacent nodesAs mentioned each node in GraphJet contains all data Contrary in Kafka Streams an applications instance processes only a fraction of the data Instances that share an application id form a consumer group Kafka assigns each consumer in a group a distinct set of partitions To circumvent this behaviour we concatenate a UUID to the idUntil now the user profile as well as the recommendations contain the ids of the artists albums and tracks respectively For users however it is more interesting to see artist names or track titles Inherently this data is relatively static Kafka Streams offers GlobalKTable to represent this kind of data We store the names in a GlobalKTable respectively and join the tables with the stream to add the names This comes with the advantages that all data is available in every application node and no repartition is necessaryThe following code shows how to perform a join for the updated tracks counts as described above: First we create a GlobalKTable that reads all track names from an input topic and stores them Then we perform the join The first argument is the GlobalKTable The second argument defines which part of the streams key-value pairs should be used for the join We use the ids from the tracks we count Lastly the third argument defines the result of the joinSo far the service consists of two sub-applications They expose different aspects of a user through their individual API Next we create a unified API with a combined user profileFurthermore this unified API helps mitigating drawbacks of the single applications If eg the user profile application receives a request for a specific user the application must look up the users partition before processing the request This may require forwarding the request to another instance of this application that is responsible for the respective partition In the unified API we can preemptively calculate the partition and therefore directly send the request to the correct hostThe default calculation of an ids partition can be found in Kafkas DefaultPartitioner The following code snippet shows our implementationWe also require a mapping of partitions to their corresponding hosts The method allMetadataForStore of the KafkaStreams class provides exactly this information We expose it through an additional API The unified REST service calls this API and updates its current mapping to forward the request to the right hostThis is not necessary for applications like the recommendation Here every node can process all requests because it has all the data However this introduces a load balancing problem The unified REST service needs to know all currently available recommendation applications As we described we set a unique application id for each recommendation deployment Consequently the approach to call an API that exposes the information returned by allMetadataForStore doesnt work We use Kubernetes LoadBalancer to solve this difficulty The different instances of the recommendation application are part of the same replica set and therefore share a load balancer which the unified API uses to forward requests toAltogether querying our REST-API results in something similar to the following extract: Overall we count 4000 events In this timeframe the queried user apparently greatly enjoyed Lindsey Stirlings music Apart from that we can see a tendency to British rock music Our artist recommendation reflect this by recommending Frank Turner The Fratellis and David BowieIn this blog post we described an extendable user profile store based on Kafka Streams We use several standalone applications to process the stream of user events and collect vastly different types of information They range from simple counts over aggregations to more advanced calculations like recommendations An API combines them into a single user profile that is updated in real time Kafka Streams queryable state stores are a great fit for such an application because of their lightweight and easily scalable nature
Cxr3BygzoMNoBihUcDSsPi,Simple right? After all everyone these days uses data warehouses You do a bit of Googling… and holy smokes you end up with 1001 technologies architectures platforms and services to choose from Where do you start? You are even more confused than beforeThis article will provide experience-born best practices for anyone creating reporting and analytics-focused warehouses This is not a final blueprint and may or may not be 100% applicable to your business Rather its a simple framework to avoid some costly mistakesThis is so obvious it doesnt even take space in my top 10 Unless you are the CIA and need your own private infrastructure you have no business owning your own metal The cloud isnt the future but the present and the norm at this pointBy the way  that last item on the list is technically not a warehousing solution but more a data orchestration exercise that needs to be solved slightly differently with different technologiesThese are the kinds of questions you should ask all your stakeholders right upfront You need to be crystal clear about your objectives which will have huge implications on your final architectureThis is controversial and might ruffle some feathersThere are many one-stop solutions like CDPs (Customer Data Platforms) that have popped up in the last 2 years and claim theyre the be-all-end-all solution for managing your golden copies and reporting 😏…weve heard these claims beforeThe reality is because they are generic solutions (with perhaps a few options to customize) there is no off-the-shelf product that will suit all your needs  sorry! The promise of a quick-start will be enticing though But you will have to compromise from day 1 and pick the product that comes closest to your budget and needs As time goes by your needs will change and these solutions will be very inflexible to adapt When that happens your internal teams will start saving data in spreadsheets and napkins And very soon you will end up having the same mess plus a new expensive toolBut there is an even bigger problem with these one-stop-solutions: data ownership Which brings me to my next point: I am a strong believer that you should own your dataIf you observe the history of technology in the last 40 years you see that every few years there is always a new-kid-on-the-block claiming that he is the true Messiah But the reality is that in 3 5 years you need to be ready to move on to more advanced technology Treat it like you would an ex-boyfriend: break up take all your belongings (in this case the data) and leave cleanlyAny solution that doesnt allow you to access all your data directly should be a No-Go If they say things like ……Dont let anyone hold your data hostageAt the core of it data warehousing is quite simple While there are 100s of choices and 1000s of tools available any near-real-time data warehousing system only has the following 3 layers (The DBs are not considered a layer in this context): How hard can it be? Heres what it looks like from 30000 feet: Of course things can get more complicated with additional requirements But this is a good starting point where you can add advanced requirements as neededOne such common complication is the need for real-time data I come across this with my clients all the time Before over-complicating your life please ask stakeholders and users this question multiple times: Do you really need real-time or would a short lag (for example 15 minutes) be good enough? • % of the time the answer is 15 minutes is awesome because most clients are still struggling with one to two-day lag data Even if the answer is No I need real-time data I would suggest setting up a near-real-time environment first Then see how many use cases are not satisfiedOne of my clients (a startup in NYC) had a really smart data team who wrote excellent code Over the course of about 3 years they wrote tons of great ETL code in Python But unfortunately over the course of 3 months the company lost its entire data team The business had evolved by then and the code was no longer working as well as it used to The lesson: smart engineers dont always equate to the best solution And you dont need to write code from scratch for everythingFor example Extracting and Loading tools (aka Data Pipelines) are so good these days that you can have the pick of the litter from free all the way to tens of thousands of dollars a month (ex: Stitch Alooma FiveTran etc) You can easily find a solution that fits your budget constraints performance needs and support expectationsAvoid coding to avoid the maintenance headache down the lineThere are many legitimate fears (investing in new technology that becomes outdated in a few months for instance) in choosing the right tool Especially when there are so many different SaaS solutions with clever marketing teams behind them But if you think of selecting tools based on the 3-layer architecture I mentioned above (1 Extract & Load 2 Transformation and 3 Presentation) and deploy at least one tool per layer you should be able to minimize the pain when you have to part ways down the roadFor example Some BI tools claim to have easy ways to transform the data Say you use these transformation capabilities to create several derived tables (Customer Master or Product Master) Well six months down the line if you have to part ways with your BI vendor for some reason now you will have to find a tool that can not only handle the BI part but also the transformation On the other hand if you had used your BI vendor only for its intended purpose (the presentation layer) then you would have had plenty of choices to swap with much easier executionThe data democratization revolution of the 2010 2012 era got things terribly wrong IMHO The concept of all business users having access to any and all data went too far (but made some BI providers multi-billionaires) Yes business users should have access to consume data However they should not be allowed to create their own data willy-nilly They should not be allowed to bring a list of tags (ex: product features) through Excel create a report and publish it to the entire company This will set you up for failureData governance is critical and needs to be a centralized function Determine whether its worthwhile to invest in a data librarian team which would be responsible for vetting and regulating all data points and KPIsOnce the data governance is in place and youve regulated data creation you can liberate the business users by setting up a BI tool where they can do their own analysis by simply dragging and dropping variablesBe Stalinist with data creation but liberal with data consumptionMost data consumers in a typical company are non-technical business folks who will never write a SQL code in their life nor care about the underlying database tables A good user-friendly interface is a key to winning this demographic Based on your use case it could be DIY Business Intelligence platforms like PowerBI or Looker or a simple scheduled PDF reporting solution But getting this part right is critical to the success of your data warehouseChoosing the correct BI/ Reporting tool can be overwhelming Especially with the plethora of choices in the market This is a topic for another time but start by understanding the following aspects better: Business users hate change Even if you build the perfect data warehouse and the perfect BI/ reporting system you will have someone on your team who will refuse to use it because the company logo now appears on the right side and not the left That is not an exaggeration! Ive seen worseAllow ample time to make the transition easy for everyone Here are a few tips: Another important aspect of change management is breaking bad (legacy) habits of your internal data and tech team(s) If you are thinking of building your data infrastructure whatever youve got now probably isnt working  and the current team may not have the correct expertise Therefore account plenty of time for hiring the right people training the existing and putting guardrails to prevent bad legacy practices from coming backDevelopment and maintenance are two different thingsDuring the initial development phase (3 6 months in a typical case) you will require a seasoned data/ system architect and at least a few heavy-duty data engineersThe architect will be responsible for assessing your existing environment understanding your future business goals and coming up with a detailed plan on how to execute the perfect data ecosystem tailored for your needs Someone who just received their AWS system architect certification last month and read a couple of articles unfortunately wont cut it! The ideal architect is someone with a decade of hands-on experience who has seen the data world evolve over time The data engineers you need at the beginning should also have some experience doing thisBut data engineers and super senior data architects are expensive you say And oh by the way they get bored and leave for more exciting projectsThis is why for most companies it makes perfect sense to contract an external team with experience during this initial setup phase rather than hiring full-time staff Overall it will be cheaper more efficient and fasterOnce the basics of the warehouse are set up the maintenance of the warehouse and the support for the BI/ reporting tools become more straightforward and less technical Hiring data analysts with business domain knowledge coupled with a junior/mid-level data engineer to attend to the minor technical changes is a good starting pointCentralizing understanding and owning your own data is a good thing And creating a data warehouse that facilitates this is a no brainer and is a strategic necessity in todays market But getting it right requires careful planning and eliminating all noise Take a deep breath Use the framework outlined in this article to simplify some of these complexities
Vk6oA2jJsWVcXfCtNkXuCJ,James Miller talks about how to become a data engineer and what he gets up to in his role at BGL Tech: Im a data engineer in the Data Solutions team at BGL Tech If you dont know what a data engineer does it involves designing integrating and building data from various sources which can then be reported on or analysed by our teams of BI (business intelligence) analysts and data scientistsMy route into data was not conventional I was training to be an accountant before I moved over to provide application support for my accountancy practices time and billing system and then for Wickes retail systems These roles were where I started learning about relational databases and SQL coding which I use daily in my current role I then moved into business intelligence and data warehousing at Travis Perkins before I joined BGL three years ago initially as a business intelligence developer in the life insurance part of the business home to brands such as Beagle StreetThere are people in our team from a range of backgrounds but what we all share is SQL coding experience familiarity with SSIS (SQL Server Integration Services) an understanding of relational databases and most likely another programming language such as Python as well as the ability to work effectively as part of a team and collaborateBefore I get into what a typical day looks like Ill explain a little about how the Data Solutions team worksWe use the Agile methodology for development as do the other teams in BGL Tech Our work is planned in two-week sprintsBefore each sprint there is a planning meeting where the team meets to review the tickets (requests for work) raised by our stakeholders and allocates them to a data engineer to work on in the upcoming sprint There are a whole range of tasks from a variety of different business areas as well as standard tasks such as monitoring overnight loads from sources such as Google Analytics transactional systems or external data feeds from third parties (each engineer does a week of system support on a rota basis) Then there is always some ongoing project work that needs to have people allocated to it for example a review of our pricing data sets or GDPR complianceTypical development requests include bringing new data sets into either the operational data store (ODS) or data warehouse (DW) adding new fields to an existing data set producing data extracts for internal or external use changing business logic or rules in our processes/ automating processes as well as system maintenance and database analyst tasks to keep the system running well indexing backups and restores etcA typical day will start with a short meeting where Ill provide a status update to the rest of the team about the work I did the previous day what I have planned for the day ahead and any problems or blockers to completing the work Im doingThe rest of the day is spent working on the tasks that have been assigned this might mean writing code developing an integration process to move and transform data Talking directly with stakeholders to better understand their requirements and to test and validate work that has been completedLast week I took my turn on the support rota and that normally means Im online a little earlier than normal to check for any job failures that need resolving The rest of the day is spent monitoring the system and triaging the support tickets before investigating and resolving themThere are some reoccurring meetings that are related to agile ways of working as well as a weekly team meeting to attend which keeps me well-connected to what the priorities are across the team and the wider businessIts a role that is fast-paced and brings me into contact with a lot of other areas of the businessI enjoy writing code and problem solving and being able to contribute to producing valuable insight that is used to inform business decisions There is plenty going on across our team whether that be using data to combat fraud or supporting our early coronavirus response to our customers Each day brings something new and varied and BGL is an innovative and exciting company to work forWith over 10 million customers BGL is a leading digital distributor of insurance and household financial services via brands including comparethemarketcom Budget Insurance Dial Direct and Beagle Street It employs more than 3000 people across its five sites in Peterborough Sunderland Wakefield London and Paris
8Y2bQ8aVavtfzhuqyssjBW,KeyBy is one of the mostly used transformation operator for data streams It is used to partition the data stream based on certain properties or keys of incoming data objects in the stream Once we apply the keyBy all the data objects with same type of keys are grouped together It is analogous to groupBy in the traditional SQLInternally KeyBy is implemented with hash partitioning Every KeyBy might cause a network shuffle that will partition the stream on different nodes which requires a lot of network communication hence its an expensive operationFlink data model is not based on key-value pairs Therefore you do not need to physically pack the data set types into keys and values Keys are virtual: they are defined as functions over the actual data to guide the grouping operatorThere are various ways we can specify the key while applying the keyBy transformation The easiest one is probably providing the index for tuples You can provide multiple indexes as to group the item based on the composite key For example in the following code first we are using Integer or first tuple item as key and using integer and string as composite key for the second keyBy transformation operationKeys can by specified using the field name eg inputkeyBy(someKey)  Field expressions is really nice way to specify the keys for nested types and even tuples inside those typesA type cannot be a key if it is a POJO type but does not override the hashCode() method and relies on the ObjecthashCode() implementationit is an array of any typeFinally we can tell Flink about the key using the Key selector functions which takes the input object and return the key from itIf you want to set the range of values as the key probably key selector can be used to create an expression to divide the stream into different keys For example in the above example if we want to split the stream into two with even and odd number of customers we will only return cabRidePassengerCount % 2; and it will split the streamReferences1 https://ciapacheorg/projects/flink/flink-docs-release-110/dev/api_conceptshtml#specifying-keys • https://trainingververicacom/lessons/keyed-streams
bCtS8JfZT44iy5mayQvmpr,State management comes out of the box for Flink and it is considered as the first-class citizen While Flink abstracts the traditional state complexities for application developers it needs to do a lot more to provide stateful fault-tolerant applications It needs to checkpoint the state frequently and restore it in case of failuresApplication checkpointing is a common technique in computer science to make applications fault-tolerant In this approach we make a copy of the application state called snapshot at a regular interval and store it When the application fails we restart our application using the last saved snapshot of the application It helps streaming applications to continue processing where they left off instead of starting all the calculations from the beginningCheckpointing in a distributed system is more complex because of the dynamic and unpredictable nature of the network Distributed Snapshots snapshots for a distributed system at any point in time will contain the state of all the processes (vertices) and their network connections (edges)Flink is a distributed stream processing engine hence it uses a distributed snapshot algorithm for checkpointing It does leverage a variant of the famous Chandy Lamport Algorithm Different distributed nodes in the Flink cluster process the data independently from each other and the checkpoint will need to re-align the nodes before taking a snapshotFlink requires a replayable data source in addition to the state backend for the checkpointing When an application fails checkpoints are used to restore the application based on the snapshotted position of data source eg Apache Kafka offset to go back and replay the lost messageCheckpointing is enabled using enableCheckpointing(n) which requires how frequent checkpointing should be triggeredAdditionally while setting the state backend persistent storage path is also required statecheckpointsdir where the checkpoint will be storedReferences:1 https://ciapacheorg/projects/flink/flink-docs-release-110/dev/stream/state/checkpointing
GQp7ZJeyUsLCtPLCLmqeHV,One of the tools BigData Republic uses for its large scale data crunching is Apache Spark While Spark at first was mainly focused on processing large amounts of data in an efficient way it is continuing to increase support for major machine learning algorithms This way it is becoming the defacto standard for data science and big data engineeringTogether with my colleagues Gerben Oostra and Alexander Backus I attended the Spark Summit in Brussels last week In this blog post I will discuss the (in my opinion) 5 main topics to keep you updated with the state-of-the-art developmentsNote that all of the talks from this Spark Summit will be freely available on the official site expected the 4th of NovemberThis is one of the coolest additions for Spark Using Tensorframes which are basically a wrapper around the TensorFlow framework it is possible to utilize the GPUs that might be available on worker nodes for the purpose of training deep neural networks In fact all machine learning algorithms implemented in TensorFlow should become available this wayThe idea is to utilize Sparks distributed features to (horizontally) distribute relevant input parameters over the worker nodes and then use TensorFlow to (vertically) optimise computations for specific node hardware including GPUs This is especially handy if you want to run compute intensive parameter tuning jobs for your machine learning modelOne of the key features that enables data passing between TensorFlow and Spark efficiently is the new Spark columnar in-memory storage format This helps porting between TensorFlow as well as other machine learning frameworks who use vectorized (column-wise) data structures to operate on as opposed to Sparks default row-based formatSince Spark was first released many new features have been introduced but run-time performance optimizations were not yet touched upon Most of the code responsible for the actual execution of computations in a Spark job consisted of many virtual function calls (a by-product of Object Oriented programming in Scala) Many standard operations in Spark code are highly generic but therefore hard to optimize for the Scala compiler and at run-time by the CPU (eg branch-prediction is hard) To solve these problems Tungsten was developed This is a whole-stage at run-time code generation component that first inspects the whole Spark job end-to-end then generates code that is optimised for the particular job instead of the job being highly genericSpecifically for Spark SQL Spark 20 also comes with improvements to the component that selects which operations are actually selected to be part of the job This is the Catalyst SQL query plan optimizer which defines strategies on which operations to use For example for joining two tables multiple types of JOIN operations could be used ranging from a map-side join using broadcasts to fully distributed cartesian product joins Tungsten and Catalyst are complementary components Catalyst first optimises the query plan for a job then Tungsten generates highly optimized code for each of the operations inside that jobAs more companies start implementing data science for their business they realise they need to integrate their machine learning models with their existing workflows and infrastructure to get a return on their investment However most of the models data scientists create are not suitable to be put into production directly Using software libraries like MLeap or H2O allows data scientists to export their machine learning models to a standardised format that can be easily integrated in (existing) big data pipelines In addition the companies backing these libraries try to provide users with a platform that makes deployment to production easier even going as far as providing a one-click production deployment solutionRelated recommended talks to watch: MLeap and CombustML: Deploying Machine Learning Models to Production (MLeap) Sparkling Water 2Sadly I was not fast enough to make a screen for this topic But nevertheless this is one of the nicest talks I have attended at the Spark Summit Most of the machine learning libraries are focused on training models in batch This limits models that are already implemented in real-time big data processing pipelines because they cannot adapt to fresh data in near real-timeHowever more effort is put in to make online training possible using Spark Streaming This means a model can be updated in near real-time whenever the big data pipeline feeds it new dataThe technical implementation in Structured Spark Streaming is roughly as follows: This looks very promising but take note that the caveat is in the details of choosing a good aggregation functionOne of the more recent development in machine learning is Factorization Machines This machine learning algorithm is especially suitable for very sparse datasets but no distributed implementation was yet available In one of the talks this was introduced as well as the Glint parameter serverThe Glint parameter server is a Spark-optimized distributed cache for storing common machine learning parameter data structures This component should prove useful for making more embedded distributed machine learning implementation possible in the future
KLwA5QsqUtS4Q8ChN4BEdP,Theres no Scala Days conference without a keynote of Martin Odersky himself This year he spoke about his current work: Dotty Dotty is the new Scala compiler that will be part of Scala 3 The first release candidate was released just hours before the keynote and comes with the compiler itself (dotc) a repl (doti) a doc tool (dotd) and an IDE It implements the MS language server protocol enabling it to serve several front ends: VS-Code and Emacs (IntelliJ support is in the works) With Dotty IDEs can use the regular compiler as the presentation compilerDotty is developed as a separate branch It should release a stable 0x in early 2018 The current rework on ctdlib collections will be part of 213 and dotty 01 Scala 3 is expected to release in 2019 or beyond Dotty adopts the release train model with iterations of 6 weeksThree important language constructs are types functions and objects Most languages do two of the three Scala should synthesize all three For Scala implicits are at the core of this: implicits are functions that pass around objects and is driven by typingContext is what is outside of the project but implicitly known Scala implicits are the canonical way to represent context Scala has pioneered the synthesis between types functions and objects and a lot of languages are moving into this space With the experience built in the past years its a good time to rebuild the basics
GqMPBmqcQashHXKzQf7U6C,For about a year I have been fully submerged in everything regarding Big Data; working with various tools and techniques throwing a bit of data science in the mix I realized there is a high entry barrier for organizations to start turning their (dormant) data into something useful With this knowledge I wanted to look at how some of the leaders and early adopters in Big Data are tackling these barriers and if they will become easier (or harder) to handle in the future Luckily BigData Republic gave me and 2 colleagues the opportunity to visit the DataWorks Summit in Munich this April providing some inside informationThe remainder of this blog describes how I as a Big Data Engineer/Architect see the changes that are coming From a broad perspective I identified 3 themes throughout BigData ecosystem also tackled at the DataWorks summit: Within the European Union strict regulations about personal data are going to be implemented by 25 May 2018 The rule is Privacy by design meaning that for every use of data you need personal compliance Not complying to the rules will carry some hefty fines up to 4% of worldwide turnover This regulation is a big legal document containing issues like what is personal identifiable information (PII) and how do you access and storage it Im not going to bore you with the details but this comes with some challenges for Big Data platforms ie: A lot of data-lakes are designed around the principles: save all append only unstructured data storage These things can create some problems You dont want to have to drain your lake with years and years of data because it is riddled with PII At the conference the answer for this problem is presented by using apache Atlas for tagging and Apache Ranger for access restriction Although these applications can help you with control access and restriction on processing they dont help you with the RTBFA presentation by Balaji Ganesan from Privacera provides some nice guidelines in handling GDPR in a checklist: From my practical experience it is important to identify the points of entry for personal data and already design storage and processing around losing this data with limited impact A good move would be storing data like website klick-troughs in two places one with personal data and the second with the remainder linking it with an anonymous keyA second subject general for the summit was the focus on generalizing open source Big Data tooling by creating user interfaces Installing a Hadoop cluster with ie Spark Flink and Kafka sometimes requires mad Linux command line skills Hortonworks tries to make the life of administrators easier by extending Apache Ambari for installing configuration and monitoring these different toolsExamples of tools Hortonworks is working on together with partners are: Although these tools look promising they are often still only distributed via Github with multiple branches and limited committers or require proprietary contracts I think these tooling work fine for purpose but when customization is required they create only more complexity Then again I write from my perspective as an engineerThe third development in Big Data also present at the summit is about early adopters like ING BMW Danske Bank and Centrica who have been working with Hadoop software for some years now These organizations never started as software organizations but are now heavily depending on software developments in the open source Big Data community like HDFS Spark KafkaTake BMW for example who has over 6 million vehicles around the world collecting over 1 terabyte of data from car sensors digital maps artificial intelligence and digital context models into a data lake each day By combining they are actively working on projects like autonomous driving increasing customer satisfaction and improving production methods These data driven projects may prove crucial in the future since BMW itself acknowledges being under pressure from new companies like TeslaSo up to some lessons learned from the field and presentations at the Summit: Data pioneers need to work together on one Big Data platform to create the highest added value A lot of organization have small data islands based on POCs and software suppliers trying to get a foot in the door Having one department makes sure people use the same data process and connected technologies this also support knowledge sharing between employees educating by examplePilots in Big Data depend heavily on reaching a business benefit fast so limited time and effort is wasted on system complexity An important part of this is cutting losses and keeping focus when new discoveries or exploration are creating too much complexity Fail fast is also applicable to the design of the software and logging that should put errors close to the source as possible making debugging previous manufactured pilot code a lot easierThe administration and distribution of data needs to be firmly embedded in the organization activities like use and access require clear guidelines The entire organization needs to be aware of guidelines regarding: Often organizations start by investing in technology like servers and licenses without having the people in the organization ready and motivated Since a Big Data project is embedded in the entire organization you require a team with the right mix skills Such a team consists of people with open minds capable of tackling any problem be it technical organizational or theoretical that are motivated by the need to innovateLooking at the BMW case they also identified several pitfalls regarding the implementations: Although the selection of lessons learned above is far from complete it is a small look at challenges to tackle before or during a Big Data project in any organization
jykxYAQjcpy5ciZPKwjvpR,Since Billie is a true data-driven company we have to deal a lot with ETL (Extract Transform Load) tasks As the name implies these tasks arise when there is a need to take some data from data source A transform it according to the desired format and finally put it to destination BOur main data sources are Facebook Google AdWords Salesforce and naturally production database Transformations usually include change of date format integration of lookup keys duplications removal etc Data Warehouse is the typical data flow destination since it is used as the single source of truth for the data analysis needsSo lets assume you have a table T1 in database D1 which you want to be the data source for table T2 in database D2 If databases D1 and D2 are deployed on different machines then you have no choice but to build a data flowMany developers in this situation say to themselves: I know Python/Ruby/PHP/Java/Klingon programming language so how complicated is it to write a short script which will connect to database D1 read table T1 connect to database D2 and finally push data to table T2 Of course it is not rocket science and such script can be written in any modern programming language but this is the perfect example of Law of the instrument
SMn2jhw4i6ibsAGbDcMTYq,What a good data engineer must do once on a while is for sure to maintain a good overall performance of your ETL toolsYou need not only to check if your database is in good health like: You need also that your ETL software should go at maximum speed how can we achieve this?First of all to an analysis of what your software should do and how is evolving You know that once you write you Mk1 version it wont be the same as the Mk15 so you need to re-check its functionality: Once you have done with the analysis lets do some dirty work: These are some of the operation you need to do to maintain your software as light and young as possible obviously if needed rewrite your entire code with a new more useful programming languageAny advice by you? I am mistaking anything? Write it on comments
n2XKsyrrrhM8yySUqgcUBi,In this story im going to tell you why and when you need to write your own ETL flow to feed your data warehouseThis depends on your needs and data base typeWell a feeding flow could be written with two main tools: 1  Drag and drop BI tools:Those tools simplify a lot the enormous amount of code you need to writeWith these tools you need only to drag and drop the feature you need and configure it Often these tools are based on Java and each feature is written with the aid of XMLPros: Every operation is easy and powerfulCons: You need to wait the producer to release the feature you need for your ETLWhen to use it?Its better to use those tools when you are not a developer and need to develop a fast paced BI project • Writing your own code:By writing your own code its an another world better or worse depending on your needs and project pacePersonally i prefer this way because you can add as many features you need at any pace you wantBut on the other side you need to be highly skilled on writing code and knowing at least one programming language between Python Javascript or Java; you can write an ETL with C but it starts to be very difficult to maintain and to debugWell this ends the thought about how and when to write your own code let me know what you think about this topic
G23tits9J34zeaAofUSz2i,Last story was about if its better to write your own ETL flow instead of using any of the tools available nowadays in this story we will cover how to write a good quality ETL flow with javascript and Node jsA good folder structure is needed in order to give to each part of your software its importance and to have a mental organisation where to find the piece of code you needThis is the usual folder structure i use for my projectsOur starting point is indexjsIn this file you should put all the logics needed to avoid any incongruence for example: Once youve done those checks lets start with the first part of our loadingIn dataLayer you should put the database structure you have for example: For each table you have to create a raw import of your data without transforming it at first (maybe only a rename should be made at this point) with a simple select * query importing only the difference from the last import A good check should be the lastInserted data and load only the data produced after that periodThis is the funnier part of the work as long as you have created the database schema you should write the import code with some transformations (eg dataTypes) and some simple computations (EgEvery time you do an insert dont forget to drop the primaryKeys every constraint and every index this allows a faster insert and avoids errorsBattling with existing dataEach time you start to import your data into dwh delete before any insert all the existing data matching the ids that are into staging this allows to implicitly update the data instead to use huge updates that are slowSo do something like this: It is obvious that you need to delete every data in staging environment before importing from source in order to make this logic workThe last but not the least here you create your data marts using SQL scripts and Sequelize to read the file and execute itWith PM2 you can create a cron-like job that starts the nodeJs scripts at the time of your preferenceWith ubuntu just use: pm2 start etland with PM2 list you can list all the jobs created in PM2An another good approach to check if the job is running well is to write each step into a loggingSql table structured like this for example: This si very useful for checking in real time what is doing the job: And at the start of each step update the table inserting the duration of previous step by doing a simple (endTime - startTime)This is obvious an example let me now in the comment what are your best practices and lets start a discussion
ShQqKyQ8uNALUEodQrA82x,Getting insights from data are what everybody is looking forMeet Stephane Hamel Stephane is an experienced digital analytics consultant cumulating a dozen years in the field He has 25 years of web experience and 30 years of working with data to understand and optimize business processesStephane: I leverage the Digital Analytics Maturity Model (DAMM) I created in 2012 to help my clients and the agencies Im coaching to uncover their strengths and weaknesses and build a realistic digital roadmap Im also known in the industry for the Web Analytics Solution Profiler (WASP) I created in 2006 as well as speaking teaching and writing about digital analytics and digital marketing for several yearsStephane: I came to the field of digital analytics from a technical perspective  Im a software engineer DBA system admin at heart who eventually got tired of being tagged as the IT guy and decided to do an MBA specialized in ebusiness This gave me a well-rounded perspective on the digital analytics industry that was reminiscent of the early days of the web itself: lots of collaboration lots of fast-paced innovationSince the first year of work Stephane started working with data to understand business processes His work with data is an endless game of Lego Keep learning new tricks or arranging the blocks in new creative waysStephane: When I worked for the Montreal Stock Exchange I learned a lot about finance trading real-time data processing and even Big Data before the name was coined When I worked for a high-end software company I learned about animation 3D and special FX When I worked for a recreational sports manufacturer I learned about manufacturing ERP CRM dealer networks and a whole lot moreSo there is a pattern: data context and creativity to find new solutionsThere are essentially two ways to leverage data: stating a hypothesis and digging in the data to see if its true or false or setting a goal and following the data to see if we are making progress toward itStephane: Either way is fine as long as you start with the right statement and everybody (all stakeholders) agree on it You can always dive in and hope to randomly stumble on something… on that needle in the haystack… but thats usually not the best approach! Im a strong advocate of optimizing a thousand things by 1% rather than trying to find the Holy Grail that will magically double your sales (or whichever metric you consider to be a measurement of success) Small continuous improvements are easier cheaper and add up to bring exponential resultsStephane: I see a lot of organizations chasing the next cool thing rather than focusing on the basics: satisfying your customers listening to them and striving to improve your products and services in the long runStephane: Many marketers boast Facebook Twitter Pinterest YouTube Instagram G+ logos on their sites and are eager to broadcast on Facebook or Twitter and watch the number of likes and retweets (or lack thereof!) instead of actually engaging a conversationStephane: It depends on the business question you are trying to answer In some cases you need a representative data set so volume is important In other cases small data might be just fine but it needs to be very accurate and precise (veracity) Thats why I always tell more junior analysts to look into the Six Sigma process of Define-Measure-Analyze-Improve-Control You dont have to go into the whole Six Sigma shebang… but its worth learning about problem solving and optimization concepts brought forward by Lean and Six SigmaStephane: Digital analytics data is a given  you want to understand your audience you want to learn how they found you (acquisition) what they did (behavior) and if it worked (conversion) This gives you a lot quantitative data and I like to augment it with qualitative sources of information such as call center data support requests surveys or… feedback on Facebook Twitter emails etcStephane: And then you have about 3000 tools in the mar-tech space to choose from! And this becomes super complex as lead generation marketing automation specialized tools ad networks back-office data are added to the mixStephane: Ah! There it is! This is where a platform like Blendo can play a role Ubiquitous and easy access to the right data at the right time to answer the right business questions (while ensuring proper privacy!) is a challenge for both data scientists and business users/marketers who want to be empowered I think if theres an affordable scalable human-friendly solution theres a great potentialStephane: I remember reading the book Becoming a Technical Leader by Ken Orr  it resonated with me because I was not aiming for the classic career path of becoming a manager but instead strived to become an expert in my areas of expertise (I can say I have achieved it!) This tinted my career path and I often made choices that sacrificed power and money in favor of liberty and creativity Some people could rightly argue one option isnt in opposition to the other I have found the balance by being a freelancer and building my own personal brand in a niche industry living in a nice historical heritage area near Quebec City working from home and allocating a fair amount of time for writing and creating new tools I rarely have more than 50% of billable time  I could make more money but would lose something: playing with data I guess thats why my motto is Data is the raw material of my craftEnjoyed Stephanes interview? You will probably like Carl Stathams tooOriginally published at blogblendo
AGDNDdEwfoGwgiAorocN2z,At Bluecore weve been using Airflow to simplify some of our most complex data workflows Over the past year weve been able to work out a few of the more complicated kinks involving Airflow Operator bugs and implementation decisions Engineers across the team have benefitted from quicker iteration speed and more stable pipelines by creating their own flexible Docker images and controlling their own deployment execution and testingMost of our application still lives on Google App Engine Standard Moving forward new projects are slated to run on Kubernetes and a few have plans to be migrated over But there is still a significant portion of our codebase that we wouldnt be able to schedule through Airflow directed acyclic graphs (DAGs) We realized we needed a way to write DAGs that could execute code on Google App Engine in order to make Airflow a useful tool for all of the engineers at BluecoreAt first glance this seems like an easy problem to solve: we could just have Airflow hit the Google App Engine application with an HTTP request Wed write some code in App Engine that would handle requests from Airflow route the requests to existing functionality in App Engine and return a useful responseAnd this is what we did! Until we realized it didnt work Google App Engine instances spawned via automatic scaling have a 60-second deadline for HTTP requests While this worked for a few tasks we wanted to trigger from Airflow a lot of the existing functionality would take more than 60 seconds to execute If we chose to throw the execution of these tasks onto Task Queues instead we lost our ability to monitor task execution or relay any type of return value This meant we wouldnt know if a task had executed at all whether it succeeded or failed or have access to its return value! Obviously this wasnt a workable solution for 99% of our desired workflowsOur asynchronous setup would look like this: Naming the solution was fairly easy: we need to execute longer running tasks asynchronously on App Engine while still monitoring their progress and allowing return values Implementing the solution posed more challengesLuckily Airflow itself provided an example for how this should be done In general individual tasks in Airflow do not communicate with one another Task B does not know or use any information from Task A aside from potential Trigger Rules But there is a workaround For DAGs where you need to communicate information between tasks you can use Airflows XComs XComs allow individual tasks in a DAG to write information to a shared database making that information available to all tasks in that DAG We realized that if we utilized XComs and wrote to this shared database from tasks being executed asynchronously in App Engine we would be able to track task execution and read return valuesWe had to make a few tweaks to our local instance of Airflow to make this happen: Putting all of this together hitting Google App Engine from Airflow now looks like: With the new and improved (read: working) App Engine Airflow Operator we are now able to leverage the full functionality of our App Engine code from Airflow without excessive code replication and dependency wrangling A significant portion of our workflows that will live in App Engine for the foreseeable future are now able to be executed via AirflowInterested in working with us? Check out our careers page here: https://wwwbluecore
JYZmHiicHGHSShjaReQsBB,At Bluecore we love data Our products rely on crunching lots of data to help our eCommerce customers provide personalized experiences through email advertising and customized on-site experiences As we continue to add more products we have had an increasing number of ways to manage data processing workflows Instead of maintaining N different systems to control our data workflows we want to consolidate our data processing onto a single platformAs Simple As Possible As Powerful as Necessary is a cultural maxim at Bluecore engineering In the early days of our product evolution we built a composable ETL system for linear data processing workflows That no longer scaled for our newer data products and thus we decided to introduce Airflow a platform to programmatically author schedule and monitor workflows created by Airbnb In this post Im not going to write about what Airflow is its basic concepts or why you might want to use it Airflows own documentation does a great job of covering this and there are a growing number of posts that give examples for those new to AirflowInstead Im going to write about a specific blocker my teammates and I hit when we decided to start running production jobs for a project Project X through AirflowWe set up a basic Airflow environment that runs on a Kubernetes cluster and uses Celery workers to process tasks Project X requires a workflow be run per customer To do this in Airflow we created a Project X DAG for each customer A DAG Directed Acyclic Graph defines a workflow in Airflow Here is a simplified version of Project Xs DAG: We vetted this for production like everyone does: we ran one customers DAG it worked we ran two customers DAGs they both worked its production ready! We were feeling confident about our environment our DAGs and our Airflow knowledge so we began running hundreds of production Project X DAGs through AirflowAnd nothing workedWe had kicked off the DAGs through the Airflow UI and saw the workers start processing tasks Since we knew the DAGs would likely take a few hours we moved on to other work while (we hoped) they would complete in the meantime But when we checked a few hours later we found that the workers had barely made any progress at allWe were surprised and decided to try the simplest solution: throw more resources at the problem We added more workers to our Kubernetes cluster and upped the concurrent task limit With somewhat reduced confidence we reset the environment and kicked everything off againAnd still nothing workedAt this point our previous confidence had disappeared Unable to pinpoint the issue from our initial understanding of Airflow Kubernetes Celery workers etc we set off to hunt through what clues the broken environment could give usIn our initial debugging we found two symptoms of our mystery issue: Thinking that the first symptom might be causing the second we searched for the cause of the evicted Kubernetes pods Luckily Googling workers evicted Airflow Kubernetes lead us to helpful Kubernetes documentation and other Medium postsThe problem was that we were not specifying memory resources for containers running our Airflow code on KubernetesBecause our containers were continuously trying to use more memory than was available our Kubernetes pods were continuously being evicted This is expected behavior as per Kubernetes documentationFun debugging side-note: our quick-and-dirty solution from earlier to increase the limit of concurrent tasks for the workers actually exacerbated the problem! By increasing the number of concurrent tasks without specifying memory resources the scheduler was scheduling even more processes to run on Kubernetes machines without the necessary available memoryTo fix this issue we specified memory resources in our Kubernetes configuration We applied the updated configuration to our cluster reset the Airflow environment and tried to run the DAGs againWe were happy to see that our workers were no longer being evicted by KubernetesSadly the Celery workers were still processing tasks that were never completing So we searched through Airflow documentation Celery documentation Kubernetes documentation and logs This is when we took very good advice from a teammate: step back simplify and try againWe scaled down to just one worker running with a limit of 32 concurrent tasks We kicked off one DAG hoping to hit the same issue in this simplified environment Luckily we didFun debugging side-note: You read earlier that we tested two DAGs successfully in production which is why we felt confident moving everything over Now Im saying just one DAG broke the environmentIn the original environment we had multiple workers and a much larger number of available concurrent tasks While we technically should have hit the same issue we basically just got luckyWe found that at the point when the Celery workers stopped completing running tasks each of the running tasks were instances of Airflows SubDagOperatorFor those less familiar with Airflow Operators the SubDagOperator is a native Operator that allows you to use a task in a DAG to kick off an entirely separate child DAG The parent DAG must wait for the entire child DAG to finish processing in order to move on to the next task So the SubDagOperator task is occupying a workers execution slot and is waiting for a child DAG to finish The child DAG cant finish (or even start!) because it is waiting for an available execution slotNow that we had diagnosed the problem Googling `Airflow SubDagOperator deadlock` quickly confirmed that other people were also experiencing deadlocks using SubDagOperatorWe replaced all instances of SubDagOperator with a new custom Operator This Operator performs the same tasks as the previous SubDagOperator but all inside of a single taskEven though it took a few people a few days to dig in debug and solve this specific issue we still think Airflow is a good choice for managing Product Xs workflow However this issue did highlight that there will always be different problems at scale (and in production) than in a development environment The best thing that you can do is understand the system so you know how to debug when the time comesInterested in working with us? Check out our careers page here: https://wwwbluecore
ZYmLFEqTPSCK82xoEUD9Ew,Deciding which database to use for a service requires some understanding of how different databases store data especially at scale Choosing between a database that stores data as rows or columns for example has a big impact on the database performance For this reason certain access patterns are better suited for row oriented databases (row-stores) and others for column oriented databases (columnar-stores) At Bluecore our production infrastructure for analytics includes both Google CloudSQL (row-store) and BigQuery (columnar-store) in our analytics pipeline We carefully chose these databases after weighing the tradeoffs of each and analyzing our access patterns Since we process data for the many millions of messages we send each day choosing the right database for our applications is vitalThere are two main types of storage: hard disk drives (HDDs) and solid-state drives (SSDs) We store the database in storage Data is written to storage in contiguous chunks We wont get into the difference in pages sectors and blocks here because it is only important to know that the data is grouped together and written/read according to its location in storage In row oriented databases these chunks contain a row consisting of all of its column values as a tuple In column oriented databases these chunks contain only the values in each row that belongs to that column Given a certain amount of space on an SSD or HDD a chunk of rows/columns data is read at once This difference has a significant impact on performanceIf the data you need to access is stored mostly in a small number of columns and it is not necessary to query each field in the rows you may be better off with a columnar-store On the other hand if you need many columns in each row to determine which rows are relevant a row-store may be a better fit There are a few more distinctions between row and column oriented databases that are important to discussRow-stores are considered traditional because they have been around longer than columnar-stores Most row oriented databases are commonly known for OLTP (online transactional processing) This means that row-stores are most commonly known to perform well for a single transaction like inserting updating or deleting relatively small amounts of dataWriting one row at a time is easy for a row-store because it appends the whole row to a chunk of space in storage In other words the row oriented database is partitioned horizontally Since each row occupies at least one chunk (a row can take up more than one chunk if it runs out of space) and a whole chunk of storage is read at a time this makes it perfect for OLTP applications where a small number of records are queried at a timeRow-stores (ex: Postgres MySQL) are beneficial when most/all of the values in the record (row) need to be accessed Row oriented databases are also good for point lookups and index range scans Indexing (creating a key from columns) based on your access patterns can optimize queries and prevent full table scans in a large row-store If the value needed is in the index you can pull it straight from there Indexing is an important component of row-stores because while columnar-stores also have some indexing mechanisms to optimize full table scans it is not as efficient for reducing seek time for individual record retrieval than an index on the appropriate columns Note that creating many indices will create many copies of data and a columnar-store is a better alternative (see When to Enable Indexing?)If only one field of the record is desired then using a row-store becomes expensive since all the fields in each record will be read Even data that isnt needed for the query response will be read assuming it isnt indexed properly Consequently many seek operations are required to complete the query For this reason a columnar-store is favored when you have unpredictable access patterns whereas known access patterns are well accommodated by a row-storeAs more records in a database are accessed the time to transfer data from disk to memory starts to outweigh the time it takes to seek the data For this reason columnar-stores are typically better for OLAP (online analytical processing) applications Analytical applications often need aggregate data where only a subset of a tables attributes are neededColumn oriented databases are partitioned vertically  instead of storing the full row the individual values are stored contiguously in storage by column The advantage of a columnar-store is that partial reads are much more efficient because a lower volume of data is loaded due to reading only the relevant data instead of the whole recordFor example if a chunk of storage can hold five values and the database has five columns (ex: one row has five values) one row will take up one chunk and be read together If only one column value is needed for the query response a columnar-store can read 5x as fast because you will read five column values in one chunk as opposed to one column value in the chunk containing the row You also avoid reading the other column values that are irrelevant to the query responseAdditionally in column-stores compression is achieved more efficiently than in row-stores because columns have uniform types (ex: all strings or integers) These performance benefits apply to arbitrary access patterns making them a good choice in the face of unpredictable queriesColumnar-stores (examples: RedShift BigQuery) are good for computing trends and averages for trillions of rows and petabytes for dataAssuming this table continues for millions of rows what if we wanted to know the sum of the amount spent on online purchases for company A? Well for company As online purchases table we would need to sum all of the online purchase values Instead of going through each row and reading the email type of purchase and any other columns this table could have we just need to access all of the values in the amount columnIn our analytics pipeline we use both Google CloudSQL (row-store) and BigQuery (columnar-store) Although the OLTP part of our application (row oriented database) is used for reporting data the analytics pipeline is a good example of how OLTP and OLAP applications typically use the same dataset but different access patterns The same data is necessary for the result but the difference in their access patterns/use cases necessitates two different database structures The analytics data stored in CloudSQL is regularly transferred to BigQuery for use with LookerFor part of our analytics pipeline Looker dashboards are available to our customers to query and analyze their data For Looker BigQuery is our preferred choice because we need to aggregate large amounts of data by a specific subset of columns Additionally our customers can create their own queries as they wish Being able to support ad hoc querying is an advantage of column oriented databases BigQuery bandwidth is great  it scales automatically allowing for querying of large amounts (petabytes) of data This is the bandwidth we need since many of our Looker dashboards compare years worth of data at a timeWe use MySQL with Google CloudSQL in another part of our analytics pipeline where we provide dashboards to our customers that detail the performance metrics of individual email send types These preconfigured dashboards allow us to take advantage of fast access via indices and reduce the amount of data scanned for each lookup A small amount of data is involved in each dashboard and the queries are known ahead of time In this use case the dashboards are more for reporting data than aggregating it Additionally being fully managed by Google just like BigQuery is advantageous because the maintenance of the database is taken care of by Google For CloudSQL in particular the replica and transactional support are helpful especially for easy rollbacksWe discussed the differences in column and row oriented databasesFor analytics both our CloudSQL and BigQuery databases contain the same data stored in different ways This is done to accommodate the different access patterns which impacts performance We can build better services by weighing the tradeoffs between response latency and data volume and understanding the difference between columnar- and row-stores When clients rely on our service for data and analytics vital to their company choosing the right database can save a lot of headaches
67GtsGuPL7ryQYAKcRJjKK,Note: real-time for our purposes doesnt necessarily mean single-digit millisecond latency so read real-time as near real-timeCurrently data at Ibotta is mostly analyzed in batches where the data is collected over a specific period usually daily and is processed all at once which means that data that arrives now can only deliver insights once the batch is complete tomorrowBatch processes: Batch requires data to sit around before it can provide answers ie high latencyData that is processed in real-time is analyzed as it arrives and results from this are produced continuously This means insights can be made from data immediately and it isnt sitting around waiting to provide business valueReal-time processes: Real-time processes therefore are as fast as possible and can answer most questions that batch can answer with the addition that it can answer now questions egAs threats increase in number and sophistication they demand that we increase the complexity and speed of threat detection and mitigation For instance real-time fraud detection offers enormous opportunity in detecting and mitigating fraud as its being attemptedDetermining optimally personalized content/advertisements in real-time leads to better campaign performance revenue boosts and cost savings Analyzing campaign performance in real-time also promises better and more timely decision makingThere are also many other opportunities to quickly repair UX issues improve product quality and stability as well as to be timely and proactive with user support amongst othersIts unwieldy to say datasets with no set time boundary that are updated as data is received so they are named streams or streaming data as shorthand The analogy of the stream is illustrative as streams have a practically infinite amount of water thats come in the past and you will have a virtually endless amount in the futureAll real-time processing is performed on streams but not all streams have real-time data or exclusively real-time data eg a stream of ad engagements that have a few hours of data added back in to reprocess correctly or a stream is emitted by an upstream process that generates hourly aggregatesStreaming data requires complex systems that have to persist indefinitely and deal with various failure scenarios How they deal with those failures defines what guarantees they can make about how or if messages are processed Streaming Semantics are terms that summarize those guaranteesThe system will guarantee that a message will be processed but wont guarantee that failure scenarios (eg retries after a server failure) wont result in duplicates being introduced into the stream This is usually achieved through checkpointing where data is cached in a resilient storage system until successfully processedThe system has no guarantees against the loss of data but will ensure that duplicates are not introduced into the stream This kind of system lacks checkpointing but through various techniques guarantees that messages it has received wont be processed againThe system will guarantee that a message will be processed and no duplicates will be introduced into the stream These are the most complex semantics and have to incorporate a wide range of techniques to ensure messages are processed only once Kafka is an example of a system which achieves thisNote: most guarantees have limitations and its essential to account for them Here is an excellent guide to the limitations of exactly once semanticsReal-time streaming has many different use cases and they tend to share a range of common challenges These challenges are solved in different ways by a vast array of infrastructure that is continually growing and changing Here are some highlights that youre likely to come across: These are unified low latency durable persistent event messaging systems that allow many sources of data to be aggregated into a single stream and read by many services independently For instance these allow core services and analytics to easily share real-time data without requiring time-consuming integrationsThese are distributed computation engines for performing analytics and transformations on high volume and real-time streaming data They tend to come with a built-in UI and a plethora of out of the box analytics APIs and integrations and are often backed by a commercial company offering enterprise supportThese are libraries that make it much faster and easier to build microservices and applications that do stream processing They are much more low level than streaming analytics platforms require being paired with third-party libraries to do machine-learning or perform statistical functions are tied to particular programming languages and sometimes to a specific streaming platformThese allow caching/persistence retrieval and querying at real-time speeds Theyre used to add state to a stream provide lookups accelerate the performance of a streaming analytics platform and sometimes as a full streaming analytics platform in and of themselves Sometimes they are independent services and sometimes theyre embeddedHere at Ibotta were pursuing real-time analytics with enthusiasmInterested in working at Ibotta? Check out https://ibotta
AHUjHe6AZPouDW7y2mWTBJ,A weather API is an essential tool for business heres whyAccording to McKinsey the cost of extreme weather events is increasing Accurate actionable weather forecasts can help businesses better prepare for weather events They can also help businesses save money and improve operationsWith the popular Dark Sky API shutting down many businesses are looking for alternativesTo help weve looked at the top weather APIs Heres what you need to consider when choosing a weather API for your project or businessIf you need to pull weather forecast data into your software applications a weather API can make it easy API is the acronym for Application Programming Interface and is a tool that programmers can use in helping them create software from apps to websites to advanced AI applications Weather APIs collect aggregate and process meteorological and other relevant weather data They also offer software tools for accessing that data This includes data like: With this data you can create updates and alerts in web and mobile apps enhance the performance of predictive models or better manage strategic business risksA weather API needs to do what you want in the way that you want it They need to provide the rights weather variables over the right time frame at the right location Weather APIs for business intelligence need to offer real-time actuals and future forecasts Weather APIs for data science and AI need to offer historic actuals and historic forecasts (hindcasts) for rigorously validating prototype applicationsHow easily does the API integrate with the software languages and tools that are most convenient for you What is the documentation like does the API offer simple tutorials and guidesWeather APIs need to be fast and available An API needs to have reliable uptime and fast response timesThe weather API that works best for you depends heavily on your goals preferences project scope and budget Hopefully this article helps in guiding your final decision Did we miss something? Let us know in the commentsAt Bytehub AI we make it simple for anyone data scientists decision makers analysts and AI experts to pull in the right weather variable at the right location to predict risk boost the performance of AI applications and drive better decisions through visual analytics
LmdB8Rd3ecEkjnmhsQub5E,I know its a vague heading… In the broader scheme of big data processing there are various options/choices available with various tradeoffs on how data can be handled The intent of this blog is to address that vagueness by comparing two options my team tried out and why we came to prefer one over the otherData lies in Capital Ones DNA All our decisions are data-driven and being a financial company with a lot of regulatory requirements we are obligated to provide more granular processing data rather than high level detailsBut before we jump into more details around the topic here is the quick gist of Apache Spark for beginners (Skip to the next section if you are already familiar with Apache SparkApache Spark is an open source big data framework for analysing massive data at scale using computing clusters The main advantage of Spark is its in-memory computation which increases the speed of application processing Also Spark supports a variety of workloads like batch machine learning streaming using same infrastructure established once Spark was started in UC Berkeley as an academic project and later contributed to the Apache Foundation in 20 Jun 2013As of writing this blog the latest version of Spark is 2Capital One is a heavy user of Apache Spark for its batch and streaming workloads The application I work on is one of the core credit card transaction processing engines for computing rewards for our card customers Our batch pipeline has a number of Spark jobs and the focus for this blog is on the first version of a job named FilterFilter takes credit card transactions as input applies a bunch of business logic and filters out non-relevant transactions for earning rewardsWe had this version in production for a few months and realized an issue in debugging a data problem Each stage mentioned in the above pipeline is a spark inner join transformation between two datasets The final output persisted qualified transactions with rewards calculated However we werent able to trace to the transaction level why/which particular business logic made it unqualified for earning rewards This was because all intermediate join result sets are computed in-memory and passed to the next stage when spark action is performedImmediately natural questions arose Could we do spark action counts at the end of each stage and cache the dataset? In fact we were already doing this But the issue was it could only give us how many transactions were filtered as part of the stages business logic not the detail of each transactionAssume ten transaction are fed as inputs After applying the business logic (Account Eligibility) only five transactions move to next stage In the next stage the same thing happens and it filters out three more transactions finally making two qualified transactions with calculated rewardsAs previously stated the core issue is with business logic data filtration using spark in-memory inner-join This makes it hard to get into the granular details of each transaction determining why/how business rules are applied for debugging After much discussion the team came up with a different design pattern of enriching the data rather than filtering it in each stepOur new job is Enrichment For this job instead of applying spark inner-join and business logic filtration in one step we do it in two stepsSame example as before Ten transactions are fed as input After applying left outer join with the business logic (Account Eligibility) it gathers the required columns for filtering in later stages So transactions are not filtered rather we are enriching the original input dataset with required data columns from each stage After applying business logic filtration in the last stage we still see only two qualifying transactions However the difference is more enriched data gained along the wayNow at each stage required data is captured using left outer join and enriched into the original dataset It captures state information for more detailed analysis/debugging later The same data columns/flags are used to apply the business logic and in the later stage yields the same result but with more granularity With this approach we were able to find the state of each business logic data column/flag for each transactionWhen we deployed Enrichment to Prod we wanted to make sure that the output was the same To verify we ran both jobs against the same input dataset and deployed comparison jobs to compare the results of Filter and Enrichment Over that period our comparison jobs consistently gave results on both filter and enrichment jobsThis gave us confidence to move forward with our new Enrichment pattern After successful verification of Enrichment jobs performance in production we replaced Filter and have been successfully running Enrichment in production for a year now Filter has been depreciated and sunset from our overall processing workflowOur choice of Apache Spark for our platform modernization efforts definitely yielded good results in terms of performance and accuracy of overall operation Our platform processes millions of transactions a day awarding millions of miles cash points to customers every day Given the high numbers  in terms of processing and accuracy we expected out of our platform  its not really a surprise we leaned towards enriching over filtering our dataPersists same set of data and grows the dataset in columnsHope this comparison helps in your use case decision tradeoffsDISCLOSURE STATEMENT: These opinions are those of the author Unless noted otherwise in this post Capital One is not affiliated with nor is it endorsed by any of the companies mentioned All trademarks and other intellectual property used or displayed are the ownership of their respective owners This article is © 2019 Capital One
imrxmYE5nbo7MG68DU4EKs,Data Engineering groups in large enterprises are typically decentralized Teams develop specialized skill sets in particular areas of data processing and have specific charters For example a team may be responsible for data acquisition Another may be responsible for cleansing transforming normalizing and analyzing data Another team of data scientists may be responsible for consuming this data and applying machine learning models to derive insights from data This results in the creation of complex data processing dependencies in a large enterprise Typically these dependencies are events generated by a given process that another process may depend on Some examples of these events could be: A process may have a dependency on a single such event or a complex combination of multiple such eventsAlso typically in a production environment processes are not run one-off They are run periodically  hourly daily weekly etc They are typically scheduled on a time basis However it is often impossible to determine the time to run a process because of the dependencies mentioned earlier Moreover a delay in running an upstream process would mean that a downstream process scheduled to run at a particular time now does not have its input ready when it is triggered Such dependencies although common can lead to wastage of resources and unpredictability in your data processing systems In such scenarios it would be much better for processes to be scheduled based on specific events  commonly known as event triggers To illustrate this lets consider a few scenarios: Solstice Networking is a large telecommunications company The data infrastructure group at Solstice has recently set out to develop a data lake The data acquisition team is responsible for acquiring data from aggregators and other sources and make various feeds available periodically for other teams across the organization to consume For example the data science team uses the device-data feed to test their models The data analytics team consumes these feeds and runs aggregations normalizations and analytics to generate periodic reports These downstream processes depend on the feeds for meeting their production SLAs As a result they would expect predictability in terms of when the upstream processes complete successfully and when feeds are available for processing In addition each of these feeds has different characteristics  be it arrival rate data size SLAs contents or location These characteristics may also change over time  eg a feed may be produced at a different location than before The downstream processes would need to know these characteristics accurately when they are triggered so that they can use this information to run correctlyPixeltube Corporation has a large customer care organization with a tremendous volume of support requests To make the customer support process more streamlined and to better help their customers they have developed a complex data processing system with the help of the data infrastructure team The main aim of this system is to help solve customer issues in the least possible time with the least possible use of resources All customer support calls have been instrumented to collect data about the issue at hand the suggested remedies and the resolution The data collection team collects this data from all the servers where it is sent and makes it available at a central location on an Apache Hadoop cluster periodically The data analytics team cleanses transforms and aggregates this data and makes it available in a standard format The same cluster also has other data such as device metadata and user profiles generated by other processes and teams The data science team generates maintains and tests their models across these datasets and derives insights that can help in predicting solutions to future customer problems A reporting team uses all these datasets to generate timely reports that can be used by the customer support team As you can see this is a complex system with a lot of interdependencies across multiple teams For this process to function correctly it is necessary for the system supporting it to allow users to express these dependencies clearly and then manage them at runtimeThe above two scenarios are representative of data engineering/infrastructure organizations of large enterprises today The common theme that cuts across all such organizations is complex interdependencies between processes If these dependencies are not managed correctly it can result in operational hazards as well as financial losses Some of the typical characteristics of such interdependent systems are: Event-based triggers in the Cask Data Application Platform (CDAP) address these requirements They allow users to trigger workflows based on specific system generated events such as: Here is a video that illustrates event based triggers in action in CDAPTo try out Event Triggers and other new CDAP features download CDAP Local Sandbox or spinning up an instance of the CDAP Cloud Sandbox on AWS or Azure CDAP is also available to install in distributed environments
Rzu2mWBcsz5uKYprQU6JQ5,So sometime in 2016 Chartbeat tried to go all BACK TO THE FUTURE on our customer base to power an interactive web-based historical dashboard To that point we were all about surfacing real-time real-time and more real-time analytics This shift would give our client publishers a peek at their audience data for the last day week month or several months We figured publishers would be interested in seeing their top articles over a period of time as well as an hourly breakdown of traffic pivoted on a few useful properties since thats the gluten-free bread and butter of our very popular real-time dashboardThe problem with doing this is that our raw dump of every visit by every audience member from each of their devices for every one of our clients would be impossible to query responsively In fact in raw form Chartbeat ingests records of 30 40K visits per second Thats over 50B page views a month! If I had a nickel for every page view I wouldnt be writing blog posts about slow database queriesThe compromise was to roll-up our visitor data into hourly buckets for each publisher article while separately storing roll-ups that attribute articles to particular authors or publication sectionsBut we couldnt break out the champagne just yet Even the roll-ups arent cheapDuring peak traffic periods for our nearly 5000 Chartbeat Publishing clients we from our Amazon Redshift visitor store import hourly roll-ups to our Postgres cluster of about 12M publisher article engagement records 16M section tags along with 800K author tags for publisher articles and 130K overall publisher site engagement recordsSo for our 2 biggest Postgres tables we are ingesting over 28M rows per day In general we are importing over 26B rows a month into the entire clusterWriting and running naive join queries of our sections or authors with our publisher article data in search of top publisher articles would feel like waiting for a crosstown bus in Union Square during rush hourOur first set of queries were pretty straight forward: sort all the things for a given publisher get the top N articles and then try to join them to find the authors for those articles We used a simple index that uniquely identified each entry for each publisher article for each hour We tried a standard Amazon RDS Postgres instance of the r32x large varietyBases-loaded upper-deck homerun game over and we win Right? Not exactly Not even closeOur month-long query response times were averaging 40 seconds which was a long drive from the 5 second response times we wanted for our publishersStrike 2: A bigger single-instance Postgres Cluster using the same indicesOk Were smart Right? We needed a 5x improvement in our response times So trying a database cluster with 4x the amount of memory had to get us a lot closer right? This wasnt just a shot in the dark We knew all about work_mem the amount of memory Postgres allocates to operations involving sorting hashing or merging And the crux of our problem was trying to sum a host of article engagement records and then sort them before joining them on our author data table So here we were willing to spend 3x the monthly hosting cost for 4x the amount of memory on a r38x large This was the walk-off win we were ready to celebrateAgain just a bit outside Our response times were still averaging 30 secondsCovering Indices: The only way to make the playoffsAfter staring long enough at the romantic poetry of the Postgres EXPLAIN ANALYZE output for one of our top articles queries it was pretty clear that fast query kryptonite is BITMAP HEAP Scan Yes the query planner is fixin to search for select rows and not sequentially scan the actual table but no a months worth of hourly article records for the average publisher site isnt a quick fetch We had to limit the number of table rows we read from disk or there would be no playoffs for Team ChartbeatThe trick as always was to break down the problem into smaller pieces Here we did it using common table expressions (CTEs) On the streets they call them WITH clauses Because of a Postgres quirk these mini-queries gave us explicit control over the query plan A CTE in Postgres is whats known as an optimization fence Most SQL flavors will push query optimization through CTEs but in Postgres they chill hard when they see CTEs Put plainly CTEs give us a way to tell the query planner I got you instead of letting the know-it-all query planner engine decide how to run the queryBy using covering indices in our CTEs or indices that contain all the columns our mini-queries needed to sum and rank article engagement metrics we were able to avoid the dreaded disk-access DETENTION There were still metadata things we needed to read from good ol disk like article titles and url But getting metadata for the top N articles from disk is a bit faster than trying to fetch the same data for each of the hundreds or thousands of a given publishers articles that would be thrown away anyway by LIMIT N In the end we fixed our queries by borrowing the same playbook for people who quickly get in and out of Trader Joes: figure out what you need for dinner before you ask a cheerful store associate to help you search all the things in all the aislesBut that was only part of the secret sauce The other part was a <drum roll> </drum roll> distributed Postgres solutionWe successfully limited the number of rows our queries had to fetch from disk which made them a lot faster but they still werent fast enough for a month and we wanted longer range queries to not be impossible as wellWe needed scale We needed shardsWe thought about rolling our own sharded Postgres solution but that seemed like way too much of a last-mile effort For one wed have to hand-write code to manage shard and tenant placement to say nothing of the good fun of implementing error-free atomic writes across a distributed cluster Oh and lastly there would be the serious migraine every time we needed to re-size our clusterSo we went with Citus Data a cloud database company whose mission as they put it is to make developers never worry about scaling Citus is an open-source extension to PostgreSQL that distributes the database and queries across multiple machines With their scaled-out Postgres solution Citus was neatly positioned to address our multi-tenant (or in our case multi-publisher) database access patternsNot only was the setup pretty easy CITUS DEVS HELPED US with our queries Let me say that again CITUS DEVS HELPED US with our queries and were a big reason why we fixed our index gameOur Python-based importers had to be changed to import to a distributed cluster and not a single-instance one Basically three main changes were needed: schema changes ingestion changes and access query changesIn sum choosing Citus was the smart choice Sure its never fun to need to tweak parts of a pipeline just before shipping a product but the lifting here was reasonable: a few days of work where a lot less than the weeks (or months) wed spend building re-building and unbreaking our own app-level sharded solution Plus Citus Datas solid technical support made our trips to the data dentist a lot less painfulThe other souvenir from the experience has been the Citus teams willingness to take 100% of our feedback from our migration and maintenance pain points and address them in future versions of their extension That didnt mean much in the very short-term of ship-or-die but has meant a lot in the time since launchOh and by the way our queries got way faster A healthy citus cluster has returned month-long queries in under 5 seconds pretty routinely which has Chartbeats historical dashboard pretty #litCovering indices are everything especially when you need to limit disk accessOh and the next time you think you feel like rolling your own sharded PostgreSQL at the last-minute get your therapist on the phone and then give Citus a serious look
LqyWPT5EuuhspyXT2yeSFR,Data Data Data  You hear that every time and everywhere Everyone realized how valuable are the loads of data any company owns This certainly applies to our company At Cheerz we run three applications that produce customizable photo products in factories all over EuropeAt Cheerz we want to create the best most magical photo printing experience for our customers Weve been building up momentum over the past Like many others in the early days our main focus was on facing increasing demand by scaling Tech and ProdOps accordingly That part is now working well and marketing has also scaled-up Today we have reached the critical size from which profitability and durability of the Cheerz brand can be targeted Thats what we aim for and data is going to help usAs the company grows the amount of data surrounding ramps up too On the first day thats your apps lying alone on top of your DB The next a shiny tracking plan adds up Then your custom Production Management System (PMS) is launched plus the brand new marketing tool Finally comes the time when your founders realize that there may be something that could help you to grow healthier and stronger It is not named yet but you are already thinking about metrics for information efficiency performanceTo start look at the scale of data usefulness in a company like Cheerz At entry-level 0 you do nothing with your data At level 100 you have Data/AI in your product (means incredible UX) in your operations (ex super-optimized processes) including marketing and in your business analytics (you know your trends you anticipate etc)From level 0 the first metrics you compute have a massive impact on your business they put numbers on the strategy quite often its an eye-opener Then immediately comes the need for refinement improving grain quality frequency In our context  we dont have a data-based product  we can reach the 60 65 mark with an easy to maintain not too pricey data stack run by a single person and a simple motto: deliver a few accurate(-enough) KPIs for all teams every dayThis is what we intend to do Below is how we make data flows from many data sources to 60 data-hungry users of our BI toolThe sources we need to collect data from are (by order of appearance) : If you want to keep it simple you need the right tool(s) to ensure smooth data collection and storage It should be accessible to not-so-technical profiles to stay focused on business understanding To do that we chose Panoply It makes scheduled data collection and database management easy and reliable In fact you dont do the management except from checking free storage space from time to time Panoply runs (and optimize) your Redshift data warehouse in your place In the end: - We dont have live data but hourly is good enough- Its ELT instead of ETL so we store raw data which is better than no data- Panoply does not connect easily to everything but no one does- Managed Redshift is not as shiny (and fast) as self-tuned-hyper-fast other tools but its the right compromise at this time of our storyPanoply cares for our data and we concentrate on improving our businessThe last tool of our two-segments-only data pipeline is LookerLooker alone copes with our entire raw data We use basic SQL and proprietary LookML to render all the business insights we need Looker materializes Core BI tables with Derived Tables and creates most views on requestThis is where we add one key thing in our motto: empower your end-users At Cheerz each person having a named account can build his own Look to track business KPIsIn that context your data guy or data girl main subject is to prepare ready-made models and to focus on data quality and delivery He can give a hand for complex analyses but business drives the roadmap not technical issuesEven if there is a long way to go to the 100-mark every day we find new clues They allow us to understand the business get our processes better grow the company while data is part of itPS Thanks Ari Bajo for your wise comments
5uTeM9CY7etTQ6X3SbQKcr,Got a question about scaling data operations? Ask John Sloboda In the next installment of our Meet the Team series John talks about his journey from Canada to Clear Street his plans for scaling our data platform and what he sees in Clear Streets futureIm originally from Waterloo Canada and grew up in the mid-90s during the tech boomComputers were on the up-and-up and I saw Bill Gates as a huge role model He was this nerdy guy who was extremely successful blazing the path for others in the technology industry I think thats really where my interest in engineering emergedI ended up studying software engineering at the University of Waterloo and joined Google after I graduatedI was part of the ads system team in Mountain View mostly working on data and infrastructure projects I was with the company for 5 years eventually moving to their office in Zurich SwitzerlandIt was a great way to start my career I got to work on projects that exposed me to data pipelines and machine learning I also worked on some of the most widely used products in the worldI learned a lot about scale and the tools businesses use to manage itI decided to sit down and make a list of exactly what I wanted in my next company and role My target was a data engineering or backend infrastructure role at a technology-focused start-up that would give me more exposure to the financial industryI wanted to be able to continue to grow my own skillset and make a substantial contribution to the companys overall growthClear Street checked a lot of the boxes on my list career-wise and culturallyIm on the data operations engineering team at Clear Street We build data pipelines that connect the core of our business  financial operations  to our engineering systemsRight now Im rewriting the architecture of the entire pipeline system which will be an ongoing project for a startup thats scaling as fast as Clear StreetWere already reaching a point where our data sets no longer fit in memory As we continue to build our data will scale by orders of magnitude so we have to think about how we will handle that in terms of data flow scalability and moreWhen we created the first version of the pipeline we were just trying to create a functional system  for example a system that generates reports correctlyNow that we have a baseline we can focus on improving the infrastructure  how we manage the data flows  using scheduling orchestration tools and distributed data processing systemsIts been a great project to work on The team works well together and there are some very interesting challenges ahead that will keep work interesting for years to comeIm most interested in Clear Streets potential for scalabilityWe have the opportunity to scale every aspect of our company: from the amount of transactions and trades that we process to the customers ability to run on our system and ultimately to the amount of revenue were able to generate as a companyThe more we can automate the more we can scale The data platform will allow us to grow at an exponential rateThats not how we operate at Clear Street We embrace discomfort to pursue innovationThe finance industry needs a change I believe that Clear Street will be the company that makes it happen
FvVY6w55WsQEN4WmqW5XdF,Im a Junior Developer working on the Data Engineering team at Clir The Data Engineering team builds infrastructure to help sift through data and organize it into a form that allows our domain experts to perform analyticsThe world of tech is fast-paced and constantly evolving It can be hard keeping up with all the new frameworks and language-specific changes while balancing all the obstacles life throws your wayFor this reason at Clir we encourage our employees to pursue their professional interests outside the scope of work through a professional development program The program can take the form of workshops online courses night school or even just a good old fashioned textbookThe benefit of the program is that work hours are designated to be used towards any of these modes of learning One of our core values at Clir is innovation which we embody by always learning new things and constantly challenging the way were currently doing things Professional development is just one way that Clir fosters innovationAn added benefit of professional development is the spread of knowledge throughout the teams At a tech company developers are constantly learning from each other Whether it be through code reviews paired programming or design meetings theres a plethora of knowledge sharing between members As a result any additional insight gained from a team member provides value for the whole groupAs part of Clirs professional development program I attended a week-long deep learning workshop The workshop provided me with many motivation experiences from speaking to researchers using time based convolutional neural networks to detect tumors in the human body to attending lectures where they went to great lengths about the vast capabilities that neural networks have Being around many brilliant minds is a very humbling but yet motivating experienceApart from figuring out that deep learning is indeed a complicated and broad subject to grasp in a week of lectures the workshop I attended left me with a few takeaways from my professional development experience
ULSkeAZ2ZbRK2EPvCH23hH,About 4 years ago a couple of users started to complain a lot about the data quality in our customer data warehouse Those users were not that numerous but they were certainly the most vocal ones If you want my opinion data quality was not that bad but we were dealing with perception issues misalignment of expectations and internal politics As we were transitioning to big data technologies that was enough to convince our management data quality should be taken care of In particular to ensure the success of our future Enterprise Data PlatformMy boss and I agreed I would take the responsibility of this topic At that time many colleagues would look at me with big eyes wondering why would I do that Lets be honest nobody wants to work on data quality because it just sounds boring but we had a deal
9zgHcBbobitZnYTVKWsx2h,"Today we will try to inspect events organised by Tech groups on Meetup Our main goal is to take a look on trending techs / IT fields and get the overall concerns about technologies development and communities expansionTo reach our objective we will harness the amazing API given by Meetup to explore its entire interesting data which gives many opportunities such as getting information about categories groups events attendees reviews etc""There's even an SDK in python and a bunch of other languages that you can use easilyWe will use the api calls to retrieve data Later we will load it into our Data Ware House cluster on the cloud then we will process it by some ETL operations to get a final reportFor this purpose we will adopt some of Google Cloud Platform solutions for workflow scheduling and data analysis which are respectively Cloud Composer and BigQueryAs intended we will use the Meetup API to get the raw data for our experiment note that you can retrieve this data using the api calls or the SDK The code below describes how we get the daily events related to Tech field in FranceHow-To: We look for all the groups labeled as Tech groups in France then we try to inspect every elapsed event (yesterday events in this example) then we store those informationHowever before saving data it would be great if we could retrieve a set of keywords related to each event For that the Google Language API could produce a very suitable solutionGiven the event description we make a call to the Google language api with this piece of information which provides us with a whole list of keywords attached to a salience value Thus we attach those information to our event data structure for further processingFinally as simple as it could be we store this data in a local file labeled by type and dateNow we will process on loading raw data For this case we will opt for BigQuery as our data warehouseFirst you should already have created a dataset named meetupThen knowing that all the exported data is formatted as JSON entities use the following command to create a table without worrying about the schemaSo now you can see what an event entry looks like in your table preview""After getting some data for our experiment it's time to make this process a periodically scheduled workflowWe will create a Cloud Composer environment which follows this architectureThus the instantiation of this one will avoid us any dev-ops operations to build a workflow orchestration serviceFor this case we will use a beta version of Composer which will gives us the opportunity to specify the most recent composer version (1100) and the version of our python SDK 37Type the following command in the GCP CLI or in your terminal to create your environment: To be sure that everything is ok under composer product in the GCP console you should get this kind of rendering: You can perceive that a new Google Cloud Storage was created for this project named as ${region}-meetup-events--* Later in this bucket we will push some code to schedule our processAdd new packages in Cloud Composer EnvironmentIn order to implement our use case we want to use external libraries Cloud composer allows us to add new ones To do so we create a requirementstxt file with the needed packagesA service account is a special Google account that belongs to your application or a virtual machine (VM) instead of to an individual end user Your application uses the service account to call the Google API of a service so that the users arent directly involvedUse this command to attach the created service account for Google Language Api to the composer workers: Straightaway our composer environment is ready to useIf you are not familiar with Airflow concepts please refer hereDirected Acyclic Graph (DAG) : In Airflow a DAG   or a Directed Acyclic Graph   is a collection of all the tasks you want to run organized in a way that reflects their relationships and dependencies""Right now lets describe a daily running DAG named 'meetup_daily_events'Get data from APINext to execute the script which consumes the Meetup data from the api we use a simple PythonOperator""Tip: We connected the task to a single threaded pool so we don't harness the Meetup api with calls from concurrent tasksLoad Data into BigQueryThe second DAG task will check if the previous one did export an event file to GCS In that case it will be pushed into a BigQuery table otherwise nothing will be done""For this pattern we use a BranchPythonOperator it's an operator that does nothing expect branching to the suitable tasks depending on some conditionsWhen the required event file is pushed to GCS we process to load it into BigQuery using this code fragment: Please note that export files are loaded to data/ directory This one refers to gs://bucket-name/data/ object repository""Having a bunch of data after days of labor let's capitalise over it and have something usefulTo do so we execute a query over the daily data and export the results in a keyword table The table is partitioned by date so data retrieving is more efficient""Trying to make things more visual here's a preview of the transformed data which shows the redundancy of some keywords in daily events in the French tech fieldSure some of them dont make sense with the actual tech topic which is classic due to the lightness of the processing and the data analysis A Data Analyst could do a better jobThroughout this article we tried to reproduce some of the standard data engineering exercises across the use of the Google Cloud Composer product and the entire Google Cloud Platform EnvironmentThe Meetup event case seemed to be interesting and very affordable so I invite you to explore the other resources like events reviews and attendance depending on topics or the days of the weekIn a future article we expect to tackle some advanced concepts of the Cloud Composer product and give several Airflow tipsHere you can find the project code source"
AamTivLpvBmdsSPSMaZyZL,A few months ago we dramatically improved throughput and error handling of our image processing pipeline (to process Real Estate listing images) But first a flashback to a little over a year ago • 15 PM: Its happening again Reports of missing listing image updates are flooding in I see Finagle networking exceptions and signs of deadlock in one of our backend Thrift services logs This is the latest in a series of image processor issues The late night debugging compounded with my failure to solve the problem has led to a dangerous rise in my stress levels over the past few monthsI reach out to our DevOps team to help debug the issue We go back and forth in our incident-specific Slack channel #war-room Someone tracks down the root failure Requests to our image processing service  the service responsible for downloading resizing and uploading listing images to S3  are timing out … again Our load balancer client and server timeouts dont match The number of image processing issues wed fixed in the past few months had fatigued us but left us open to more radical solutionsOur listing pipeline pulls and processes listings with their images from APIs provided by third-party data providers called Multiple Listing Services (MLSs) It includes four major steps included in the below diagram of our listings pipeline: The initial version of the pipeline used a synchronous API provided by our image processing service for step 3 This image processing API accepted requests containing a listing object with image URLs For each request the service downloaded images resized them and uploaded them to S3As we scaled with more MLSs this synchronous image processing system slowed down the rest of the pipeline and broke more oftenWe fortunately gave up on patching our synchronous system to accommodate increased scale Trying to decrease the likelihood of transient system failures and overload was an inferior approach to gaining greater control over the load and impact of failures on this systemThe past incidents related to the image processor highlighted three key issues with our synchronous image processing flow The listings pipeline: Following in the footsteps of many data engineers before us we decided to solve our problem by making our image processing system asynchronous We chose message queues as our async communication mechanism based on the four key benefits they would bring to our listing pipeline and its image processing flow: We decided to host our message queues on our newly created internal Publish-Subscribe (PubSub) library backed by Amazons SNS and SQS In this new system request and response queue items replace the synchronous image processing requests and responses decoupling image processing from the listings pipelineThe request queue items include unprocessed images and just enough information  a unique foreign key an MLS source tag and a region name  to keep track of the listings to which each set of images belongs The response queue items include processed images and the same listing tracking informationOur first new component the PubSub Image Request Processor processes images from the request queue and pushes the processed image results onto the output queue Our second new component the PubSub Image Response Processor consumes from the output queue and saved processed results to region-specific image MongoDB collections
movtWssfyCeTdYsW7iMgXu,Debezium is a log-based Change-Data-Capture (CDC) tool: It detects changes within databases and propagates them to Kafka In the first half of this article you will learn what Debezium is good for and how it works The second half consists of an experience report in which I describe the joys and pains of running Debezium in productionThis article intended for architects or (data) engineers who want to learn about a great ETL tool to add to their tool belt as well as teams who are currently on the fence about introducing Debezium to their setupTheres a number of reasons why you would want to ingest your databases contents into a publish-subscribe systemThe reasons are manifold When thinking about how to go about this most people would naturally come up with two approaches: Dual Writes and SQL pollingAs you can see both approaches are not ideal Especially when consistency is a concern you probably wouldnt want to pick either But can we do better? Of course we can otherwise you wouldnt be reading this article 😉 Ill introduce you to a third approach: DebeziumImagine this: every time you insert update or delete a record in your database an event holding information about the change is immediately emitted This process happens automatically: not a single line of code needs to be written on your part almost as if it was a feature of the database itself Even better: it is ensured that every single change is captured; no updates are missed In other words it is guaranteed that database and Kafka are eventually consistent What is this sorcery? you might ask The answer is a process called log-based CDC Have a look at the following imageLets consider this ordinary CRUD application (User Service) that stores user data in an SQL database Our goal is to propagate the user data via a Kafka topic (Users Topic) to other components Furthermore a Debezium instance is running Debezium can be deployed either as a standalone self-contained process (as we did in this example) or as a Java library that can be included into an existing applicationNow lets walk through what happens when the application inserts a user into the database: The change event Carol was added can now be picked up by whoever subscribes to the users topicIt is important to note that the application is entirely oblivious to the fact that there is a Debezium instance or even a Kafka cluster running From the perspective of the user service it inserts a row into a database and thats it No line of Kafka-specific code was addedIn one of our projects were running Debezium in production and while the experience has not been the smoothest one (see next section) there are many good things to be said about it Heres what we liked in particularIn practice Debezium works nicely and it does what its supposed to do reliably However there are a few things that we wish we had known or considered before we started using itWhen youre starting off with Debezium it might be tempting to leave the default settings unchanged and simply expose your existing tables as-is to Kafka However this is a bad idea for the simple reason that youre effectively exposing your database schema to the outside world Why is this bad? Once you start pushing records to your downstream consumers they will expect that the message format to not change anytime soon The problem is that now your message format is directly tied to your database schema and we all know that database schemas need to be evolved more often than you think Now every time you want to change the database schema you need to deal with a (possibly breaking) change of your API Your nicely decoupled architecture isnt so decoupled anymoreThe issue becomes worse when youre using Avro serialization Debezium infers the Avro schema from the database schema Since both schemas are now effectively the same thing youre now imposing Avros schema compatibility rules onto your database schema In pracice this means that you have to be really cautious when adding or deleting columns otherwise you might break compatibility with the consumersWith this in mind it becomes clear that you need to establish some sort of alternative read model to feed into your clientsIf you thought you could simply create an SQL view and let Debezium publish that you are mistaken Debezium simply doesnt support any kind of views be it materialized or not The reason is that views dont manifest themselves in the WAL and hence there is no way changes to views can be detected What you should do instead is to make use of the outbox pattern The idea of the outbox pattern is that you maintain a separate table specifically for the purpose of holding outgoing events Only this table should be monitored by Debezium The trick is that this table needs to be updated transactionally eg via database triggers In the user example from above you would need to create an create/update/delete trigger on the users table and let that trigger write update events the outbox table Thankfully Debezium provides means to streamline this but nonetheless this makes matters quite a bit more complexWhen youre used to building applications for the cloud you might have certain expectations of how a service ought be provisioned For example you might expect applications to be configurable via environment variables I was mildly shocked to find out that the only officially supported way to provision Debezium is to POST a config file to a REST API of the running Debezium instance The config is then persisted in a log compacted Kafka topic which needs to be cleaned up manually after Debezium is torn down (BTW this is not the fault of Debezium itself but rather the Kafka Connect framework that Debezium is built upon) This behavior contradicts the idea of immutable infrastructures and makes automated deployments rather awkward If youre like me and prefer the environment variable way of configuring applications feel free to use this custom Debezium Docker image that I created to alleviate our deployment painsWhen you have a relational database its not a far stretch to assume that your schema has relations The thing is though that Debezium works on a table-level: it only considers tables in isolation and as a result of that relationships are not consideredIf you absolutely cannot do without joins and you dont want to deal with outbox tables you have to swallow the bitter pill and make the downstream clients join the Kafka topics themselves Depending on your schema this might be viable but for us it was not Resolving a 1:10000 relationship via Kafka Streams turned out to perform not too well We ended up having to de-normalize the schema insteadAlthough Debezium supports RDS out of the box (some config changes are required but they are well documented) it doesnt run as smoothly on RDS PostgreSQL as we had hoped After rolling out Debezium on our staging environment we soon found that our database was steadily filling up the disk even though there was barely any data in the database and no writes happening at all Even more curious was the fact that as soon as we started inserting sample data some of the storage would be freed again After a bit of digging we found a blog post explaining the cause of the issue (which btw lies in AWS not in Debezium) If youre interested in the details I encourage you to check out the blog post but to save you some time Ill skip right to the solution: the workaround we ended up implementing involved and I kid you not writing junk data into the database in regular intervals via cron job LambdaThe issue has since been addressed by the Debezium team As of version 11 you can configure Debezium to run heartbeat queries which rids the need to set up a separate Lambda function Still very awkwardAs you can see our experiences with Debezium were not without issues When we started off with Debezium we had a somewhat naive idea in our minds of how much work it would take off of us While Debezium takes care of the most critical aspects of ETL we absolutely underestimated the amount of elbow grease we would have to put into making it work in our setup We didnt think about schema evolution and (de)serialization we didnt think about creating boundaries around our database schema and decoupling our system Then we had issues running it on AWS Next thing is monitoring: when youre running a production system you might want to monitor every component of it thoroughly Debezium runs in the system just like any other microservice and needs to be monitored accordinglyAll things considered theres a lot more to it than spinning it up and let it work its magic as we had initially thought Theres a commitment involved when running Debezium and I wish this would have been more clear to usNow the question would I personally use it again? On the one hand there were times where we suffered great pain: having to announce to the team that our database had run out of space again because the heartbeat Lambda failed to run; failing production deployments because the database schema had changed triggering an implicit Avro schema evolution and breaking compatibility with a downstream consumer; reworking the database schema over and over to cater to Debeziums needs; etc Debezium broke our system again has almost become a running gag in our dailiesOn the other hand Debezium kept its promise of keeping our topics consistent with the database All faults and failures that we inadvertently caused were ultimately recoverable Who knows how much time we would have spent chasing consistency bugs if we had chosen the dual writes or SQL polling approach? I dont even want to think about it So all in all yes I would prefer Debezium over the alternatives any day
SgtjTLZuTDGZd4Aw5mVd4v,Google Cloud Platform provides some tools that let us manage our data We mainly use Cloud Composer  GCPs managed Airflow service  to schedule some of our data pipelines with all of our data ending up in GCPs data warehouse BigQuery for analysisWe previously had development DAGs next to our production DAGs in our single Composer instance This made cloud environment separation a bit undesirable as we were checking filenames for _dev when setting environment variables At the beginning of this year we made the decision to streamline our DAG development by having separate development and live Composer instancesCurrently in April 2019 you cant turn on and off a Composer environment; you can only create or destroy To minimise costs and overhead we went down the route of having a development Composer instance spinning up and down every weekday between 9:45am and 6:15pm being scheduled by our production Composer instanceHeres what our spin up DAG looks like: As installing the PyPi packages takes a while we spin up at 9:45 which causes the environment to be ready shortly after 10The bash file composer_setupsh referenced looks like: This imports the PyPi packages and copies over the environment variables from our GCS bucket We also modify the airflow config with core-dags_are_paused_at_creation=True so that we need to explicitly turn DAGs on and stop them from backfilling automaticallyWe also create a new connection bigquery_gdrive because we want our Composer to be able to interact with CSVs hosted on Google DriveThe composer_settingssh referenced: When we want to keep the development environment on overnight for scheduling purposes we can simply switch the tear down/up DAGs off When we switch the DAGs back on we wouldnt want them to run multiple times for the days missed; using catchup=False in both DAGs prevents thisNote: catchup=False started working again in Composer version composer-182-airflow-110Our teardown DAG: composer_teardownsh: This exports our variables to the same GCS bucket as above We went with the decision to export variables from the dev environment when spun down but some may prefer for the dev variables to be ephemeralBear in mind that due to the Composer instance being destroyed you wont be able to recover the logs via the Airflow UIBefore having this development environment our Cloud Composer costs were ~£250/month By doing this method we only see an increase of ~£60 to our monthly Cloud Composer costs That makes sense that by having a development environment up for 25% of the week it only increases the monthly cost by 25%By spinning the environment up and down thats ~£190/month saved that the business has to not invest in Bitcoin! Bargain
M76WDTQys3HYUTuDwSDrRh,Datacasts 19th episode is my chat with Tristan Bergh a systems thinker and data scientist living in Cape Town South Africa Give it a listen to learn about his aeronautical engineering education his enterprise software engineering background his skills as a full-stack data scientist and the tech community in South AfricaTristan Bergh is a systems thinker and data scientist living in Cape Town South Africa He studied aeronautical engineering before working as an enterprise software engineer He worked as an architect on AFIS implementations in Southern Africa moving into middleware for large fin-tech systems He restarted his system modeling and machine learning consulting services through 2014 delivering predictive analytics models in healthcare and other domainsTristans skills are that of a full-stack data scientist using primarily PySpark to deliver predictive analytics models in a rules-engine environment He backs up his machine learning at scale using R and Python to find optimal models and feature sets for use in modeling His current area of interest is in setting up customer next-best-action processes at scaleIf you enjoyed this piece Id love it if you hit the clap button 👏 so others might stumble upon it You can find my own code on GitHub and more of my writing and projects at https://jamesklecom/ You can also follow me on Twitter email me directly or find me on LinkedIn
3wZ5yffzFAq3x5vfkXiCLL,Im creating a new podcast about Data Science called Datacast! The 9th episode is my conversation with Mark Sellors the Head of Data Engineering at Mango Solutions Give it a listen to learn about his decades experience working with analytical computing environments his thoughts on Dev Ops and Data Science his recommendation for resources on data engineering plus many moreMark Sellors is the Head of Data Engineering at Mango Solutions a UK based Data Science consultancy He has more than a decades experience working with analytical computing environments DevOps and Unix/Linux He uses his experience to help Mangos customers transform their analytic capabilities to ensure they can make the most of their dataIf you enjoyed this piece Id love it if you hit the clap button 👏 so others might stumble upon it You can find my own code on GitHub and more of my writing and projects at https://jamesklecom/ You can also follow me on Twitter email me directly or find me on LinkedIn
BA5KaJzvH5xGfM5KoBtwYq,According to an IBM study bad data cost the United States 31 trillion dollars in 2016 To picture how big that money is its twice Canadas GDP in the very same yearWhy is bad data so costly? It is hard to detect difficult to fix and every so often impossible to trace It stays idle and only reveals itself in case of incidents Data engineers data analysts managers decision-makers use the same data at some point along the pipeline When transferring data from one place to another it carries a hidden cost of correcting and fitting data to a specific useThomas C Redman defines in his article on Havard Business Review a model of The Hidden Data Factory When Department B requires some data from Department A they spend extra time to correct and modify the data to their proper use This extra step can potentially add other types of errors that are invisible to Department B but noticeable to othersData-driven decision-making processes are extremely sensitive to bad data But once in a while we stack layers of erroneous information to our data without even realizing how much it can cost us in the long runBad data not only costs us money but it also consumes human resources It takes place in our data centers and blends in with good data to falsify our data processing systems In our business bad data is the root of all evilOn an April day the number of audiences on all campaigns were blown up on the Reporting tools The incident was acknowledged as a data-related issue This time the impact was not about money but the user experience was gravely disturbed The unforeseen number of users were visible in one of Criteos advertising management platformsThe flawed data were identified in two hours on the same day This part of data was chained up to many reporting pipelines thus the whole data flow will be affected The routine started to take place: emergency ticket created team leaders got pinged and responsibilities were asked to be takenNothing suspicious was spotted out in the systems that produced the initial data We understood that we must understand the root cause at the end but to react a quick fix was proposed The team that manages the data agreed to refill the inaccurate data with the adjacent hours We then had to backfill the rest of the pipelines in strict order Backfilling implies the action of recreating the history data to correct an abrupt set of dataInappropriate data can equal to money loss or client dissatisfaction But as long as we understand the root cause and swiftly put on a concrete action plan well have everything under control The above story is one in various examples of a typical data-related incident we have to deal with on a daily basisAt Criteo we know how critical data can be to our business Our predictive models ingest hundreds of terabytes training data It learns how to deliver an outstanding outcome to our clients advertising campaigns Our data analysts execute ad-hoc queries or make use of reporting pipelines to answer clients needsUnderstanding how vital data infrastructure is we dont hesitate to roll up our sleeves and get our hands dirty to keep a decent architecture We maintain our own data centers build our in-house data pipelines and try to keep a viable consistency across the systemWe have dedicated teams to regulate data flowing through the whole system The data observability and transparency enable us to trace back bad data and identify contagious databases to isolate We create adequate data models suitable for each use case data integrity monitoring and inconsistent quality tracking toolsWe dont neglect the value of data transparency and we do not internally hide our data behind security walls We hope to provide a documentation layer to the existed data ecosystem to ease the examination of a distinct data setWe have some textbook procedures when it comes to handling data-related incidents A quick response code that requires stakeholders to take actions under a limited period enables us to promptly determine the data origin The sole bottleneck is to know the right people to ask forIn the near future we would like to focus on improving the data quality monitor to move it closer to the sources and also defining better the data ownershipWe are the SRE Core Data & Governance group composed of four small teams with a handful of data engineers Our main products range from reporting data to data documentation Data quality is on the list of our top concerns to maintain a healthy data systemWe started the initiative by defining a clear vision for data quality across Criteo R&D: Data Quality ensures that data is fit for its intended uses in operations decision making and planningAt the time of this writing we have almost 1000 consistency checks and more than 1500 stability checks actively running on our Core Data Models The Core Data Models (or CDM in our colloquial language) consists of approximately 200 tables used for reporting purposesCDM nowadays provides Criteo data users with a stable robust and reliable data source for reporting Our tables go directly to client reports performance dashboards or campaign management centers One of our core challenges is to maintain high data integrity It is critical in our job to keep those data clean and accurateBesides the data quality checks we also have monitoring tools such as Tableau dashboards to visualize those checks result for daily surveillance We also expose our works along with our data documentation tool to help end-users easily navigate through datasetsMaintaining reliable data pipelines is not all about monitoring As data has become an indispensable asset big tech companies now treat data as a part of the development cycle Think software but for data Deployment debugging observability There are more and more promising initiatives coming from tech players across the globe Each one has its method to combat bad data but they share a common goal of improving user experienceAs someone who works closely with data one of my duties is to question the data we consume and ensure the quality of the data we produce Ive been intrigued by how we can better supervise our data quality Ive come up with a framework named triple As akWe have little to no tolerance for poor data quality We aim at constructing fundamental principles to create a security layer between our data and the client-facing presentations Today we seek to enrich our force with enthusiastic data engineers to join us in the engagement against bad dataThe risk of having bad data will always be present but the most suitable solution is to be prepared We understand no one can avoid bad data because good and bad data are just two sides of the same coin There is no bad data by its nature only an inappropriate contextI would like to thank Lucie Bailly and Guillaume Coffin whose time and effort have made valuable contributions to this articleDont forget to head over to the latest contributions from the Data Team Criteo Labs to the Medium community
bw7nQU7wJuRoaSZyiLGPPy,As 2019 is coming to an end we reflect on our progress to move away from legacy systems and build a modern Data Platform that is flexible scalable and reliable The goal is to reduce struggles for data scientists and analysts elevating their freedom in terms of possibilities to leverage all kinds of data for their daily tasks Hence we realized that it is a good opportunity to start a blog and write about our experiencesFollowing this article we will publish regularly about data stories at BabbelTL;DR: we are happy to share the Terraform Module we built to easily migrate tasks from AWS Lambda to AWS Fargate in a convenient wayThe central figure of our legacy system is a MySQL data warehouse (AWS Aurora) that contains most of the processing logic expressed in SQL in the form of stored procedures As almost the whole company depends on the data we deliver we have to find a way to migrate to a modern Data Platform built around a proper data lake in our case built on Snowflake without affecting everyday business operations The first step was extracting logic locked in those procedures in order to monitor and maintain it more easilySince then the core component of our infrastructure has been AWS Lambda for multiple reasons AWS Lambda enabled us to migrate legacy pipelines to more elastic and simple components as well as to quickly build new parts of our platform Introducing a clear DevOps strategy alongside was also a fundamental part of the processToday we orchestrate hundreds of lambda functions performing all kinds of ETL jobs from fetching data coming from external services to data cleansing and processing Some of these lambdas react to CloudWatch Events and others are part of more complex workflows built on top of AWS Step FunctionsWe package all our lambdas with AWS Chalice following this code structure: The biggest limitation of AWS Lambda in our case is the runtime limit However AWS enabling 15 minutes runtime in October 2018 bought us some time to keep focusing on fighting technical debt without investing too much energy in designing a new more complex infrastructure for our pipelinesWe currently process around 70M events per day and as Babbel is a fast-growing company the quantity of data that our platform ingests and processes is continuously increasing Thus lambdas memory and runtime demand increase as well Following the growth the need for new data services emerged for which more computational resources are neededRethinking our development process was not an option The productivity we reach with our team was mostly a consequence of the clean code structure paradigm that we embraced over time which also cuts the time required for code reviewsA complex system that works is invariably found to have evolved from a simple system that worked A complex system designed from scratch never works and cannot be patched up to make it work You have to start over with a working simple systemThe two main drivers for the decision were the possibility to keep our development cycle and use the same code structure as in our lambdas Thus we were able to remain very productive while adopting new technology and use tools that we developed to foster fast and unified development of lambda functionsWe started writing Terraform modules for managing ECS clusters for multiple stages and deployment of single-purpose Fargate tasks triggered by CloudWatch events The scope of the initiative was to build a small framework to port our lambdas to Fargate limiting the changes to our code structure and potentially having the possibility to share code between lambdas and new tasksIn particular the module we are presenting here aims to manage the entire lifecycle of a task including privileges and permissions granted to itIts written in Terraform 012 and comes with a complete example that sets up an entire stack of additional services needed for a successful deployment (VPC Subnets NAT Gateway ECS Cluster Task Execution IAM Role)This module creates the following resources: Task definitions and IAM policies are defined in JSON format the same way we were doing for our lambdas in the configjson and policyjson files of Chalice The process of adding new tasks to an ECS Cluster is as simple as pushing the docker image to AWS ECR writing the JSON configurations mentioned above and creating a new module resource in the infrastructure codeThe module is available through the official Terraform RegistryRunning Docker containers on AWS Fargate enables us to gain flexibility when writing tasks worrying less about runtime and memory when dealing with data operations Overall its a convenient transition from AWS Lambda in the context of migrating pipelines and with the help of this Terraform module we are saving time and effort while doing itThis post was co-edited by Dženan Softić (Team Lead Data Engineering) Thanks also to Andrzej Neumann (Team Lead Data Engineering) and Mehdi Talbi (Data Engineer) for their support during the project implementation and release
ZWX2XcD2Xg4b6BLQAVfSZx,Im happy to help announce the launch of a new community  the Data On Kubernetes Community The Dokcommunity is an openly governed and self-organizing group of curious and experienced operators and engineers concerned with running data-intensive workloads on Kubernetes DoKc takes inspiration from the CNCF and Apache foundations and aims like each of them to be open vendor-neutral and extremely inclusiveUnlike each of these foundations the purpose of DoKc is to openly develop share and discuss engineering and operation patterns as opposed to particular projects While DoKc is an open-source community it is a community focused on knowledge generation and sharing as opposed to becoming a home for open source projects (and their trademarks)DOKCs is for all those that are interested in using Kubernetes to serve and manage data in their organization 2020 has emerged as the year that data really migrated onto Kubernetes with the common workloads including CI/CD machine learning ecommerce systems such as Magento instrumentation and monitoring and others Almost any up to date stateful workload has seen a rapid increase in usage on KubernetesIn short now that Kubernetes can handle high-value stateful workloads  it is all the more critical that the broader cloud native community share best practices The potential of managing data on the same platform used to orchestrate is enormous and is just being uncovered I am confident that the DOKC community will bring together practitioners and developers building Kubernetes and container attached storage and other related projects thereby sparking more understanding and innovationOur founding sponsor MayaData; creators of the open-source project OpenEBS and the Kubernetes native chaos project Litmus has interacted with thousands of companies and individuals running data workloads on Kubernetes These experiences helped uncover the need for an independent community to discuss and investigate useful patterns for the use of Kubernetes as a data layerMany others feel similarly  DataStax Confluent Arista Yugabyte Optoro 2nd Quadrant and others have volunteered to participate in upcoming conversations and investigationsPlease fill out this simple form to get in touch with me about forming a part of the communityRelative links:Youtube channelThe DoKcommunity SlackTwitter: @DoKcommunityWebsiteMeetup
NTPZpN65K8Ypy9kZ6EpYV2,This is the first post of a series that will be posted once a month highlighting what is happening in the Data Engineering and Big Data landscape ranging from new tools news tutorials interviews and benchmarks
DPXQErq2gePRi5SfYDhJwP,In this edition theres a great article about data reliability and which metrics matter in a Data Platform (such as data downtime) how to measure them and how they can impact other teamsAnother great article comes from Netflix and how they manage the costs of their data platform by integrating metrics from the AWS billing with their S3 data inventory data catalog and Job Platforms Netflix is able to track their costs per area and even suggest TTL for table partitions saving money in the endWe also highlight the release of Flink 1110 with a new Source API and support for Change Data Capture on Flink SQL Theres also the release of Hadoop 330 with support to ARM architectures and Java 11 And the release of Samza 150
UvVYTS3CRzuDphJ7GDjR92,On October we had one of the most important conferences on Big Data AI and Data Engineering The videos of the presentations are already online at the conference site
YppHknGNEwNyj7WYzZiUcs,We explain our experiences in designing building and running a large corporate Datalake Our platform has been running for over two years and makes a wide variety of corporate data assets such as sales marketing customer information as well as data from less conventional sources such as weather news and social media available for analytics purposes to many teams across the company We focus on describing the management of data and in particular how it is transferred and ingested into the platform
KirmHU9gFePnmXLPsU769p,Apache Hadoop is open-source software that facilitates communication between multiple computers Such functionality allows to store and process large amounts of data Hadoop is fault tolerant and can run on commodity hardware therefore traditionally clusters have been relatively cheap and could easily be expanded for more space and processing power These features led to significant adoption rates and the framework has been one of the catalysts for the emergence of Big Data in the late 00sBefore we begin the natural questions is why would someone build a Hadoop cluster in 2020? A couple of reasons: TOTAL: £202This specific guide is for a headless Raspberry Pi setup using a MacBook machine This means that the Pis do not have their own dedicated screens and keyboards but instead will be controlled using the MacBook (simply referred to as machine in this guide) • Flash the Raspbian OS image onto the SD cardsThis one is straightforward Mount the Pis to the acrylic case tape the charger and router and connect the wiresThe next step is to write some simple MapReduce programs for the cluster to run Afterwards the intention is to enhance the cluster by installing additional components of the Hadoop ecosystem like Spark or HueThe clusters computing and storage utility has no chance in competing even against the budget laptops of 2020 however it contains most physical and software components that have powered companies like Google and Facebook over the last decade Overall building a Raspberry Pi Hadoop cluster is a highly enjoyable and engaging educational experienceA special thank you to Jason I Carter oliver hu and Alan Verdugo for their guides that served as inspiration
7eoNsmEZRUNrSTeNAKZ2Wi,Original authors: Eric Liang and Aaron DavidsonThis blog post is part of our series of internal engineering blogs on Databricks platform infrastructure management integration tooling monitoring and provisioningAt Databricks engineering we are avid fans of Kubernetes Much of our platform infrastructure runs within Kubernetes whether in AWS cloud or more regulated environmentsHowever we have found that Kubernetes alone is not enough for managing a complex service infrastructure which may encompass both resources created in Kubernetes (eg pods services) and external resources such as IAM roles Management complexity comes from (1) need for visibility into the current state of the infrastructure (2) reasoning about how to make changes to the infrastructureTo help reduce management complexity it is desirable for infrastructure to be declarative (ie described by a set of configuration files or templates) The first advantage is that one can easily inspect the target state of infrastructure by reading the configuration files Second since the infrastructure is entirely described by the files changes can be proposed reviewed and applied as part of a standard software development workflowKubernetes YAML configuration files already implement declarative updates to objects in Kubernetes The user only need edit an objects YAML file and then run $ kubectl apply -f theobjectyaml to sync the changes to Kubernetes These YAML files may be checked into source control and if needed the user can query Kubernetes to inspect the difference between the live version and the local fileHowever one runs into pain points when attempting to apply this methodology to a larger production environment: In this blog post we describe how we use Googles Jsonnet configuration language to solve these problems walk through an example of templatizing a service deployment using Jsonnet and propose a Jsonnet style guide for the infrastructure templating use case Weve found that Jsonnet is easy to get started with and scales well to complex use casesLate 2015 we started experimenting with Jsonnet as part of the effort for Databricks Community Edition our free tier Apache Spark service for education Since then Jsonnet has exploded in popularity within Databricks engineering with over 40000 lines of Jsonnet in 1000+ distinct files checked into our main development repository These templates expand to hundreds of thousands of lines of raw YAML once materializedTeams at Databricks currently use Jsonnet to manage configurations for Kubernetes resources (including internal configurations of services running on Kubernetes) AWS CloudFormation Terraform Databricks Jobs and also for tasks such as TLS cert management and defining Prometheus alerts Over the past two years Jsonnet has grown to become the de-facto standard configuration language within engineeringJsonnet is a configuration language that helps you define JSON data The basic idea is that some JSON fields may be left as variables or expressions that are evaluated at compilation time For example the JSON object {count: 4} may be expressed as {count: 2 + 2} in Jsonnet You can also declare hidden fields with :: that can be referenced during compilation eg {x:: 2 y:: 2 count: $x + $y} also evaluates to {count: 4}Jsonnet objects may be subclassed by concatenating (+) objects together to override field values eg suppose we define the followingThen the Jsonnet expression (base + {x:: 10}) will compile to {count: 12} In fact the Jsonnet compiler requires you to override x since the default value raises an error You can therefore think of base as defining an abstract base class in JsonnetJsonnet compilation is completely deterministic and cannot perform external I/O making it ideal for defining configuration Weve found that Jsonnet strikes the right balance between restrictiveness and flexibility  previously we generated configurations using Scala code which erred too much on the side of flexibility leading to many headachesAt Databricks we extended our kubectl tool (dubbed kubecfg) so that it can take Jsonnet files directly as arguments Internally it compiles the Jsonnet to plain JSON / YAML before sending the data to Kubernetes Kubernetes then creates or updates objects as needed based on the uploaded configuration
PEWKEQLmiur6LreP9sNCdf,Datacoral supports several database engines as part of its transformation technology Using existing databases allows us to leverage the significant R&D effort that specialized vendors have invested in functionality performance and scalability As database technology continues to evolve it is important for us and our customers to understand the new capabilities that are being developed and take advantage of themDatacoral supports Amazon Redshift and was in fact show cased as a Redshift Ready Partner in the Global Partner Keynote at the AWS re:Invent 2019 event Redshift is changing at a rate of hundreds of new features and enhancements per year Hence keeping up with what is happening with Redshift requires some work that is crucial for us in order to help our customers get the maximum value from a product that is growing in complexityThe annual AWS conference re:Invent returned to Las Vegas in early December and there were plenty of announcements and sessions about numerous topics Much has changed since the first conference in 2012 when it had about 6000 attendees to the 2019 version which was at least an order of magnitude largerAmazon Redshift was announced at the 2012 re:Invent and became an instant hit and was the fastest growing AWS service until it was surpassed by Aurora couple years ago Much of Redshifts success was due to its pricing slogan of $1000 per TB per year for certain instance types which was dramatically lower than people were used to spend on databases and a very good deal for a managed database service At this years conference it was claimed that Redshift is the most popular cloud data warehouse technology with tens of thousands of customersHighlighted at the conference were recent and upcoming Redshift features as well as trends in Redshift usage like customers moving to data-lake architectures There were three particular areas of customer demand: Among the existing benefits that were highlighted were integration with a variety of other AWS technologies scalability and performance the fact that it is a managed service and that it has a variety of security features and compliance certifications Also highlighted was the cost that is hard for anyone to beat without having major benefits from separating compute and storageIn addition a major focus was features and functionality that are making Redshift increasingly data-lake friendly When Redshift was first rolled out it was a traditional MPP database where the data was stored on the local disks of the nodes of a cluster Over the years features have been added to facilitate queries against other sources of data like S3 and to separate compute and storage which were tightly coupled in the original architecture Features like Spectrum fit into this general data-lake trendAmong newer stuff it was claimed that Redshift has had 200+ new features and enhancements in the last 18 months Obviously not every single one was presented in detail Some of the newer features have been rolled out very recently while others are in beta preview and wont be publically available until next year Below are a few features that we believe will help our customers who use RedshiftSome of the more prominent features that were discussed at the conference are listed belowThe RA3 is a new type of instance that promises an unprecedented combination of scalability performance and price while allowing the separation of compute and storage costs It seems like the Redshift people are finally taking the separation of compute and storage really seriously with Concurrency Scaling as a key feature Supposedly key ingredients for RA3 are managed storage high-speed caching and high-bandwidth networking RA3 instances are already generally available with some happy beta customers (including Western Digital and Yelp) mentioned at the presentationOne of the new features that is to become available next year is Redshift AQUA the Advanced QUery Accelerator There is a more detailed description of the architecture here This is hardware acceleration based on FPGAs sitting between the Redshift clusters and storage When you think of hardware acceleration through FPGAs its hard not to think about Netezza Netezza was a data warehouse appliance vendor that IBM bought in 2010 Using FPGAs can be an inexpensive way to provide processing power to CPU-intensive workloads and AWS claims that AQUA can be up to 10x faster However the benefits of hardware acceleration are highly workload dependent It can also have the benefit of reducing the data traffic from storage to the DB servers through storage-level predicate evaluation and projection Redshift already has that ability when using range-restricted scans (aka zone maps) on local disks where the columnar storage provides projections On S3 the Parquet storage format provides similar benefits So it will be interesting to see what impact this feature will have on various workloads In any case the technology will be available with RA3 instancesAZ64 is a proprietary compression encoding that promises high degrees of compression and fast decompression for numeric and time-related data types It was originally announced in October AWS claims 35 percent less storage and 40 percent faster than LZOAnother performance feature currently in beta preview is materialized views It seems like a fairly standard MV feature with rewrite and complete and incremental refresh It currently has quite a few limitationsA new data type Geometry has been introduced to support ingesting analyzing and storing spatial dataAnother feature in preview is federated queries that can access data in Postgres databases on RDS or Aurora Apparently its part of the story of Redshift becoming more data-lake friendly The ability to access data in other databases is highly useful so this is a welcome addition They gave an example of creating a UNION ALL view in Redshift with branches combining hot data in stored Aurora recent data stored in Redshift and archived data stored in S3Another feature is that data can be unloaded from Redshift to S3 in Parquet  another data-lake friendly feature In S3 the data can be accessed by a variety of servicesAnother item was the optimization of table design and maintenance based on usage patterns using ML algorithms It included automation of analyze vacuum and sorting as well as advisors for sort keys and distribution stylesOther highlighted management features included a new and improved management console Auto WLM and a scheduler for elastic resizing of clustersStored procedures have made their way back to Redshift They existed in the original source code for ParAccel that AWS had acquired as the basis for Redshift but were removed because of concerns about security Its about time they came back since stored procedures are highly useful Redshift uses the PL/pgSQL format which is no surprise since it has a Postgres-based front endAt Datacoral we cant wait to try out all of these features! We take it as our responsibility to figure out which of these features will truly benefit our customers We will report back our findings in a future blog postPlease reach out to us at hello@datacoralco if you have comments or want to try out Datacoral
nhMJgznZ2MZRqwns94tEBs,"When youre a software developer you need to think about interactions with users and high availability APIs This is always a priority because the content needs to be refreshed ASAP for the user and sometimes do you have a lot of usersLets talk about Data Engineers right now They will be worried about how all your users data will be stored and accessed Not by the user but for dashboards or data scientists""To understand the difference lets start with an example of Web Developers' challenge Imagine a web page that receives thousands and thousands of requests and architecture for thatNow for data processing we need something like Lambda Architecture that enables high size data traffic that cant be handled by traditional JSON APIs An example of this architecture could be explained a lit bit in this article and its image is very useful as an explanation: Commenting on the components in this figure: Lambda Architecture is resilient fault-tolerant and enables data to process as it comes without fear of lose something It needs an orchestrator like Azkaban to start jobs and keep streaming systems running Azkaban itself has 2 kinds of API to make new jobs but can be abstracted with Auror Python project and define some job flowsA flow shows the direction that a process should take It has some jobs and these jobs could have other job dependencies So to start a job C you need to finish job A job E needs job B job C and job D etc… All these flows have time to start and can be scheduled as a unix cron but following Quartz patternThis architecture is very strange for most of the software developers that start to work with Big Data because there is no need for APIs or Caches but you need to take care of: Thanks Pedro Schuwarte for english review"
SiSnoFG5uFA5CHufHdYgQL,Companies large and small are interested in building machine learning and AI based systems to help customers and everyday users But little do they realise they have so much data within the organisation to improve day to day activity of every individual within the corporate worldLets take an example of an IT organisation involved in selling their product & supporting them Each department within this organisation starting from HR finance sales IT engineering & business teams and CXOs use different disparate systems and interact with them everydayNow lets connect these data: Suddenly the organisation has capability to draw out insights and do more with the data which otherwise isnt more than reportsSales and account executives can get meaningful insights on customers how the systems are used and what more can they do to provide valueEngineer will have a knowledge base to support the customerCXOs will have access to resource utilisation profit analysis and other corporate insights in real-timeContinue: https://linkmediumAt HorizonX (http://wwwhorizonxcomau) we work with enterprises and help them realise the data potentials within and outside the organisation
nzt29HZMvxqmU9aYEdsSZY,Data processing with a general-purpose distributed data processing engineApache Spark written in Scala is a general-purpose distributed data processing engine Or in other words: load big data do computations on it in a distributed way and then store itSpark provides high-level APIs in Java Scala Python and R and an optimized engine that supports general execution graphs It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing MLlib for machine learning GraphX for graph processing and Spark StreamingTo run Spark you can either spin your own cluster or use Amazon EMR with/without Amazon Glue or use Google DataprocApache Spark contains libraries for data analysis machine learning graph analysis and streaming live data Spark is generally faster than Hadoop This is because Hadoop writes intermediate results to disk (that is lots of I/O operations) whereas Spark tries to keep intermediate results in memory (that is in-memory computation) whenever possible Moreover Spark offers lazy evaluation of operations and optimizes them just before the final result; Sparks maintains a series of transformations that are to be performed without actually performing those operations unless we try to obtain the results This way Spark is able to find the best path looking at overall transformations required (for example reducing two separate steps of adding number 5 and 20 to each element of the dataset into just a single step of adding 25 to each element of the dataset or not actually doing operations on part of the dataset which will eventually will be filtered out in the final result) This makes Spark one of the most popular tools for big data analytics currentlyHadoop saves intermediate states to disk and communicates over a network If we consider the logistic regression of ML model then each iteration state is saved back to disk The process is slow Whereas Spark keeps all data immutable and in-memory It achieves this using idea from functional programming such as fault tolerance which works by replaying functional transformations over original datasetsSpark laziness (on transformation) and eagerness (on action) is how Spark optimizes network communication using the programming model Hence Spark defines transformations and actions on Datasets (and RDD) to support this The transformations (such as where) are lazy and so their resultant output is not immediately computed Actions (such as take) are eager Their results are immediately computedThe Hadoop ecosystem includes a distributed file storage system called HDFS (Hadoop Distributed File System) Spark on the other hand does not include a file storage system You can use Spark on top of HDFS but you do not have to Spark can read in data from other sources as well such as Amazon S3While Spark doesnt implement MapReduce one can write Spark programs that behave in a similar way to the map-reduce paradigmFor Big Data processing the most common form of data is key-value pairs In fact in a 2004 mapReduce research paper the designer states that key-value pairs is a key-choice in designing mapReduce During the transformations a key generally acts as a group against which aggregated value is calculated in a transformationSimilar to Hadoop partitioning in Spark(Hash partitioners or Range partitioners) can bring enormous performance gains especially in the shuffling phaseData streaming is a specialized topic in big data The use case is when you want to store and analyze data in real-time such as Facebook posts or Twitter tweets Spark has a streaming library called Spark Streaming although it is not as popular and fast as some other streaming libraries Other popular streaming libraries include Apache Storm and Apache Flink Kafka or Kinesis are useful when using Spark StreamingThe easiest way to try out Apache Spark is in Local Mode The entire processing is done on a single server You thus still benefit from parallelization across all the cores in your server but not across several serversSpark runs on JVM It exposes a Python R Scala and SQL interfaceFor Python Spark provides Python API via PySpark which is available in PyPI and so can be installed via pip It can be imported or directly invoked as pyspark to get an interactive shellSimilarly Scala and R also provide interactive shells spark-shell and sparkR respectivelyFor other installation options check sparkapacheorg/downloadshtmlThe Spark cluster mode overview explains the key concepts in running on a cluster Spark can run both by itself (standalone) or over several existing cluster managers (Hadoop Apache Mesos Kubernetes)The system currently supports several cluster managers: Spark is organized in a master/workers topology In the context of Spark the driver program is a master node whereas the executor nodes are the workers Each worker node runs the same task and returns the results to the master node The resource distribution is handled by a cluster managerSpark applications run as independent sets of processes on a cluster coordinated by the SparkContext object in your main program (called the driver program)Specifically to run on a cluster the SparkContext can connect to several types of cluster managers which allocate resources across applications Once connected Spark acquires executors on nodes in the cluster which are processes that run computations and store data for your application Next it sends your application code (defined by JAR or Python files passed to SparkContext) to the executors Finally SparkContext sends tasks to the executors to runEach application gets its own executor processes which stay up for the duration of the whole application and run tasks in multiple threads This has the benefit of isolating applications from each other on both the scheduling side (each driver schedules its own tasks) and executor side (tasks from different applications run in different JVMs) However it also means that data cannot be shared across different Spark applications (instances of SparkContext) without writing it to an external storage systemEach driver program has a web UI typically on port 4040Spark gives control over resource allocation both across applications (at the level of the cluster manager) and within applications (if multiple computations are happening on the same SparkContext) The job scheduling overview describes this in more detailWhile pysparkSparkContext is the main entry point for Spark functionality and represents the connection to Spark cluster and can be used to create RDD and broadcast variables on that cluster the pysparksqlSparkSession is the entry point to programming Spark with the Dataset and DataFrame APISpark uses functional programming language Scala The reason behind this is that functional programming is perfect for distributed systemsIn functional programming if f(x) = x + 5 then f(3) is always 8 But this is not always true in non-functional languages such as PythonAs a result running the same code again can give different results The problem in this example is easy to see not that easy when you have dozens of machines running code in parallel and sometimes you need to restart a calculation if one of the machines has a temporary issue These unintended side effects can lead to a major headaches The confusion comes from sloppy languageHow DAG and lazy evaluation helps in the running of memory: Just like bread companies make copies of the starter from their mother dough every Spark function makes a copy of its input data and never changes the original parent data (that is original data is kept immutable) Because Spark doesnt change or mutate the input data it is known as immutable This makes sense when you have a single function but in case you have multiple functions then you chain together multiple functions that each accomplish a small chunk of the work Youll often see a function that is composed of multiple sub-functions and in order for this big function to be peer each sub-function also has to be a peer It would seem that Spark needs to make a copy of the input data for each sub-function If this was the case your Spark program would run out of memory pretty quicklyFortunately Spark avoids this by using a functional programming concept called lazy evaluation Before Spark does anything with the data in your program it first builds step-by-step directions of what functions and data it will need These directions are like recipes of your bread and in Spark this is called a Directed Acyclic Graph (DAG) Once Spark builds the DAG from your code it checks if it can procrastinate waiting until the last possible moment to get the data Thus spark has the opportunity to optimize the overall transformation process This is exactly what you would do if you were making bread You want to grab the flour bring it back to your bowl and then go back to the pantry and get some sugar and add it to the bowl and then go back to the cupboard for the salt and so on for every ingredient This would be the cooking equivalent of thrashing Instead you look at the recipe before you start mixing ingredients together to see what you can grab and mix together in one big step In fact you often mix all your dry ingredients then mix all your wet ingredients and combine them together before baking In Spark these multi-step combos are called stagesFunctional programming in Python: Anonymous functions can be thought of as a Python feature for writing functional-style programmingDeclarative programming is concerned with What and in Spark can be done with Spark SQL using Spark SQL built-in functions Whereas imperative programming is concerned with How and in Spark can be done with Spark DataFrames and PythonA Dataset is a distributed collection of data Dataset interface provides the benefits of Resilient Distributed Dataset (RDD) with the benefits of Spark SQLs optimized execution engine The Dataset API is available in Scala and Java Python does not have the support for the Dataset APIA DataFrame is a Dataset organized into named columns It is conceptually equivalent to a table in a relational database or a data frame in R/Python but with richer optimizations under the hood DataFrames can be constructed from a wide array of sources The DataFrame API is available in Scala Java Python (pysparksqlDataFrame) and RNote that the pysparksqlSparkSession is the entry point to programming Spark with Dataset/DataFrame API A SparkSession can be used to create DataFrame register DataFrame as tables execute SQL over tables cache tables and read parquet filesOne of the key differences between Pandas and Spark dataframes is eager versus lazy execution In PySpark operations are delayed until a result is actually needed in the pipeline This approach is used to avoid pulling the full data frame into memory and enables more effective processing across a cluster of machines With Pandas dataframe everything is pulled into memory and every Pandas operation is immediately applied Another key difference is that Spark lets you use parallalization using partitionsParquet is a columnar file (unlike CSV which is row-based storage) format that saves both time and space when it comes to big data processing and it is a file format that includes metadata about column data types Parquet for example is shown to boost Spark SQL performance by 10x on average compared to using text thanks to low-level reader filters efficient execution plans and in Spark 160 improved scan throughput! Refer to this article by IBMSo if data is in CSV format it makes sense to load those files in Spark convert and save them in parquet format and then use them whenever they are requiredWhen using CSV files into DataFrames Spark performs the operation in eager mode (that is all of the data is loaded into memory before the next step begins execution) while a lazy approach is used when reading files in parquet formatAlso note that while working with Spark for distributed clusters it doesnt make sense to load files (or write files to) from local storage We should rather be using Amazon S3 or HDFSA pysparksqlDataFrame (and pysparksqlGroupedData) is equivalent to a relational table in Spark SQL and can be created using various functions in pysparksqlSparkSessionUse <spark_session>read to access pysparksqlDataFrameReader to load DataFrame from external storage systemsUse <data_frame>write to access pysparksqlDataFrameWriter to write DataFrame to external storageUse sqltypesStructType with a list of sqltypesStructFields to represent a data type representing a rowFor example: Before we continue further with the example lets briefly look at some categories of commonly used functions: continuing with the exampleWhen you use Amazon S3 (preferred) youre separating the data storage from your cluster One of the downsides is that you have to download your data across the network which can be a bottleneck (but it wont be the case is Spark is running on AWS itself) Another solution is to store the data on your Spark cluster with HDFSSpark and HDFS are designed to work well together When Spark needs some data from HDFS it grabs the closest copy which minimizes the time data spends traveling around the network But there is a trade-off to HDFS You have to maintain and fix the system yourself For many companies from small startups to big corporations S3 is just easier since you dont have to maintain a separate cluster Also if you rent clusters from AWS your data usually doesnt have to go too far in the network since the cluster hardware and the S3 hardware are both on Amazons data centers Finally Spark is smart enough to download a small chunk of data and process that chunk while waiting for the rest to downloadThe spark-submit the script in Sparks bin the directory is used to launch applications on a cluster It can use all of the Sparks supported cluster managers through a uniform interface so you dont have to configure your application for each oneIf your code depends on other projects you will need to package them alongside your application in order to distribute the code to a Spark cluster For Python you can use the --py-files argument of spark-submit to add py zip or egg files to be distributed with your application If you depend on multiple Python files it is recommended to package them into a zip or eggOnce a user application is bundled it can be launched using bin/spark-submit a script This script takes care of setting the classpath with Spark and its dependencies and can support different cluster managers and deploy modes that Spark supports: Some of the commonly used options are: For Python applications simply pass a py file in the place of <application-jar> instead of a JAR and add Python zip egg or py files to the search path with --py-filesSparks Limitation: Spark Streamings latency is at least 500 milliseconds since it operates on micro-batches of records instead of processing one record at a time Native streaming tools such as Storm Apex or Flink can push down this latency value and might be more suitable for low-latency applications Flink and Apex can be used for batch computation as well so if youre already using them for stream processing theres no need to add Spark to your stack of technologiesWhen you are working on a distributed Spark cluster errors in your code can be very hard to diagnoseNormally when a function is passed to a Spark operation (such as map or reduce) is executed on remote cluster worker nodes they work on separate copies of all the variables used in the function These variables are copied to each machine and no update to the variable on the remote worker machines are propagated back to the driver programSupporting general read-write shared variables across tasks would be inefficient However Spark does provide two limited types of shared variables for two common usage patterns: broadcast variables and accumulatorsBroadcast variables allow the programmer to keep a read-only variable cached on each machine rather than shipping a copy of it with tasks They can be used for example to give every node a copy of a large input dataset in an efficient manner Spark also attempts to distribute broadcast variables using efficient broadcast algorithms to reduce communication costsSpark actions are executed through a set of stages separated by distributed shuffle operations Spark automatically broadcasts the common data needed by tasks within each stage The data broadcasted this way is cached in serialized form and deserialized before running each task This means that explicitly creating broadcast variables is only useful when tasks across multiple stages need the same data or when caching the data in the deserialized form is importantBroadcast variables can be created from a variable v by calling SparkContextbroadcast(v) The broadcast variable is a wrapper around v and its value can be accessed by calling the value methodAfter the broadcast variable is created it should be used instead of the value v in any function run on the cluster so that v is not shipped to the nodes more than once In addition the object v should not be modified after it is broadcast in order to ensure that all nodes get the same value of the broadcast variable (eg if the variable is shipped to a new node later)Accumulators are variables that are only added to through an associative and commutative operation and can therefore be efficiently supported in parallel They can be used to implement counters (as in MapReduce) or sums Spark natively supports accumulators of numeric type and programmers can add support for new typesAs a user you can create named or unnamed accumulators As seen in the image below a named accumulator (in this instance counter) will display in the web UI for the stage that modifies that accumulator Spark displays the value for each accumulator modified by a task in the Tasks tableTracking accumulators in the UI can be useful for understanding the progress of running stages (NOTE: this is not yet supported in Python)An accumulator is created from an initial value v by calling SparkContextaccumulator(v) Tasks running on a cluster can then add to it using the add method or the += operator However they cannot read its value Only the driver program can read the accumulators value using its value attributeAccumulators do not change the lazy evaluation model of Spark If they are being updated within an operation on a DataFrame their value is only updated once that DataFrame is computed as part of the actionEach driver program has a web UI typically on port 4040 that displays information about running tasks executors and storage usage Simply go to http://<driver-node>:4040 in a web browser to access this UI The monitoring guide also describes other monitoring optionsThe web UI provides the current configuration for the cluster which can be useful for double-checking that your desired settings went into effectThe web UI also shows you the DAG the recipe of the steps for your program A Spark application consists of as many jobs as many actions (like saving DataFrame to a database or taking some records back to the driver for inspection) regarding the code Jobs are further broken up into stages Stages are units of work that depend on one another The smallest unit within a stage is a task Tasks are a series of Spark transformations that can be run in parallel on different partitions of our DataFrame So if we have 10 partitions we run 10 of the same task to complete a stage Tasks are the steps that the individual worker nodes are assigned In each stage the worker node divides up the input data and runs the task for that stageBy default Spark master uses port 7077 to communicate with the worker nodes port 4040 shows active Spark jobs the web UI for your master node is on port 8080 (or 4040)
Z7fJzJqhDveL2DGMd8NxvH,In simple terms data warehousing is a process used to collect and analyze data from multiple sources The importance of data warehouses has been boosted with the availability of large volumes of business data that is now available for analysis and reporting among business usersHowever data warehousing is not a recent development and has been in the making since the late 1980s Before the evolution of cloud technology traditional data warehouses were operated on in-house infrastructure which was more expensiveAdditionally these data warehouses contained data from selected sources including relational database tables and data files Today data warehouses include these data sources and additional sources including CRM & ERP systems social media platforms IoT devices big data and much moreSo how has data warehousing for the enterprise evolved since the 80s? How does a data warehouse on the cloud compared with the traditional model? Lets examine this and much more in the remaining sectionsAn Enterprise Data Warehouse (or EDW) is a centralized database (or a collection of databases) that stores business data from multiple data sources This business data is then made available for data analytics and for deriving valuable insightsIn todays age of big data an enterprise warehouse is a central repository that collects data from multiple sources (as illustrated in the figure) This includes structured unstructured and partially structured data that goes through an ETL (extraction transformation and loading) process before being stored in the data warehouseAccording to Oracle the enterprise data warehouse is a data management system that has been designed for performing business intelligence or BIWith the availability of all the business data in a centralized warehouse the data can now be analyzed using data analysis or BI tools for more accurate insights For example a data warehouse in an airline company can be used to determine the current crew assignment frequent flyer programs and air route analysisIn recent years companies have moved their enterprise data warehouse architecture from traditional in-house warehousing to cloud-based architecture An enterprise cloud data warehouse provides the following business benefits as compared to the traditional warehousing model: Next we shall discuss the Enterprise data warehouse architecture along with its key componentsFirst we shall discuss the key components of the enterprise data warehouse architectureTypically a three-tier enterprise data warehouse architecture consists of the following components namely: As a technological innovation an enterprise data warehouse can support many workloads in any business enterprise including manufacturing financial reporting supply chains and customer satisfaction analysisAmong the primary reason to migrate the data enterprise to the cloud data scalability can be a major challenge as organizations keep adding new or more complex workloads This growth can create multiple workloads including analytical and operational workloadsFor proper planning businesses must consider the following key categories of EDW workloads: Now that we have an understanding of the EDW components and workload categories lets move on to the process of implement the EDW on a cloud platformWith the emergence of cloud-powered technologies business enterprises are implementing (or migrating) their enterprise data warehouse in cloud platforms Cloud-based data warehousing services are offered on technologies such as Amazon Redshift Google BigQuery and IBM DB2Before migrating your EDW to a cloud platform any business must assess a few migration considerations This is crucial to ensure that the cloud-based data warehousing workloads are integrated seamlessly with the in-premise workloads thus minimizing any downtimeEach of the following considerations can be used to determine the priority of the workloads to be moved to the cloud: For large data volumes migrating the data warehouse from the premise to the cloud can be a long and time-consuming process Hence you must define the data sets early in the process This ensures smooth connectivity for data movement and the design of a project schedule for a time-bound data migration processA cloud-powered EDW requires all your ETL processes and data to be enabled beyond in-premise warehousing As a consideration evaluate if your existing ETL tools are valid for cloud operation and can be integrated with cloud-based EDW technologiesCloud-powered EDWs offer capabilities that can be a challenge for developers who are used to working on in-premise warehouses Through proper training and learning ensure that your project developers gain the necessary expertise and resources required to adopt a cloud EDWBusiness costs of moving data from an on-premise warehouse to the cloud can overshoot in the event of an inefficient migration process As a business evaluate the overall costs of migration before proceeding with the sameImplementing data warehousing for enterprise on the cloud requires a phase-wise process comprising of new deployments as well as the migration of the existing workloads to the cloudLets discuss each of these phases in detail: This is the initial phase where you can define the business metrics and KPIs for measuring the EDW project deliverables For defining target workloads engage and brainstorm with your team members including business analysts BI programmers and database administrators You can also identify the use cases for the cloud-powered EDWDuring the assessment phase list and map out your workloads to data sets database tables and other structures For better governance and compliance define the necessary security controls Additionally you can define the roadmap for a minimum viable cloud (MVC) as well as the staffing for executing the MVC build and other EDW operationsIn the third phase design and implement the MVC build including elements such as connectivity routing access controls and other deployment tools Additionally you should configure separate environments for development testing and production activities Along with building database instances you can configure the staging environment for data replication and transformationFinally we come to the migration phase where you can move your datasets to the cloud EDW platform Additionally you can implement the ETL processes during this phase to ensure integrity between the on-premise and cloud premise workloads Identify the individual workloads (for development testing production) and migrate them along with the data workflows At last reconfigure your data analytical models for execution on the cloudThe final phase is the operational phase for executing an operational and automated response to any system events During this phase identify the operation team personnel and if necessary train them to identify and respond to system eventsWith this phase-wise implementation you can complete the migration of your EDW system to the cloud Next we shall look at how a cloud-powered enterprise data warehouse benefits any organizationCloud-hosted data warehousing for enterprise offer multiple business benefits including the following: Any on-premise infrastructure involves multiple cost avenues including license costs hardware manpower and even deployment and maintenance-related costs For cloud-powered EDW you only pay for what you consume Additionally cloud platforms free up your internal resources for other work which would otherwise have been allocated to managing your data warehousing activitiesCapabilities like effective data analytics and business intelligence are no longer just an IT domain but are now used by business users and departments In this environment its no longer feasible for business functions to wait for IT personnel to set up a data warehouse for themOn the other hand cloud deployments offer the advantage of faster and agile data warehousing for the enterprise and business usersCloud deployments have often raised business concerns about data security and privacy This has been addressed by cloud-powered tools like Microsoft Azure and Google Cloud Storage that place a high value on their security measures For example measures like data encryption on the Google Cloud Platform has improved data securityFor any industry or business domain data warehousing for an enterprise has its share of benefits and challenges To simplify decisions you must first understand your data-related needs and the feasibility of migrating your data warehouse to the cloudWith its customized services in cloud engineering and analytics Countants has enabled its global clients to move to the cloud platform and reduce their operational costsWant to leverage your business from our BI and cloud-powered services? Get in touch now with your business queries and needsOriginally published at https://wwwcountantscom on January 18 2020
TjyDhVnRPsqH3HsMckFRZo,Learn about Cloud ComputingLearn different Challenges that need to procure with Cloud ComputingWhat is Cloud Computing? In short It is on-demand allocation of computing resourcesWikipedia says Cloud computing is the on-demand accessibility of computing resources such as data storage and compute engines with no explicit active management by the end-user Following are the characteristics of Cloud computing: A report by RightScale from January 2018 on State of the Cloud survey major challenges in Cloud Computing were found as follows: Security and Privacy: When we say security and privacy we are talking about the user data that is stored on cloud service providers (CSP) data centres A CSP should abide by the rules of not sharing confidential data or any data that matters to the users The data centres must be secure and privacy of the data should be maintained by a CSPThere is always concern about the actual location of your data where it is stored and processed Elevating these concerns many cyber attacks hacked API and interface compromised credentials etc have happened in the near past Before on boarding yourself on the cloud computing platform one should always check the data security and data recovery (in case of disaster) policy of the CSPManaging Cloud spend: Cloud Computing can be expensive if you dont know how to manage your computing resources and take maximum advantage of them Many times organisations dwell in a mindset of pay-as-you-go and spend more on cloud than they would have on on-premise infrastructure One should always optimise the cost by financial analytics and reporting the usage for better monitoring of costLack of Resources/Expertise: As the usage of cloud technologies is increasing tools to manage it are getting sophisticated finding experts on top of this in cloud computing is becoming a bottleneck to many organisations Many companies are adopting automated cloud management technologies but its always better to train individuals to satisfy the need of time DevOps tools like Chef and Puppet are heavily used in the IT industryGovernance/Control: In cloud computing infrastructure resources are under CSPs control and end-users or companies have to abide by the governance policies from CSP Traditional IT teams have no control over how and where their data is and processed IT governance should assure how infrastructure assets from CSP are being used To overcome the downfalls and challenges on boarding to cloud IT must adapt its orthodox way of governance and process control to induct cloud Now IT is playing an important role in bench-marking cloud services requirements and policiesCompliance: When organisations are moving their native data to cloud they need to comply with particular general body policies if the data is from public sources Finding a cloud provider who will comply with these policies is difficult and one needs to negotiate on that frontMany CSPs are coming with flexible compliance policies for data acquisition and cloud infrastructureManaging multiple clouds: In the above-mentioned survey 81% of companies are having multi-cloud strategies and have a hybrid cloud structure(public and private clouds) Managing multi-cloud infrastructure contrary to a single cloud is very challenging given all the above data-driven challenges Companies are opting for a multi-cloud scenario because some of the services are cost-effective in public and to manage cost-effectively this cloud model has been very successful in recent years Managing such highly networked architecture is a difficult taskComputing Performance: Cloud Computing is on-demand compute service and supports multi-tenancy thus performance should not suffer over the acquisition of new users The CSP should maintain enough resources to serve all the users and any ad-hoc requestsBuilding a private cloud: Building a private cloud is very difficult as it involves many tasks such as grabbing an IP address cloud software layer setting up a virtual local area network (VLAN) load balancing firewall rule setting for the IP address loading the correct version of RHEL server software patch ups arranging nightly backup queue Many companies are planning to do so because the cloud will on-premise and they will have all the data authority over shared cloud resourcesPortability: This means that if the users want to migrate from one CSP to others the vendor should not lock-in customer data or services and the migration should be easy There are different laws over data in different countriesService Quality: The service quality should be good and is a major concern of the end-user The whole ecosystem of Cloud Computing is presented in virtual environments and thus the CSP should give what is promised in terms of service be it compute resources or customer satisfactionInter-operability: CSPs services should be flexible enough to integrate itself into other platforms and services provided by other CSPs The data pipeline should be easy to integrate and should drive improved performanceThere are a lot of challenges in Cloud Computing like Big data long hall transfer transferring data problems but still it is the best computing resource available to dateAvailability and Reliability: The data and service from the CSP should be available at all times irrespective of the external condition is the ideal condition The computing resource should be available for the users and their operability should be reliable Cloud Computing challenges are basically on the CSP side rather than the userCloud Computing with lots of ups and downs is still the best engineering service of our generation As more people come on board using cloud services CSPs are providing enhanced services increasing adoption of Cloud Computing If you found this tutorial helpful please do share it with your friends and leave a clap :-) If you have any queries feedback or suggestions do let me know in the comments Also you can connect with me on Instagram and LinkedIn There is so much to share with all of you and Im just getting started
Pev5nAaZgG5DWgsNs87ebu,One of Dataforms key motivations has been to bring software engineering best practices to teams building ETL/ELT pipelines To further that goal we recently launched support for you to run Continuous Integration (CI) checks against your Dataform projectsCI/CD is a set of processes which aim to help teams ship software quickly and reliablyContinuous integration (CI) checks automatically verify that all changes to your code work as expected and typically run before the change is merged into your Git master branch This ensures that the version of the code on the master branch always works correctlyContinuous deployment (CD) tools automatically (and frequently) deploy the latest version of your code to production This is intended to minimize the time it takes for new features or bugfixes to be available in productionDataform already does most of the CD gruntwork for you By default all code committed to the master branch is automatically deployed For more advanced use cases you can configure exactly what you want to be deployed and when using environmentsCI checks however are usually configured as part of your Git repository (usually hosted on GitHub though Dataform supports other Git hosting providers)Dataform distributes a Docker image which can be used to run the equivalent of Dataform CLI commands For most CI tools this Docker image is what youll use to run your automated checksIf you host your Dataform Git repository on GitHub you can use GitHub Actions to run CI workflows This post assumes youre using GitHub Actions but other CI tools are configured in a similar wayHeres a simple example of a GitHub Actions workflow for a Dataform project Once you put this in a github/workflows/<some filename>yaml file GitHub will run the workflow on each pull request and commit to your master branchThis workflow runs dataform compile - this means that if the project fails to compile the workflow will fail and this will be reflected in the GitHub UINote that its possible to run any dataform CLI command in a CI workflow However some commands do need credentials in order to run queries against your data warehouse In these circumstances you should encrypt those credentials and commit the encrypted file to your Git repository Then in your CI workflow you decrypt the credentials so that the Dataform CLI can use themFor further details on configuring CI/CD for your Dataform projects please see our docsOriginally published at https://dataformco
6Q4rQ3FZCZMeineszpftfW,"Ensuring that data consumers can use their data to reliably answer questions is of paramount importance to any data analytics team Having a mechanism to enforce high data quality across datasets is therefore a key requirement for these teamsOften input data sources are missing rows contain duplicates or include just plain invalid data Over time changes to business definitions or the underlying software which produces input data can cause drift in the meaning of columns  or even the overall structure of tables Addressing these issues is critical to creating a successful data team and generating valuable correct insightsIn this article we explain the concept of a SQL data assertion look at some common data quality problems how to detect them and  most importantly  how to fix them in a way that persists for all data consumersThe SQL snippets in this post apply to Google BigQuery but can be ported easily enough to Redshift Postgres or Snowflake data warehousesA data assertion is a query that looks for problems in a dataset If the query returns any rows then the assertion failsData assertions are defined this way because its much easier to look for problems rather than the absence of them It also means that assertion queries can themselves be used to quickly inspect the data causing the assertion to fail  making it easy to diagnose and fix the problemLets take a look at a simple example""Assume that there is a databasecustomers table containing information about customers in the databaseThe following simple query will return any rows violating these rules: We may also want to run checks across more than one row For example we might want to verify that the customer_id field is unique A query like the following will return any duplicate customer_id values: We can combine all of the above into a single query to quickly find any customer_id value violating one of our rules using UNION ALL: We now have one query we can run to detect any problems in our table and we can easily add another unioned SELECT statement if we want to add new conditions in the futureNow that weve detected the issues in our data we need to clean them up Ultimately choosing how to handle data quality issues depends on your business use caseIn this example we will: Rather than editing the dataset directly we can create a new clean copy of the dataset  this gives us freedom to change or add rules in the future and avoids deleting any dataThe following SQL query defines a view of our databasecustomers table in which invalid rows are removed default customer types are set and duplicate rows for the same customer_id are removed: This query can be used to create either a view or a table in our cloud data warehouse perhaps called database_cleancustomers which can be consumed in dashboards or by analysts who want to query the dataNow weve fixed the problem we can check that the above query has correctly fixed the problems by re-running the original assertion on the new datasetAssertions should be run as part of any data pipelines to make sure breaking changes are picked up the moment they happenIf an assertion returns any rows future steps in a pipeline should either fail or a notification delivered to the data ownerDataform has built in support for data assertions and provides a way to run them as part of a larger SQL pipelineThese can be run at any frequency and if an assertion fails an email will be sent to notify you of the problem Dataform also provides a way to easily create new datasets in your warehouse making managing the process of cleaning and testing your data extremely straightforwardFor more information on how to start writing data assertions with Dataform check out the assertions documentation guide for Dataforms open-source framework or create an account for free and start using Dataforms fully managed Web platformOriginally published at https://dataformco"
9hWa5CVJcgCXUL2KNHKut9,The process of integrating data which we went over in a previous blog post is time consuming because of the multiple handoffs needed between cross-functional analytics teams and data integrations teams to compensate for the data subject matter experts (SMEs) being separate from the data integration processThe solution then is to bring data integration closer to data SMEs saving the time spent waiting for data on the analytics side of the house and freeing up precious technical resources within data teams and IT departments to focus on complex data challengesAt the same time self service without governance can mean disaster for an enterprise Allowing multiple analytics teams to have what amounts to edit access to many data systems some of which contain sensitive data could mean a lack of visibility or a lack of a coherent and holistic data strategyOf course its crucial that that does not happenWhich is why self-service is only half of a viable solution For a self-service vision a vision of speed and agility to work there needs to be robust data governance and management capabilities in placeThe tools that are strong in self-service lack in governance capabilities rendering them useful perhaps for smaller businesses but completely inept for an enterpriseOn the other hand the tools that are strong in governance are entirely out of reach for non-technical or even semi-technical usersThats why its time to switch over to a data integration software that makes it super simple to get usable data into a centralized analytics environment regardless of technical know-how essentially making data integration part of a cross-functional teams self service toolkit without compromising on data governance and managementIn other words the enterprise needs a solution that provides teams with the ability to extract data from any source within their enterprise (or from external data sources if needed) by simply clicking to select what data is neededUsers should also be able to manipulate data formats and schemas on the fly to ensure that when the data arrives in their analytics environment it arrives in the exact specifications it was needed in These transformations should also be able to be applied visually without needing any code to ensure true self-service is providedFinally loading the data into an analytics environment or having the data sync on set intervals of time must be no more an involved process than clicking a button isThis simple workflow is only half of what an enterprise grade data integration software needs to bring to the table The other half (you guessed it  the governance half) happens behind the scenes in the form of forced sensitive data obfuscation activity logs role based action controls administrative dashboards and so on Data governance is serious business and data integration softwares needs to take it that way in order to be suitable for the enterpriseData integration is just a bridge from raw data to answers and its time to make that bridge shorter than ever because when teams within your enterprise are armed with the data they need youll see unbelievable resultsLearn more and reduce the time and energy spent on data integration so that your cross-functional teams can carry data-driven projects end to end and do more faster
KfHxsKWogNtVrQHP2KsA5p,Automation powers revolutionsThroughout history we have seen automation drive step functions in efficiency and productivity across industries across cultures across centuriesThe time is ripe now to bring automation to bear on data processing Lets talk about whyThe benefits of making data-driven decisions are vast and well understood A quick google search will reveal an endless number of statistics and figures pointing to thatHeres one of them: Heres another: We can keep going but the point is clear Data-driven = more customers profits market share etcSo every time data is created whether it be by IoT edge devices marketing campaigns company orders or a myriad of other data sources your enterprise gains more potential for data-driven valueGiven the tremendous investments made in data infrastructure as well as in data analytics software and talent over the past few decades the modern enterprise is poised to put data at the heart of every strategic business decisionIts simple: the bottleneck to becoming data-driven is no longer the availability of raw data; and its no longer the availability of analytics capabilities; the bottleneck is the painful repetitive manual work of actually transforming raw data into usable data  usable data that can power analytics and data-driven decisions We call this manual work data processing Data processing includes manually mapping source schemas to destination schemas manually deduplicating records manually sorting through data to find and obfuscate PII and a myriad of other repetitive manual tasks  all done in the pursuit of usable dataIn telecommunications we brought automation to the table When you pick up the phone to make a call you no longer talk to an operator; youre connected automaticallyIn financial services we brought automation to the table When you want to trade an equity you no longer call someone on the floor of the NY stock exchange; you just click a button and the trade happens automaticallyIn manufacturing we brought automation to the table When you want to build a car you dont drive around town collecting parts and then assemble them by hand; you utilize automated supply chains and factory floorsAnd all of these industries were revolutionized  by automationThats exactly what Data Process Automation (DPA) is: an opportunity to revolutionize your enterpriseBringing automation to your data processing can drastically reduce the time and effort needed to get from raw data to usable dataGetting data from point a to point b sounds simple enough but legacy ETL tools make it surprisingly difficult requiring both knowledge of the complicated platforms and the creation of ad-hoc pipelines Our DPA platform moves data via one dead-simple API endpoint and intuitive user interfaceDeduplication is a problem that seems to be as old time one that many analysts must be shocked still exists Shouldnt they have come up with a way to fix this by now? is a question anyone who has had to manually dedupe a data set has asked in frustration We think they should have so we did Our DPA platform provides an incredibly simple way of understanding whether two records are the same This can be integrated into any data integrity workflowHeres where things get tricky Every device collecting data has its own schemas creating outputs in different formats Every ERP HR finance marketing etc system has its own schema as well Wouldnt it be nice to get them into the same format? Without hours of manual tedious labor? Our DPA platform provides effective data modeling that provides the foundation for every team in your organization to create use and share data and insightsNow what if you wanted to actually unify all of those systems? In other words what if your organization needed to take its gazillion systems with all of their gazillion formats and schemas and get all of the data that lives in them into one place in one format? Dont worry about it our DPA platform does that tooFinally even if your data is all standard and accessible you still need to be able to work with it and share it free of compliance concerns Our DPA platform will automatically identify sensitive data in both structured and unstructured formats and mask it so that you can do the work you need to without worrySo imagine a world where you can move and analyze quality data from system to system in a standardized format without compliance concerns Now imagine a world where that happens automatically so that just like when you pick up the phone to call someone how that call gets connected doesnt even cross your mindThats what data process automation is thats what will transform your enterprise into a data-driven machine and thats what were bringing to the tableOne final note  overhauling enterprise wide data processing systems can seem hard maybe even impossible and we understand that Datalogues DPA platform can replace your legacy ETL system  but it doesnt have to Every essential DPA pillar can be implemented to work in tandem with existing systems and processes so that implementation takes about 60 days not years
RsBjF95m5K8acLR5oz9xzC,Today we think about data all the time So much so that the phrase data-driven is now a core part of corporate lexicons synonymous with good or well informed and the opposite of lets say bad or speculative Data and data analytics are so prevalent today its hard to imagine a time where they werentBut that time existed and not too long agoWhile companies have been collecting data for arguably hundreds of years it was only in the 80s that storing large amounts of data became fiscally feasible as the cost of hard drives started to plummetAnd it wasnt until the 90s that the idea of Business Intelligence (BI) was solidifiedAs with every new technology mass adoption of data warehouses and BI tools did not happen overnight That is to say that in the early 80s not every enterprise was diligently collecting and storing large amounts of data in structured ways  the early adopters wereSimilarly on January 1st 1990 not every employee of every large company went into work that day to discover a BI platform installed on their desktop computers Mass adoption of new technologies while it happens faster than ever today still takes timeBasically until quite recently enterprises didnt know the value their data could bring themThat means that when processes that included data generation or collection were designed they were often not accompanied by the building of data platforms systems and warehouses that were standardized in formats schemas credentials etcA messA mess of disparate data systems housing data of disparate formats with different levels of accessibility without a clear map of what data lives whereMost of the behemoth enterprises we know today got to be as big as they are through mergers and acquisitions With each merger and with each acquisition companies acquire the data systems of their subsidiaries In other words they inherit the mess of the companies acquired and that mess is multiplied in severity when you start talking about merging it with the pre-existing mess of the parent companyIf even the explanation sounds messy it should come as no surprise The data landscapes formed by these conditions are often a nightmareLets add in yet another source of complexity; external data sources Big companies rely on many various suppliers and vendors to fulfill customer orders and manufacturing requirements The problem? Each vendor or supplier an enterprise interacts with generates data How that data is generated ( in what formats etc) varies from vendor to vendorTodays society is global and so is the modern enterprise While international presence is great for sales brand equity and ultimately profit it does not do much in mitigating the challenges of a complex data landscapeDifferent data systems now housing data in different languages and country-specific formats become part of the equation (as if that equation wasnt difficult enough to solve already)Havent had enough? Since the late 90s/early 2000s when IoT devices have become more and more prevalent the amount of data generated by the enterprise has once again explodedHowever not all IoT devices output data in the same format and structure In fact data outputs change not just from device vendor to vendor but from model of device to model of device of the same make as well YikesAnd finally everyones favorite topic data security Regulations like GDPR HIPAA and HITRUST have made the already challenging task of data integration even more well challengingWith GDPR data must remain in region and must have subsequent access restrictions More generally these regulations make integrating and working with your data safely difficult especially when youre unsure where your sensitive data resides All of this creates challenges in accessing manipulating and moving data aroundIn a perfect world all these compounding factors weve laid out would be positives not negativesBecause most if not all are simply symptoms of having a massive amount of data at hand Today thats worth more than gold if you could only harness it and get it into the hands of the right people safely and securelyGood news is you finally can Why dont you get started today and reduce the time and energy spent on data integration so that your analytics teams can do more fasterNot ready to get started? Read on for more information on what an enterprise grade data integration solution must bring to the table to be successful
QYXsLJXGAuQsdb8GUhbZY9,Analytics are important for your enterprise Its why your business is likely spending a ton of money a year on self-service analytics software cross-functional teams can use hardware to run that software and talent to wield itHowever its just as likely that the returns on those investments and efforts arent where you want them to beIts a common problem and if you ask a CEO why that problem exists they likely wont knowAnyone working within a cross-functional analytics teamThats because theyre the ones waiting for weeks to get usable data into their self service toolkits (data warehouses data wrangling and data analytics softwares) Once the data is in there Im sure theyd tell you insights are almost never more than hours or a few days awayWhile cross-functional analytics teams are generally expected and empowered to complete data-driven projects end to end and while they are the ones who know the data they need for analysis best they are almost never the ones charged with data integrationIn fact in most organizations data integration is regarded as a squarely technical pursuit that lives in technical data teams whether that be IT or a specialized central ingestion team And when you look at data integration tools on the market today that makes senseLegacy data integration tools such as Informatica Oracle IBM etc are hard to use Training an analyst with a myriad of other responsibilities and areas of focus to use them would take months an investment of time most companies are unwilling to front-load Its also an investment that often needs to be repeated as employee churn brings in new people who have yet to be trained in these toolsThere are also newer very powerful tools on the market such as Databricks Spark Confluent amongst many others but they require deep technical expertise such as distributed computing knowledge to useThe players that have sprung up as would-be disruptors the self-service data integration providers of the world while easier to use lack the powerful data transformation capabilities and governance practices that the modern enterprise needsSo data integration remains where its lived for years: in technical teams relatively divorced from the data subject matter experts (SMEs) who reside in these cross-functional analytics teams and who know not only what data they need but the formats and schemas they need it inHighly iterative and not in the good wayThe back-and-forths required between technical and analytics teams during the data integration process to ensure the data is delivered to the analytics environment of choice in the exact format and schema its needed in are a drain of time as well as both analytic and technical resourcesAnd by the way if youre wondering just why traditional data integration processes are so lengthy check out our blog post that dives into the complex landscape of where data lives and what formats it lives in in a typical enterpriseWhatever your reason for having analytics teams performing data analysis  those are the goals that are on the line when access to usable data is a bottleneckNow imagine that you could cut the time it takes to go from raw data to insight by 75% or more You could answer those strategic questions you have posed and have time left over to run all the projects that are sitting on the back burner todayTrust us your cross-functional teams have the capacity to do more than you could imagineThats why youre investing in these kinds of teams Becase their multidisciplinary skill sets allow them to take a project from its inception to its completion They just need clean usable dataTake for example a college intern who figured out aeronautical vehicles carried way more water than was generally needed on trips saving the industry millions in fuel costs and helping the environment to bootBy creating a model that accurately predicts how much water an aeronautical vehicle needs depending on that vehicles routeThe only thing that intern needed to create that model was a creative approach to problem solving and usable data Your enterprise has already hired the talent needed for creative problem solving The last step is empowering them with usable dataInsights and strategic decisions are key for a successful enterprise but their ROI is generally hard to measure Another more concrete consequence of expending precious and scarce technical resources on menial data integration tasks is the accrued cost of pipeline building and maintenanceIts probably time to switch things upRead on for how we propose you reduce the time and energy spend on data integration so that your analytics teams can do more faster
BF8fGZEJyMazeBEL7Ss4EQ,Datagrip is a great database IDE just like the other IDEs from Jetbrains: Pycharm IntelliJ … In this blog I describe how to connect an AWS Athena database to Datagrip for my own reference and hopefully helpful for someone else as well This assumes that you already have data on S3 which is connected to Athena and you can already query it in the AWS consoleYou can find the JDBC driver here Store it somewhere on your hard drive Then open Datagrip and go to File -> Data Sources Click the + sign in the top left and select Driver  Complete as shown below: Go to the AWS IAM console and create a user with the following inline policy (or attach the policy to an existing user): Dont forget to replace THE-S3-BUCKET-TO-QUERY with the actual S3 bucket that Athena uses to query and THE-S3-BUCKET-TO-STORE-QUERY-RESULTS with an S3 bucket that Athena can use to store its query results When you query Athena using the AWS console its something like s3://aws-athena-query-results-1234567890-eu-west-1 Make sure to create and download an Access Key for that user You can do that by going to that user and then to the Security Credentials tabOnce there click the Access Key buttonCopy the Access key ID and the Secret access key somewhere Those will be the username and password you need to connect to Athena in Step 3Open Datagrip again and go to Data sources again (File -> Data Sources) Click the + symbol again and now select Athena Driver In the General tab add the following url: jdbc:awsathena://athenaeu-west-1amazonawscom:443/ Change the region if needed Also set the user and password As you couldve guessed those are the Access key ID and the Secret access key which you created in step 2 Next go to Advanced and set the s3_staging_dir to the S3 query results bucket you configured above Now go back to the General tab to test your connection It should workIf you dont see all the tables you expected you can go to the Schemas tab and select the schemas you needThats it Happy querying massive amounts of data on S3 from the convenience of Datagrip Do be wary of the costs In Athena you pay $5 per TB scanned And in Datagrip you dont see how much data each query scanned As a general tip: In reality weve observed Athena costs only very little for what it delivers Remember a small RDS database quickly costs about $35 per month An example aggregation query on a table of 1 billion rows stored in Parquet format scanned about 1GB Remember it only scans the columns it needs in the aggregation and Parquet compresses really well It took 6 seconds to execute that query To compare that to a small RDS database with Athena we can run 7000 of those queries for the same $35 Asking a small RDS database to run 7000 aggregation queries on 1 billion rows will probably take several months to execute And would need a lot more storage than 1GB
abvCgn5JJDnDKtW9QoowDR,Probably most of you know the basic join types from SQL: left right inner and outer Since these are supported by most of the data-oriented frameworks like pandas SQL or R it is easy to restrict oneself to these 4 approaches during the data flow design stageHowever Spark DataFrame API (since version 20) has 2 other types: left_semi (alias leftsemi) and left_anti In the following we will explore what they doLets define some data to work with: The left_semi join type is essentially a filter on the left table based on the join condition: Notice that my second article (In preparation) is missing from the output: it has been filtered out because it has no corresponding entry in the views DataFrame The output schema is identical to the left tables schemaThe left_anti does the exact opposite of left_semi: it filters out all entries from the left table which have a corresponding entry in the right table In our case only my article in preparation remains: Unfortunately these join types are only available for DataFrames the Datasets do not support these
niuSjF9rLsCWcAQLUMcipz,This is another one of those how to blogs that can hopefully help people get up-and-running quickly because it took me a while to figure it out Here is the full code if you want to try this yourself: https://githubcom/datamindedbe/spark_on_azure_batch_demo Obviously this is all just demo code with a lot of hardcoded strings and shortcuts This is not production-grade So take what you can use and adapt it to your contextYou want to run spark jobs on Azure But cost is a concern So youre looking into Azure Batch because you can use Low-Prio VMs which are not available on HDInsights or Azure Databricks Cloud See pricing here: https://azuremicrosoftcom/en-us/pricing/details/batch/ You quickly realise that you can save up to 80% which is significant We already use a lot of SPOT instances on AWS (the\xa0equivalent\xa0of\xa0low\xa0prio\xa0VMs) even in Production workloads because we aim to write our jobs in such a way that they can easily recover from failure and simply run again With Compute typically being the biggest cost in big data workloads this will have a very positive impact on the bottom lineHeres another piece of the puzzle: you have to run a bunch of daily jobs which individually are not really that big But you have to run it for 100 days You can look into AZTK which can run large spark clusters on Azure Batch But I dont really need one big Spark cluster Just give me 100 single node Spark enginesThis is where Azure Batch comes in You simply wrap your Spark code in a Docker container and schedule 100 of those containers on a pool of 20 Azure Batch nodes and let them execute the 100 jobs in parallel You get to use Low Prio VMs out of the box and you only pay for what you need No long-running Spark or Hadoop clustersIn order to run this demo you need to set up: Chances are you already have these running in Azure If not its simple click and install on the Azure Portal For each of these 3 services you need to get the Keys and add them to a configpy: For this demo we need a simple spark job Any job will do really The only thing you need to be aware of is that it has to be able to read from Blob Storage So we need some extra libraries when we initialise Spark In this case Im doing a simple aggregation on airline data which you can find hereWhat Im doing here is taking a full year of airline data grouping it by year month day and then calculate the average arrival and departure delay A very basic example To get this to work you need a few azure and pyspark dependencies which you typically add to requirementstxt and then do pip install -r requirementstxt ideally in a virtual environment (which is out-of-scope for this demo): The next step is that we wrap this job in a Docker container and push it to the Azure Container Registry A simple Dockerfile can be as follows: It starts from a python36 base image installs openjdk and some monitoring tooling and then all the python dependencies as listed in requirementstxt Finally it copies your code to a srcfolder You can build it with the command: Now you could already run this script locally as follows: This simply says to docker: please run the above docker image and within that image execute the python command with input a CSV file and output a Parquet file on Azure Blob storage Well need this later to schedule the same command on Azure BatchNext you need to log in to the Azure Container Registry with datamindeddemo replaced by your own registry of course Enter the admin username you see on the Azure portal: Finally you can tag your docker container: And upload it to ACR: Again replacing this with your own ACR path and credentials Note that the first time you push this Docker container it will be huge about 1GB: it has an ubuntu image a bunch of spark libraries a JDK … The next push you do of this container will only push your latest code changesOk so weve written a sample Spark application with the right Azure libraries Weve dockerized it and uploaded it to Azure Container Registry Now were finally ready to run our code on Azure Batch This is going to be quite a long python script but it does a lot of things as well: And finally you will see your jobs run on Azure Batch: Note that all nodes in a pool are completely separate This is not a one big Spark cluster concept You can log in to those individual nodes with a handy connect button: By the way Azure Batch Explorer is downloadable from github: https://azuregithubio/BatchExplorer/ It makes things slightly more convenient than looking in the azure Portal Once connected you can see how those nodes are performing using a tool such as htopYou can find all code to reproduce this here: https://githubThis solves 90% of the needs of a project were working on
DyCF3yd8tVyggzcevbi9tV,Here is a typical situation in enterprises that recently took on streaming and kubernetes in their Big Data infrastructure: Two environments for production and testing each with their own K8S cluster and their own MSK (kafka) cluster and a firewall in between In our case its all on AWS and you cannot reach the kafka clusters directly Now the testing environment has a new version of an application installed but the prod data doesnt get routed to that environmentWe have 3 raw incoming streams from an on-premise environment that feeds our production kafka I want to feed 10k messages of each stream into the test environment to see if everything works out as expected We have a dashboard that shows the message streams of each of the three streams in prod and if everything looks clean raw messages on the left lead to a lot of traffic in our egressNow some shell magic with kubectl kafka-console-consumer and kafka-console-producer Now inside of this container I execute some commands to collect messagesThis creates three files in /messages which we can now grabkubectl cp kafka-helper-977c4988d-p78gb:/messages You will have to change the container name here Check it with kubectl get pods Now connect to the different clusterNow my kubectl is linked to the new clusterEt voilà: All messages went through our flow and the results look good in our egressRun all of this in a single script from your machine
ivW6qhm9JhNm95zqUsiYkp,Your organisation wants to dive head-first into data and AI but you dont really know where to start? Data&AI is on the radar of most C-level leaders Its often seen as a differentiator from competition as an enabler for more customer engagement as a tool for cost reduction And there is no lack of industry analysts and strategic reports that highlight the impact that data already has on the corporate world But how to get started? Weve done quite a few tours of duty in data analytics Weve reflected on what comes back across clients and here is what we learned: Organisations who are successful in data have one common trait: they build data products not data projects What is the difference? In projects you know what you want you know how much you want to invest in it you make a plan for how to build it and you build it Once build is done only maintenance is needed Building a bridge is a project You know you want to connect two sides of a river You get money from both communities you hire an architect and a temporary (contractor) team and you build it Whats important in a project: Deliver predefined scope on-time within budgetFor products you need a completely different mindset You are focused on business outcomes And you give teams the freedom and responsibility to iterate towards that outcome That fits much better with data&AI initiatives Why? Because you often know very clearly what you want but you dont have a clue of how to get there even though you do recognise that data will be an important factorA typical example: customer satisfaction You want that to be as high as possible Its very clear what you want But there are many ways to Rome Do you want to increase customer satisfaction by: Maybe a combination Or maybe something else entirely Either way in a product mindset you enable your data and customer teams to work closely together to iterate towards a solution fast and then reliably run that solution for as long as it adds value That is called a data productEvery data product goes to the following phases whether they like it or not: And then the circle starts again You experiment again you implement new pieces deploy them again and monitor the outcomes The faster you go through this cycle the faster you will learn and the faster you will deliver value to your stakeholdersBased on our experience plenty of things can go wrong in this lifecycle as illustrated below: Many organisations we see still dont have their data available on a secure platform in a self-service mode Often there is no actual analytics environment or the BI department hords and protects data from business users And when there is an analytics environment the lack of data governance turns data lakes into swampsAnother recurring issue is that the notebook environment IS the production environment No code reuse no data reuse no best practices no tests no version control Nothing You change the notebook you change the data product That might be fine for some innocent reports But the more complex and customer facing your data product becomes the less acceptable that way-of-working isDeployments are another pain point Besides the production environment there are not a lot of testing grounds for data products This is due to data privacy complexity of setting up an environment and complexity of CI/CD processesOnce a product is live the best monitoring tool that organisations have is customers complaining that the product is not available shows incorrect data or contains outdated informationAnd finally and maybe the biggest painpoint of all most companies are struggling with finding enough data talent And once they do find them they are not allowed to do structural improvements to this product lifecycle because a next feature awaits At one of our first clients our contract was nearly terminated because I spent time pushing our code to a git repository and integrating with a build server You were hired to build features not be an operational expense Ouch Luckily I could persuade the manager to let me stayI know data scientists have the sexiest job of the 21st century And while their genius contributions can often make a profound impact on the value being created especially in the experimentation phase it is not the whole storyProper data governance can give you a business glossary a data catalog data lineage … I know these concepts are not sexy But they do accelerate the build of new data productsProper data engineering can bring automation scalability security flexibility reliability … Again no business sponsor gets excited when they hear these notions But you cannot live withoutWhat are your own experiences building data products? Any pain points that stand out? Happy to hear your feedback and interact
5TjjhuaGXShVrHHf5PvDAq,As a data engineer building data pipelines in a modern data platform one of the most common tasks is to extract data from an OLTP database or data warehouse that can be further transformed for analytical use-cases or building reports to answer business questionsApache Sqoop quickly became the de facto tool of choice to ingest data from these relational databases to HDFS (Hadoop Distributed File System) over the last decade when Hadoop was the primary compute environment Once data has been persisted into HDFS Hive or Spark can be used to transform the data for target use-caseAs adoption of Hadoop Hive and Map Reduce slows and the Spark usage continues to grow taking advantage of Spark for consuming data from relational databases becomes more importantBefore we dive into the pros and cons of using Spark over Sqoop lets review the basics of each technology: Apache Sqoop is a MapReduce-based utility that uses JDBC protocol to connect to a database to query and transfer data to Mappers spawned by YARN in a Hadoop cluster When the Sqoop utility is invoked it fetches the table metadata from the RDBMS If the table you are trying to import has a primary key a Sqoop job will attempt to spin-up four mappers (this can be controlled by an input argument) and parallelize the ingestion process as it splits the range of primary key across the mappersIf the table does not have a primary key users specify a column on which Sqoop can split the ingestion tasks Without specifying a column on which Sqoop can parallelize the ingest process only a single mapper task will be spawned to ingest the dataFor further performance tuning add input argument -m or  num-mappers <n> the default value is 4To only fetch a subset of the data use the  where <condition> argument to specify a where clause expression example : For data engineers who want to query or use this ingested data using hive there are additional options in Sqoop utility to import in an existing hive table or create a hive table before importing the dataApache Spark is a general-purpose distributed data processing and analytics engine Spark can be used in standalone mode or using external resource managers such as YARN Kubernetes or Mesos Spark works on the concept of RDDs (resilient distributed datasets) which represents data as a distributed collection Spark engine can apply operations to query and transform the dataset in parallel over multiple Spark executors Dataframes are an extension to RDDs which imposes a schema to the distributed collection of data Dataframes can be defined to consume from multiple data sources including files relational databases NoSQL databases streams etcLets look at a how at a basic example of using Spark dataframes to extract data from a JDBC source: Similar to Sqoop Spark also allows you to define split or partition for data to be extracted in parallel from different tasks spawned by Spark executors ParitionColumn is an equivalent of  split-by option in Sqoop LowerBound and UpperBound define the min and max range of primary key which is then used in conjunction with numPartitions that lets Spark parallelize the data extraction by dividing the range into multiple tasks NumPartitions also defines the maximum number of concurrent JDBC connections made to the databases The actual concurrent JDBC connection might be lower than this number based on the number of Spark executors available for the jobInstead of specifying the dbtable parameter you can use a query parameter to specify a subset of the data to be extracted into the dataframeOnce the dataframe is created you can apply further filtering transformations on the dataframe or persist the data to a filesystem including hive or another databaseNow that we have seen some basic usage of how to extract data using Sqoop and Spark I want to highlight some of the key advantages and disadvantages of using Spark in such use casesNext I will highlight some of the challenges we faced when transitioning to unified data processing using SparkIn the Zaloni Data Platform Apache Spark now sits at the core of our compute engine Data engineers can visually design a data transformation which generates Spark code and submits the job a Spark Cluster One of the new features  Data Marketplace enables data engineers and data scientist to search the data catalog for data that they want to use for analytics and provision that data to a managed and governed sandbox environment ZDP allows extracting data from file systems such as HDFS S3 ADLS or Azure Blob relational databases to provision the data out to target sandbox environments Apache Spark drives the end-to-end data pipeline from reading filtering and transforming data before writing to the target sandboxSome of the challenges we faced include: In conclusion this post describes the basic usage of Apache Sqoop and Apache Spark for extracting data from relational databases along with key advantages and challenges of using Apache Spark for this use case In the next post we will go over how to take advantage of transient compute in a cloud environment
hhCXVVNb6tnEJYds6MKSYr,You just triggered your Airflow DAG that sends data to your clients and you being confident that the DAG will succeed (Why will it not  you wrote it There is no way it can fail) you go to have coffee with your colleagues in Companys kitchen where the awesome Coffee Machine is waiting for you to serve the most delicious coffee ☕ You discuss how you can make the company better (Of course you dont talk about how awesome the new Avengers trailer is !!) And then you finally decide to go back to your seat being a smirk to see the green status on your DAG But wait…… what just happened your DAG failed  of course it was not your fault the letters DAG decided to change their order to DGA when you were having coffee hence your DAG failed At the same time your boss comes to you and asks you  Hows your work going? Was the data sent to all clients? And you wished that there was some way you could have received an alert on your mobile when you were having coffeeWell I can go on and on with this stupid story but the fact is you need alerting when your DAG fails so that you can take actions at the earliest Airflow has a built-in capability to send alerts on emails but well it gets lost in the pile of other 1000 unread emails And it is just easier to get alerts where your entire team has an eye on  SLACKThere are 2 ways in which you can integrate Slack with AirflowLegacy tokens are an old method of generating tokens for testing and development and Slack themselves dont recommend to use this but it is the simplest method  hence you can still use it but bear in mind that it can get deprecated anytimeFollow this steps: You can try this example in iPython or using Jupyter notebook as follows: However this is just an example to send a message on slack and not alerts on task failures Each task in Airflow contains a parameter called on_failure_callback (of callable type) to which you pass a function to be called when a task fails
2EdeYf7iQeKuZMLapMRpJt,Code quality has been a big topic of discussion in the last decade More people are talking about it now as more people have to read more code on a daily basis Reading other peoples code is hard We all know that! A lot of tools IDEs have made our lives easier by providing a standard framework to work within These tools enforce some level of sanity but a lot still needs to be done by the developerGood readable code saves more time than we think The first step I think to writing good readable code is to be stylistically consistent  be it indentation the spaces between two functions the way you declare variables even the way you import libraries All this has to be taken care of while youre writing your code for the first time After that there are tools to help you outA linter is one such tool that helps identify potential issues with the code based just on style guides naming standards typos and bugs too Linter is a part of the larger picture of code quality Most languages based on their stylistic grammar and syntactical structure have found standard ways of writing code in them Yes there are debates over tabs and spaces but most of the other debates are more or less settled Based on set patterns tools that enforce these patterns are available in the marketAs engineers have realized the importance of writing readable code This trend has spiked the use of fancy IDE features which help you write code better It could be as simple a feature as autocomplete (eg Intellisense) or a query beautifier Software developers data engineers data scientists infrastructure engineers data analysts  all these people spend a large chunk of their time writing code  code that would be maintained code that other people would read An average data engineer uses at least 2 IDEs  one for writing Python or Scala scripts for ETL orchestration etc the other one for writing good old SQLGood readable code saves more time than we thinkAs Python is more or less the de facto language for data engineers and data scientists lets talk about it One of the first proposals to use a style guide for uniformity and consistency was from the original authors of Python themselves It is PEP8 Since the PEP8 a couple of other major linters have evolved and taken over the market like  Pylint and Flake8Flake8 is essentially a wrapper around the following  PyFlakes pycodestyle and Ned Batchelders McCabe script Every major IDE code editor supports this For instance if youre using VS Code you can follow this link to understand how to set up or disable linting there Googles Python style guide is based on pylintIf you write code in Java Scala or any other language there are linters available for them tooAlmost all major SQL IDEs have an option to enable syntax checkers and query beautifiers and linters SQL is also for some reason one of the most abused languages when it comes to style consistency and readability Try reading someone elses queries I have Bothered by that issue a while ago I wrote about the benefits of using a style guide while writing SQLSimilar to Python there are a number of style guides for writing SQL too But there are different challenges when it comes to styling SQL Different databases data warehouses support different features of SQL some are specific to the platform being used It is therefore hard to come up with a unique style guide that works for Snowflake Redshift SparkSQL MySQL PostgreSQL MS SQL Server and Oracle at the same time Not only that but the style guides also have to be version-specific tooTry reading someone elses queriesThe most comprehensive and sensible style guide I could find was written by Simon Holywell There are a couple of other style guides like GitLabs SQL Style Guide and Mozillas owner SQL Style Guide Both of these are also well thought-through All thats good but how to check if your SQL code follows a given style guideThere are two well-maintained linters for SQL  sql-lint and sqlfluff You can choose one of them or write your own and integrate it into your CICD pipeline Lets now talk about the last bit I said  the CICD pipelineFinally linting should be a part of your CICD pipeline Executing the linter manually on your code is again moving in the wrong direction The idea of linting was to reduce the possibility of inducing human errors Running the linter in the IDE hence shouldnt be the only place where linting is run Just like automation tests linting has to become a part of the pipeline Before your code is merged to the next level it should be linted automatically  and if the lint fails the pipeline should failLinting is more than just checking for style it can be about whatever you want it to be under the purview of code quality You can write your custom checks inside the linter A linter can contain any kind of logic that you want The idea is that theres a piece of code that essentially goes through your piece of code to check if it follows the rules you have defined
RKER2BSEfWd4QjDTh2MiFy,There are only two means of ascertaining death One is by no means infallible and involves a brain scan and cardiogram That requires expensive machinery The other and the only one that is certain is putrefaction And that requires timeTheres a surprising amount of truth in this line from The Serpent and the Rainbow  biologically speaking indicators of life are fallible Slow shallow breaths can be hard to observe the human pulse can be affected by drugs and someone who just fell off the Titanic isnt going to have a normal body temperature even if theyre still alive There are a lot of cases a medical professional has to take into account and a lot that cant be anticipated  theres a need for adaptabilityIdentifying dead individuals in a dataset is also harder than you might think and theres still a need to take into account a number of cases An individual may stop appearing in a claims dataset because theyve died Then again this could also be because theyve switched jobs and their insurance changed In that case say Walgreens data would no longer have the customer and instead theyd be in an Aetna dataset And as of 2011 a state can prohibit the Social Security Administration from publishing (even for research purposes) who has died in that state  the result is that there arent even stable government data sets that can be used for researchA comprehensive dataset of deceased individuals is vital in order to understand the efficacy of new therapies or to be able to predict life expectancy from a procedure At Datavant weve aggregated and deduplicated data from the Social Security Administrations death master file and third party data from public and private obituaries to allow death data to be connected to the rest of the health ecosystemFrom a technical perspective some of the biggest challenges when building data pipelines are bad data (like an accidently inserted comma) and unintuitive edge cases (like the redaction of an incorrectly marked death) To ensure we had as robust a pipeline as possible we needed to create a robust test data set Even though our tests would be automated I felt there was high value in creating a readable test dataset When future developers need to make a change to the pipeline or are trying to understand how it works its important that theyd be able to quickly and easily understand whats going on For an easily accessible pipeline that runs every Sunday night with an increasingly high body count there was no other choice for test data than Game of ThronesBut really Game of Thrones provided the PERFECT test data as shown here by a smattering of applicable cases (**Spoiler Alert**): 2 Our data source also reports when someone theyve previously reported as dead is still dead but has updated data For example if date of death was previously reported wrong and now they have the correct date this amendment would be reported In the land of Game of Thrones where characters never seem to get his name right Dickon Tarly would probably be initially named as Rickon Tarly • Both our data source report fields like Title In a show where everyone seems to have badass titles and is constantly trying to claim more of them this made for very funny data My favorite example applies to both Robert Baratheon and Viserys Targaryen  whoever is King gets to be called King of the Andals and the First Men Lord of the Seven Kingdoms and Protector of the Realm in a very long-winded column of the test data set • If the same person appears as dead in both of our source data sets but some of their data (such as the day of death) is different we have to have a system to resolve these two sources of truth In Game of Thrones if the Lannisters reported the death of Myrcella Tommen and Joffrey their last name would be reported as Lannister: however an official at Kings Landing would have reported their last name to be BaratheonGame of Thrones made for a rich test data set that was very fun to create But the importance of testing our pipeline has impacts far beyond the realm of Westeros Working as an engineer especially with fake data gives you a too-easy opportunity to step back You can easily forget the context of what you work on and what it means for real people The truth is that I worked for weeks with a data set of almost everyone who has ever passed away in the US Every week when I ran our pipeline and received new data each row was more than just data entries It represented one persons life coming to a close and the lives of so many others forever changing as a consequence And so I do my best to acknowledge the truth of the data Im working with Every week I take time to honor the lives of individuals whose living life impacted so many people and whose life will continue to impact the future of life-saving research
UtqvnuWPMkXtAbLPFaDjNf,Its been three years since my first exposure to programming and almost two years since I started working professionally as a software engineer Ive been wanting to build side projects for a while (with a failed attempt late last year due to poorly defined scope) and now I finally got my act together! In this post I will share more about the app that I just built how I implemented it and my learnings from the processSo first up here are the objectives I set out to achieve before I started this project: Lets have a sneak peek of the end product before going into the details of its implementation Visit the app hereAlright lets dive into the implementation process and details now Some of the implementation details can get a little long so feel free to skip to my overall learnings at the endThe first step was to find a problem to tackle and define a clear scope To make sure I dont repeat the same mistake with my failed project last year I was more careful in the planning stage I made sure that I kept the scope small clear and realistic and I did research to ensure the feasibility of the idea In total I spent about 3 4 hours over two days exploring public data in different domains and conducting relevant researchI eventually decided on visualising Londons property transaction data on a map as I am interested in the property market I verified its feasibility by checking if the data can be accessed easily and if there is a map API that I can leverage on for map visualisation Also I made sure to define the scope of the visualisation very clearly which is as follows  to showcase how the number of residential property transactions vary across locations in London  and nothing more Having this clear scope proved to be beneficial later on as it kept me focused rather than jumping on every new idea that popped up along the wayThe next step was to identify key components of the app and tackle each component one at a time Each component was tackled in a similar iterative manner  identify sub-problems brainstorm and research solutions conduct proof-of-concept dive into implementation; repeat when necessaryKey components of the app: There is a lot more that can be done to improve and optimise the app but I am really happy to have achieved what I set out to do as follow: Please drop by the app (link here) explore around and leave a feedback or thumb up there
Y5LyNcK2rUJPGoFAmae4XS,Trade Me like many companies around the globe is leveraging the capabilities of the public cloud The countless services and the cost-saving opportunities are only a few advantages the cloud brings to businessesThis was a collaborative project between Business Intelligence and Data Engineering and this blog post - written by Maikel Penz (Data Engineer) and Lashwin Naidoo (BI Developer) - shares our experience building a data warehouse in the cloud with SnowflakeSnowflake is a relational SQL data warehouse provided as a service It runs on top of either AWS Azure or Google Cloud There is no infrastructure management involved which means that BI developers spend their time delivering business value rather than installing/maintaining and patching softwareThe scope of this project was to understand Snowflake and deliver business value using a targeted but large data set BI and Data Engineering played their roles to write the ETL and build the integrations around the technology to feed data and make it available to consumersTrade Me runs a Data Lake stored on AWS which is used as a powerful data source for teams around the business to build data driven products Our goal is to continuously push data from the Data Lake into SnowflakeEvents are logged from Trade Mes website and sent to S3 A Data Pipeline kicks off (lets call it Lake Pipeline) to process data files and make them available for consumptionThe Data Lake follows a tiered storage architecture We split the data into multiple S3 buckets with their own retention periods and holding data generated from different stages of the Lake Pipeline This model helps us save on storage costs and gives us access to both raw and processed data filesSnowflake provides a continuous data ingestion service called Snowpipe The service is managed by Snowflake and we only need to hit their service and pay for how much compute resource is needed to push data through their pipelineOur requirements: There are two approaches to set up Snowpipe to move files from S3 to Snowflake: 1 - Using a feature called auto ingest This feature uses a Snowflake managed SQS queue to ingest files Messages get pushed to their queue by adding it as an event to an S3 bucket or as a subscription to a SNS topic • - Programmatically calling the Snowpipe REST APIApproach 2 works better for us considering the requirements listed above The following image introduces the Lake Pipeline and shows how we plugged in a Lambda Function called Snowflake Ingestion as part of it You will notice that subscribing a Lambda Function to a SNS topic adds great flexibility as we can manually call the function to replay files add logic to pause the pipeline and all of this without interfering with the Lake PipelineAuthenticating users to Snowflake can be done in multiple ways from the simple user/password to more sophisticated methods like Federated AuthenticationSince the beginning we knew that maintaining users on Snowflake wouldnt scale well as more analysts started using the tool With this in mind the obvious preference was to authenticate users through Trade Mes existing user management platformAs mentioned above Snowflake provides Federated Authentication This means that Snowflake acts as the service provider (SP) and the company as the identity provider (IdP) Fortunately Trade Mes Single Sign-On is done through OKTA and OKTA provides native support for SnowflakeIntegrating OKTA with Snowflake is straightforward without diving into the details here is what needs to be done: 1 - On OKTA: Create a Snowflake application and point the sub domain to the Snowflake account • - On Snowflake: Update account settings to use a certificate and SSO URL generated from the first step on OKTA and enable SSO login • - once the above is done you will see this cool Single Sign On button on Snowflakes login pageGotcha: AD users are not syncd with Snowflake automatically which means that every individual needs to be created and set with the appropriate Idp email address on SnowflakeNow that people can log into Snowflake we need to make sure they are allowed to access the right stuffSnowflake provides a set of System-defined Roles to cover administration tasks like user management and billing However besides administration you might also want to configure granular access to objects users interact with like Databases and WarehousesFollowing the same security principles of most Databases on Snowflake you can configure read/write access to Schemas Tables and Views In our case we created an Analysts role with read access to the data and separated roles for ETL (data transformation/aggregation) and the Pipeline (to cover the move of files from the Data Lake to Snowflake)Warehouses exist for compute Think of them as machines that dont hold the data permanently but provide CPU and Memory to run ETL and AD Hoc Querying To help us better understand our cost we created a warehouse for Analysts one for the ETL and another for Backfilling Permissions were set to make sure an Analyst can only use the shared Analysts WarehouseETL was one of the big decisions to be made as part of this project Snowflake doesnt come with a built-in ETL tool but instead provides an extensive list of 3rd-party ETL partnersWith plenty of options in front of us we wanted to find a low-cost solution that would also check a few boxes: In the end we didnt find a one size fits all solution and instead decided to combine a set of technologies Lets now go through the ETL architecture in more detailSnowflake comes with a Python connector allowing us to use code to fire SQL queries to Snowflake to transform and aggregate data Python is a widely used programming language and lets us extend the ETL by using libraries and doing error handling out of SnowflakeHere we introduce Prefect Prefect is a workflow management system built to combine Python functions in a robust workflow where tasks can either run in sequence or take advantage of parallel execution Besides its power the other strong reason we chose Prefect was due to the the low effort to get up and running with itFinally we had to figure out where to run the ETL from and also how to schedule it Prefect provides its own scheduling system but this means we needed to have a machine running 24/7 To get around it here comes Docker AWS ECR and AWS batch • - We package our Python functions along with the Prefect workflow and generate a docker image • - This image gets pushed to AWS ECR (AWSs managed container registry) • - A job definition gets created on AWS Batch using the previously uploaded image AWS Batch dynamically allocates the resources to run the container so theres no need to run a machine all the time • - We create a schedule through Cloudwatch to fire jobs to AWS Batch and run the ETLThe following image summarizes our approachSnowflake uses its own in-house SQL That said we found querying in Snowflake intuitive and natural The query language is based off the ANSI SQL standard Snowflake has added to this to cater for analysis and working with semi-structured dataOne particularly nifty feature is the LATERAL JOIN This concept is similar to to SQL Servers CROSS APPLY and Oracles LATERAL joins but can be used to parse through semi-structured dataIn practice parsing through nested JSON and arrays can be tricky Its usually parsed via code outside of the database This involves iterating through the nested sets via nested loops and transforming it into tabular form for loading into the data warehouse The LATERAL JOIN in Snowflake works much like other joins letting you read through nested subsets by joining them and returning a standard result setParquet files from the Data Lake are ingested into Snowflake  We land the data into a variant column storing in a JSON semi-structured formatSnowflake profiles and optimizes reads of JSON formatted data This provided us with two approaches for transforming the data into facts and dimensions through our ETL • - The first option was the traditional approach of splitting out the data into its columnar form as transient tables Then running transformations on this tabular data • - For the second approach we created views on top of the semi-structured data This removed the step to copy out the data into intermediate tables before the transformation stepAfter testing both approaches we settled for the first option In practice running transformations off the semi-structured data was not as fast as tabular The slow down was more apparent as we scaled up to larger data sets and ran both sets of ETL end to endSnowflake provides JDBC and ODBC drivers which connect to a myriad third party clients and visualization tools These drivers cater for authentication via SSO and OktaThe Snowflake Worksheets query interface is more than adequate for running ad hoc queries If youre looking for additional query features though you could easily connect to your favorite non proprietary query tool We found that DBeaver worked quite wellTrade Me runs Power BI dashboards and reports through a Power BI Gateway Setting up Snowflake as data source was fairly straight forward Power BI much like other modern visualization tools has two options for reading data The first being import mode where all report data is cached into memory on a scheduled refresh  The second is direct query where queries are sent and data retrieved as users interact with the report Further to this Power BI and other visualization tools usually run parallel queries off the data warehouse to get at the data quickerVisualization tools optimize under the hood for fast report response times Coupled with the fact that Snowflake charges by query time tuning reports to minimize costs becomes a significant matter Its important to minimize query times by choosing the connection method suited for the report data set size and expected usage In our case we created a compute warehouse to monitor Power BI report usage and analysed the report query history to determine the most cost effective approach for each reportSnowflake is an excellent tool and seems to be a good fit for Trade Me We found it simple to interact with the UI and the flexibility to integrate Snowflake with external tools/pipelines put ourselves in a good place to expand and take on more projects like this in the futureThis project put us in front of new technologies and we believe that besides the continuous push to learn and explore new tools we also need to revisit our methods to be ready for the cloud Alongside the wins we did face some surprises specially around the cost incurred on large backfills and ETL Trade Me has a strong on-premises background and the move to the cloud will force us to improve the ways we currently operateTo sum up the overall outcome of the project was very positive as we now better understand the technology and the small frustrations made us learn where to be cautious with it in the futureOur personal feedback is based on the current state of Snowflake Their team is constantly working on updates and making releases that may in the future invalidate parts of the contentThe Prefect team has another product - currently under preview - called Prefect Cloud This is a management interface on top of Prefect that solves the problem of having to provide your own execution environment
7Qqasw3XQTzyHYnoURDBg5,When it comes to data engineering there are two common approaches for transforming and loading data The first approach is ETL(Extract Transform Load) The other is referred to as ELT (Extract Load Transform) One easily notices that the transform and load steps in the acronym TL vs LT are swapped However this small swap in wording has much larger implications in data processing To explain this lets start with a quick explanation of ETLETL stands for ExtractTransform and Load ETL is a high-level data processing and loading architecture The first step of ETL is Extract which refers to the process of extracting data from a source This can be through hitting an API to receive the data picking up the data from SFTP or S3 or downloading the data from a URI The next step is Transform which are the data transformations to the data to conform it clean it and aggregate it to its final stage This can involve modifying the data format converting JSON to CSV or other modifications such as joining the dataset to other data sources Finally the last step is load which refer to loading the fully transformed data into a databaseIn our simplified example we will build an ETL process for pulling and gathering data for an Ice Cream store We will need to hit an API to extract data about ice cream stores We need to roll the data up to a store level and store the final result in our databaseWe are hitting the API to get the JSON response We then flatten the JSON response using pandas and write the result to CSV This is a simplified example in real life schema validations data quality and data type checks would also be appliedIn our final table we need the total sales per store In order to do this we will need to perform an aggregation step We do this by creating a python script and using the Pandas library Our script will do the following: 1Converting datetime to date • Calculating the total price by date and store • Finally we want to save this result as flattened format in this case a CSV fileThe final step is to load the transformed data to the datbasaeFinally we will load our results to the target tableIn ELT(ExtractLoadTransform) the Transform and the Load components are flipped The main idea behind ELT vs ETL is that the transformation in ELT happens on the target database instead of through batch processing through a script or toolIn our case we need to modify the transform code to perform the transformation on the database To do this we converted our pandas transform statement in SQL We then issue this query on the database to create the final tableWe will need to make a modification to our load code to load the source file instead of the final fileUsually the transform step is not as simple as rolling up data with a group by This was purposely kept simple to drive home the main differences between the two processes There are also a number of data cleaning steps as well as data enrichment such as filling in blanks or performing joins to other datasetsThese cleaning steps can makes the process of going directly from Extract to Load difficult If the data is not structured properly many databases will reject the load Depending on how unstructured and/or unclean the data is there may be an intermediate step to perform a number of transform to clean the data Examples of these file issues include quote characters in the CSV dates formatted incorrectly data type incompatibility etcLuckily many database tools such as Redshift and Snowflake have options built in to handle many of these cleaning steps automatically cleaning the dataset on loadIn our second article we will evaluate the trade offs between ETL vs ELT approaches We will discuss what scenarios would benefit from ETL or ELT and advice on how to implement these architectures in the real world
XdUoykvspf33StiJHaxVYj,I am thrilled to welcome our new colleague Leo Thomas to Development Seed! Leo joins the team as a Data Engineer and brings with him expertise of developing cloud based tools to make geospatial data accessible to researchers and the scientific community He will work on our efforts to ingest catalog and distribute new commercial datasets with groups like NASA and other data providersLeo is no stranger to tech startup culture Before joining DevSeed Leo was based in Beirut Lebanon working for a drone information tech startup where he refactored their applications AWS backend by migrating to a NoSQL database and GraphQL API and redeployed all the resources with autoscaling Before that Leo was based out of his hometown of Paris France working for a 3D printing startupHaving lived around the world Leo has picked up the knack for languages and speaks French English and Spanish fluently and enough Arabic to order his favorite meal: beid qawarma He prides himself on his culinary talents but is currently focused on perfecting his sourdough starter a nod to the San Francisco region where he grew up
cW3XgQa3wQEod48yckBxsd,According to Google a Google Cloud Certified  Professional Data Engineer enables data-driven decision making by collecting transforming and visualizing data The Data Engineer designs builds maintains and troubleshoots data processing systems with a particular emphasis on the security reliability fault-tolerance scalability fidelity and efficiency of such systemsThe Data Engineer also analyzes data to gain insight into business outcomes builds statistical models to support decision-making and creates machine learning models to automate and simplify key business processesA Google Cloud Certified  Professional Data Engineer has demonstrated in Googles assessment their ability to: You can find the full exam guide hereFor me this certification includes everything you need to know to work on data projects with (and even without) Google Cloud Platform You will need to prove your knowledge and expertise on the following topics: As you can see the range of topics which is covered is very broad The certification does go into technical details Make sure you are familiar with the basic coding needed for each GCP productDo you want to get yourself a cool badge like this one and prove to the world your knowledge on all modern data analysis systems? Let me walk you through the resources I found useful while I was preparing for the exams in November 2018The type of training you need depends also on your background Below are a few suggestions from a guy with a technical background who worked for a little more than 10 years on data collection and data analysis projects but with limited experience on Google cloud (Im talking about me by the way)There is a lot of high quality studying material online that you can use to get yourself familiar with the necessary topics These are the ones Ive used The order of appearance is the order I would suggest to use them while studyingI addition to everything I mentioned above I found the following links really helpful when I was working on my study plan: I hope this will help your advance your GCP skills and save you some time as you are familiarizing yourself with Google Cloud Platform! You can also check the rest of my articles on my blog (https://wwwtzamtzisgr) if you are interested in digital analytics
jcmCGoZ5eQejHLcraS5yZ8,"Every vehicle on the road has a 17-digit vehicle identification number (VIN) attached to it as illustrated below But when someone buys or sells a vehicle they want to know more information about the vehicle That leads to the process of VIN decoding The data engineering team at TrueCar oversees the VIN decoding service which is one of the most heavily used APIs at TrueCar The service runs a given VIN through the process of matching it to a specific make model and potentially trim Our article will focus on why VIN decoding is important to our business why its a challenge a high-level overview of the core VIN decoding process and finally a technical overview of the VIN decoder at TrueCarVIN decoding is very important to TrueCar It is used across our data pipelines at TrueCar and almost every application uses VIN decoding in some mannerSo what makes VIN decoding difficult? The 17-digit VIN itself is not always sufficient to decode the trim Trims of a vehicle are how Original Equipment Manufacturers (OEMs) identify what set of features is standard on the vehicle For example the L trim of a Toyota Corolla may not have leather seats as the default but the SE trim may The drop down menu on the image below shows the trims for a 2020 Toyota CorollaAnd why is it important to decode to a given trim? Pricing Pricing differs between trims Depending on the vehicle price differences between trims can range from a couple hundred to even tens of thousands of dollars For example an Audi RS 7 performance trim is $131675 while the standard trim is $114875 The importance of VIN decoding led TrueCar to initially have multiple VIN decoders before settling on one""Lets go through an example for VIN 1FTEW1E42KKC69677 The VIN goes through the regular VIN decoding process but maps to ten trims as shown below We then go to the next step which is build data In this instance we have build data to decode to the correct trim The trim selected is Lariat SuperCrew 55' Box 4WD If build data information is not available we end the VIN decoding process and calculate the VIN decoding confidence scoreBased on this understanding of the challenges importance business use cases and details of our VIN decoding process we will now dive into the technical details of our one and only VIN decoder in TrueCarLots of work has been done to make UVD the TrueCar VIN decoding service operate the way it does today UVD is currently the most heavily used API throughout the company Average response time is 10 20 millisecond for each request decodes to one trim 93% of the time and can handle up to 15000 requests per second As mentioned before it leverages multiple data sources from different providers which we load into memory upon deployment The service does a look-up against HBase We have designed the tables in such a way that all reads to HBase are point lookups which on average take milliseconds which explains how the response time can be so quick We can easily scale horizontally should we ever need to in the future When there are new updates to the cached files UVD uses the wormhole design Whenever files that UVD uses have any update a message is sent to UVD which will reload the files for every running instance of UVDVIN decoding is a bit tricky since much of the time the VIN alone is not sufficient Using different data sources can help and that is how TrueCar has been able to decode to the correct trim 93% of the time Hopefully this post has given you insight into how TrueCar tackled VIN decoding problems how VIN decoding is used within the company why it is important and where we are today with UVD"
dAyiwYNpa8R3zGXtGtcEyg,This is part 3 of a series of posts about how Dwelo software uses Twilio Sync as its IoT communications platform If you missed part 2 you can find it hereIn the previous posts of this series we described how Dwelo uses Twilio Sync to manage state synchronization for all of the IoT devices that are found in Dwelos smart apartments We shared how sensor state is represented in the Sync cloud by using Sync maps which are dictionaries of key/value pairs that record the state of each IoT device We also shared how we use those maps to represent commands to the devices in the form of desired stateNow in this post we will share what we do with those state changes once the Dwelo cloud receives them from SyncTwilio Sync notifies connected clients about changes to Sync maps using a publish-subscribe model And Twilio provides libraries so that developers can create clients that subscribe to map updates from the Sync cloudHowever due to the cost of connecting each client directly to the Sync cloud and subscribing for updates  eg one connection for every instance of the Dwelo IoS and Android apps being used by our customers  Dwelo chose instead to have a single entity in the Dwelo cloud receive all Sync notifications via a webhookWe created a service that receives the Sync map updates from the Twilio cloud and then publishes the updates internally for our clients to consumeThe sync-reflector service was created to provide a mirror of all activity that occurs in the Twilio Sync cloudWe use the sync-reflector to: The service is comprised of the AWS components shown here: When the Sync cloud has a change to report it publishes the event through a webhook to an API Gateway The gateway then passes the update event to a lambda function for processingThe lambda first authenticates that the source of the event was the Twilio Sync cloud using a helper library from Twilio The helper library hashes the incoming request using HMAC-SHA1 and with the Dwelo auth token as a key to validate against a special header in the request called the X-Twilio-SignatureOnce the event source has been authenticated the lambda publishes the event to an SNS topic and also puts the event record into a Kinesis Firehose for our analytics engine to consumeWhen publishing to SNS the lambda packages a set of message attributes that SNS subscribers can use to filter on The set of attributes are: For lambdas that subscribe to the SNS topic they can create a filter using the attributes above in order to receive a pre-filtered list of events to processFor example if a service wanted to be notified of all battery level events so that it could send a notification email to users when the reported level was low the service would register this filter: This would cause that service to receive events from all devices in the system that reported BatteryLevel Other services might want to subscribe to map_item_created events so that they can take action whenever a new device is created in the systemKnowing how customers are using their devices is certainly a must-have feature of large-scale IoT systems At Dwelo every light that turns off and every door lock that locks is carefully recorded in our analytics engineAfter the events are reported and processed by our sync-reflector service they are put into a Kinesis Firehose The Firehose bundles the events together and sends them on their way to data warehousing provided by an Amazon Redshift cluster Once in Redshift the events are available for queriesDuring each records travel through the analytics engine we mutate it into a format that takes advantage of the columnar storage properties of Redshift Besides normalizing field names this also includes extracting elements from nested data that we might want to search on later and putting those elements into their own fields For example door unlock events are mutated so that the identifier for the user code that unlocked the door is extracted and available for queriesStoring our data lake in S3 gives us tremendous flexibility in manipulating the data as well as in controlling its lifetime Also it provides great *replayability* into our data warehouse When we first developed our analytics solution we chose AWS Redshift to leverage its proximity to the data source as well as to take advantage of its out-of-the-box pipeline Recently however we have migrated to Google BigQuery This was optimal for the large analytical queries that we need to run But we were able to do so without much difficulty because of our existing S3 infrastructureWith Twilio Sync as our source and using some simple AWS components we created a highly available pub/sub messaging service that has configurable streams for all of our sensor state changes as well as a robust and highly scalable data warehousing solution! These pipelines have enabled Dwelo to receive and process more than 5M sensor events daily with very little configuration or investment in infrastructureCheck out the 4th and final post of this series where we describe our service for handling all of the IoT device state that is received by the Dwelo cloud
Z2pbtXH3ntedJBqZoKjidx,In this blog we will walk through automation script for deploying Azure Data Factory (ADF) data pipelines & its entities (ie Linked Services Data sets) to copy data from Azure virtual machine to Azure Storage accountIn this article we will perform following steps: Prerequisites: Azure PowerShell: Azure PowerShell is a set of cmdlets(lightweight command used in windows powershell environment) for managing Azure resources directly from the PowerShell command line It simplifies and automates to create test deploy Azure cloud platform services using PowerShellBefore starting to install and run the latest Azure PowerShell module use the following command to sign into the Azure portal It will prompt you to enter username & passwordAzure Data Factory is a cloud-based data integration service that allows to create data-driven workflows in the cloud for orchestrating and automating data movement and data transformationThe Data Factory services allows us to create Pipelines which help to move and transform the data Pipeline can have one or more activities to perform move/transform dataFinally we can now see Data Factory created on azure portalIn the next steps we will create data pipeline & its dependencies for copying data from azure virtual machine to Azure table storageBefore creating a pipeline we need to create a data factory entities ie in order to copy file data from Azure VM directory file path to Azure Storage Table we have to create two linked services: Azure Storage and Azure VM File system Then create two data sets: Azure storage output data set (which refer to the Azure Storage linked service) and Azure VM FileShare Source data set (which refer to the Azure VM directory path linked service) The Azure Storage and Azure VM File system linked services contain connection strings that Data Factory uses at run time to connect to your Azure Storage and Azure VM respectively and also in order to run copy activities between a cloud data store we have to install Integration run time The installation of Self hosted integration run time needs on an on-premise machine or an azure virtual machineLinked Services: Linked services are much like connection strings which define the connection information needed for Data Factory to connect to external resources ie ADF manages activities across a variety of data sources such as Azure Storage Azure SQL Azure Data Lake SSIS on-premises systems and more To communicate with these sources ADF requires a connection definition known as a Linked ServiceCreate Azure VM Linked Service: AzureVM Linked Service is to link VM Directory file path to our data factory This is used as Input (Source) store This linked service has the AzVM connection information that the Data Factory service uses at run time to connect to itCreate Azure Storage linked service: Azure Storage Linked service is to link storage account to our data factory DemoADF This is used as output(sink) store The linked service has the Storage account connection key information that the Data Factory service uses at run time to connect to itWe can now see two Linked Services created on data factoryData set: Data set is a named view of data that points /references the data want to use in our activities as input and output Data sets identify data within different data stores such as tables files folders and documents ie a data set is not the actual data It is just a set of JSON instructions that defines where and how the data to storeIn our scenario we need to create two data sets one for pointing to Azure storage linked service(output) & another one pointing to AzureVM Linked service(input)Create Azure VM FileShare dataset: 2We can now see source & target data sets created on data factoryPipeline: A pipeline is a logical grouping of activities that together perform a task The pipeline allows you to manage the activities as a set instead of each one individually ie pipeline operates on data to transform & to analysis data on top of itWe can now see pipeline created on data factoryTo invoke/run the pipeline we can add schedule triggers or event triggers to demonstrate now we will invoke the pipeline using powershell cmdletsPipeline invoke generates a run id for each specific run & we can see successfully executed pipeline run from ADF monitor tab & final target output on Azure storage accountAlso the data on the final destination target table on Azure Table storage account: Summary: In this blog we have seen Azure Data Factory concepts and the ways we could create data pipelines Linked Services data sets and invoke using PowerShellhttps://docsmicrosoftcom/en-us/powershell/azure/?view=azps-33https://docsmicrosoft
Vc5Q2pH7cGEwayXULvheYy,Recently I had the opportunity to sit down with Dr George Watts at Egens Learning & Innovation Series A behavioral scientist who has spent 20+ years in executive coaching & personality awareness Dr Watts has built a career around teaching people to understand their innate strengths and how to leverage them in a professional settingDetermining what ones natural strengths are is a key factor in finding something you will be successful at Building a career around your strengths leads to long-term passion and dedication towards the job Work fatigue is something all professionals encounter at some point during their careers However this can be mitigated if ones career is based on something you have a deep interest and aptitude forAt the same time simply finding ones professional calling is not enough Dedication and desire to continuously improve yourself is key to reaching the next level Dr Watts offered himself as an example; even after 20+ years working as a behavioral scientist he still takes 2 3 hours every week to academically mature his strengths Instead of expending effort on your weaknesses you should nurture your talents in order to achieve higher levels of excellenceAs someone who is in the earliest stages of career progression I found lots of value in the information Dr Watts presented His strategies are tailored around the Five-Factor Personality Model which holds that there are 5 core personality traits found in every person These traits can help predict the degree of social and professional success an individual may experienceWhile no single trait is better than another some personality types can indicate an aptitude for certain career paths When I spoke one on one with Dr Watts his assessment was that my top two personality traits were agreeability and extraversion This was unsurprising as these factors lend themselves well to someone working in business/communications due to the often collaborative nature of that spaceAt the same time I was curious to find out what personality traits are important for those who choose to work in more technical positions I sat down with Nishant Sharma one of the Egen data engineers in attendance to find out what role professional development has played in his career Nishants top two personality traits were open-mindedness and emotional stability; both of which he believes to be essential for developers and engineers Software engineers and architects work in an environment where they constantly face failure Its vital that anyone contemplating entering this field has a rational outlook and can remain level-headed in high pressure situations Nishant also values open-mindedness as a critical trait for an engineer as unorthodox solutions to complex problems are often requiredHowever having a dominant personality type does not mean your career options are limited During our workshop one data engineer identified as conscientious while another was extroverted Which personality trait makes one a better engineer than the other? Trick question- the conscientious data engineer is more likely to help his customers by telling a story with data and the other one is more likely to form a team to find a collaborative solution Neither approach is better than the other as having a diverse team with a variety of strengths is a workplace advantageDr Watts presentation was valuable for helping employees understand where their natural strengths lie For those new to the working world identifying your dominant personality traits can narrow down the areas in which you are particularly suited to work In our fast-paced world it may be hard to find the time for professional development- but if you can focus on refining your strengths rather than obsessing over areas of weakness Ensure you are presenting the best version of yourself by developing & maturing your innate talents: both academically & practically Using your strengths strategically in your day-to-day tasks as well as demonstrating sincere dedication towards self-improvement will be recognized & lead to upwards career progressionEgen is strongly committed to providing its employees with the best resources for personal and professional enrichment Every 6 weeks we host a Learning & Innovation Series Talk at Egen HQ We invite speakers & industry leaders to present technical talks business insights and professional development seminars to ensure employees are continuously growingAbout DrDr George Watts is a behavioral scientist & executive coach with over 25 years of experience He is the Chairman & founder of Top Line Talent a professional training service dedicated to optimizing career success & unlocking innate potential
6rpAHiKx5BQSUDwc9NSpz6,Windows PowerShell is a tiny yet powerful utility tool used by DBAs & Microsoft developers all around the world It can do everything from creating folders to moving files to restoring databases by using PowerShell Scripting: a compact scripting language that simplifies life for us developersIn\xa0this\xa0blog I would love to share my learning on this cool PowerShell feature for excel import using ImportExcel moduleHave you ever heard of the traditional Import/Export Wizard in MS-SQL? Its an add-on in MS-SQL that allows you to manually import flat files into your database Its a manual work for loading files into SQL using a wizardInstead imagine that you want to automate the process of loading Excel data into SQL Server\xa0How would you do it? Some people might suggest using SSIS to load the data easily on SQL Server Theyre entitled to their perspectives but in my opinion SSIS can be a bit tricky when it comes to loading Excel dataImagine creating an Excel based SSIS package & loading new data into the database on a weekly or daily basis & every single time the dynamic Excel file is updated Its possible to handle this in SSIS but its more complex than it seemsWhat else can we do? What are our other options? Thats where Powershell comes in: its a tool which can quite literally take care of almost everythingI dont mean to demean other options or even SSIS in general I have worked on SSIS & I do appreciate how it can simplify ETL tasks These tasks cant be done in PowerShell that easily but Im just here to provide an alternative way to load Excel dataLets start with a very interesting tool for executing PowerShell Scripts: PowerShell Integrated Scripting Environment (ISE) If you have not tried this & still execute commands in a command prompt you should give PowerShell ISE a try In this article we will examine an alternate option for securely loading Excel files into the SQL Server databaseHeres a module which you should be aware of when you are loading excel data into SQL: ImportExcel A fabulous module which has a bunch of functions of which the one most important function for us here is ConvertFrom-ExcelToSqlInsert This function basically accepts your target table name & excel file path It can also convert the Empty strings to NULL & can use MS-SQL Syntax which is pretty cool! Heres how you use it: When you run this above piece of code youd get a bunch of insert statements written in a format suited for MS-SQL on a per-row basis You would need headers in your excel spreadsheet The header names should match the column names in the SQL TableWhat is a per-row basis?  Imagine in your excel spreadsheet you have the first row as a header and the following rows as data for those headers youll have something like this: Impressed? To be frank so was\xa0IYoull generate your insert statements and will run those statements all at once in PowerShell using Invoke-SqlcmdAt this point youre probably wondering how it will handle large amounts of data How does the Import-Excel module compare to SSIS & which option is better for efficient utilization? I havent yet tested it for large amounts of Excel data but its important to remember this method focuses on efficiency & simplicity rather than quantityTo answer the inevitable follow-up question- yes you are able to dynamically select any files in PowerShell rename it to something generic and use that file to load data into SQL Server Its ingeniously simpleNow lets talk about another key aspect of writing code: security Its important to remember that were all prone to cyberattacks when writing a PowerShell Script or any other code In this case a cyberattack is definitely possible but if proper encryption & decryption logic is followed the risk can be mitigated or even eliminated If you are going to connect to SQL Server you need to guarantee security How do we do this? Here is a sample encryption-decryption method which you can followIf we are going to execute SQL queries using PowerShell we need to encrypt the SQL credentials by using Powershell & SQL Agent jobWhy SQL Agent Job: Suppose you want to encrypt a password used in PowerShell and you want to schedule this PowerShell script then you have to encrypt the password using SQL Agent jobWhy? The reason being: the person who encrypts the password is the only one who can run the decryption on it\u200a\u200aat least in PowerShell To avoid user-dependent processes we should always use SQL Agent jobs for encryption & decryption If you want to encrypt it as a Windows user you run the PowerShell script in ISE (using the following script) and can run the decryption script with the same Windows credential you use to log into your systemBelow is a PowerShell script with a sample name: encryptPasswordThe above script accepts the password as a parameter & encrypts the password in a particular location You can run this Powershell script in a PowerShell Integrated Scripting Environment (ISE) or a SQL Agent jobWhen you use PowerShell ISE it will encrypt the password sent in as a parameter Remember- only your login will be able to decrypt it so if you end up leaving the company and someone else wants to decrypt the password theyre out of luckA good way to deal with this problem is by creating a one time SQL Agent job: Encrypt Password jobThen create a new step called Test Encryption and use the above PowerShell script to pass the password as a plain text: Click OK & create the job Next execute the job and watch a successful password encryption file appear at the location you chose (NOTE: This location has to be provided in PowerShell script & it has to be accessible by the SQL Agent Job user)Remember: This is a one-time thing You should delete this job once the encrypted password file is generatedThe above PowerShell script is a sample that executes SQL Command I have used Invoke-Sqlcmd here but you can use any relevant SQL command The only difference is the first two lines of code which are essential for decrypting the passwordWe use Marshal Class in our script as this class uses a method SecureStringToBSTR(SecureString) This method allocates an unmanaged binary string (BSTR) and copies the contents of a managed SecureString object into itThe other method PtrToStringAuto allocates a managed String and copies all or part of an unmanaged string into itThe below query is used for user-specific decryption- so only the specific user who encrypted the password can use this command: Marshal Class Reference: https://docsmicrosoftcom/en-us/dotnet/api/systemruntimeinteropservicesmarshal?view=netframework-4Since our password was encrypted in SQL Agent job this PowerShell script should also be passed in an existing SQL Agent job as a step These methods will allow you to successfully decrypt the password at runtime without other users knowing the actual password for the fileThank you doug finke for this super cool module ImportExcel as well as Casey Catasus for helping me identify the importance of encryption in PowerShell
C99PdVLv3MhMZy4u8Z5C9G,Enterprises often perceive the consolidation of data in Data Lake as a strategy for deriving value out of data Within the same context ingestion of data is kicked off without understanding who or how data will be made available to the rest of the organization Enterprises even fail to consider operational aspects of data like security governance quality etc which if not solved earlier can leave the whole initiative stranded resulting in the loss of millionsHere is a series of blogs divided into three parts where each part focuses not only on the challenges faced by organizations in democratizing data but also discusses about creating a data-driven culture across the organization: Part 1 the first in the series discussed challenges principles and success factors which are the key pillars of a successful data strategyPart 2 focused on setting up operational aspects like Security and Governance It also covered the comparison scenarios and patterns where a variety of products (Open Source/ Proprietary) can be leveragedPart 3 the last in the series discusses/compares varied tools/technologies and patterns for setting up end-to-end a DataOps pipeline for enabling data-driven organizationsIn Part 1 we discussed the key pillars for setting up enterprise data and Part 2 was focused on operational aspects of enterprise data which need to be considered for setting up a secure environment for data federation In the last part of the series we will talk about the strategy tools and technologies needed for enabling data-driven organizations Let us discuss the various aspects which need to be considered for enabling data-driven organizationsEnabling a data-driven culture requires the setup of an end-to-end automated lifecycle/journey of data within an organization which is also known as DataOps DataOps not only expedites the process of sourcing/ingestion and curation but at the same time it also ensures that the sanctity of data is maintained and ready dashboards are available for tracking/auditing and discovery of the enterprise data There are 3 main aspects of DataOps: 1 Data Engineering: Automated Data Pipelines for ingest/transform/curate/consume and discovery and insights • Data Governance: Metadata management and maintaining the quality and lineage of data • Data Operations: Monitoring the usage/consumption of data and providing ready dashboards for further analysisThough all these functions solve different problems they have to go hand-in-hand to define the journey of data Unavailability or lack of automation or gaps in any of the above functions will not only slow down the whole process but can also raise questions on the quality of dataThe picture below shows the lifecycle/stages of data within the enterprise and the role each of the data functions needs to perform in each of the stages: Let us move ahead and discuss each of these data functions and tools available for efficiently performing various tasks in the lifecycle of dataData Engineering: DE (Data Engineering) talks about the processes and tools for ingesting curation/transforming and consumption of enterprise data DE teams should provide self-served automated tools which can be leveraged by the data stewards for ingesting the data into the Enterprise Data LakeData Governance: DG (Data Governance) is the process of providing controlled access to the enterprise data based on ACLs and RBAC which should also be auditable and traceable It also defines the overall process of defining and managing enterprise data Refer to Part 2 for more detailsData Operations: DO (Data Operations) is one of the important aspects of overall data management and DataOps It is entrusted to for defining various dashboards for Data reconciliation auditing tracing and viewing of the curated form of enterprise data along with its qualityConsidering all the above aspects here is the quick glimpse of the products/tools available in the market along with their capabilities to support various functions of DataOps It also highlights a few other aspects of DataOps which are influential in deriving the decision for products chosen for implementing DataOps in any enterpriseThe above comparison matrix clearly states that there is no product/tool (except IBM) which is capable or self-sufficient to meet all the functions of DataOps for an enterprise But the good news is they do provide seamless integration with the products which specializes in one or the other functions of DataOpsThough IBM provides a whole lot of integrated product suites for managing end of the data lifecycle it comes with a cost and should be justified with the value provided by the use cases enabled by the IBM suite of productsDataOps is not a one-time aspect As businesses evolve over a period of time the requirements and capabilities provided by DataOps should also evolve Organizations need to keep measuring the effectiveness of their DataOps capabilities/functions Based on the findings appropriate actions should be taken to ensure that it is still relevant and providing value to the organization and its people Every organization is unique and may require some customization to the measuring criteria But to begin with there are a few industry-wide accepted frameworks like DCAM DMM and DMMA which can be leveraged for assessing the maturity of an organization in managing dataIt is essential for an organization to establish a successful data strategy for democratizing enterprise data which needs to be automated by leveraging appropriate tools and technologies All 3 parts in the series define the various aspects to be considered and implemented for setting up a successful data strategy which will also enable data literacy across the organization At the same time it is also essential to understand that nothing happens one fine day Big bang approaches usually dont work but a value-based approach does provide sufficient reasons for the investment in data strategy
LCMQ6kyWiMhBddNQNNcUPS,In this post I describe the challenge of Data Warehouse consumption by users and systems and how we have solved it in Applift In case you cannot wait until the end spoiler we have developed our own homegrown application that is now open source for you: https://githubAs part of a data strategy of an organization one of the critical activities is to harmonize a variety of data sources into a single database (ie sources from system such as production marketing sales etc) This database is called DWH (data warehouse) and allows one to easily access all data in one centralized location It is fast in data retrieval contain meaningful business terms and is a one stop shop for all of the answers To build a DWH data is extracted loaded and transformed from APIs databases files and other data sourcesAccessing a DWH is usually done via query language (SQL) or BI tools such as Tableau and is performed by analysts and business users As the DWH becomes a more centralized tool and the single source of truth in an organization there is a growing need to access it not only by Analysts and business users but also by engineering and app developers The increased usage brings new challenges and a question whether traditional solutions such as SQL BI tools and standard API are the right interfaces to access the DWHConsuming data from the DWH with traditional interfaces such as SQL BI tools and API development is problematic for several reasonsTraditional API solutions require engineering effort to develop (ie Java development) which leads to slow development An endpoint needs to be well defined (by the product owner) and a series of tests need to be conducted before the end point can be consumedTo solve these problems we need a way to give back the power to data owners in the organization so they can decide how exactly the data needs to be consumed We need a simple way to do so in order to allow the team of data owners (BI developers and Data engineers etc) to focus on their main task: reliable harmonized and centralized dataDWH API is a product that allows a user to create an API endpoint (exposing a data set) via web interface using SQL only This end point can be consumed like any other REST API solution and requests history can be trackedCreating an endpoint: Configuring a new endpoint is done by writing SQL A developer can do that purely via web interface First the developer configures the database access by providing the database credentials (currently support Postgres) Then the user define the endpoint by creating a SQLConsuming an endpoint: Consuming the end point is done via REST API  GET method The consumer passes the endpoint uri token filters paging and other parametersTracking: The system tracks every API call request The log information containing which endpoint was accessed by which token from which IP the parameters requested duration of execution etcThe DWH API became a centralized solution in Applift with more than 50 endpoints serving internal and external systems and users It is used by internal system who need DWH data for exposing performance and grading analysts who access deep historical data and external partners who want to see online feedsWithin a few minutes/ hours an analyst or a developer can create a new endpoint While not only reducing development time dramatically the DWH API has increased flexibility of making small modificationsAs a result of the DWH API implementation more questions can be answered integration can be done faster and better decisions are takenWe have been using DWH API for the last 2 years in Applift and decide that it is time for other data owners in organization to benefit from itDownload it here: https://githubInstall it on the cloud or on your premise and connect to your own database or data warehouse to easily expose your data via API to your consumersGive us feedback or contribute to improve this solution
H8fnpvkNL89AAjPUbqoPAz,As a part of the Cortex Data Lake platform team at Palo Alto Networks we are building a Batch processing pipeline to support running different application jobs written in Spark PySpark Hadoop MapReduce etc at scale running at regular intervalsFor this purpose we have identified Apache Airflow as our choice of Workflow Engine to support the scheduling of these application jobs at scaleWhen we started on our journey to explore the Apache Airflow on Google Cloud we experimented with a few methods for running Airflow on GKE Along the process we identified the need for a self-managed solution Lets start with a little background on these components and how we got to our current solutionApache Airflow is a Platform created by the community to programmatically author schedule and monitor workflows Since Airflow pipelines are configured as code they are dynamic in nature Airflow provides the ability to define our own operators extend the libraries to suit our environments and scale for a larger number of jobs with the support of the message queue to orchestrate any number of workersIn the process of experimenting different ways of running Airflow on Google Cloud we have identified certain flexibilities/requirements that led us to start exploring the necessity of a Self Managed Airflow Solution: Due to the benefits and flexibilities listed above we currently build our own custom Airflow Docker images based on top of open source available base images and deploy these Airflow services as Kubernetes pods on GKE using Spinnaker pipelineAlthough there are several benefits and flexibilities to managing our own Airflow deployment solution scaling it for a large number of DAGs and tasks was a more critical and difficult problem to solve for our continued successThe Memory and CPU graphs for the pods below are for a 10 DAG Run setupDatabase Connections : Prometheus based Metrics for monitoring the queue size and scheduler Delays: We operated on the below configuration by this time in our scaling process: Usage Graphs and Results for 65 DAG run setup each running hourly : Worker Scaling - Identifying how to scale worker : Worker Scaling - Scaling workers in number : GCSFUSE : LOGS : REST API plugin : Airflow has now become an important part of data infrastructure at Palo Alto Networks and we are in the process of aggressively scaling it to support 1000s of DAGs We hope this post helps anyone who is looking to explore Airflow and its deployment using Kubernetes A big shoutout to our Batch Compute team (Wenfu Feng Nimit Nagpal Chandan Kumar Tejdeep Kautharam Punit Mutha Benoy Antony) in achieving great success with building data processing solutions that empower our customers
asUDsG82FsS92wM5edYpJf,Maxime Beauchemin summarizes hard-won lessons about building repeatable and reasonable data processing systems: Functional programming brings clarity When functions are pure  meaning they do not have side-effects  they can be written tested reasoned-about and debugged in isolation without the need to understand external context or history of events surrounding its execution
LTiWajkhHBxWLUkaCs29qM,Ive been very lucky to be an intern at Optimizely for two summers now as part of their talented Business Systems Engineering team This teams mission is to enable Optimizely to take action on its own internal data Considering Optimizelys mission is to let our customers take action on their data it only makes sense that we have methods to make data-driven decisions ourselves The Business System team builds rock-solid performant data pipelines that take messy raw data streams and transform them into a neat homogeneous OLAP schema in the Data WarehouseYou might think why does Optimizely need a Data Warehouse or a Business Systems team? As a startup it makes sense that we purchase or subscribe to a product like Zendesk a helpdesk system rather than hire a team of engineers to build our own Optimizely uses a myriad of these products but this creates a problem: useful data about our customers is siloed inside these products There is no easy way to gain insights about our customers across all the systems we use Furthermore most external systems we subscribe to do not have the functionality to write complicated queries that an analyst would need to perform A Data Warehouse allows us to elegantly solve both of these problems Being data-driven is so important to us that there are TVs around the office with charts which are powered by the Data WarehouseOptimizely Engineering is insistent that interns become part of the team as another engineer not simply an intern This is unlike most other places where an intern is assigned work to do in isolation Intern work at Optimizely is code reviewed just like every other engineer and is (hopefully) eventually pushed into production Interns follow the same engineering processes at Optimizely that full-time engineers follow I worked on several exciting and high impact projects during my tenure hereOne of the most impactful projects I completed was overhauling the Zendesk Data Pipeline Historically this pipeline caused a great deal of grief to the team with frequent failures affecting the ability to monitor Success service stats in real time I re-wrote it using a clean object-oriented structure new API endpoints and extending the functionality to track SLAs from our Success staff Tracking these over time is critical to Optimizelys Customer Success team especially as the company prepares to roll out an exciting new initiative in the near futureAnother project I worked on was implementing a RESTful API called SpeedFeed that is being used to interview full-time candidates in a take-home assignment The SpeedFeed API assignment more closely represents the daily work of a data engineer compared to a traditional phone screenI also worked on building several new data pipelines Two of these included Google Cloud and Amazon Web Service costing that allow Optimizely to track hosting costs at a granular level Another one was for Docebo our e-course management system that allows analysts to answer a plethora of important questions about customer engagement with our education platformOptimizely also enables engineer creativity to be showcased during Hackathons I worked on two small projects during a special intern hackday The first project came from an insight I gained using the Data Warehouse I determined by using the Levenshtein string distance function that many customers likely misspelled their email addresses when signing up for Optimizely To solve this we integrated with Mailcheckjs which offers suggestions for email misspellings A second project involved increasing the security of our product by integrating with Castleio which detects suspicious login activity We know a security incident can end the whole company which is why we try to be as proactive as possible for example by adding 2-Step VerificationOverall I had an excellent two summers interning at Optimizely There are plenty of fun activities available in the Bay Area Summer intern trips included an SF Giants Game miniature golf escape the room group volunteering activities and weekend hiking excursions I strongly recommend any great aspiring software engineer to intern here at Optimizely Optinauts have a bright future ahead with a world-class engineering team coming up with brilliant solutions that delight our customers If this sounds interesting to you check out our careers page
2GP6BKQvPVAwMkUx4oFd2V,Well wait a while you think… feel free to use Google image searchCongratulations! You now have pictures of spreadsheets ones and zeroes or maybe even spreadsheets filled with ones and zeroes in your head With this sort of pop culture imagery its not surprising that data may seem very serious and abstract trapped in computers and disconnected from the real worldOne key discipline within data engineering is ETL (Extract Transform Load) Simply put this is the process of moving data from one place to another while making changes in the middle Production ETL pipelines do not reveal their inner workings in ways that lend themselves to spectating unless you count watching progress bars of file downloads as entertainmentIn contrast a Rube Goldberg Machine is a device that performs simple tasks in hilariously complicated ways You have seen them in music videos or science museums and many engineering groups host friendly competitions around building them (see: zipping a zipper erasing a chalkboard toasting bread) Creative inefficiency is very fun to watch These devices are the engineering embodiment of appreciating the journey over the destinationIf this combination is intriguing to you you should have joined the enigmachine projectOur team of data wranglers gathered to build a contraption for processing data about the world Over the course of several days we designed a process that required digital and physical stages to work togetherThe enigmachine is a physical-digital data pipeline If you give it an image it produces animated LED charts of data from Enigma Public about the things in the picture Lets take a look at how it worksStep 1: take a picture of the subway While the computer is thinking the plastic oil rig starts movingStep 3: Enjoy your animated data visualization! Repeat step 2 to view a new columnOur goal was to complete a functional and extensible data pipeline which crossed physical-digital boundaries and have fun learning some new technologies along the way We used: This diagram shows how these pieces fit together Red lines are internet connections black lines are physical wired connectionsHeres what happens under the hood: After deciding on an overall initial project architecture we developed the stages in teams of two Everyone contributed code and worked with some form of physical electronics By defining clear interfaces we were able to develop our pieces in parallel and have confidence that the pieces would fit together when we combined them at the endIf we had more time and people we would have liked to add more stages to the pipeline such as a vacuum for cleansing dirty data Since the stages were linked together using standard RESTful interfaces through the internet the pipeline could theoretically span multiple physical locationsMost people regard data as a serious affair trapped in the world of computers (even when its leaked) We hope our project provokes people to think creatively about how to make digital data more approachable and brainstorm novel ways to interact with the digital worldIf you have an idea for a piece (or a whole) Rube Goldberg data pipeline let us know! To build one backed by the worlds largest repository of free public datasets you can get started with the Enigma Public APIWith thanks to Jenny Kang (Design) David Rubinstein (Engineering) and Martin (Photography) for insights debugging assistance and support through the design processInterested in solving complex problems and working on interesting projects? Were hiringOriginally published at wwwenigmacom on June 12 2018
RrtLPf8GgQ2xF9YPvhKmir,As a data technology company Enigma moves around a lot of data and one of our main differentiators is linking nodes of seemingly unrelated public data together into a cohesive graph For example we might link a corporate registration to a government contract an OSHA violation a building violation etc This means we not only work with lots of data but lots of different data where each dataset is a unique snowflake slightly different from the nextWrangling high quantities and varieties of data requires the right tools and weve found the best results with Airflow and Docker In this post Ill explain how were using these a few of the problems weve run into and how we came up with Yoshi our handy workaround toolAirflow is designed to simplify running a graph of dependent tasks Suppose we have a process where: Considering each task as a node and each dependency as an edge forms a directed acyclic graph  or DAG for shortIf you are familiar with DAGs (or looked them up just now on Cracking the Coding Interview) you might think that if a DAG can be reasoned within the time of a job interview then it cant be that complex right? In production these systems are much more complex than a single topological sort Questions such as how are DAGs started? or how is the state of each DAG saved? and how is the next node started? are answered by Airflow which has led to its wide-spread adoptionIn order to understand how Docker is used its important to first understand how Airflow scales The simplest implementation of Airflow could live on a single machine where: Unfortunately this system can only scale to the size of the machine Eventually as DAGs are added and more throughput is needed the demands on the system will exceed the size of the machine In this case airflow can expand to a distributed systemNow this system can scale to as many machines as you can afford* solving the scaling problem! Unfortunately switching to a distributed system generally exchanges scalability for infrastructural complexity  and thats certainly the case here Whereas it is easy to deploy code to one machine it becomes exponentially harder to deploy to many machines (exponentially since that is the number of permutations of configuration that can go wrong)If a distributed system is necessary then its very likely that not only is the number of workers very high but also the number of DAGs A large variety of DAGs means a large variety of different sets of dependencies Over time updating every DAG to the latest version will become unmanageable and dependencies will diverge There are systems for managing dependencies in your language of choice (eg virtualenv rubygems etc) and even systems for managing multiple versions of that language (egUnless you have been living in a container (ha-ha) for the last five years youve probably heard of containers Docker is a system for building light-weight virtual machines (images) and running processes inside those virtual machines (containers) It solves both of these problems by keeping dependencies in distinct containers and moving dependency installation from a deploy process into the build process for each imageDocker is not a perfect technology It easily leads to docker-in-docker inception-holes and much has been written about its flaws but nodes in a DAG are an ideal use-case They are effectively enormous idempotent functions  code with input output and no side-effects They do not save state nor maintain long-lived connections to other services  two of the more frequently cited problems with DockerDocker exchanges loading dependencies at run-time for loading dependencies at build time Once an image has been built the dependencies are frozen This is necessary to separate dependencies but becomes an obstacle when multiple DAGs share the same dependency When the same library upgrade needs to get delivered to multiple images the only solution is to rebuild each image Though it may sound far-fetched this situation comes up all the time: The double-edged sword endemic to Docker containers should sound familiar to anyone working with static builds One common approach to solving this problem is to use plug-ins loaded at run-time At Enigma we developed a similar approach for Docker containers that we named Yoshi (hence the Nintendo theme for this entire blog post)By keeping code we suspected might need to be updated frequently in Yoshi keeping the interface to Yoshi stable and injecting the latest code at run-time we are able to update code instantly across all workflowsInjecting code at run-time allowed us to use all of the benefits of Docker containers but also create exceptions when we needed At first this seemed like the best of both worlds but over time we ran into flaws: Yoshi caused more bugs and complexity than we wanted but by revealing where our code changed most frequently it also revealed a simpler way to deploy updates across many DAGsHeretofore images were built one-to-one for each DAG but it does not have to be that way Each workflow has its own set of dependencies so an image is built for those dependencies but each node in the DAG could use a different image Additionally Docker images are referenced by URL The image stored for that URL can change without changing the URL This means that a DAG node executing the same image URL could execute different imagesEventually this led us to inject code by inserting updated images in the middle of a DAGImage injection not only allowed us to build workarounds to the double-edged sword of static Docker images  without the compatibility challenges of code injection  but building a Yoshi image also opened new doors to run Yoshi utilities from a command-line anywhere and run a Yoshi serviceIt took us a long time to get there but our final solution allowed us to have the best of both worlds and then someGame Over*There is a limit to the number of machines that can connect to the same redis host but that is most likely a lower limiting factor  especially for a start-upOriginally published at wwwenigmacom on April 10 2019
mkwwrckSLDt7QEhYdx5m2y,At Everoad we ensure we keep on our objective to revolutionise the trucking industry through making the most of our Big Data practices and keep innovating in all possible manners My predecessor wrote a great article here which I definitely recommend you to read outlining how they not only accomplished their objectives for 2018 but also how they went forward and beyondI was on-boarded to this amazing startup functioning right out the heart of Paris to bring in new ideas and insights into the existing architecture and figure out ways to develop it to make sure that as we grow in size our Data Architecture remains robust and ready to take up the increasing challengesAs cool kids of this generation Im sure you know who we are however for the other cool kids who may have not heard of us were a marketplace enabling shippers and carriers to find the right match Were young growing and ambitious with our goals heres where you can check out who we are and what we do and heres where you can get some deeper knowledge on whats going on at Everoad Our Analysts are always on the go performing hardcore analytical work creating products to help us provide our services better find new innovative techniques to leverage data to the best of our abilities and much more For this to happen we need to make sure they are provided with as much data as required in order to facilitate them to do what they do best In addition to providing the data we need to ensure it reaches our analysts in a timely fashion and that it is updated so that our analysts can get as many insights as possible from itOur AirFlow structure in the past was a robust architecture which met our needs as we wanted it to however in the recent months the architecture started to slow down with the first signs of Data Overload As our data ingestion grew drastically we needed to act quick and recreate our data pipelines to handle this new heavy load but at the same time make sure we optimise it in terms of runtimeThe following are the recent changes we put in place to optimise our DAGs: As a growing Data Engineering Team we are focused on finding innovative solutions and creating the best data architecture for our analysts in order to facilitate them with all they do and meet all their data needs This post has just been a recap of what weve been able to achieve so far in terms of optimisation and we have many more ideas to implement and deploy and well keep you updated with all we do through our next posts You might even find a little spark in the upcoming posts if you know what I mean
7zkZw9pegAYTF6wB9WiKLg,One of our clients approached me with the following story: A naive solution they attempted is to maintaine a last_updated timestamp column and periodically pull all changes since the last pulled timestampThis approach holds several shortcomings: Debezium is an open source distributed platform for change data capture Start it up point it at your databases and your apps can start responding to all of the inserts updates and deletes that other apps commit to your databases Debezium is durable and fast so your apps can respond quickly and never miss an event even when things go wrongSince Debezium is reading database logs: Debezium records the history of data changes in Kafka logs from where your application consumes them This makes it possible for your application to easily consume all of the events correctly and completely Even if your application stops (or crashes) upon restart it will start consuming the events where it left off so it misses nothingThis is where Delta Lake comes into play While in the past it was hard to reliablly perform UPSERTS and DELETES on data lake tables Delta Lake along with its other benefits enables it
ayQ8hP56Ggm4ywdgtnooEr,At Fandom were working currently on A/B testing various approaches for content recommendations for our readers One of the most popular algorithm to provide these based on users behavior is Collaborative Filtering so among other strategies we decided to give it a tryAll our data-related workloads: importing data from external providers marketing efficiency analysis and reporting is controlled now by Airflow DAGs running workloads on Amazon EMR clusters Fortunately Apache Spark one of the most famous data processing frameworks has built-in support for Collaborative Filtering so our first choice was to implement the recommendations model with SparkQuickly it turned out the Fandom scale is challenging  on weekly data with 25 million unique users and 84 million unique items (Fandoms articles views) it takes ages to calculate satisfying multi-iterations model with Spatk due to data shuffling between cluster nodes What is more the method was very cost-ineffective as a lot of new machines needed to be startedWhen browsing the Internet looking for Collaborative Filtering model calculation on a dataset with the comparable scale we found a great Python library: implicit by Ben Frederickson The API is simple and just right provided example matches our needs and  what was the most important in our case  the author is focused on optimizing the time of model building and response delivery by providing code that runs on GPU using Nvidia CUDAWhile implicit library worked great in manual proof-of-concept phrase it has 2 drawbacks when comparing to the classic Apache Spark approach: While the first issue can be addressed easily with 3-steps Airflow DAG we were looking at how to address the second one Basically we imagined the flow like this: The in-cloud part had to meet a few requirements: The first idea was: hey somebody must have already addressed the problem so we went to the operators directory on Airflows GitHub and found there AWSBatchOperatorWhile Amazon is doing a lot of efforts to make Elastic Container Service GPU-friendly and even supports GPU-based scheduling and out of the box support on certain instance flavors the features are not available in AWS Batch Actually setting up the compute environment as a bunch of P2- or P3-flavoured nodes gives capabilities to run nvidia Docker runtime via SSH but AWS Batch job submission API lacks this optionThere is an official AWS blog post on how to create a custom AMI image for GPU cluster but it lacks information on resource allocation or how to prepare and run custom Docker image Also it seems a bit outdatedHowever we were highly motivated to try this approach! The next sections describe how we overcame all these difficulties to create a working Airflow pipelineTo create a working AMI container its good to start with Deep Learning Base AMI Start a new GPU node with this image (we used g34xlarge) Then login into the node and: 3 Restart docker: sudo service docker restart • Verify that runtime is properly configured: 5 Create an AMI image from AWS console using the official manual and create a new AWS Batch compute environment with this image When asked for max CPUs  multiply the number of CPU cores (not GPU cores or GPU units!) of your selected EC2 flavor per desired parallelism level • Create a job queue in AWS Batch redirecting all jobs to just created compute environmentWhen your AMI is ready its time to prepare a Docker image that containers will be built from AWS Batch cant deploy tasks directly on AMI thats why this additional virtualization layer is needed hereFor implicit library mentioned before we created an image using this Dockerfile: Please note that the base image is devel one (not the base we used to test the AMI instance)  this one provides header files required by libraries installed via pipImages you want to start from AWS Batch need to reside in public DockerHub repository or on your private ECR We decided to use a private oneThen go into AWS Batch and create a job definition: Write a python script that reads data from the local filesystem and writes results back to the filesystem as well (in our case directories are named input and output) This way allows to write integration tests easily and helps with development The Docker image contains awscli tools so integration with S3 can live outside a scriptFinally put a script on S3 so it can be updated without touching base imageAirflow glues all the parts together It uses AWSBatchOperator to submit a job definitionAirflow task takes responsibility of setting input data location application code and requested output location This provides some kind of flexibility we need to: The described method works perfectly fine in our environment even with the final multi-level virtualization pattern (AWS Batch -> Node -> Container -> Application)AWS Batch provides the required level of flexibility  it runs GPU nodes only when there are some jobs waiting in the queue and is not limited to Airflow as a jobs submitter Keeping the application code outside Docker container allows us to simplify deploy pipelines to just synchronizing git repository with the S3 bucket without the need to rebuild imagesFrom the maintenance perspective  AWSBatchOperator waits for the job to complete by default and benefits from Airflow capabilities like retries notifications and monitoring (job logs are available via AWS Cloud Watch)Its worth to mention that AWS Batch artifacts prepared for this pipeline (job definition queue and compute environment) can be used also outside Airflow by data scientists that want to test new models
apMmEzqZSQR3S8M5RNwugh,Recently I got the opportunity to work on both the Streaming Services Kafka vs Kinesis often comes up And believe me both are Awesome but it depends on your use case and needsI have heard people saying that kinesis is just a rebranding of Apaches Kafka And I dont agree with them totally Ill try my best to explain the core concepts of both the bigshotsLike many of the offerings from Amazon Web Services Amazon Kinesis software is modeled after an existing Open Source system In this case Kinesis is modeled after Apache KafkaKinesis is known to be incredibly fast reliable and easy to operate Similar to Kafka there are plenty of language-specific clients available including Java Scala Ruby Javascript (Node) etcAmazon Kinesis has a built-in cross replication while Kafka requires configuration to be performed on your own Cross-replication is the idea of syncing data across logical or physical data centers Cross-replication is not mandatory and you should consider doing so only if you need itEngineers sold on the value proposition of Kafka and Software-as-a-Service or perhaps more specifically Platform-as-a-Service have options besides Kinesis or Amazon Web Services Keep an eye on http://confluentioApache Kafka is an open-source stream-processing software platform developed by Linkedin donated to Apache Software Foundation and written in Scala and Java APIs allow producers to publish data streams to topics A topic is a partitioned log of records with each partition being ordered and immutable Consumers can subscribe to topics Kafka can run on a cluster of brokers with partitions split across cluster nodesKafka has five core APIs: Kafka has the following feature for real-time streams of data collection and big data real-time analytics: As a result Kafka aims to be scalable durable fault-tolerant and distributed However Kafka requires some human support to install and manage the clusters Also the extra effort by the user to configure and scale according to requirements such as high availability durability and recoveryAmazon Kinesis has four capabilities: Kinesis Video Streams Kinesis Data Streams Kinesis Data Firehose and Kinesis Data Analytics The Kinesis Data Streams can collect and process large streams of data records in real time as same as Apache Kafka AWS Kinesis offers key capabilities to cost-effectively process streaming data at any scale along with the flexibility to choose the tools that best suit the requirements of your application It enables you to process and analyze data as it arrives and responds instantly instead of having to wait until all your data is collected before the processing can begin Lets focus on Kinesis Data Streams(KDS)The high-level architecture on Kinesis Data Streams: Kinesis Data Streams has the following benefits: As a result Kinesis Data Streams is massively scalable and durable allowing rapid and continuous data intake and aggregation; however there is a cost for a fully managed service KDS has no upfront cost and you only pay for the resources you use (eg $0015 per Shard Hour) Please check Amazon for the latest Kinesis Data Streams pricingKafka or Kinesis are often chosen as an integration system in enterprise environments similar to traditional message brokering systems such as ActiveMQ or RabbitMQ Integration between systems is assisted by Kafka clients in a variety of languages including Java Scala Ruby Python Go Rust Nodejs etcOther use cases include website activity tracking for a range of use cases including real-time processing or loading into Hadoop or analytic data warehousing systems for offline processing and reportingAn interesting aspect of Kafka and Kinesis lately is the use in stream processing More and more applications and enterprises are building architectures which include processing pipelines consisting of multiple stages For example a multi-stage design might include raw input data consumed from Kafka topics in stage 1 In stage 2 data is consumed and then aggregated enriched or otherwise transformed Then in stage 3 the data is published to new topics for further consumption or follow-up processing during a later stageBoth Apache Kafka and AWS Kinesis Data Streams are good choices for real-time data streaming platforms If you need to keep messages for more than 7 days with no limitation on message size per blob Apache Kafka should be your choice However Apache Kafka requires extra effort to set up manage and support If your organization lacks Apache Kafka experts and/or human support then choosing a fully-managed AWS Kinesis service will let you focus on the development AWS Kinesis is catching up in terms of overall performance regarding throughput and events processingComments would be AppreciatedFollow us on Twitter 🐦 and Facebook 👥 and join our Facebook Group 💬
SbTcfPJMfau9Eo5uRjYeZs,Orchestrate parallel jobs on K8s with the container-native workflow engineArgo Workflows is an open-source container-native workflow engine for orchestrating parallel jobs on K8s Argo Workflows are implemented as a K8s CRD (Custom Resource Definition) As a result Argo workflow can be managed using kubectl and natively integrates with other K8s services such as volumes secrets and RBAC Each step in the Argo workflow is defined as a containerYou can list all workflows as: Get the list of all Argo commands and flags as: Heres a quick overview of the most useful Argo command-line interface (CLI) commandsYou can also run workflow specs directly using kubectl but the Argo CLI provides syntax checking nicer output and requires less typingFirst install argo controller: Examples below will assume youve installed Argo in the Argo namespace If you have not adjust the commands accordinglyConsider sample micro-service application instana with its source-code named robot-shopIt can be deployed as easy as: Getting a list of application Pods and wait for all of them to finish starting up: Once all the pods are up you can access the application in a browser using the public IP of one of your Kubernetes servers and port 30080: For a complete description of Argo workflow spec refer to spec definitions and check Argo Workflow ExamplesArgo adds a new kind of K8s spec called a Workflow The entrypoint specifies the initial template that should be invoked when the workflow spec is executed by K8s The entrypoint template generally contains steps which makes use of other templates Also each template which is container based or steps based (as opposed to script based) can have initContainersThe kubectl apply issue with generateName field of Workflow: Argo CLI can submit the same workflow any number of times and each time it gets a unique identifier at the end of name (generated using generateName) But if you use kubectl apply -f to apply an Argo workflow it raises resource name may not be empty errorLets look at below workflow spec: Here is how steps look like in JSON: This suggests that top-level elements in steps have to be a group of steps and they run sequentially whereas the individual steps within each group run in parallelHere is another example of nested steps: The whalesay template takes a parameter (as array element containing a key-value pair in inputs
EXrH9qmkdVwkSQFFHjQKCG,Heartbeat is a lightweight daemon that you install on a remote server to periodically check the status of your services and determine whether they are available Unlike Metricbeat which only tells you if your servers are up or down Heartbeat tells you whether your services are reachableHeartbeat is useful when you need to verify that youre meeting your service level agreements for service uptime Its also useful for other scenarios such as security use cases when you need to verify that no one from the outside can access services on your private enterprise serverYou can configure Heartbeat to ping all DNS-resolvable IP addresses for a specified hostname That way you can check all services that are load-balanced to see if they are availableWhen you configure Heartbeat you specify monitors that identify the hostnames that you want to check Each monitor runs based on the schedule that you specify For example you can configure one monitor to run every 10 minutes and a different monitor to run between the hours of 9:00 and 17:00Heartbeat currently supports monitors for checking hosts or endpoint via: Below I will show you how the magic is • Heartbeat monitor: look at the example closely and you will find that I have set up a few fields there like env tags team etc That is the magical part of monitoring We will talk about it more at a later stepAfter heartbeat is up and running you can check the detail from Uptime view and data structure • Setup watcher to generate alert for notification the key point is that using top_hits aggregation to bring more details then use the field you bring in for the alerts • In the end you will have the beautiful alert with all the details you need Your on-call engineer will be very happy for having such riches alerts And the amazing thing is that you can minimize the time for finding out the issue isFollow us on Twitter 🐦 and Facebook 👥 and Instagram 📷 and join our Facebook and Linkedin Groups 💬
iSopfhAgreLATwZs4SV8LZ,At Flatiron a large number of systems work together to process patient data and de-identify it One of these systems the Client Packager is responsible for the post-processing of our Enhanced Data Mart (EDM) products which are the datasets that we deliver to cancer researchers This data system must carefully aggregate verify and format the EDMs before they can be shipped off for delivery As the number of data products we deliver and clients we deliver to grows we have noticed that the original system design is no longer supporting our current scale of operationThe heart of the issue lies with the infrastructure A single AWS EC2 instance is used to execute the Client Packager While the instance itself is capable it is shared across several engineering teams for batch processing continuous integration and ad-hoc remote execution Resource contention can become a limiting factor for computationally-intensive jobs Job status is communicated through Slack and email with application logs remaining on the host Needless to say this infrastructure design works fine for a nimble startup but starts to break down as the frequency and length of jobs increases over time Current jobs can take up to ten hours to completeIn an ideal state we would have individual teams operate their own cluster of machines allowing data pipelines to scale according to each teams requirements Teams should manage the security and permissioning of their own systems enforcing the principle of least privilege Its also important that application logs and intermediate data pipeline results be transparent and accessible In order to achieve these goals and allow our particular data system to scale we adopted Apache Airflow an open source task orchestration frameworkIn Airflow data pipelines are defined as a series of tasks and dependencies Together they define a workflow that can be represented as a Directed Acyclic Graph (DAG) Each task in Airflow can be independently distributed to a different worker in a cluster allowing workflows to scale Additionally the status of each workflow is easily visible through the Airflow UI allowing transparent monitoring of data pipelines Within Flatiron Airflow infrastructure is administered such that each teams cluster is provisioned separately allowing for isolated job scheduling and permissioningWhile there are several benefits to migrating the Client Packager to Airflow doing so requires overcoming a series of challenges around distributed code execution and message passingIn order to distribute any process across a cluster of workers we need a way to distribute the source code Syncing a large codebase and installing the proper execution environment can take several minutes For longer running processes this is not a significant amount of time However this introduces unnecessary failure points and limits shorter running tasks from being effective A common solution here is to containerize the application which is what we have done using Docker To keep our containers lightweight we use Bazel an open source software build tool to package subsets of code into libraries so that only the necessary dependencies can be included By organizing the code and execution environment into a single image file we can quickly distribute our tasks to cluster workersAnother challenge of distributing our tasks across cluster workers is that these tasks need to pass large datasets to each other When we were running the Client Packager on a single instance we could safely assume the local filesystem was always available for persistent reads and writes Once we moved the workflow onto a cluster we had to be more deliberate about data communication since tasks that share data no longer share a local filesystem One solution was to leverage AWS S3 as a remote filesystem to download inputs and upload outputs at the end of each task This had the advantage of allowing each task access to the data output of any dependencies that ran further up the DAG Uploading intermediate results to AWS S3 also made it possible to review the data outputs for correctness and completenessThe refactored Client Packager running on a distributed Airflow cluster drastically improves its scalability This is just one example of how Flatiron rethinks infrastructure as the company continues to grow
johHnDzs5aSzq92zrnnLxe,When we on-board a new partner at Flipside Crypto and start parsing their blockchain data my curious colleagues will often check in on our ingestion progressSince I spend all day writing queries in the SQL console my instinct is to pull out my go-to monitoring query run it and send a code snippet of the query results to them over SlackBut as my gracious co-workers have ever-so-gently let me know this is a sub-optimal way for them to receive updatesSomething like this: The chart above shows progress on our UDM Event Ingestion UDM stands for Universal Data Model At Flipside we use this model to compare data across blockchains I wrote about how we do that more hereHow to read the chart: Well get into how to interpret these results belowTo build the chart we use an out-of-the-box scatter plot in Mode with drag-and -drop columns and attributes and easy-to-edit labels and interactive filtersThe workflow in Mode is: The way these stages are seamlessly linked together is a massive improvement over copy/paste results sharingBut most importantly my coworkers can access richer data and context and explore it whenever without waiting on me to run another query for themMy original monitoring query would produce results that looked like this: From these results I learned how to extract a few different layers of information: When I translated this query to a visual though having an intuition about different statuses became a lot easierCompare similar results in chart form: In the chart above you can see that several chains are clustered around the Real Time Line meaning the last block time and our last ingestion time for the chain are almost the sameAlso the chain that is currently in Catch-Up Mode is a big obvious point shifted to the right The large size comes from the high number of events weve ingested in this window because there is more chain for us to process The shift to the right comes from lag in latest block that was processed We are further back in their history so there is more lag when that block was published and the time we are processing itAnd again you can use the an interactive sidebar on the chart to toggle on and off different points if you want to see any one more clearlyWhile any one report will give you a quick snapshot of progress like all good computer tools you can maniacally refresh the results tooWhile not much changes second-to-second in our pipeline or on the chains themselves we recently upgraded a parser for some of our chains and decided to replay a small portion of each chain to ensure that we didnt miss any data during the migration from the old parser to the new parserOver the course of the hour that we did the migration and replay you can clearly see the backfill moving closer to real timeFollow the Big Green Dot: Each of the frames in the gif above are minutes apart You can see the Big Green Dot moving through hours of data but also notice the smaller replay chains moving from right to left and from up to down as we scaled up the ingestion process to meet the replay throughputYou can see in the examples at the end of this article how important the right visual is when it comes to explaining data to our clientsLately Ive been learning that empowering our internal teams to make sense of pipeline and process data is equally importantWe can get answers for our clients faster and speak more authoritatively about our data when all stages of data processing are easily accessible and more readily understandableHaving the right tools to quickly move from queries to reports makes this process easy Reporting building should become a standard part of our engineering workflows going forward
ZSWPZLnX7pZpSWzBMqKhTo,You may already be familiar with blockchain as an immutable public ledger  a way to capture and store information as a shared source of truthIn this post we want to explore those questions highlighting what kinds of questions are naturally and easily answered by blockchain-backed storage and what kinds of questions are more easily answered by utilizing other storage formatsAt Flipside Crypto our customers  crypto projects who want to deeply understand their users  need to answer a whole series of questions that are pretty much impossible to answer by pulling data directly from the blockchain Instead we transform store and aggregate data into more traditional relational formats in order to calculate metrics like network activity or visualize trends in adoption and retentionWell get into those metrics below but first we should explore the data that we can pull directly from the chainWhile any particular blockchain might have its own implementation at a high level you can generalize the shape of blockchain data as being a series of linked blocks each composed of a group of transactions where the transactions are interactions between addresses  cryptographically controlled identifiers on the chainCommon blockchain libraries allow you to query for things like the latest block in the chain or balance for a specific address The query interfaces all make sense when you consider how the chain gets built up Users who control addresses submit transactions they would like to record to a pool of potential transactions These transactions are then mined where the miners attempt to group them into a valid block that will eventually be recorded on the chainUsing the Python web3 library as an example we can explore the Ethereum blockchain over JSON-RPC Once you have a connection to an Ethereum node an easy entry point is pulling the latest block: Which returns: At the block level there is: We can in turn ask for the transaction information using the transaction hash: Which returns: The transaction includes: You can see a cleanly decoded version of this transaction here on EtherscanioYou could continue pulling block after block and decoding transactions which is what we do in our ETL flow to parse each chain but lets step back and ask what were doing here from a data access perspectiveIn order to record transactions on the blockchain miners need to be able to perform a couple simple lookups: And when they mine the block they will record inputs and outputs for each transaction On a smart contract platform like Ethereum these outputs can get very complicated because they contain the results of executing any arbitrary contract call  think of like the output of any function call in any given programming language…and think about what it would take to encapsulate all those possible outcomes to be decoded later onMiners do not need to compute: In other words the data stored on the chain and the interfaces built to access it serve to mostly answer point-lookup-style queries not aggregate analyticsSince our customers want to answer questions about adoption and usage trends and not just do point lookups we have to go beyond just crawling the chain We have store transaction data in a way that lets us flexibly segment and aggregate it The good news is that for many of the questions we end up asking at Flipside Crypto a straightforward relational data model is all we needFor each block we decode all the transactions and store the them as one or more individual records An Ethereum transaction looks pretty similar to the rpc response above with the following fields that get pulled from the block the transaction some additional metadata and decoded outputs from any potential contract calls: With these values we can slice and dice the network data by project to show larger trends like economic throughput (explained in depth in the context of Bitcoin here by Nic Carter)Our individual transactions easily roll up by project and so we can calculate average transaction size and number of transactions over time If we plot average transaction size against number of transactions on a project-by-project basis we can start to see how different projects are used: Projects farther to the left are more similar to payment systems like Paypal that is a high number of smaller value transactions In contrast projects with larger transfer sizes act more like stores of value A projects placement in this plot could confirm whether users are doing what the creators intendedWhile high-level averages across all projects is interesting to help us get oriented to the ecosystem as a whole our customers want to go deeper to better understand their users Since we start at the individual transaction level we can refine the aggregate stats and break down transfer metrics in other waysFor a single project we can start to understand how active their user base is over time In the plot below we are looking at average transfer size again but this time we can compare it to the number of transfers sent per user per month to reveal the concentration of user activity: Often crypto projects we work with have a profile of their ideal user behavior By looking at these behavioral groupings over time they can better understand if usage of their project is trending towards or away from their idealFinally our analytics become even more powerful when we let projects map their off-chain efforts to on-chain activityBy capturing retention from on-chain data we can help them track and measure their off-chain engagement efforts in a much more concrete wayAs a data engineer its been fascinating to see how a transformation in data storage  from blockchain to relational  has enabled so much insight for our customersJust last night we were discussing some of these metrics with a data scientist who looked at us and said What are you talking about? There are no customers on the blockchain! We hope youre a little surprised by these insights We think it is possible to explain how people are using blockchain-based projects in the way that helps projects refine their missions and ultimately leads to a healthier ecosystemWe have many more data transformations ahead of us too Each time we experiment with a new format we find new questions we can answerIf youre interested in answering these questions and exploring how to provide more insight into whats being built in the blockchain space feel free to reach outFind me on twitter or email data@flipsidecrypto
aCnq5MUpPK9WvcpkHMRgqi,We should start by noting that Flo has long been committed to having a data-informed culture We operate using OKR and prefer to ascertain the necessary information and rely on accurate figures rather than feelings This approach requires effort skill and the ability to overcome all kinds of obstaclesDifficulties can be categorized into those related to the product analytics and technology My post touches on the technical side of thingsYou need to experiment any time you launch a new functionality In addition to running product A/B tests (also known as bucket tests or split-run tests) youll need to run technical updates since they can also lead to a decrease or increase in certain metrics Thats why our motto is everything is an experimentIf you arent familiar with analytics but want to learn more about why experiments are necessary and how to conduct them you can read this articleBasically in terms of mathematical statistics it just isnt enough to conduct an A/B test and compare the metrics of the funnels (CTR CPA etc) or some average values (LTV session length etc) Thats because you cant be entirely sure whether the results are reliable To assess the level of reliability you need to conduct an experiment under specific rules and analyze the results using mathematical methods of statisticsAnalysts usually handle this part while developers tend to run the test itself As such bottlenecks form in the experimentation process increasing the experiments duration and decreasing their frequency However there are testing tools that can help resolve the situation Some companies use third-party services while others create their own (were in the latter group)Feature toggling is a mechanism for enabling and disabling new features Server-side this is the control process through the Dynamic Config service (which I will talk about in more detail below) Client-side everything looks extremely simple in most cases: The mechanism is simple yet reliable and convenientAs I said any new product idea in the form of a new or changed feature must first undergo experimental testing If we decide to transfer the feature to release we move on to rollout gradually expand the audience and finally send the feature to release only if it proves to be successfulThe primary purposes of rolling out a feature are to: ab Prevent metrics degradation due to unforeseen effects As I said everything is an experimentThe difficulty with feature rollout is that you cant increase the rollout percentage during the experiment Users start moving between test groups and spoil everything Simply put situations arise in which the experiment is already underway when a user that was initially in the control group ends up in the treatment group The user is therefore able to participate in both experimental groups How do you measure such a users behavior and impact on metrics? Thats a good questionOur experimentation process looks like this: ● Product managers and analysts formulate a hypothesis and then determine what kind of experiment is needed and for which audience● They select metrics to record the uplift If the required metric isnt available it has to be added to ETL to calculate the results of the experiment (see below for details)● The analysts then calculate the experiments required sample size and duration We dont conduct sequential testing yet and we dont use Bayesian multi-armed bandits either In other words we only follow the classic fixed-horizon approach But were working on other areas so stay tuned● After that a task is assigned to backend developers in Jira to prepare the experimentWhat Developers Do: ● Meanwhile the programmers prepare the necessary configs to send from the server to the client using the Dynamic Config service for the experiment● Then they check the validity of the predicates (see below for more details) and look for possible problems or overlaps with other experiments as needed● Client-side developers prepare client-side code with feature toggling to ensure that the experiment works correctlyThe general experimentation flow is as follows: Things to Keep an Eye on: ● Sometimes a user is assigned to the right group and the right experiment but the key event for the experiment doesnt occur (for example the push notification doesnt reach the user) Therefore its not enough to assign the user to the desired experiment; its vital to ensure that the desired event occurs The so-called fill rate (the ratio of all targeted events to those that actually happened to users) is often measured One example is the ratio of sent push notifications to those actually displayed● Various novelty and network effects make it difficult to assess the outcome of the experiment clearly (for example working offline or users sharing information about the experiment with each other)● When working with subscriptions it becomes necessary to implement the so-called waterfall configs approach This involves building several blocks with conditions one after another before ultimately applying the block with the settings that the user falls under● We dont know anything about users who only install the application and start using it without registering Nevertheless we need to conduct experiments right away (especially on prices and subscription formats) This somewhat complicates the usual procedureUsers are divided into groups using the internal User Profile service and the so-called predicate mechanism In terms of our internal analytics we can assign any number of parameters to the user that will subsequently help reach him or her  for example a parameter indicating a womans age or the fact that shes pregnantParameters can be chained together using AND OR and comparison operators It looks something like this: A special service validates this set of parameters If its validity is confirmed we can determine on the server-side whether the user falls under the desired experiment For instance: The predicate age<=18 AND experiment (new_feature 30) returns true for a random 30% of female users aged 18 or youngerWe also distinguish between sticky and non-sticky experiments The former assumes that the user is included in the experiment and falls under a certain group once and for all ie if the same experiment is repeated he or she will not find themselves in a different group (control/treatment)This can be achieved in two ways: using a hash function or remembering the user Randomization itself is essential The division into two groups must meet the criterion of randomness ie the closer we are to real randomization the better The hash function is fast and makes it possible to obtain the same value for the user every time thus remembering them In this case the main thing is that the hash function itself allows you to fully split two groups of users into the desired proportions of 50/50 20/80 etc In addition users fall under these groups independently of any other factorsWeve implemented the following procedure: ● We take the md5 hash based on the experiment name and user ID● We then take 8 low-order bytes take the remainder after dividing by 1000 and divide the result by 10 This gives us a percentage accurate to one decimal place This percentage determines the group the user belongs to● The function returns a number between 00 and 1000 When calling an experiment in an expression such as experiment(test40855) the function will return true if the user falls between 400 and 855After we create the required features and experiments we need to send this information to the client somehow To do this we use a special service called Dynamic Config which we use to transfer the necessary configurations to the required devicesAt the very beginning when the user has just installed the application we still dont know anything about them However we learn more and more about the user throughout onboarding The decision to send them to a certain experiment (for example to a new promo screen) or not must be made quickly There isnt time to wait for the server to send a new config (there may be internet connection issues or the server may change its mind) We need to strike while the irons hot as the saying goesTo do so we came up with so-called waterfall configurations that differ from the usual kind in that the conditions for entering the experiment follow one after another with this type of configurations For instance: As we move through the interface we learn more about the user and try to apply new rules to him/her each time If one of the rules is triggered as the waterfall progresses the user ends up in one of the specified experimentsThe Experiment Dashboard is recalculated and updated regularly throughout the day The calculation uses a proprietary ETL that operates on an hourly basis The dashboard allows you to view a list of running and stopped experiments track uplifts for metrics and statistical significance indicators (p-value) and see the sample size and which group each experiment belongs toYou can also go to each experiment on a separate page showing the changes in p-value based on the cumulative calculationThe ETL itself is based on Spark (Scala) Scala and Spark are generally the de facto standard for Flo when developing such solutions For some ETLs its easier to use Python Pandas and PySpark since analysts use Python in their workAirflow is a clear convenient and reliable tool that we use to run these tasks on a scheduleOur ETL works as follows: all metrics are described in the form of SQL which calculates a value (for example CTR or some other conversion indicator) It looks like this: We use Presto on top of data from AWS S3 as our main database This is a powerful and robust distributed SQL query engine that runs on top of terabytes of our own raw dataThe p-value is then calculated as part of a parametric or a nonparametric test This is a chi-square by default and the final dashboard calculates the significance with this statistical criterion by defaultIf this is insufficient we can use the bootstrap (each analyst decides for themselves) We also have two environments deployed for Data Science: JupyterHub and Amazon SageMaker Studio These allow us to load the required dataset from the experiment for advanced analysis Of course a single system with a single tool would be better Well do this when we have enough feedback about these toolsFirst we want to save our analysts from doing rote work as much as possible and to learn how to conduct experiments with minimal involvement from analysts Since not a single change or feature is rolled out without experimental testing this takes a lot of time resources and effortETL has already outlived its usefulness as a tool for calculating experiments Experiments take too long to calculate and adding new metrics is extremely difficult We need to make this process more flexible For example we can randomly add metrics in the form of SQL queries that are then executed using PrestoLooker lacks the functionality and flexibility to display experiment analyses Developing a proprietary UI will allow us to act independently to visualize the results of experimental analyses and conduct the experiments themselvesWe use the fixed-horizon approach to calculate the duration of experiments and the size of audiences While this is a traditional approach to conducting experiments its already largely outdated since there are already more advanced (but also more complex) experimental methods Take for example the sequential approach● We take great care of our users personal data and never provide such data to third parties Since weve already implemented the feature-toggling mechanism (as with internal analytical storage) conducting experiments was the next logical step● They lack flexibility and transparency To validate the results of an experiment we need access to raw data and methods of calculation We cant rely on data that we cant trust● Third-party services dont offer much benefit since we have our own talented analysts and data engineers The biggest challenge in conducting experiments generally involves the following two tasks: you need to correctly divide users into groups and correctly assess the significance of the experiment (taking into account all of its specific characteristics) If these two tasks are essentially solved a third-party service would just be another possible implementation of whats already workingConducting experiments and developing your own service to do so is a difficult and intense endeavor The aforementioned approach to conducting experiments and performing the corresponding calculations is neither an exclusive one nor the only correct one However the examples given above should be of use to anyone who wants to create their own system for experimentation
BSxFk9bDXVw8Av3jpwnWHJ,Cloud Data Fusion is the brand new fully-managed data engineering product from Google Cloud It will help users to efficiently build and manage ETL/ELT data pipelines Data Fusion intents to shift the focus from code  where data engineers can spend lot of days/weeks building connectors from a source to a sink- to a focus on insights and action Built on top of the open-source project CDAP it leverages a convenient user interface for building data pipelines in a drag and drop mannerNote: this is a cross-post from the Fourcast blog Read more thereData Fusion comes at a time where companies struggle to deal with a huge amount of data spread across many data sources and to fuse them into a central data warehouse The key challenges of integrating all these data are as follows: Data Fusion is addressing these challenges by making it extremely easy to move data around with two main focuses: The following screenshot shows the interface with a simple pipeline First step is the connector to the raw database then there is a wrangling step that does some transformation on a set of columns and finally the data is sent to two sinks: BigQuery for analytics purposes and Cloud Storage for backup of the dataSome of the other relevant features of Data Fusion are these (described by one of the early adopters): Below is a list of business challenges where Data Fusion will excel: Data Fusion is providing a fabric which allows user to fuse a lot of different technologies and products that are available on GCP in a much easier more accessible secure and efficient manner as shown on the following chartData Fusion is the new backbone for data analytics and will become in the months to come a major game-changer for doing data engineering Our engineers at Fourcast are already familiar with this new GCP product and will be glad to give you a demoJust send us a message at info@fourcastio and we will get back to you
N4HeovWkz9okNF2nhp78dr,As a data engineer it is quite likely that you are using one of the leading big data cloud platforms such as AWS Microsoft Azure or Google Cloud for your data processing Also migrating data from one platform to another is something you might have already faced or will face at some pointIn this post I will show how I imported Google BigQuery tables to AWS Athena If you only need a list of tools to be used with some very high-level guidance you can quickly look at this post that shows how to import a single BigQuery table into Hive metastore In this article I will show one way of importing a full BigQuery project (multiple tables) into both Hive and Athena metastoreThere are few import limitations: for example when you import data from partitioned tables you cannot import individual partitions Please check the limitations before starting the processIn order to successfully import Google BigQuery tables to Athena I performed the steps shown below I used AVRO format when dumping data and the schemas from Google BigQuery and loading them into AWS AthenaStep 1Step 2Step 3Step 4Step 5Step 6So why do I have to create Hive tables in the first place although the end goal is to have data in Athena? This is because: So Hive tables can be created directly by pointing to AVRO schema files stored on S3 But to have the same in Athena columns and schema are required in the CREATE TABLE statementOne way to overcome this is to first extract schema from AVRO data to be supplied as avroschemaliteral  Second for field names and data types required for CREATE statement create Hive tables based on AVRO schemas stored in S3 and use SHOW CREATE TABLE to dump/export Hive table definitions which contain field names and datatypes Finally create Athena tables by combining the extracted AVRO schema and Hive table definition I will discuss in detail in subsequent sectionsFor the demonstration I have the following BigQuery tables that I would like to import to AthenaIt is possible to dump BigQuery data in Google storage with the help of the Google cloud UI However this can become a tedious task if you have to dump several tables manuallyTo tackle this problem I used Google Cloud Shell In Cloud Shell you can combine regular shell scripting with BigQuery commands and dump multiple tables relatively fast You can activate Cloud Shell as shown in the picture belowFrom Cloud Shell the following operation provides the BigQuery extract commands to dump each table of the backend dataset to Google Cloud StorageIn my case it prints: Please note: --compression SNAPPY this is important as uncompressed and big files can cause the gsutil command (that is used to transfer data to AWS S3) to get stuck The wildcard (*) makes bq extract split bigger tables (>1GB) into multiple output files Running those commands on Cloud Shell copy data to the following Google Storage directoryLets do ls to see the dumped AVRO fileI can also browse from the UI and find the data like shown belowTransferring data from Google Storage to AWS S3 is straightforward First set up your S3 credentials On Cloud Shell create or edit boto file ( vi ~/boto) and add these: Please note: s3us-east-1amazonawscom has to correspond with the region where the bucket isAfter setting up the credentials execute gsutil to transfer data from Google Storage to AWS S3 For example: Add the -n flag to the command above to display the operations that would be performed using the specified command without actually running themIn this case to transfer the data to S3 I used the following: Lets check if the data got transferred to S3 I verified that from my local machine: To extract schema from AVRO data you can use the Apache avro-tools-<version>jar with the getschema parameter The benefit of using this tool is that it returns schema in the form you can use directly in WITH SERDEPROPERTIES statement when creating Athena tablesYou noticed I got only one avro file per table when dumping BigQuery tables This was because of small data volume  otherwise I would have gotten several files per table Regardless of single or multiple files per table its enough to run avro-tools against any single file per table to extract that tables schemaI downloaded the latest version of avro-tools which is avro-tools-182jar I first copied all avro files from s3 to local disk: Avro-tools command should look like java -jar avro-tools-182jar getschema your_dataavro > schema_fileavsc This can become tedious if you have several AVRO files (in reality Ive done this for a project with many more tables) Again I used a shell script to generate commands I created extract_schema_avrosh with the following content: Running extract_schema_avrosh provides the following: Executing the above commands copies the extracted schema under bq_data/schemas/backend/avro/ : Lets also check whats inside an avsc fileAs you can see the schema is in the form that can be directly used in Athena WITH SERDEPROPERTIES But before Athena I used the AVRO schemas to create Hive tables If you want to avoid Hive table creation you can read the avsc files to extract field names and data types but then you have to map the data types yourself from AVRO format to Athena table creation DDLThe complexity of the mapping task depends on how complex the data types are in your tables For simplicity (and to cover most simple to complex data types) I let Hive do the mapping for me So I created the tables first in Hive metastore Then I used SHOW CREATE TABLE to get the field names and data types part of the DDLAs discussed earlier Hive allows creating tables by using avroschemaurl So once you have schema (avsc file) extracted from AVRO data you can create tables as follows: First upload the extracted schemas to S3 so that avroschemaurl can refer to their S3 locations: After having both AVRO data and schema in S3 DDL for Hive table can be created using the template shown at the beginning of this section I used another shell script create_tables_hivesh (shown below) to cover any number of tables: Running the script provides the following: I ran the above on Hive console to actually create the Hive tables: So I have created the Hive tables successfullyAs discussed earlier Athena requires you to explicitly specify field names and their data types in CREATE statement In Step 3 I extracted the AVRO schema which can be used in WITH SERDEPROPERTIES of Athena table DDL but I also have to specify all the fiend names and their (Hive) data types Now that I have the tables in the Hive metastore I can easily get those by running SHOW CREATE TABLE First prepare the Hive DDL queries for all tables: Executing the above commands copies Hive table definitions under bq_data/schemas/backend/hql/ Lets see whats inside: By now all the building blocks needed for creating AVRO tables in Athena are there: If you are still with me you have done a great job coming this far I am now going to perform the final step which is creating Athena tables I used the following script to combine avsc and hql files to construct Athena table definitions: Running the above script copies Athena table definitions to bq_data/schemas/backend/all_athena_tables/all_athena_tableshql In my case it contains: And finally I ran the above scripts in Athena to create the tables: There you have itI feel that the process is a bit lengthy However this has worked well for me The other approach would be to use AWS Glue wizard to crawl the data and infer the schema If you have used AWS Glue wizard please share your experience in the comment section below
BqXTis2MaKuVwjvT4dhVhy,These days everyone talks about open-source software However this is still not common in the Data Warehousing (DWH) fieldFor this post I chose some open-source technologies and used them together to build a full data architecture for a Data Warehouse systemI went with Apache Druid for data storage Apache Superset for querying and Apache Airflow as a task orchestratorDruid is an open-source column-oriented distributed data store written in Java Its designed to quickly ingest massive quantities of event data and provide low-latency queries on top of the dataDruid has many key features including sub-second OLAP queries real-time streaming ingestion scalability and cost effectivenessWith the comparison of modern OLAP Technologies in mind I chose Druid over ClickHouse Pinot and Apache Kylin Recently Microsoft announced they will add Druid to their Azure HDInsight 40Carter Shanklin wrote a detailed post about Druids limitations at Horthonworkcom The main issue is with its support for SQL joins and advanced SQL capabilitiesDruid is scalable due to its cluster architecture You have three different node types  the Middle-Manager-Node the Historical Node and the BrokerThe great thing is that you can add as many nodes as you want in the specific area that fits best for you If you have many queries to run you can add more Brokers Or if a lot of data needs to be batch-ingested you would add middle managers and so onA simple architecture is shown below You can read more about Druids design hereThe easiest way to query against Druid is through a lightweight open-source tool called Apache SupersetIt is easy to use and has all common chart types like Bubble Chart Word Count Heatmaps Boxplot and many moreDruid provides a Rest-API and in the newest version also a SQL Query API This makes it easy to use with any tool whether it is standard SQL any existing BI-tool or a custom applicationAs mentioned in Orchestrators  Scheduling and monitor workflows this is one of the most critical decisionsIn the past ETL tools like Microsoft SQL Server Integration Services (SSIS) and others were widely used They were where your data transformation cleaning and normalisation took placeIn more modern architectures these tools arent enough anymoreMoreover code and data transformation logic are much more valuable to other data-savvy people in the companyI highly recommend you read a blog post from Maxime Beauchemin about Functional Data Engineering  a modern paradigm for batch data processing This goes much deeper into how modern data pipelines should beAlso consider the read of The Downfall of the Data Engineer where Max explains about the breaking data silo and much moreApache Airflow is a very popular tool for this task orchestration Airflow is written in Python Tasks are written as Directed Acyclic Graphs (DAGs) These are also written in PythonInstead of encapsulating your critical transformation logic somewhere in a tool you place it where it belongs to inside the OrchestratorAnother advantage is using plain Python There is no need to encapsulate other dependencies or requirements like fetching from an FTP copying data from A to B writing a batch-file You do that and everything else in the same placeMoreover you get a fully functional overview of all current tasks in one placeMore relevant features of Airflow are that you write workflows as if you are writing programs External jobs like Databricks Spark etc are no problemsJob testing goes through Airflow itself That includes passing parameters to other jobs downstream or verifing what is running on Airflow and seeing the actual code The log files and other meta-data are accessible through the web GUI(Re)run only on parts of the workflow and dependent tasks is a crucial feature which comes out of the box when you create your workflows with Airflow The jobs/tasks are run in a context the scheduler passes in the necessary details plus the work gets distributed across your cluster at the task level not at the DAG levelFor many more feature visit the full listIf you want to start with Apache Airflow as your new ETL-tool please start with this ETL best practices with Airflow shared with you It has simple ETL-examples with plain SQL with HIVE with Data Vault Data Vault 2 and Data Vault with Big Data processes It gives you an excellent overview of whats possible and also how you would approach itAt the same time there is a Docker container that you can use meaning you dont even have to set-up any infrastructure You can pull the container from hereFor the GitHub-repo follow the link on etl-with-airflowIf youre searching for open-source data architecture you cannot ignore Druid for speedy OLAP responses Apache Airflow as an orchestrator that keeps your data lineage and schedules in line plus an easy to use dashboard tool like Apache SupersetMy experience so far is that Druid is bloody fast and a perfect fit for OLAP cube replacements in a traditional way but still needs a more relaxed startup to install clusters ingest data view logs etc If you need that have a look at Impy which was created by the founders of Druid It creates all the services around Druid that you need Unfortunately though its not open-sourceApache Airflow and its features as an orchestrator are something which has not happened much yet in traditional Business Intelligence environments I believe this change comes very naturally when you start using open-source and more new technologiesAnd Apache Superset is an easy and fast way to be up and running and showing data from Druid There for better tools like Tableau etc but not for free Thats why Superset fits well in the ecosystem if youre already using the above open-source technologies But as an enterprise company you might want to spend some money in that category because that is what the users can see at the end of the dayRelated Links: Originally published at wwwsspaeticom on November 29 2018
e4ZyiGbaj5Vjr5BRmfAp7g,This post will look at how we added velocity to our legacy databases and also assumes prior working knowledge of Kafka and event messagingAs part of our transformation to having a global platform we are breaking down responsibilities of some of our core apps In doing this we are moving a percentage of our loans managed to the newer platform dual running alongside the rest of our loans on the older platformThe newer platform has a much lower latency as it is event driven and feeds directly into our cloud Data Lake and Salesforce instance while the older platform has batch ETL jobs that feed into our legacy Postgres DWH and Salesforce at a set intervalsThe challenge is to have loan information from both sources available to Salesforce and the Data Lake at the same time to avoid any referential or reporting conflictsAdding velocity to all the loan information from our legacy batch based ETLs and getting them into Salesforce in near realtime is key in allowing our sales agents convert abandoned or half completed loan applications into revenue quickly before the window of opportunity is lostA lot of this work is also a precursor for a complete migration of reporting from our postgres data warehouse to the cloud (S3 + Athena/Redshift) ingesting data in near real time from our application Kafka topics This will also us to have any data on Kafka landing into our Datalake in near real time which open the doors to numerous benefits self-serve reporting and automated GDPR redactions to name a fewTo add velocity to our legacy app databases we decided on an open source tool called DebeziumDebezium is a connector that sits on top of Kafka Connect that captures change across different database types using that databases logging functionality For example with MySQLs Binary Logs and Postgres Logical Replication it then parses changes from these logging mechanisms serialises them as Avro and sends them to a corresponding Kafka topic with some metadata about time and database operation (Insert Update Delete) attached Debezium sits on top of our Kafka connect cluster as a long living concurrent process the amount of concurrency controlled by assigning it more or less task processesIn order to accurately capture change we also have to have a starting point or initial snapshot Debezium comes with functionality out of the box to do this but at the time the current version could not snapshot a table with enforcing a full table lock for the duration of the process To mitigate this we implemented our own snapshotting method built on AWS glue This process which looks at an RDBMS table and replicates it as is in S3 to serve as a starting point on which all of our streamed events will be appendedAfter the initial snapshot we set up Debezium connectors for the entities that we were interested in and began sending them to an S3/Salesforce Sink These events will then land alongside our initial snapshot and then are able to be used for reporting and marketing purposesBelow is an example debezium connector for a Postgres source this can be deployed by submitting a POST request to a Kafka connect cluster with a debezium plugin configuredThere are multiple different release versions of Debezium on the project GitHub page so we initially went with the latest version on confluent hub (v09) but quickly found some issues with Postgres and heartbeating We quickly switched to building a newer version built from source (v0100Beta2) so depending on your version some of the above options are subject to changeAvro is our chosen file format for all of our event data as it allows easy schema evolution for backwards compatible changes as well as self describing data so all messages are discrete and self-describingDebezium will also keep track of Schema changes on the source tables we are monitoring and the tables database schema is converted to an Avro schema and then stored in our Schema RegistrySome things worth noting turning on the WAL for the Postgres database introduces a dependency that all subscribers to a replication slot be active and listening to allow the WAL to be purged periodically If subscribers do not acknowledge that the WAL has been frequently read Postgres will hold onto it causing a steady increase in disk space used until the DB server crashes Debezium handily released a heartbeat mechanism in version v0100Beta2 to allow consistent acknowledgementAnother common gotcha is ensuring you either have a mechanism for creating Kafka topics for your Debezium topics or ensure that you have allowed automatic creation of topics in the Kafka settings as there will not be any errors thrown if the topics do not existThank you for reading! Make sure to follow us on Twitter and Medium
X9kbuLcQGkffaPEyfWRVH9,Disclaimer: the data used in this article is generated following rules for formal correctness/consistency and reflects peculiarities of real use case data These are not real dataSuppose your company has a fleet of vehicles used during operations and each of this vehicle has a tracking device that record several data from the vehicle itself like speed accelerations engine parameters and the position by a gps sensor and send all this data to a recording system in cloud with near real time connectionYour primary target may be related to safety and fleet monitoring but you still record a lot of data that you can use for risks analysisAfter some experiences with several solutions available on the market we landed on Geotab (https://wwwgeotabcom) a very flexible device with a lot of features a strong rules engine and a Marketplace full of addonsThis solution is hosted on Google CloudIf you have the same solution you can use some of the script I wrote to dump data from your devices here is the repo: https://githubNow your client asks for vehicles tracking data that are collecting speed data from vehicles to build a road risk mapThe dataset is quite simple a csv containing LatitudeLongitude Speed ieYou can find samples dataset in this repo I split the big one in a smaller slices for testing purposes: https://githubIn the dataset positions-speedscsv you will have just the data about positions and speedsFirst of all I wish to add the max road speed related to that specific positionTo do that I found several options the most effective are using the OSM database directly in a GIS software or use the Overpass API to get the dataSince using the public Overpass API to get that amount of data could not be so reliable and install on premise can be a little tricky I preprocessed the dataset with QgisFirst download the layer with road speed data I found it packed here: http://downloadgeofabrikde/europePlease note that the italian shp is not available as a single file (at the time Im writing) You have to download the 5 parts and merge together very easy operation with any GIS softwareLoad the road layer and import the csv of your datasetI used the plugin Join Attributes by Nearest to get the speed value for a specific position from the road layerRemember to select the field(s) you wish to copy between layer: It is quite fast also with this big datasetNow you can export the layer as a new csv with the metered speed and the posted speed for each positionFor your convenience the preprocessed dataset is in the github repo please download the multipart positions-speeds-maxspeedzipCreate a bucket in Google Cloud Storage and upload your datasetStart Dataprep from the Navigation menuDataprep is a very powerful tool to clean data and make quick evaluation of big dataset before going deeper in the analytics pipelineFirst you need to import the data click on Import Data and select GCS as a datasourceKeep in mind that you are working on a sample subset of your data not all values are in this sampleThe first issue I can see is that I have a lot of zeros in the Maxspeed column These are locations that for some issue dont have a posted speed recorded on the OSM databaseWe want to remove this from our analysisAnd select the rows you want to delete: Now take a look to the recorded speed column: I see that the dataset has a max recorded speed of 169km/h and there isnt any mismatched value or missing oneTo improve the analysis Ill leave the high values but I prefer to remove the lower one filtering out all the values under 30 km/hLast transformation is to add a date field in our datasetWe dont have a recording date with the day in which a value has been recorded but we can add a date field to be able to spot in the future a trend in the fleet we are monitoring With a loaded data field I can easily spot if my drivers are improving their safety behaviors or notNow its the time to move down in the pipeline and move these data to our big data storage solutionBy default the output action will be to create a new object in the Google Cloud Storage you loaded data fromWe want to push the data directly to BigQuery click on Add Publishing Action and select BigQuery and the datasetIn this case we will create a new table but in future we can just append data to the existing tableThere are indeed several options to define your needs from auto generating a new table and/or you can schedule the run of the job periodicallyPlease match the region in which your data reside usually to avoid egress costs moving data between regions in this case Dataflow will not process data across regionsFinally you can fire the job and give it some minutes to complete the loading processMeanwhile you can have details of the running job click on the right of the job name and select View Dataflow job A new window will open giving you access to the Dataflow transformations pipeline and you can see the transformations pipeline in actionOne awesome feature of BigQuery is the possibility to load a dataset directly from Google Cloud Storage so you just have to create a bucket in GS and upload the csv dataset to that bucketClick on Create table in BigQuery select create table from Google Cloud Storage and paste the URI of your bucket and filenameWe dont need to import the data with the BigQuery menu because we used a Dataprep/Dataflow pipeline to make some little tweaks to our datasetA quick check on the table after the job finish: Now you can move to the awesome new Gis visualizer: https://bigquerygeovizappspotWrite your query type in the project (dot) the dataset name (dot) the table nameWe want to build a geographical point from Latitude and longitude to do that we can use the ST_GeoPoint function that takes two values and converts it to a positionI suggest starting with small queries using limit clause to reduce the amount of data to process during the testing stepPlay with the style and you will easily plot all your values to the map in an eye catching layoutLets try some filtering of high risk value to spot potential high risk areas like intersections: This is visualizer supports a set of Geospatial function for instance with this query you can check the risk zone in a range of 9 to 12 km of radius from the Coliseum (1249230841890182) limiting the analysis to the Ring road around RomeAnd playing with the styles you can quick build a risk map based on your data: In the next part Ill show how to use more advanced geoprocessing functions of BigQuery and visualization of the data in Google Datastudio
7eHacKbmGHRv6d4vgighYF,As the Firefox data engineering teams we provide core tools for using data to other teams This spans from collection through Firefox Telemetry storage & processing in our Data Platform to making data available in Data ToolsTo make new developments more visible we aim to publish a quarterly newsletter As we skipped one some important items from Q4 are also highlighted this timeThis year our teams are putting their main focus on: Last year we started to investigate how our various tools are used by people working on Firefox in different roles From that we started addressing some of the main issues users haveMost centrally the Telemetry portal is now the main entry point to our tools documentation and other resources When working with Firefox data you will find all the important tools linked from thereWe added the probe dictionary to make it easy to find what data we have about Firefox usageFor STMO our Redash instance we deployed a major UI refresh from the upstream projectThere is new documentation on prototyping and optimizing STMO queriesOur data documentation saw many other updates from cookbooks on how to see your own pings and sending new pings to adding more datasets We also added documentation on how our data pipeline worksFor experimentation we have focused on improving tooling Test Tube will soon be our main experiment dashboard replacing experiments viewer It displays the results of multivariant experiments that are being conducted within FirefoxWe now have St Moab as a toolkit for automatically generating experiment dashboardsTo make working with events easier we improved multiple stages in the pipeline Our documentation has an overview of the data flowOn the Firefox side events can now be recorded through the events API from add-ons and whitelisted Firefox content From Firefox 61 all recorded events are automatically counted into scalars to easily get summary statisticsEvent data is available for analysis in Redash in different datasets We can now also connect more event data to Amplitude a product analytics tool A connection for some mobile events to Amplitude is live for Firefox Desktop events it will be available soonTo enable low-latency views into release health data we are working on improving Mission Control which will soon replace arewestableyetcomIt has new features that enable comparing quality measures like crashes release-over-release across channelsFor Firefox instrumentation we expanded on the event recording APIs To make build turnaround times faster we now support adding scalars in artifact builds and will soon extend this to eventsFollowing the recent Firefox data preferences changes we adopted Telemetry to only differentiate between release and prerelease dataThis also impacted the measurement dashboard and telemetryjs users as the current approach to publishing this data from the release channel does not work anymoreThe measurement dashboard got some smaller usability improvements thanks to a group of contributors We also prototyped a use counter dashboard for easier analysisTo power LetsEncrypt stats we publish a public Firefox SSL usage datasetThe following datasets are newly available in Redash or through Spark: For analysis tooling we now have Databricks available This offers instant-on-notebooks with no more waiting for clusters to spin up and supports Scala SQL and R If youre interested sign up to the databricks-discuss mailing listWe also got the probe info service into production which scrapes the probe data in Firefox code and makes a history of it available to consumers This is what powers the probe dictionary but can also be used to power other data toolingPlease reach out to us with any questions or concerns
XZbDPSnFr3AgxgTi4JURBy,The data platform and tools teams are working on our core Telemetry system the data pipeline providing core datasets and maintaining some central data viewing toolsTo make new work more visible we provide quarterly updatesA lot of work in the last months was on reducing latency supporting experimentation and providing a more reliable experience of the data platformOn the data collection side we have significantly improved reporting latency from Firefox 55 with preliminary results from Beta showing we receive 95% of the main ping within 8 hours (compared to previously over 90 hours) Curious for more detail? #1 and #2 should have you coveredWe also added a new-profile ping which gives a clear and timely signal for new clientsThere is a new API to record active experiments in Firefox This allows annotating experiments or interesting populations in a standard wayThe record_in_processes field is now required for all histograms This removes ambiguity about which process they are recorded inThe data documentation moved to a new home: docstelemetrymozillaorg Are there gaps in the documentation you want to see filled? Let us know by filing a bugFor datasets we added telemetry_new_profile_parquet which makes the data from the new-profile ping availableAdditionally the main_summary dataset now includes all scalars and uses a whitelist for histograms making it easy to add them Important fields like active_ticks and Quantum release criteria were also added and backfilledFor custom analysis on ATMO cluster lifetimes can now be extended self-serve in the UI The stability of scheduled job stability also saw major improvementsThere were first steps towards supporting Zeppelin notebooks better; they can now be rendered as Markdown in PythonThe data tools work is focused on making our data available in a more accessible way Here our main tool Redash saw multiple improvementsLarge queries should no longer show the slow script dialog and scheduled queries can now have an expiration date Finally a new Athena data source was introduced which contains a subset of our Telemetry-based derived datasets This brings huge performance and stability improvements over PrestoFor the next few months interesting projects in the pipeline include: Please reach out to us with any questions or concerns
BxTKdqsgc3SxAFMnJffva8,As the data platform & tools team we provide core tools for using data to other teams This spans Firefox Telemetry data storage and analysis to some central data viewing tools To make new developments more visible we publish a quarterly update on our workIn the last quarter we continued focusing on decreasing data latency supporting analytics and experimentation workflows improving stability and building Mission ControlTo enable faster decision making we worked on improving latency for important use-casesMost notable is that the main pings now arrive much faster which power most of our main dashboards and analysis The new rule of thumb is 2 days until 95% of the main ping data is available from activity in the browser to being available for analysisIn Firefox Telemetry we can now record new probes from add-ons without having to ride the trains which greatly reduces shipping times for instrumentation This is available first with events in 56 and scalars in 58The new update ping provides a lower-latency signal for when updates are staged and successfully applied It is queryable through the telemetry_update_parquet datasetSimilarly the new-profile ping is a signal for new profiles and installations which is now queryable through the telemetry_new_profile_parquet datasetThe new first-shutdown ping helps us to better understand users that churn after the first session by sending the first sessions data of a user immediately on Firefox shutdownThis year saw a lot of cross-team work on enabling experimentation workflows in Firefox A focus was on enabling various SHIELD studiesHere the experiments viewer saw a lot of improvements which provides a front-end view for inspecting how various metrics perform in an experimentAn experiments dataset is now available in Redash and Spark which includes data for SHIELD opt-in experiments and is based on the main_summary datasetThe experiment_aggregates dataset now includes metadata about the experiment and its reliability and speed have improved significantlyOther use-cases can build on the ping data from most experiments using experiment annotations which is available within 15 minutes in the telemetry-cohort data sourceOur data tools make it easier to access and query the data we have Here our Redash installation at sqltelemetrymozillaorg saw many improvements including: Mission Control is a new tool which makes key measures of a Firefox release like crash counts available with low latency An early version of it is now available hereOn the Firefox side about:telemetry got a major redesign which makes it more easy to navigate added a global search and aligns it with the photon designTo make analysis more effective two new datasets were added: For analysis jobs run through ATMO the reliability was greatly improved which resulted in a big decrease of job failuresAlso support was added for using of different EMR versions with different ATMO installations allowing us to test changes to our EMR configuration much more thoroughly prior to deploymentSome of the things that we will work on in the next months include: Please reach out to us with any questions or concernsYou can also find us on Twitter as @MozTelemetry
iv2oBhqFGbsqRWpwnsVsRD,The data platform team is working on our core Telemetry system the data pipeline and providing core datasets with support from the Firefox data engineering and the Data tools teamTo make new features more visible we intend to provide quarterly updates starting with this oneOn the data collection side we added scalars and added engagement measures on top We now support recording histograms in child processes and added categorical histogramsWe improved documentation starting from the Telemetry wiki page and updated onboardingThe data pipeline work powers results for re:dash and custom analysis among other thingsPlease reach out to us with any questions or concernsYou can find us on IRC in #telemetry and #datapipelineThe main mailing list for data topics is fhr-devBugs can be filed in one of these componentsYou can also find us on Twitter as @MozTelemetry
7VbkHT6nvXiPWqvHrRwn2v,When building software systems we usually deal with data from external sources This can be user input data coming from other systems etcAny data that we dont know completely ahead of time can and will behave differently than what we expected A classic example for this is user input say a text field If we dont limit the length and contents somebody will eventually enter a book lengths worth of data or try to use it to attack a systemBut the same problem extends to data from systems we control and that we might have faith in At Mozilla we have a variety of teams and products which are deployed to a large and diverse population of users across the world We may think we know how those products and the data they generate behave but practically we always find surprisesLets consider a hypothetical mobile application The mobile application has a basic system to collect product data and makes it easy to collect new string values To make it easier and more flexible for our teams to add something we dont impose a hard limit on the length of the string We have documentation on the various instrumentation options available making it easy to choose the best for each use-caseNow this works great and everybody can add their instrumentation easily Then one day a team needs data on a specific view in the application to better understand how it gets used Specifically they need to know how long it was visible to the user which buttons were interacted with in which order and what the screen size of the device was This doesnt seem to directly fit into our existing instrumentation options but our string recording is flexible enough to accommodate different needsSo they put that data together in a string making it structured so its reasonable to use later and we start sending it out in our JSON data packages: The change gets tested and works fine so it gets shipped Some time later we get a report that our product dashboards are not updated An investigation shows that the jobs to update the dashboards were timing out due to unusually large strings being submitted It turns out that some users click buttons in the view 100 times or moreWhats more a detailed review shows that the user churn rate in our dashboard started to increase slightly but permanently around the time the change shipped The favored hypothesis is that the increased data size for some users leads to lower chances of the data getting uploadedTo be clear this is built as a bad example There is a whole bunch of things that could be learnt from the above example; from getting expert review to adding instrumentation options to building the data system for robustness on both ends However here i want to highlight how the lack of a limit for the string length propagated through the systemNo software component exists in isolation Looking at a high-level data flow through a product analytics system any component in this system has a set of important parameters with resulting trade-offs from our design choices The flexibility of a component in an early stage puts fewer constraints on the data that flows through which propagates through the system and enlarges the problem space for each component after itThe unbound string length of the data collection system here means that we know less about the shape of data we collect which impacts all components in the later stages Choosing explicit limits on incoming data is critical and allows us to reason about the behavior of the whole systemChoosing a limit is important but that doesnt mean we should restrict our data input as much as we can If we pick limits that are too strict we end up blocking use-cases that are legitimate but not anticipated For each system that we build we have to make a design decision on the scale from most strict to arbitrary values and weigh the trade-offsFor me my take-away is: Have a limit Reason about it Document it The right limit will come out of conversations and lessons learnt  as long as we have one
NP7smkQc79NY5hgQPeuT2q,One of the successes for Firefox Telemetry has been the introduction of standardized data types; histograms and scalarsThey are well defined and allow teams to autonomously add new instrumentation As they are listed in machine-readable files our data pipeline can support them automatically and new probes just start showing up in different tools A definition like this enables views like thisThis works great when shipping probes in the Firefox core code going through our normal release and testing channels which takes a few weeksHowever often we want to ship code faster using add-ons: this may mean running experiments through Test Pilot and SHIELD or deploying Firefox features through system add-onsWhen adding new instrumentation in add-ons there are two options: Neither are satisfactory; there is significant manual effort for running simple experiments and adding featuresThis is one of the main pain-points coming up for adding new data collection so over the last months we were planning how to solve thisAs the scope of an end-to-end solution is rather large we are currently focused on getting the support built into Firefox first This can enable some use-cases right away We can then later add better and automated integration in our data pipeline and toolingThe basic idea is to use the existing Telemetry APIs and seamlessly allow them to record data from new probes as well To enable this we will extend the API with registration of new probes from add-ons at runtimeThe recorded data will be submitted with the main ping but in a separate bucket to tell them apartWe now support add-on registration of events from Firefox 56 on We expect event recording to mostly be used with experiments so it made sense to start hereWith this new addition events can be registered at runtime by Mozilla add-ons instead of using a registry file like EventsyamlWhen starting add-ons call nsITelemetryregisterEvents() with information on the events they want to record: Now events can be recorded using the normal Telemetry API: This event will be submitted with the next main ping in the dynamic process section We can inspect them through about:telemetry: On the pipeline side the events are available in the events table in Redash Custom analysis can access them in the main pings under payload/processes/dynamic/eventsAs mentioned this is the first step of a larger project that consists of multiple high-level pieces Not all of them are feasible in the short-term so we intend to work towards them iterativelyThe main driving goals here are: This larger project then breaks down into roughly these main pieces: Phase 1: Client workThis is currently happening in Q3 & Q4 2017 We are focusing on adding & extending Firefox Telemetry APIs to register & record new probesEvents are supported in Firefox 56 scalars will follow in 57 or 58 then histograms on a later train The add-on probe data is sent out with the main pingPhase 2: Add-on tooling workTo enable pipeline automation and data documentation we want to define a variant of the standard registry formats (like Scalarsyaml) By providing utilities we can make it easier for add-on authors to integrate themPhase 3: Pipeline workWe want to pull the probe registry information from add-ons together in one place then make it available publically This will enable automation of data jobs data discovery and other use-cases From there we can work on integrating this data into our main datasets and toolsThe later phases are not set in stone yet so please reach out if you see gaps or overlap with other projects
eSyvDghcySuHBeReBaGuZM,The project reflects MongoDB changes on Redshift in the big data platform in near real-timeDuring streaming we do bunch of different transformations than ETL That naturally may cause data to be alteredStreaming benchmark written with Python 3 just like many other parts of Big Data Platform prepares a set of ids that are ensured to be the documents which had been moved to Redshift in both ways of ETL and streaming so that we could not miss streaming documents as they may be potentially smaller in size compare to those moved with ETLWe have set up performance criteria that we consider in benchmark results Those are computed after each MongoDB and Redshift sourced data are transformed and being prepared for the comparison partThe streaming benchmark should also be run in an environment with Spark However Hadoop is not required in this case due to the small size of data that are compared And we do not require any distributed computing along the way Spark dependency is due to the MongoDB transformer which requires Spark SQL engine to extract DataFrames out of MongoDB dataDuring development of the streaming I discovered this annoying problem where the type of a field changes and the type of streamed field does not follow the schema generated by ETLThis problem can be overcome by running ETL again since it would infer the schema types over again and resolve it with the correct field typeFor example ETL can label the type of a field as integer However MongoDB is a NoSQL database which allows double typed values in that field When the double typed value is present in the streaming job the job fails during transforming processThe recovery system automatically detects if a streaming job fails and runs ETL job then starts rerunning streaming jobs This may not be a perfect solution and the data will not be updated in longer timeStreaming is the modern way to moving data in near real-time compared to mass ETL However there are still many problems emerging from the underlying understanding of streaming the data One may state that streaming is not as secure and guaranteed as the ETL In my opinion in contrast the risk can be reduced and the overhead is worth it as you can have your data in near real-time
UZwX8FW6fHRLYpKAcdivb8,From the world of AWS RedshiftAfter 12 months of using redshift as a central data store for a project I discovered that there are many performance issues related with it coupled with the fact that the cost is not friendly for the project(for startup) I was working on at the time Some of the issues are : Discouraged by the limitation Redshift presents I had to break the ego of using AWS tools for everythingBigQuery is Googles Data warehouse Solution It is super fast highly scalable cost-effective and fully managed cloud data warehouse Also known for high performance and scalable query engine on the cloud The strength of BigQuery lies in its ability to handle large datasets For example querying millions of records might take only a few seconds to get the result
AP7Zf6QbNWbq3gxipGtcaQ,As a Google Developer Expert I often have the opportunity to do amazing things some of which would be unthinkable considering where I came from The last of those was the invitation to be a guest speaker in the course Artificial Intelligence: Cloud and Edge Implementations at Oxford UniversityThe experience of teaching at one of the top universities in the world was really amazing and it made me reconnect with my inner passion of teaching othersSo in order to celebrate that today I want to bring that content to you as wellThis is the first part of a series of articles It will mostly focus on some definitions that will be needed for the ones to followThe first thing I want to do is to invite you to think about data problems Nowadays in the buzzword rich environment that we live it is very common that business acquire technologies just for the sake of it instead of using a problem driven approach For example in my career Ive faced too many times the scenario where a customer had purchased a Big Data stack because someone said it was fast or scalable or the next big thing and they actually didnt have a Big Data problemThats why I always like to remind people to think about the business first before trying to fit any kind of technology in the solution specIt is surprisingly hard to find a textbook definition of Data Engineering although it is quite easy to find data engineer job descriptions For the lack of a textbook definition per se I decided to create my own based on my personal experience and a bit of exploration on data engineer job ads: Data Engineering is the design study and development of data products pipelines and services to enable all functions of the business to have data-driven capabilitiesThe key point here is that data engineering is an enablement function in an organisation It is not the endgame but the means of achieving data-driven capabilitiesThe data engineering function should be responsible for ensuring the business has correct data in a timely manner and support other business functions to build products on top of it like machine learning models BI dashboards and so onThe first time the term Big Data appears on literature is on a paper from NASA (1997) about visualisation problems: Visualisation provides an interesting challenge for computer systems: data sets are generally quite large taxing the capacities of main memory local disk and even remote disk We call this the problem of big dataYou may stumble upon several definitions of Big Data from 3 to any number of Vs (the typical one being Volume Velocity and Variety) but I dont think that any of those are as timeless as NASAs definition: datasets large enough that dont fit on main memory or even local diskThat means that the concrete definition of Big Data evolves in the same pace as hardware evolves Is 1 GB of data Big Data nowadays? Its not but in the 90s it definitely wasIs a relational database with 2TB of data a problem of Big Data? The answer is: it depends! One shouldnt look at volume alone to define the problem but you also need to look at consumption patterns If you need to read all of it to build a model then it may be but if you are only consuming the newest rows each time to build your reports it is definitely notThats when the any-number-of-Vs definition may help you but dont let it limit you As long as your workload doesnt fit the capabilities of a single machine you will definitely need to implement some data engineering technique to handle itIve wrote that before but Ill repeat it again because I love this definition so much It is from my former boss Larry Ellison (at the Churchill Club): cloud computing is just a computer connected on the internetTechnically that pretty much sums it but cloud computing is a bit more than technical specifications It is all about service levels and a shift in the paradigm of owning the infrastructure versus renting itAccording to NIST to be considered a cloud offering the service must have five capabilities: 1) self-service 2) ubiquitous access to the internet 3) resource pooling 4) rapid elasticity and 5) metered usage and billingId say that capabilities 4 and 5 are the secret sauce here as they allow us to use top infrastructure without needing to pay top dollar for it Compare this scenario to the on-premise world where you need to plan you purchases every 3 years taking into account growth support depreciation and dozens of other factors In that world you end up paying upfront for a lot of infrastructure that you may or may not use That gets even worse if your business has sazonal peaks as you will need to size for that peak instead of the average usageCloud infrastructure makes our life simpler by reducing the risk of acquiring infrastructure As our business grow we can grow the infrastructure almost instantly… and if something bad happens along the way we are free to scale back and get relieved of the financial hurdle of unused infrastructureThat scenario is particularly important in the data engineering world especially because Big Data platforms can be really hard and costly to manage As long as you are minimising the infrastructure cost you are also minimising the risk of your businessData pipelines are the bread and butter of a data engineer job I like to see then as a single unit of work consisting of a data source a data processing component and one or more data sinks Optionally you may also have a scheduler if that workload is batch-y in natureYou may also hear about ETL or ELT pipelines ETL comes from Extract Transform and Load where the extraction process happens at source is transformed by the data processing pipeline and loaded to a data sink The ELT approach tries to swap those two steps to leverage data locality and process data in place using the tooling available at the data sinkThis will make more sense when you add to the context the type of technology a data sink consists of For instance if your data sink is Google BigQuery you have massive parallel processing capabilities that you may want to leverage for the transform stepThe tradeoffs to consider are processing costs in the pipeline versus the data sink the performance and possible bottlenecks If any of those are ambiguous you may need to revisit the business requirements technology limitations and the expertise in your teamIn terms of data sources the Variety of the Big Data Vs plays a big role here as it is the source data that will determine which technology you will be seeing here Maybe your source is a relational database maybe it is a key value store or maybe even it could be an event source platform The possibilities are limitlessAs soon as you know your data you may chose the best strategy and technology to process it The most common frameworks for handling Big Data nowadays are Apache Spark and Apache Beam but you may find several others including Apache Storm and Apache Flink Ill cover specially Spark and Beam in a follow up articleThe good thing about distributed computing frameworks like Spark and Beam is that they do a lot of the work of handling the distributed part of things leaving the developer with an API that resembles writing code for a local process You arent exempt from the responsibility of know what things run local and what runs in a distributed fashion but they surely make your life easierFinally talking about data sinks they usually have the same technology considerations as data sources but you probably want to consolidate your data in a common platform that enables your business to explore all kinds of data setsSome data may be needed in its raw form though and it is the combination of the structured and un-structured data that will make up your data lake By Wikipedias definition a data lake is: A data lake is usually a single store of all enterprise data including raw copies of source system data and transformed data used for tasks such as reporting visualisation advanced analytics and machine learningThe scheduler like Ive mentioned before is an optional component for a data pipeline as it may or may not need it depending on the pipelines nature If it is a batch process with a well defined beginning middle and end it will benefit from scheduling whenever you need repeatable results eg for daily summariesAlternatively you may have real-time or near real-time workloads that are always running and doesnt need scheduling at all although sometimes you may see scheduling used in those cases as a keep alive mechanismThe most common platform used for scheduling nowadays is Apache Airflow although some contenders are surfacing recently like Spotifys Luigi Airflow per se is much more powerful than a simple scheduler though and for the sake of brevity well revisit it in a future article in the seriesIn this article weve covered an overview of how to handle data problems from a top down approach and got an initial understanding of the key components of the tech stack for a data project For the next article well see a practical data problem and how we start to solve it using the building blocks aboveClick here for part 2: framing data problems
9vmeAWHbCXKUJJYJ8v5Gnn,In the previous article we discussed the fundamentals of a data project and covered some basic definitions This time we are going to explore the reasoning on how to approach a data problem until we get to a solution designSpoiledTomatoes is a media company specialised in streaming bad short movies Their website is open and can have both anonymous and logged in users Their service is free and their main stream of revenue is based on ad sales On the last month theyve recorded about 50 M unique users with average 13 video views per user and 25 minutes per sessionYou were hired with the main goal of increasing company revenuePlease remember the diagram from the previous article: we need to move from the business requirement to the technology and not the other way around So while we may end up solving the problem with technology we shouldnt assume anything at the beginning before digging deeper into the problemThe business requirement is (purposely) vaguely stated as increase revenue But increase how? There are infinite answers to that question and in the real world you will want to brainstorm a few ideas before starting to filter the solution space One process for that is the divergence and convergence technique from Design ThinkingI wont did deeper into that but I just want to highlight that whenever you face ambiguity in the brainstorming phase it is useful to frame the problem in the context of the business mission statement vision and valuesLets say that Webflops mission is to deliver the best (worst?) content of the world through technology making it accessible to all people in the worldThat means that if you came with the solution close the platform and charge the users for a fee it will be filtered out as it would be a change in the core business that contradicts the mission statementThe next point for analysis is what the domain knowledge tell us It is a web platform but also a media distribution system We need to take into account how the revenue stream works as wellFrom the ads point of view one hypothesis we can come with is by increasing user engagement we can increase ad revenueTo increase user engagement we want to show the user content that is more relevant to their taste so they will use the website more and see more adsThis is a problem that fits into the information retrieval domainBefore getting to the actual solution here are some questions I like to ask before tackling any kind of data problem: (This list is by no means restrictiveIn this scenario we 1) know which videos the user has seen (video views) 2) we know the fact as soon as the user clicks a video 3) we expect massive amounts of data as we can have 50M users watching videos simultaneously (and that number of users is unbounded) 4) we have the freedom to store it in anyway we want as we own the tech stack 5) the data changes in realtime and 6) yes we want to deliver content as fast as possible because new videos appear on the site every minuteA typical solution for this information retrieval problem to show the user relevant content is a recommender system (also known as recommendation system)The main purpose of a recommender system is to predict the users preferences and try to show them content that is relevant to them but they are usually* not aware of its existenceThey are particularly effective for distributing content in the long tail and are vastly used by the retail and media industries*Note: this may be tuned towards one way or another depending on the business needsA top level architecture for a recommender system may be seen below: First we have a content server that delivers the items we want to render on the page You may consider it your traditional backend serviceThen following the arrow upward we have a message queue The purpose of this queue is to capture the video views events Next a consumer processes take those video views and consolidate them into session data Please note that this first three blocks are a data pipeline like we described in the previous articleThe next pipeline is about reading the session data and processing it with a recommendation algorithm and writing it back to a recommendation DB Because of the nature of the recommendation algorithm we may need a scheduler to run it from time to timeFinally we have a recommendation service that delivers the recommendations written to the recommendation database The content server will have the logic to render those for the clients to seeFor the sake of simplicity weve omitted some components that would be required in a real world architecture like load balancers cache layers and AB testing services We will also be using a single recommendation algorithm instead of an ensemble of themSo far weve tackled a procedure of how to solve data problems while avoiding the pitfalls of choosing the technology first approach You may notice that this is the end of the second article and we still havent picked up the tech stackNevertheless we are making progress We know the problem we want to solve we have a hypothesis and a proposed solution to test that hypothesis In the next article we will dig a bit deeper on how recommendation systems works and start talking about the components of such architecture
48NFTSPX9ev2LrbexFCBEM,At Grab the scale and fast-moving nature of our business means we need to be vigilant about potential risks to our customers and to our business Some of the things we watch for include promotion abuse or passenger safety on late-night ride allocations To overcome these issues the TIS (Trust/Identity/Safety) taskforce was formed with a group of AI developers dedicated to fraud detection and preventionThe teams mission is: The TIS teams scope covers not just transport but also our food deliver and other Grab verticalsIn our early days when Grab was smaller we used a rules-based approach to block potentially fraudulent transactions Rules are like boolean conditions that determines if the result will be true or false These rules were very effective in mitigating fraud risk and we used to create them manually in the codeWe started with very simple rules For example: Rule 1: To quickly incorporate rules in our app or service we integrated them in our backend service code and deployed our service frequently to use the latest rulesIt worked really well in the beginning Our logic was relatively simple and only one developer managed the changes regularly It was very lightweight to trigger the rule deployment and enforce the rulesHowever as the business rapidly expanded we had to exponentially increase the rule complexity For example consider these two new rules: Rule 2: Rule 3: The system scans through the rules one by one and if it determines that any rule is tripped it will check the other rules In the example above if a credit card has been declined more than twice in the last 3-months the passenger will not be allowed to book even though he has a good booking historyThough all rules follow a similar pattern there are subtle differences in the logic and they enable different decisions Maintaining these complex rules was getting harder and harderNow imagine we added more rules as shown in the example below We first check if the device used by the passenger is a high-risk one eg using an emulator for booking If not we then check the payment method to evaluate the risk (eg any declined booking from the credit card) and then make a decision on whether this booking should be precharged or not If passenger is using a low-risk device but is in some risky location where we traditionally see a lot of fraud bookings we would then run some further checks about the passenger booking history to decide if a pre-charge is also neededNow consider that instead of a single passenger we have thousands of passengers Each of these passengers can have a large number of rules for review While not impossible to do it can be difficult and time-consuming and it gets exponentially more difficult the more rules you have to take into consideration Time has to be spent carefully curating these rulesThe more rules you add to increase accuracy the more difficult it becomes to take them all into considerationOur rules were getting 10X more complicated than the example shown above Consequently developers had to spend long hours understanding the logic of our rules and also be very careful to avoid any interference with new rulesIn the beginning we implemented rules through a three-step process: Sometimes the use of English between steps 2 and 3 caused inaccurate rule implementation (egOnce a new rule is deployed we monitored the performance of the ruleBased on implementation each rule had dependency with other rules For example if Rule 1 is fired we should not continue with Rule 2 and Rule 3As a result we couldnt keep each rule evaluation independent We had no way to observe the performance of a rule with other rules interferingAs Rules 2 and 3 depend on Rule 1 their trigger-rate would drop significantly It means we would have unstable performance metrics for Rule 2 and Rule 3 even though the logic of Rule 2 and Rule 3 does not change It is very hard for a rule owner to monitor the performance of Rules 2 and Rule 3When it comes to the of A/B testing of a new rule Data Scientists need to put a lot of effort into cleaning up noise from other rules but most of the time it is mission-impossibleAfter several misfiring events (wrong implementation of rules) and ever longer rule development time (weekly) we realized  No one can handle this manuallyWe decided to take a step back sit down and closely review our daily patterns We realized that our daily patterns fall into two categories: These two categories are essentially divided into two independent components: Based on these findings we got started with our Data Orchestrator (open sourced at https://githubcom/grab/symphony) and Griffin projectsThe intent of Griffin is to provide data scientists and analysts with a way to add new rules to monitor prevent and detect fraud across GrabGriffin allows technical novices to apply their fraud expertise to add very complex rules that can automate the review of rules without manual interventionGriffin now predicts billions of events every day with 100K+ Queries per second(QPS) at peak time (on only 6 regular EC2s)Data scientists and analysts can self-service rule changes on the web portal directly deploy rules with just a few clicks experiment and monitor performance in real timeBefore we decided to create our in-built tool we did some research for common business rule engines available in the market such as Drools and checked if we should use them In that process we found: Given the above constraints we decided to build our own rule engine which can better fit our needsThe diagram depicts the high-level flow of making a prediction through GriffinIn an abstract view a rule inside Griffin is defined as: Rule: We allow users (analysts data scientists) to write Python-based rules on WebUI to accommodate some very complicated rules like: This significantly optimizes the expressive power of rulesValues of a rule When a rule is hit more than just treatments users also want some dynamic values returned Eg a max distance of the ride allowed if we believe this booking is medium riskOur answer is NoThe below screenshot is the process snapshot on C5large machine with 2 vCPU Note only green processes are activeA lot of trial and error performance tuning: One of the most popular aspects of Griffin is the WebUI It opens a door for non-developers to make production changes in real time which significantly boosts organisation productivity In the past a rule change needed 1 week for code change/test/deployment now it is just 1 minuteBut this also introduces extra risks Anyone can turn the whole checkpoint down whether unintentionally or maliciouslyHence we implemented Shadow Mode and Percentage-based rollout for each rule Users can put a rule into Shadow Mode to verify the performance without any production impact and if needed rollout of a rule can be from 1% all the way to 100%We implemented version control for every rule change and in case anything unexpected happened we could rollback to the previous version quicklyGriffin evolved from a fraud-based rule engine to generic rule engine It can apply to any rule at Grab For example Grab just launched Appeal automation several days ago to reduce 50% of the human effort it typically takes to review straightforward appeals from our passengers and drivers It was an unplanned use case but we are so excited about thisThis could happen because from the very beginning we designed Griffin with minimized business context so that it can be generic enoughAfter the launch of this we observed an amazing adoption rate for various fraud/safety/identity use cases More interestingly people now treat Griffin as an automation point for various integration pointsOriginally published at https://engineeringgrabcom
G9DCqM6GfszqtojYu3zDED,Amazon Redshift is the primary data warehousing solution used at GumGum Apart from the real time reports which are powered through Druid Redshift fuels the majority of our reporting capabilities Being a fully managed solution from Amazon we do not have to maintain the Redshift cluster be it the hardware or the redshift engine Redshift is column oriented and its massively parallel processing architecture (MPP) makes it petabyte scalable Amazon Redshift also includes Redshift Spectrum which can directly query unstructured data stored in S3A simplified picture of our ETL process looks as below: The raw ad server logs are parsed by the Spark jobs which are loaded into Redshift raw fact tables The raw tables are then aggregated into tables with less granularity so that the queries can run faster These tables are queried by our reporting applications However at times we have to support ad hoc requests which require us to reparse the ad-server logs This is because the data in raw tables is only retained for 35 daysOur Redshift cluster consists of 14 dc18xlarge nodes each which has 256 TB SSD storage Our current data usage is almost 56% of this storage out of which 50% is used to store 35 days of raw tables alone Hence retaining raw tables for longer duration demands increased storage needs of the cluster which is not always ideal This data is not queried often But Redshift architecture before Spectrum forced us to add nodes whenever we needed more storage Adding more nodes resulted in adding more storage as well as more compute Since it added both storage and compute it was more expensive as well Our real need was to just add more storage at a lower costThat is why Spectrum was a big relief to us Spectrum and Athena make separation of compute with storage possible It uses S3 as a data store and in S3 you pay for what you use Compute is charged per query basis Hence we decided to use Redshift Spectrum to support ad hoc querying which needs data from raw logs for longer durations of time Our ETL process just had to be modified slightly to store the data in parquet format in S3 Parquet format ensured that the data is compressed storage and query efficient For optimal S3 performance all data files were decided to be around 128 MB The same data can also be used by Amazon Athena for querying The modified ETL looks like this nowSpectrum and Athena works on pay per query model where the charges are $5 per 1TB of data scanned For efficient use Spectrum and Athena supports partition keys which provides a lookup to the S3 folders that should be queried As most of our ETL jobs run hourly our partition key is a timestamp field with hourly granularity The structure of the S3 folder should be similar to s3://path/to/the /folder/partitionKeyName=year/month/day/hour to support this partition key For example a valid folder name would be s3://parquet-logs/ad_events/partitionKey=2017/12/25 and the partition key also has to be named partitionKey in the schema definition of the external tableThe structure of our external tables in Spectrum looks as below: We alter the table hourly to add the new partition value to the table The values of the partition keys values and the S3 location it refers can be seen in the system table svv_external_partitions The queries in Spectrum and Athena should use the partition key to limit the data being scanned by the engineAmazon Redshift Spectrum external tables do not support statistics The database engine uses heuristics or simple row counts to determine the join order By default when there is a query which joins external table with the Redshift table external table is assumed to be the bigger one Otherwise it looks for a table property called numRows to determine the larger table We set this property every day as a part of ETL for efficient queryingBased on the basic benchmarking we found that Redshift Spectrum is 2 7 times slower than Redshift Athena and Redshift Spectrum results are comparable however when queried over long range of time Athena seems to slightly outperform Redshift Spectrum Redshift Spectrum has an advantage that it supports joins and other query operations with the tables stored in Redshift while Athena does notTo conclude Redshift Spectrum removes hassles of scaling maintaining servers or provisioning by its serverless architecture In addition it separates storage from compute and allows us to scale separately as needed We are optimistic that Redshift with Redshift Spectrum will further democratize our data and ease the ad hoc reporting needs reducing constraints on the data storageWere always looking for new talent! View jobs
GqVEwHZUXeVFqFRmAXcnxF,Much can be said about the cutting edge technologies developed and used by GumGum but a core component of GumGums business involves the effective display of digital advertisements We work with agencies and brands to create engaging and relevant ads that are shown on the websites of our publisher partners To accomplish this we rely on being able to carefully target ads to specific user groups in specific browsing situationsA typical behind-the-scenes conversation between GumGum and one of its advertising partners might proceed as followsAdvertiser: We want 100000 BMW ads displayed to males age 25 40 living on the West Coast making $150k per year using a mobile deviceSo a Serious Business Question is How many spaces for ads will be available in the near future subject to campaign-specific restrictions? Hence the need for InstavailsTo elaborate a unit of inventory is a piece of real estate on a publishers page where an ad can potentially be placed GumGum has various types of ad products (eg In-Image In-Screen and Native all described here) so a unit of inventory could be an image (for In-Image) a small section of a page (for Native) or the whole page itself (for In-Screen) Of course the publisher must support these different product types for it to count as inventoryThe goal of Instavails is to give GumGum a clear picture of how much inventory will be available in the near future subject to various restrictions like those mentioned above It finds the number of matching impressions from the last 365 days and predicts future numbers based on those results The end result looks something like thisThe goal of this post is to describe just how we arrive at this pictureEssentially all of Instavails lives in the AWS cloud • Ad impression logs are stored in S3 (This happens continuously unrelated to this project) • Log data is sampled via a data pipeline and stored in S3 (This happens each night for the previous days logs) • Past years samples are loaded into a Spark SQL database on an EMR cluster (The data is updated each night) • Queries are sent to the cluster they executed and forecasts are made then everything is returned to the serverBefore discussing these components we should mention more about counting inventory To predict available inventory in the future we must have a way to count inventory from the past This would be easy if we had a master list of all of our publishers web pages but the web is dynamic: publishers add/delete pages constantly and it would be unreasonable to expect each publisher to update us with all such changes Instead we consider inventory impressions Each time a user visits a page we consider that an event and we log it (usually in JSON format) with information about the publisher page user etc Thus we can count inventory by searching log data This data is generated at a rate of 100s of GB/dayHere are a few words about Apache Spark for those who are unfamiliar It is an open source cluster computing framework that provides an API for parallel fault-tolerant in-memory (ie fast) computation The main data abstraction in Spark (16) is the DataFrame which can be thought of as distributed data with a schema (much like a distributed SQL table) Spark supports a number of programming languages (we primarily use Scala) and data sourcesOn our AWS cluster Spark jobs are controlled by the driver which runs on the master node Each worker node has one or more executors which process tasks related to the overall jobEach day an AWS Data Pipeline runs to sample the inventory impression logsSimply put we have an enormous amount of log data (petabytes per year) and querying all of it would take too long Sampling means we keep a small but representative portion of the data and query that insteadThe most basic type of sampling scheme is called uniform sampling It selects elements from the base dataset with equal probability that is from a uniform probability distributionUniform sampling is easy to implement and it is very fast but it has many downsides For example we want to consider some impressions to be the same even though they might contain different data: impressions from the same user on a given web page over the course of a day should be treated as copies of the same impression In this way one user who happens to view our publishers pages too often will be over-represented in a uniform sample This is troublesome because one factor in how GumGum serves ads is a frequency cap: we dont show ads to the same user to frequently (eg more than once an hour) Because a uniform sample will be biased towards commonly-occurring items it is the best choice for this projectAs an alternative we use a sampling scheme that samples uniformly from the distinct items in our set of inventory impression logs The name of the algorithm is AMIND(M): Augmented min-hash distinct-item sampling with at most M distinct items This sampling scheme is described in Sampling Algorithms for Evolving Datasets Rainer GemullaAs the name indicates a major ingredient in AMIND(M) is a hash function Let H be a prime number and choose A B ∈ {0 … H  1} randomly The parameter H is called the hashmod since we now define h(x) = Ax + B (mod H) where x is the integer representation of an item in the dataset (We dont explain exactly how x is obtained here) One can prove that values of h look uniformly random in the set {0 … H  1} which is crucial for creating a representative sample Here is the (deceptively simple) algorithm for creating an AMIND(M) sampleWith an appropriate data structure for the sample the complexity of INSERT is O(log M) and the complexity of MAXHASH is O(1) For example we use a sorted map where the keys are the hash values and the values are lists of corresponding impressions Note that sample creation requires scanning the entire dataset to try to insert each item Also the algorithm is a priori not parallelizable since we have to keep a sorted collection of hashesThis may seem like a major drawback given that the base dataset could be hundreds of gigabytes in size However a useful fact about AMIND(M) samples is that if M and h are fixed the AMIND(M) samples of a dataset form a monoid under a special union operation Namely we can combine separate AMIND(M) samples A natural application of this is to sample each hours log data in parallel and then combine the resultsHeres the algorithm for combining different AMIND(M) samples • Create samples for all hours in parallel (with same M h) • Combine the samples and group by hash values • Sort by hash value and take all items with M smallest hash valuesAs mentioned the sampling occurs nightly as part of a data pipeline The algorithm is implemented in Scala as a Spark job One basic question is how to process all hours simultaneously? In other words what is a way to have Spark operate on separate DataFrames simultaneously? The naive approach (ie pointing Spark to a list of data sources) does not workHere foldLeft allows evaluating a given 2-argument function (in this case union) against consecutive elements of a collection where the result of that function is passed as the first argument in the next invocation and the second argument is the current item in the collection The only limitation with this approach is that each hours data must fit in one clusters RAMHow to relate the sample to the original dataset? In other words if we execute a query on the sample how does that result compare to the same query executed against the original dataset? It turns out that it is enough to find Mth smallest hash value Recall that R is the dataset S is the sample and the hash function is h(x) = ax + b (mod H) where H primeThere is some important impression data that is not stored in the logs For example we need to enhance each sample with BlueKai demographic data from a Cassandra table One such demographic category is 27729: Interest in Robots Monsters & Space ToysBlueKai data is associated with users who view publisher pages and we use the DataStax connector to access Cassandra from Spark The data in Cassandra is stored as tuples (visitor ID bluekai category ID) The idea is to get all distinct visitors from our sample do a group concat to get all corresponding category IDs per visitor ID as a string then join that with sample Spark makes this simpleHere foldLeft allows evaluating a given 2-argument function (in this case union) against consecutive elements of a collection where the result of that function is passed as the first argument in the next invocation and the second argument is the current item in the collection The only limitation with this approach is that each hours data must fit in one clusters RAMHow to relate the sample to the original dataset? In other words if we execute a query on the sample how does that result compare to the same query executed against the original dataset? It turns out that it is enough to find Mth smallest hash value Recall that R is the dataset S is the sample and the hash function is h(x) = ax + b (mod H) where H primeThere is some important impression data that is not stored in the logs For example we need to enhance each sample with BlueKai demographic data from a Cassandra table One such demographic category is 27729: Interest in Robots Monsters & Space ToysBlueKai data is associated with users who view publisher pages and we use the DataStax connector to access Cassandra from Spark The data in Cassandra is stored as tuples (visitor ID bluekai category ID) The idea is to get all distinct visitors from our sample do a group concat to get all corresponding category IDs per visitor ID as a string then join that with sample Spark makes this simpleFor those unfamiliar the AWS Data Pipeline framework provides a way to manage complex data processing workloads in a scalable fault tolerant and repeatable way It is easy to handle scheduling failure notification retries etc Heres the directed acyclic graph corresponding to our sampling pipelineThis pipeline runs nightly after previous days logs have been saved to S3The main way that users interact with our database is through Spark Job Server which is a RESTful interface for submitting and managing Spark jobs contexts and jars When running on our cluster it provides a web UI to see all jobs contexts and jarsThe main benefit of Spark Job Server is that we can start one Spark SQL context create our DataFrames (described below) and the data will be available indefinitely for queries To use it simply extend the SparkSqlJob trait to persist the Spark SQL ContextThe RunJob method is where the actual interesting stuff happens such as executing queries The Validate method just checks the inputTo create the Spark SQL database we read each daily sample from the last year and enhance (ie join) with extra dataThe extra data comes from MySQL via Sparks built-in JDBC support This data includes publisher data that changes very frequently (so it needs to be fresh) and it is not present in the impression logsThe final dataframe is the union of the daily samplesWe partition the final DataFrame by IP address so that it is (mostly) evenly distributed across the cluster Giving executors even amounts of data reduces stragglers We arrived at 400 by trial and errorUsers can put restrictions into a web interface and these are translated into simple SQL-style queries Roughly restrictions map to SQL predicates Heres an example of a query that is sent to Spark Job ServerThese numbers are converted into a graph on the frontendThe initial query doesnt do everything we need so we have to process it to account for date and frequency cap For the frequency cap we literally cap the count of IPsThe result is close to a date → count map but there is a bit moreSpark SQL allows for SQL style queries so most of the hard work (eg group by) is already done Once the query is parsed we just need to extract the two relevant fields and apply the estimatorThe result now is a date → count map which we turn into a JSON string Spark Job Server returns another JSON as outputOur tables have several multi-label columns such as keywords Keywords describe the content of the page where the impression happenedOur first attempt was to store keywords as comma-separated strings and query with the LIKE operatorThere were several major problems with this approach First it yields inaccurate results (eg angel vs angels) Second and most importantly is performance There could be thousands of keywords in a users restriction so thousands of predicates in the resulting query This means slow execution timesAnother idea was to store keywords in arrays Unlike MySQL Spark SQL supports array data types (among others)
XnVx77Crnpf6A6ghhjYtF7,The above video contains two talks presented at the Kafka meetup hosted at GumGum on 19th November 2019Abstract:In this session Alex Woolford will walk us through some practical near real-time examples that touch on analytics systems integration and machine learningBio:Alex is a systems engineer at Confluent the company founded by the original creators of Apache KafkaHes been working with a diverse set of open source technologies for the past 7 years specifically Hadoop et al StreamSets and most recently Kafka Alex maintains a YouTube channel of geeky practical examples which can be found at https://wwwyoutubecom/c/alexwoolfordOutside of work he enjoys floating in a tank playing guitar (very very badly) and yelling from the sidelines at his kids sporting eventsAbstract:Today is the day that you have to upgrade your Apache Kafka production cluster You trained well on your staging cluster but there are a lot of steps to upgrade a clusterYou are afraid that you will miss some steps on some brokers leading to production failures or configuration drift You wish that you could deploy Kafka just like any of your stateless application Immutable infrastructure can help you It means that components of your infrastructure get replaced at every deployment not just updated in-place It ensures you a consistent reliable simple predictable deployment processAt GumGum we are running 4 Kafka clusters along with Kafka Connect and Schema Registry  more than 90 nodes in total Using the techniques discussed in this talk we improved simplified and accelerated operations on this infrastructure we manage What you can take home from this talk:- How to build a Kafka ecosystem as an immutable infrastructure- Considerations to take into account to be able to have auto-healingclusters- How to speed up the configuration updates upgrades and recovery ofyour streaming platform with minimal headaches- How to monitor your stack in this immutable infrastructureBio:Karim is a French Software Engineer SRE at GumGum He holds a masters degree in Computer Science and he has been in charge of GumGums Kafka infrastructure for the past 2 years
Xp5vByVEmGMtbHW5wLAaTw,When the organizations scale and the data explodes it becomes vital to have scalable data architecture This post revisits the problem statement discussed here but for an entirely different scale To give a quick recap the goal is to forecast the inventory impressions per day given a set of targeting rules and sample data This time the inventory being forecasted is programmatic inventory In part one of the blog post Jatinder Assi discussed in detail about data architecture and distributed sampling on the programmatic inventory In part two here I will focus on enabling the forecasting application at this scale using Delta Lake and Delta Caching in Apache Spark on DatabricksThe application which reads the sample data and performs the forecast is called Search and Forecast application The Search application reads the sample data along with the targeting rules which are provided by the user and builds the time series of the form impressions per day This resulting time series is used by the Forecast application to train the forecasting model and predict the time series for the next n days We use the same forecasting models as mentioned in the previous post The main topic of discussion here is the Search application which has to deal with an entirely different scaleIf the Search application has to read the sample data from S3 for every forecasting request we cannot adhere to the SLA of the forecast response time of less than 30 seconds The obvious choice is to cache the data in memory on Spark however this is an expensive choice For the scale at which we operate for programmatic inventory we will need a huge cluster if we were to cache the entire data on a Spark cluster which is not viable An alternate choice is to cache the data on the disk Though the performance wont be as good as in-memory caching we can still achieve the SLA But as the sample data gets refreshed daily through the daily pipelines we will need to recreate the cache every single day which is tedious Moreover after evaluation we found that we will need at least 35 c42xlarge nodes as we will still need enough memory for the IO and compute operations to build the time series data We thought of alternatives and landed on Delta lake with Delta cachingDelta Lake is an open source storage layer originally developed by Databricks and later open sourced at Spark Summit 2019 Delta Lake brings ACID transactions to Spark but for our use case the feature of most interest is scalable metadata handling The transaction log in Delta Lake keeps a record of every single transaction that has occurred on the Delta Lake (and hence supports features like versioning and time travel) So when new sample data gets written into Delta Lake Spark checks the transaction log for the new writes and updates the table automatically without having to explicitly refresh the table dailyWe schedule OPTIMIZE VACUUM and ANALYZE on the cluster dailyTo cache the Delta table on the cluster we use Databricks Delta caching (previously called IO cache) Delta cache stores the data on the disk using a fast intermediate format which supports accelerated reads Successive reads of the same data are always done locally which saves IO time drastically Delta caching is enabled by default for i3xlarge instance typesThe data is entirely stored on the disk which frees the memory for map-reduce operationsThe data stored and read from Delta cache is typically faster than Spark Caching We can monitor the Delta cache metrics on Storage tab of Spark UI which shows how much data is cached on each node volume of data read from S3 volume of repeated reads from Delta Cache and so onWe dont need to invalidate or load the delta cache explicitly But to warm up the cache in advance CACHE SELECT command can be used If the existing cached entries have to be refreshed REFRESH TABLE statement can be used which is lazily evaluatedThe high level architecture of the Search application can be summarized as below: With Delta Lake and Delta Caching our cluster size dropped to 25 i3xlarge nodes which is more efficient and cost effective when compared to disk caching and in-memory cachingWere always looking for new talent! View jobs
BWpJ6CWYN6kDUeGWxzFwQ5,GumGum receives around 30 billion programmatic inventory impressions amounting to 25 TB of data each day Inventory impression is the real estate to show potential ads on a publisher page By generating near-real-time inventory forecast based on campaign-specific targeting rules we enable the account managers to set up successful future campaigns This talk Real-Time Forecasting at Scale using Delta Lake and Delta Caching which Jatinder Assi and I presented at Spark + AI Summit 2020 highlights the data pipelines and architecture that help us achieve a forecast response time of less than 30 seconds for this scale Spark jobs efficiently sample the inventory impressions using AMIND sampling and write to Delta Lake We talk about how we enable time series forecasting with zero downtime for end-users using auto ARIMA and sinusoids that capture the trends in the inventory data and discuss about AMIND sampling Delta Lake Databricks Delta caching and time series forecastingWe also discuss the details around this solution in two tech blogs here: Were always looking for new talent! View jobs
TEfMd7YaTiW5Cd92FnX7o4,Our advertising data engineering team at GumGum uses Spark Streaming and Apache Druid to provide real-time analytics to the business stakeholders for analyzing and measuring advertising business performance in near real-timeOur biggest dataset is RTB (real-time bidding) auction logs which amounts to ~350000 msg/sec during peak hours every day It becomes crucial for the data team to leverage distributed computing systems like Apache Kafka Spark Streaming and Apache Druid to process huge volumes of data perform business logic transformations apply aggregations and store data that can power real-time analyticsEvery ad-server instance produces advertising inventory and ad performance logs to a particular topic in the Kafka cluster We currently have 2 Kafka clusters along with centralized Avro Schema Registry with over 30 topics with producer rate ranging from 5k msg/sec to 350k msg/secIn order to process huge volume of advertising data in near real-time we leverage Spark Streaming on Databricks by running spark application per Kafka topic We then apply data transformations and build Druid Tranquility connection on every single spark worker to send transformed data in real-time to druid in a distributed fashionDruid Tranquility ingests data via HTTP push in realtime to druid and providing abstraction for connecting to druid handles partitioning replication and seamless druid realtime node service discovery and data schema changes Due to our nature of our advertising data and business logic we clean validate and explode on various columns which are nested and list data-type thus resulting in 10x more data that we send from tranquility to druid real-time nodesIn Druids data model every dataset is stored in datasources (similar to tables in relational DB) Key components of every datasource are timestamp dimensions metrics Timestamp serves as primary partition column dimensions are used to filter query/groupBy and metrics are values that can be aggregated and must be numericDuring real-time ingestion real-time nodes will roll up every datasource based on predefined query granularity (1 min or 1 hour in our use cases) dimensions and perform aggregation on all numeric metrics This rollup process significantly reduce storage in druid data nodes post-aggregation Once the data is aggregated and it will be permanently stored in historical nodes and deep storage(S3) based on the datasource retention policyDruid broker nodes are entry point for any user data query based on the query params druid broker can either query real-time data segments from real-nodes and/or query post-aggregate processed data from historical and deep storage nodesDruid supports two types of query languages: Druid SQL and native queries via REST endpoint Native queries are most efficient way to query data from druid in real-time We use Apache Superset (native queries) for engineering teams and our BI Platform Looker (Druid SQL) for data analysts and business teams to build data insights with real time data pointsAs with any distributed computing and storage system there will be handful of challenges and tuning that will need to happen Lets talk about some of those challenges we facedAfter closely debugging spark-streaming app and real-time druid nodes we realized that a lot time on spark-streaming end is spent on network out to druid which resulted in idle CPU time for spark workers On Druid real-time nodes we were not able to handle peaks hours for datasources with high cardinality dimensions and had to continue horizontally scaling real-time nodesTo tackle real-time ingestion cost and performance we implemented micro-batch (~15sec) aggregation in spark-streaming so instead of Druids real-time ingestion rate at 31M msg/sec we reduced it to 280k msg/sec with additional computation step in spark-streaming app to aggregate data every ~15 secs before we send it out to druid via tranquilityThis resulted in 60% cost reduction for spark-workers as there was significantly less CPU idle time and 70% cost reduction in druid real-time nodes as ingestion rate dropped to ~280k msg/secApache Spark and Apache Druid has been crucial at GumGum to provide real-time insights for the business After multiple iteration of performance tuning we have been able to build a capable and cost-effective system but theres still more work to do Our data volume and rate of ingestion is constantly increasing as are the business users and their query use cases As next steps we will be exploring options to build exactly-once semantics real-time Analytics pipeline with up-to 1 hour data aggregation windows instead of micro-batch aggregations in sparkWere always looking for new talent! View jobs
fLhT8k6g4VBjrQf7cZJmq2,Maxime Nay Lead Data Engineer at GumGum gave a talk explaining GumGums Data Architecture and challenges associated with it on February 15th 2018 at South Bay Java Users Group GumGum produces over 50 TB of new raw data every day It amounts of more than 100 billion events per day These events are processed using a typical lambda architecture For a given use case we have a batch pipeline and a real time pipeline Data produced from both of these pipelines is them merged to give a complete view of the data GumGum has more than 70 such pipelines Some of them do not have the real time component Processing data at such scale involves maneuvering through many challenges Maxime talks about the challenges and the steps taken to solve some of the problems we facedProcessing 100 Billion Events a Day from GumGum on VimeoThe slides used in this presentation can be viewed at http://bitWere always looking for new talent! View jobs
nnQUPZ7YL4yM7BGuvHapEH,Can you move your data pipelines to a serverless architecture? Should you? At GumGum we just built such a pipeline at scale using AWS Here are our tips and our feedback on the limitations of such an architectureWere not going to talk about pipelines that use Amazon Lambda Serverless here means fully managed services when the servers are abstracted away from the user Think about the services such as S3 IAM or DynamoDB  it is not your responsibility to deal with the AWS instances behind themSo here our focus is on the ETL data pipelines that use S3 Glue Athena and Redshift SpectrumIn the industry its common to differentiate batch from streaming data pipelines At GumGum we utilize both and they work greatHere are the caveats with our classic pipelinesWhat if we could have serverless data pipelines that we dont need to keep updating and that would let us run SQL queries on all our data stored on S3? The first serverless data pipeline we set up at GumGum receives an average of 20 TB of compressed files a dayTo facilitate this we use Amazon S3 as the storage rather than a database such as Redshift Thats the main difference with our classic data pipelines The good news is that if you use AWS it is very likely that your data is already on S3Glue is the central piece of this architecture Amazon brands it as a fully managed ETL service but we are only interested in the Data Catalog part here using the below features of Glue: This service is very powerful because it lets us design a pipeline in a generic way that is not specific to a schemaBoth Athena and Spectrum are services to run SQL queries on files stored on S3 Both are integrated with the Glue catalog so you can query the tables created by your Glue crawlers right awayYou can use either or both Spectrum requires an existing Redshift cluster so its more relevant if you are already using Redshift Otherwise Athena can be usedBehind the scenes they use different engines developed by different AWS teams Result is that they dont behave exactly the same but well come back to thatUsing these 3 or 4 services we get this simple architectureIn a nutshell your data is stored as files on S3 Glue crawlers scan these files and update the Glue Catalog Athena and Spectrum let you query these files using the schema defined in the Glue catalog
T9U9aV9EU2tDukekBFJ5G8,AWSs Database Migration Service (DMS) is often misunderstood as a service that can only migrate your data to the cloud But the service could be very useful for replicating data between the two datastores within the cloud as well In our case both of our datastores  RDS (MySQL) and Redshift were already in the cloud The MySQL database is our primary OLTP database and many of our employees login to an internal application to modify data within this database This database contains all of our dimension tables Our reporting dashboard connects with our data warehouse built on Redshift This database contains all of our fact tables In order to run the reports accurately its important that all the modifications done to the dimension tables in MySQL are transferred to Redshift in timely mannerBefore using DMS we had a Groovy script that was transferring data from MySQL to Redshift every night This script would simply read all the rows from a MySQL table delete all the rows in Redshift table and insert all the rows in the Redshift table This would take down our reporting dashboard for 30 minutes every night But we started opening offices worldwide (London and Sydney) and this wasnt an option anymore Furthermore some users were demanding that changes to some tables are reflected in the warehouse immediately Our script was simply not capable of replicating changes in real timeThere are many options available in the market for such a replication  FiveTran Alooma Informatica There is even an open source solution  Tungsten But we were always looking for a managed solution When we learned that AWS offered a similar solution it was a no brainer Our security conscious ops team didnt like the idea of sending sensitive data to an outside company server and bringing back in again Furthermore it would require no approvals from anybody to use a one more AWS service DMS also have a good API which makes it easy to useOnce decided the implementation was fairly easy DMS has concept of replication tasks and replication instances You can define source endpoints (MySQL) and target endpoint (Redshift) Creating replication tasks to replicate data from one table to another table was few clicks in the console You can place one or more replication tasks on one replication instance You need to enable ROW level binlog in MySQL and create the replications tasks DMS will watch the bin log for any changes and as soon as the changes appear in binlog DMS will replicate it to the target tablesThe motive behind this post is to share the knowledge we gained while implementing DMS at GumGum Here are some of the implementation issues we faced: Here are some of the issues we faced in production: 3 Beware that outages in other AWS services can break DMS replicationIn 90% cases simply restarting a replication task works! In cases where you cant restart the task you can simply reload the table and start replication from scratch We take out permissions for our dashboard to access the target table in Redshift and then reload the table using DMS Taking out permission ensures that while DMS is reloading a table if a report tries to access the table it errors out Its better to error out than show a report with wrong data Having a script to reload table comes handy in such cases Since these tables are fairly small reloading wont last more take 2 to 3 minutes And certainly dont forget to look at the DMS log to see the root cause of the problem DMSs replication log is usually very helpfulWere always looking for new talent! View jobs
2VkABG4a7uh9W6hyAuKyUN,Before I started as a Data Engineer two years ago I had no idea what the role entailed or how it differed from data science and data analytics Job titles with the word data in them are known to be an enigmatic black box Thats true even for folks in technical roles This post is what I would have wanted to read when I was trying to fit the pieces of the data pipeline togetherAt Gusto Data Platform Engineers support the other data teams  Data Analysts and Data Scientists If those teams need access to data we move and adjust it to a usable format for them If they need a server that can crunch numbers for them quickly we maintain that server If they need a tool to explore data interactively we make sure they have it  whether its an external tool that we keep connected or a tool we build ourselvesPrimarily our work impacts the business side of Gusto For example we recently switched our customer satisfaction (CSAT) surveys to a new vendor and the customer-facing teams who rely on the surveys needed to be able to access the data in a similar way as beforeMy job was to collect the raw responses from the third-party vendor transform the data to make it query-able and then make sure a partner on the Data Analytics team had access to it From there the Data Analyst made graphs and tables so that the customer-facing teams were able to see their stats in a way that was already familiar to themIf you only needed to use data from one database you may not need to move it beyond creating a read-only cluster specifically for this purpose Even then its likely that the database is structured in a way that works for the application or the engineers rather than for the people who need to analyze the dataIf your users (ie data analysts and business teams) need information from more than one database or want to use data from third party tools youll need some way to group all the data together (ie a data warehouse) If your database doesnt keep a record of each write you may need some way to allow users to get a historical view of the databaseI could keep going but you get the idea The data is very unlikely to be in the most ideal state for queries that yield fast insights which is why it needs to be copied and restructuredIn our case we gather data from our four internal databases which contain the data created by our apps For example whenever a customer runs a payroll multiple rows are created or updated across multiple tablesWe also gather data from external tools via their APIs These include Qualtrics Marketo and Zendesk Chat among othersAnother way to get data from an external tool is to connect to the database directly We do this with Salesforce but we also copy data into certain Salesforce tables from our internal databasesIn our case we expose a data lake to the Data Analytics and Data Science teams The data lake is meant to be a place of discovery for these teams Since the data is raw it takes less work for the Data Engineering team to manage but it doesnt eliminate data that could be useful for skilled explorersMore broadly within Gusto we expose a data warehouse of tables that are structured to be queried quickly and only contain a subset of all the data in the lake For Gusto all of our data goes through the lake before it gets to the warehouse and only the data that we know is useful and worth cleaning gets to the warehouse These tables are meant to be more easily understood and allow for varying levels of access to sensitive data through different schemasOverall we can think about the difference like that between a lake and tap water Lake water in some places is safe to drink but usually its not and in most cases getting the water is a lot less convenient than drinking from the tap Tap water on the other hand is enriched with fluoride is stripped of lead and bacteria and is at your fingertips when you want itSimilarly the lake is a huge amount of data (all that my team collects) To make any use of it you need to know whats in it and how to treat it to write a query that returns anything a human can make sense of The warehouse has been cleaned for you; its in tables that make sense for known use cases; and you can get answers out of it quicklyFirst we run a scheduled ruby CLI command to get raw data from the data sources and store it in S3 in the rawest possible format Storing raw means that if we decide later on that we want an additional field or a mapping changes we wont have to reload everything well just change the next stepNext we use Apache Spark or Amazon Athena to perform a simple ETL ( Extract Transform Load) on the data that we want in the data lake An ETL could be as simple as reading a table from json files making small changes to the structure and writing the new table to csv files The data lake is also stored in S3 but all files are parquet whereas the raw data could be collected in any format like json or csv For our purposes this ETL stage leaves the data with the same columns as in raw and switches the file format to parquet Changing files to parquet a compact columnar format means that Presto (or more specifically in our case Athena) can query them very quicklyFinally once we know what to put into the data warehouse we run Redshift SQL queries to perform another ETL which loads the new arrangement into a table in the data warehouse This ETL stage does things like combine tables unnest complex column types like arrays or structs and remove sensitive fieldsThe short answer is we dont For the most part the Data Analytics team decides what should be available and in what format Sometimes we are given the requirement and then write the code to move it Sometimes we build the tools that allow a Data Analyst to build and maintain a table on their ownNot all Data Engineering teams will do the same work Some Data Platform teams focus exclusively on infrastructure while ETL teams might exclusively handle the process of moving and cleaning dataI cant say exactly what youll need to know but heres what helped me feel confident in my role: Originally published at https://engineeringgustocom on August 22 2019
Sth4f45uCcnCipFqnrFbLj,Today there are 6500 people on LinkedIn who call themselves data engineers according to stitchdatacom In San Francisco alone there are 6600 job listings for this same title The number of data engineers has doubled in the past year but engineering leaders still find themselves faced with a significant shortage of data engineering talent So is it really the future of data warehousing? What is data engineering? These questions and much more I want to answer in this blog postIn unicorn companies like Facebook Google Apple where data is the fuel for the company mostly in America is where data engineers are mostly used In Europe the job title does not completely exist besides the startup mecca Berlin Munich etc They are called or included in jobs like software engineer big data engineer business analyst data analyst data scientist and also the business intelligence engineer Myself I started as a business intelligence engineer and using more and more time on the engineering rather the business part thats why I am starting this blog from the data warehousing angleTo use the analogy to a physical retail-type warehouse you want to sell very structured products in the most efficient way to your customers In a data warehouse (DWH) you have typically structured data and optimised them for business users to query If you dig a little deeper you offload data from the trucks in the back of the physical shop before it gets sorted and structured into the warehouse for the customers to buy In a DWH you basically do the same just with data As you see in the DWH architecture below the offloading area in the back of the store is your stage area where you store the source data from your operational systems or external dataA traditional Data Warehouse architecture by Wikipedia: The physical warehouse where the customers buying the articles is in a DWH normally the so-called data mart The data processed between each layer seen in the architecture above is called ETL (Extract Transform Load) This is not to confuse with ELT (Extract Load Transform) which is the common mythology data lakes (more in my recent post) In a DWH you always transform to get data as clean and structured as possibleBesides the obvious reasons of a shop explained above a data warehouse gives you big advantages: Data engineering is the less famous sibling of data science Data science is growing like no tomorrow and so does data engineer but much less heard Compared to existing roles it would be a software engineering plus business intelligence engineer including big data abilities as the Hadoop ecosystem streaming and computation at scale Business creates more reporting artefacts themselves but with more data that needs to be collected cleaned and updated near real-time and complexity is expanding every day With that said more programmatic skills are needed similar to software engineering The emerging language at the moment is Python (more in the chapter below) while used in engineering with tools alike Apache Airflow as well as data science with powerful libraries Where today as a BI-engineer you use SQL for almost everything except when using external data from an FTP-server for example You would use bash and PowerShell in the nightly batch jobs But this is no longer sufficient and because it gets a full-time job to develop and maintain all these requirement and rules (called pipelines) the data engineering is neededIn order to get high quality and frequently updated data sets it is important to distinguish between data pipelines that are done and cleaned by data engineers and all the others that are mostly exploratory We at Airbus use a folder that is called cleaned and all data sets produced there are constantly updated documented and of the highest quality Based on these data sets you create your own We use the data lake solution Palantir Foundry (brand name of Airbus: Skywise) which provides you with a map where you see the data lineage easily Documentation and metadata to each data set are crucial as otherwise you lose the overview of your data which is also one main task of a data engineerAnother important task or service which a data engineer provides is automation that data scientists or data analysts do manually A good overview what task this includes are provided by Maxime Beauchemin the founder of Apache Airflow a tool that helps a data engineer to lift the majority of tasked mentioned: While the nature of the workflows that can be automated differs depending on the environment the need to automate them is common across the boardI believe that not every company is in need of data engineers His skills are mostly required if the company either: If English is the language of business SQL is the language of data and Python the language of engineering While technology disappears often SQL is still here This means you need a reliable understanding of: Stitchdatacom anticipated that as company size increased so would the focus on scaling-related skill However thats not the story the data told Instead data engineers at larger companies tend to be more focused on enterprise skills like ETL BI and data warehousing whereas data engineers at smaller companies focus more on core technologies: Programming languages have always come and gone but in the last couple of years Python rises on top of the popularity The question is why One valid reason for sure is because of the rise of the data engineers but also the use of libraries for data science and data analyticsAccording to the Codeacademy and their source data from Stack Overflow they say its connected to the rise of data science This and machine learning were the biggest trends in tech 2017 Additionally Python has become a go-to language for data analysis With data-focused libraries like pandas NumPy and matplotlib anyone familiar with Pythons syntax and rules can deploy it as a powerful tool to process manipulate and visualise dataRelated to the rise of data science and data engineering its clear to me that Python is here to stay and its becoming the Swiss Army Knife of programming languagesBut for what can you use Python in data engineering For example you use it for data wrangling (reshaping aggregating joining disparate sources etc) which mostly done with the library Pandas small-scale ETL API interaction (our presentation usually happens in Tableau which has Python APIs) and automation with Apache Airflow which is also natively in PythonApache Airflow has several building blocks that allow Data Engineers to easily piece together pipelines to and from different sources Because it is written in Python Data Engineers find it easy to create ETL pipelines by just extending classes of Airflows DAG and Operator objects And this allows us to write our own Python code to create any ETL we wish with the structure given by Airflow Airflow uses several packages mentioned all ready to do the job: boto for S3 handling pandas for obvious advantages with data frames psycopg2 for popular integrations with Postgres and Redshift and several more said by David DalisayAccording to a job description as a data engineer at Facebook in Menlo Park in Seattle he needs to have the following qualification and responsibilitiesA picture of such a persona could look like (quoraThe salary of a data engineer is hard to say as it is very new especially in Switzerland where the following salary-report is from But on the following table you see all the jobs that are related or close by (unfortunately only in German sorry for that) and their salary for a full year ins Swiss Francs (CHF) created by Robert Halfch: As already mentioned in further up the data engineer has the skillset of a business intelligence engineer plus also solid programming and big data skills I believe BI-engineers will transit over to a data engineer anyway depending on the size of a companyAs the power of computers and especially the speed of the internet is growing more data can be collected and needs to be analysed Therefore many parameters around the data warehouse environment have or will change Below the points that I see with most influence: Furthermore the way we do ETL is changing as well as Maxime Beauchemin data engineer at Airbnb quotes: Product know-how on platforms like Informatica IBM Datastage Cognos AbInitio or Microsoft SSIS isnt common amongst modern data engineers and being replaced by more generic software engineering skills along with understanding of programmatic or configuration driven platforms like Airflow Oozie Azkabhan or Luigi Its also fairly common for engineers to develop and manage their own job orchestrator/scheduler He is also saying that code is the best abstraction there is for software rather than using drag and drop tools (ETL-tools) Most important what I see as well that the transformation logic is of a higher need and shouldnt be locked away exclusively for BI developersAs you cant change ETL without modelling differently also this is changing: Facebook Airbnb and other companies taking it a step into so-called Data Camps or Data University to educate internal employees in respect of data to get more data savvySo after all is the data engineer the new business intelligence engineer? I would say in the long run yes I imagine that data warehouses  in any way  will always be a need for the business where the data is fully structured and easily accessible But how we build DWHs or a similar type will change and therefore more engineering and data engineers are neededOriginally published at wwwsspaeticom on March 8 2018
B7e97jSdqiX9N9k5iijbeA,Recently I am working on migrating our currently pipelines (mostly pyspark) to JVM based Our plan is to use spark for batch processing and flink for real-time processingOne of our most critical pipeline is the parquet hourly batch pipeline It generates the data for our data warehouse and people use presto to query it The original design is very simple and robust It is a pyspark batch job It consumes secor logs and produces data on s3 Secor is used to ensure data exact once Since the parquet job needs to wait for the secor batch finished it generally brings delay to the dataSo is that possible to make the data available in near real time? I think flink probably a perfect fit here Pipeline itself is simple I need to have a Kafka source connected to a Parquet sink Thats itIn order to archive this I still need to write some custom code: 2 A parquet sink writer Sink I choose flinks BucketingSink since we are creating a warehouse here We certainly want partitions in it Lets take a look at flinks sink writer API We need to implement write open getPos flush close and duplicate But if you look at the Actual parquet writer (Here we choose AvroParquetWriter) you will notice there is no flush method and getPosThen you probably will google around just try flink parquet writer There are few people taking about Most of them are talking about flinks batch API This makes sense if you calm down and think about The actual parquet writer implementation is not record by record it writes row group by row group Some interesting link: To conclude: I am sure my opinion is biased If anyone has a better idea or implementation please share with me I really appreciated
jBCwtM4GHPmhrq6Xi4b6ep,From last post we learned if we want to have a streaming ETL in parquet format we need to implement a flink parquet writer So Lets implement the Writer InterfaceWe return getDataSize in both getPos and flush because there is no getPos and flush method in AvroParquetWriter So is this legit? The answer is NO Lets take a closer lookSo now we learned: Remember we want to use BucketingSink in the last post? Lets look at where getPos get called in BucketingSink It controls when the sink closes the file It doesnt matter much since who cares if the file is slightly bigger or smaller than the batch sizeWe know Flink uses Checkpointing to ensure exact one So Lets take a look at the snapshotState function We Learned BucketingSink stores the current file length so when it handleRestoredBucketState it truncates the invalid file to a valid positionWe want to implement flush because the #3 solution works but not perfect Imaging if your checkpoint interval is small eg 10s you will end up having tons of small files We all know we dont like small files in big data worldYou probably noticed two more functions getFooter & appendFooterWe all know parquet file stores its metadata in its footer When we snapshot the current state we also need to snapshot the footer because when we restore the state we also need to put the footer at end of the parquet fileThats all we need to implement a Parquet Writer in Flink In order to implement it you need to understand how Flink checkpoiting and how parquet works I hope I covered all the why & how If I didnt make it clear enough please comment down below
ivmJXc9UfVFHsoR7MCabGz,Presto is an open source distributed SQL query engine for running interactive analytic queries against data sources of all sizes ranging from gigabytes to petabytesThe most important problem we want to solve here is the Data Visibility We Already have data sitting on S3 and HDFS we want to have a Minimum effort to bring Data to everyone When we start looking we want to find a tool to fits our needs: Above diagram shows our current Presto setupOur Presto cluster is busy at work hours and less busy at night We give it more resources when it busy We leverage our Mesoss Marathon framework to achieve this We run Presto under Marathon if we want to scale it up we just need to add more instances At night we scale it down by reducing the presto workers Marathon will also restart the worker or coordinator if it diesWe are also experimenting metrics based scaling In the future when the CPU or Memory Usage is tight we will scale it automatically when it is idling we take back the resourcesWe did some benchmark on Presto but performance is not the most important factor There are lots of Presto performance benchmarks available online
26BoFtBoX3aRviVTxN6KZp,It is not often we write pieces like this: Here at Hashmap we ❤ new ways to interact with Snowflake however; we regret to inform you that the new ADF Snowflake Connector is a technology that feels like it was from early 2018 not mid-2020 If ADF is trying to compete with other data integration tools (Fivetran Matillion and Qlik) then the new ADF Snowflake connector falls short in several key areas We find this a disappointing release from Azure Data Factory Team as we were hoping the tool would be more akin to these technologies in terms of functionality and scope This quote summarises these feelings about the new ADF connector succinctly: A delayed game is eventually good but a rushed game is forever badWhile MrMiyamoto is talking about games here his words ring true in any product situation From video games to architecture to data: if you rush a product that reputation then sticks with you no matter how much you improve after launch Products like No Mans Sky Zune Intels i9 10900k: all of these products were rushed to completion and were/are critically and commercially panned No amount of Day 1 patches or future content updates will draw in customersThe new ADF Snowflake connector feels the same way as if to mitigate market share loss rather than a proactive move or at the very least a reactive one We hope that the Azure Data Factory Team is not discouraged by this but rather they can learn grow and become a true competitor to these tools mentioned above To do that they will need to fix four key areas of their product Those such areas are Ease of Setup and Configuration Sustainability Management overhead and most importantly PerformanceHere is a step by step instruction on how to configure the tool: 2 Set Up the Snowflake Linked Service and Connection you will need your Azure account name and the account that has the correct privileges to run on the Snowflake environment Then you will give the database and the warehouse you wish to run on • Create your Copy Data command and configure the Source and Sink Tabs • Create your Table and Schema in your Snowflake environment • Run your Azure Data Factory Pipeline to load the data into Snowflake You should see in Snowflake a call to the ODBC in your history table This is how you know ADF has successfully landed data in Snowflake If you check the Command run you should see something to this effectNotice here for a moment the name of the Azure Data Factory Stage This is important to note as this is the name of the intermediate blob storage area that gets created in Azure I will return to this later in the Sustainability section Just know it is not idealWhen firing up this tool Senthil and I found the Setup and Configuration to be the most challenging part The ADF Snowflake Connector seems to restrict the copy functionality we love from Snowflake If we deviated from these predefined accepted parameters in any way we get a validation error from Azure Data Factory that directs us to a new problem such as wild cards not being defined in the source but in the copy command itself These error messages took some time to fix and were not intuitive Here is a list of the pain points we encountered: What particularly concerns us are the Validation Errors What happens if your data does not fall into the nice format ADF expects? You will have to waste more time creating an Az Function to handle that transformation yourself Here is a picture of showing what needs to be enabled to allow the connector to run without the Validation ErrorsWhen it comes to Sustainability it is often easiest to understand the mechanics behind a process Snowflake has many ways to Copy Data into the underlying Tables It is important to note how the Azure Data Factory Connector handles copying data into Snowflake The Connector currently has two modes: A Direct Copy and a Staged Copy The Direct Copy is limited in its use as any of these scenarios below force the user to perform a Staged Copy • Source data format does not meet the criteria of ADF loading into destination data store • Source data that is on-Prem/Hybrid cloud environment and needs decompression to efficiently load into destination data store • Securely transport data from source and destination without opening up 443 portsIf your data environment requires any of these things you will not be able to utilize the Direct Copy and will be forced to use Stage Copy This presents its own challenges not from a security or functionality perspective but from a cost perspectiveA Staged Copy brings the source data to an interim temporary stage location then performs the necessary transformation and will finally send the data to the destination store After the transfer of data is complete the data will be expunged from the temporary locationSo how does one configure the Staged Copy? Well in the Snowflake Sink Options in the Azure Data Factory Pipeline you must enableStaging set to be TRUE Snowflake automatically compresses the using their standard AUTO compression however you can set the compression based on your preference the options depending on the file type There are your standard GZIP BZ2 and DEFLATE to name a few for Flat Files Parquet has either SNAPPY or LZO There is a list of compression options that can be found hereAnother aspect of Sustainability is the cost of execution currently the execution of an ADF pipeline to Snowflake will cost costs #DIUs * copy duration * $025/DIU-hour in addition to the cost Snowflake will charge This is because ADF still generates an intermediate Blob area where it writes the data We initially thought this step would have been cleaned up or dropped thanks to the new ADF Connector This is something that seems like a large oversight in this caseTo give credit where credit is due the ADF Snowflake connector does move data from Blob to Snowflake well However no Auto Adjust with regards to source schema nor any CDC functionality leaves the connector feeling a little bare-bones from a data management perspective This problem is only exacerbated at scale What happens when a company needs to load data from sources other than flat files from blob into Snowflake Well they will need to revert back to building the pipeline manually or using a tool like Fivetran Matillion or Qlik What if you need CDC functionality? Then your team needs to either build their own custom CDC Tool or find a tool that has it baked in No enterprise has just flat files they need loading to Snowflake By using this tool you are committing your business to add additional methods and tools for your developers to learn unnecessarily ultimately slowing down development and your insightsThe lack of features provided from the Azure Data Factory Team dooms this tool as is a net negative to development stacks that would like a native Azure experience to connect to SnowflakeTo utilize copying data from other methodologies forces the user to adopt a staged approach thus making it more expensive to operate than a direct copyWith the lack of features you would hope that the ADF Connector performed better in a speed test as compared to setting the stage yourself and performing a COPY INTO Command Unfortunately this is not the case: Here are the results of loading a 5ADF is 20% slower than creating a stage yourself and using the copy into command on Snowflake I think this is the final nail in the coffin for the ADF Connector The connector isnt faster scalable lacks cdc harder to set up and/or manage and is not sustainable The benefits gained from other tools and even doing it by yourself are greater than using a tool that is already obsolete in every way Save your future self the headache of having to redesign around this tool for loading other sourcesWe would like to start our conclusion by saying this is not meant as an attack against the Azure Data Factory Team Here at Hashmap we strive to provide the best experience for our customers We pride ourselves on being among the first to adopt and learn about new technology regardless of platform Today we were hoping that this connector could help our customers leave some vendor paid tools and potentially lower costs while at least maintaining functionality This is not the case today We believe that the Azure Data Factory Snowflake Connector will grow features in time but we wont be recommending our customers to switch any enterprise data loads to this method anytime soonI hope youll check out some of my other recent stories also: Feel free to share on other channels and be sure and keep up with all new content from Hashmap here To listen in on a casual conversation about all things data engineering and the cloud check out Hashmaps podcast Hashmap on Tap as well on Spotify Apple Google and other popular streaming appsKieran Healey and Senthil Anandarangan are Cloud and Data Engineers with Hashmap providing Data Cloud IoT and AI/ML solutions and consulting expertise across industries with a group of innovative technologists and domain experts accelerating high-value business outcomes for our customers
UUecnxg24WFhrV7k72kriL,Nature as we all know it thrives in diversity The more the merrier The more varied the merriest! To truly appreciate nature one has to take in all its myriad possibilities all of its variability One has to accept that change is the only constant Perhaps that is what makes it so interestingEr…but not so charming when youre a data engineer You are tasked with ingesting data with more-than-you-care-for variability in its schema typical with data that comes from heterogeneous sourcesDeal with structured data formats such as CSV you create as many stage tables as are required to host each individual dataset to support its schema and then write some SQL code to perform expensive JOINs and/or UNIONs to bring it all under one hoodYou whip together some code fine-tuned to accept the known schemas (some more praying here) and transform the datasets on-the-fly All before you host them in your database and hopefully move on to the real deal  whatever that may be for your use caseOh but wait! heres nature calling again (no pun intended) and you find that one of the data sources has changed (aka evolved) its structureBut as inconvenient as having to deal with datas structural variability might be theres really no avoiding it And perhaps this is what gives analysis its power  data captured with so many details from so many different points of viewTherefore we must endeavor to make this whole process of schema harmonization easier and as close to standardization as possible Or even better we should try to retain the original schemas of all the datasets while somehow storing them together within the same data structure so that we can later access as if they are a single physical entityIf only there were a tool a system out there that empowers data engineers to store data with all of its raw pieces of information intact and yet combine them without too much pre-processing/modeling effortEnter Snowflake Cloud Data Warehouse and its native support for semi-structured data formats like JSON AVRO and Parquet to name a few Why semi-structured format you ask? Read on about our real-life scenario where converting structured CSV to JSON made the data pipeline and hence our life so much easierAt Hashmap while working with a client in the energy industry we came across a use case which dealt with field data being collected from different sites within the same business process but where data which originated from different sites were not guaranteed to look like one another  and didnt The requirement was that all of this data had to be ingested through a single processing pipeline that aimed at generating important operational KPIs to be delivered in real-timeThere was one solution already in place which utilized Apache Spark scripts that performed the tasks for unionizing the disparate datasets before applying the required business transformations Although the solution worked it was cumbersome to understand ergo maintain The solution was not modular enough to give it the flexibility to absorb changes to underlying datasets easily or quickly Besides this was an on-prem solution which meant that the cluster sizing and tuning was completely the onus of the data engineering team that built itTo make things simpler Hashmap re-worked the data pipeline to transform the original datasets into JSON retaining each datasets native structure and simply loaded each into a Snowflake table such that each data point became a row with its own specific schemaSnowflake allows for storing data in semi-structured formats such as JSON AVRO and ORC (to name a few) under a column of type VARIANT It stores the underlying data in its native format compressed for efficient storage and access which provides amazing flexibility and frees the data engineer from having to transform the data any further to make it conform to a strict schema definition In other words it enables the engineers to not only have access to all data in one place (a single relational table) but with the originality of each data point intactSo we loaded our disparate datasets to create a pool of data in one Snowflake stage table and then utilized it all to build the necessary operational metrics using Snowflakes SQL functionality for extracting data from a JSON backed column The data came from varied sources but we got to treat it all as one table with uniform SQL access patterns We combined Snowflake with another nifty tool by the name of dbt (Data Build Tool) to organize our SQL source and even represent our transformational data pipeline as Select SQL to generate the metrics Dbt handled the DDL for us and in the dialect used by SnowflakeAt this point even if the structure of any of the existing datasets were to change or new variants were to be introduced into the pipeline nothing would need to be changed in the rest of the pipeline as far as ingesting them goes given the simple function of data reformatting into JSON is in place The only change could be adding new columns to the KPI calculations themselves as-and-when required This untangled a very messy knot at the beginning of the pipeline made the overall design more simplified and modularized  hence easier to maintain and evolveNot to forget that Snowflake is an elastic Data Warehouse built on the cloud! So there is literally zero cluster provisioning/tuning required to be done by data engineers while implementing a caseBut just to be clear in comparison to the previous solution there is still a pre-processing step involved to deal with the disparity But it does not include any complex logic of UNION-izing the datasets by loading them into a data processing engine and dealing with them one-by-one Only a simple operation that converts the source data format CSV in our case into JSON (or any format supported by Snowflake as VARIANT)  something that can be done with a simple serverless processing stepThere have been numerous NoSQL databases in the market for some time now which allow data to be stored as documents in more weakly-defined structures and hence could be helpful when dealing with colorful data But none of them have proven to provide better data interactivity than a trusty SQL engine nor with as much ease as Snowflake Moreover in the case of Snowflake the sizes of the datasets which can be ingested potentially have no upper limit in size and neither do the compute resources that would be needed to dig into themFeel free to share on other channels and be sure and keep up with all new content from Hashmap hereChinmayee Lakkad is a Data Engineering Consultant with Hashmap working on designing and developing data pipelines on Big Data Data Warehousing and Cloud Platforms for the better part of the last 6 years
ewsuShunrA5k64nmmcpgwd,In this blog I will like to present the thought process of working with a Data Engineering problem using Apache Spark I assume the readers to have some programming skills in python and some understanding of Apache Spark All the codes will be available in the Github repository hereBefore we begin lets discuss the development environment I will be running the code on my Macbook Air(2015) with the latest macOS(1015 Catalina) 8 GB of RAM Intel i5 processor with an SSD HDD The application will be written in Python and will include Scala later and will be running with Apache Spark 244 and unit testing with PyTest There are no other dependenciesThe New York City Taxi and Limousine Commission (TLC) created in 1971 is the agency responsible for licensing and regulating New York Citys Medallion (Yellow) taxi cabs for-hire vehicles (community-based liveries black cars and luxury limousines) commuter vans and paratransit vehiclesIn partnership with the New York City Department of Information Technology and Telecommunications (DOITT) TLC has published millions of trip records from both yellow medallion taxis and green SHLs Publicizing trip record data through an open platform permits instant access to records which previously were available only through a formal process (FOIL request) The TLC does not collect any information regarding its passengersThe yellow and green taxi trip records include fields capturing pick-up and drop-off dates/times pick-up and drop-off locations trip distances itemized fares rate types payment types and driver-reported passenger counts The data used in the attached datasets were collected and provided to the NYC Taxi and Limousine Commission (TLC) by technology providers authorized under the Taxicab & Livery Passenger Enhancement Programs (TPEP/LPEP)  The trip data was not created by the TLC and TLC makes no representations as to the accuracy of these dataFor our project we will be working with data from the year 2017 and 2018 for Yellow and Green Taxies The total volume of raw data is ~21 GBNew York Citys Medallion (Yellow) taxi cabsYellow taxicabs are the iconic yellow vehicles that have the right to pick up street-hailing passengers anywhere in New York City Taxis can also be dispatched by TLC-licensed E-Hail companies By law there are 13587 taxis in New York City and each taxi must have a medallion affixed to it Vehicles can be owned by drivers or leased to them by licensed garages Grant funding is available for wheelchair accessible taxisNew York Citys Green taxi cabsGreen taxis also known as boro taxis and street-hail liveries were introduced in August of 2013 to improve taxi service and availability in the boroughs Green taxis may respond to street hails but only in certain designated areasThe raw data and data dictionaries can be downloaded from hereAs a Data Engineer I usually divide my projects into three iterative processes until you reach the desired data quality The three phases are as follows • Exploratory PhaseThe exploratory phase is where you enrich your knowledge about the data and by gathering inputs from various data creators like business users and software engineers • Development PhaseThe development phase is where we build the ETL pipelines Decisions on job scheduling data extraction transformation and storage format are to be decided The schedule transformation and storage format depend on the end-user requirements • Validation PhaseThe validation phase is where we perform data quality checks and accuracyWe will talk more about these phases as we go on Now that we have done with all the boring stuff lets get our hands dirty Lets explore the data bit starting with record countFirst we create in an instance of SparkSession You free to choose any appName as you please I will discuss about the master in the coming postsSparkSession  The entry point to programming Spark with the Dataset and DataFrame API You can read more about SparkSession hereThen we use the SparkSession instance to read CSV file/s The runtime argument can point to a single CSV file or to folder containing CSV files The CSV files are read into a DataFrame object Remember Spark is lazy loaded and nothing happens at this pointdfcount()  This an example Spark action this triggers all the previous Spark transformations to execute Here we just have one that is to read the CSV files and print the record countBefore running the script make sure you have installed pysparkAs mentioned earlier I have taken 2 years worth of data which is about ~21 GB Please free to download as much or as little as you want
mPtjMqBDehYMDKQPwYoYN7,This is a continuation of my previous post here We begin with the exploration phase of Data Engineering Our goal here is to learn more about the dataset Here we will work on a sample dataset and apply the findings to the rest We will work with April 2018 dataFiles organized by year and each file correspond to a monthly data for the yellow or green taxiLets start with schemaNow all the columns are of type String which is inaccurate To get the right data types we can set another option inferSchema as TrueWe have the right data types for all columns This way is costly since Spark has to go through the entire dataset once Instead we can pass manual schema or have a smaller sample file for inferring the schemaHere we have created a manual schema and specified it while reading the CSV fileNow we get the above warning before printing out the results We built the schema based on green taxi data The columns ehail_fee and trip_type were missing from yellow taxi data hence the warningAs a solution we can read yellow and green data into separate data-frames with different manual schemas We will then add the columns ehail_fee and trip_type to the yellow with defaults 00 and N/A pick-up and drop-off time columns are renamed as pickup_datetime and dropoff_datetime and a new column taxi_type is also added to identify yellow and green taxi Finally we perform a union on the data-framestaxi_df contains data for both yellow and green taxies and has over 101 million recordsLets find the min and max of the pickup_datetime and dropoff_datetime columns on the combined data-frameThe dataset contains data from the past and future These invalid records and must be filtered out Lets find the records that are not from April 2018We have found 655 records that are not from April 2018 We need to filter out such records and store them separately To filter out these invalid records we need to match the year and month from the pickup_datetime column with the corresponding filenameWe are adding a new column file_name with the sparks input_file_name function returning the absolute file nameRemember the filename has the format <TAXI_TYPE>_tripdata_<YEAR>-<MONTH>csv The above function extracts the value for taxi_type and file_year_and_month column from the file_name column file_year_and_month column will have the data in the format <YEAR>-<MONTH> We also find the year and month for each row and stored in the data_year_month columnThe transformations are applied to both yellow and green taxi dataValid and invalid data are separated filtered out into separate datasets We consider only valid datasets for further processingIn the next post we will further explore the dataCode upto this point can be found in my Github repository here
3GnkXu4vFks3Cvfm88mePp,Everyone is talking about data engineering these daysMost of us database architects and developers have been performing data engineering our entire career It has just been in the last few years the term data engineering has become the de facto way to describe moving large amounts of data from multiple types of sources and then loading and transforming it for analysis by business and data scientistsThe term is also used to differentiate the work done by database developers in the data pipeline and the data science work done after data engineering All the tools available to do data science on a large scale has prompted companies to start big machine learning projects However many of these projects are failing because the data is not clean conformed or useable Hence the current emphasis is now on data engineering to enable the data science and other reporting analyses companies need and want to make the data more accurate and more useableSimply put: data science is data wranglingHaving all the processing power available in the cloud and the data lake to store all corporate data in one place has changed the data movement pipeline we have used for years from extract  transform  load (ETL) to extract  load  transform (ELT) And the transform part has become much more complex Data can now come from anywhere in any format and may require complex transforms that tools like SSIS or Informatica may not support Set-based transforms are not always the best way to achieve some of this workIn ETL the pipeline does the heavy lifting This is where SSIS excels but you will run into resource limits In ELT the work is done in the cloud and all the tools are available thereTime to look at a new toolsetDatabricks is a product created by the team that created Apache Spark On the Microsoft Azure platform it hides all the complex work required to create clusters of multiple machines with distributed data and queries It provides a unified processing platform for large amounts of data in a performant and scalable mannerBut the killer feature for data engineering is the support for multiple languages and data pipelines You can use SQL Python or Scala all in the same process It can also support streaming and graph data and comes with connectors for many different sourcesBecause we now have multiple types of data in all kinds of formats we need a toolset that encapsulates all those different needs You use the appropriate language and its features for the appropriate task If it is manipulating relational data you use SQL If you need to do some JSON parsing or string manipulation you might use Python or something requiring object-oriented support like ScalaThe Databricks framework allows you create code using any of the above languages as well as others in the same process This is something we have never been able to do beforeWe do many modern data analytics projects with Azure Data Warehouse where enterprise data warehouses are being built out of data from many data formats not just relational databases Before we can get the data into Azure Data Warehouse there is a lot of processing that needs to happen  especially around ensuring the data is correctly delimited has no in-text line feeds and no other common data migration problems Databricks transforms can be built using Python (for non-set-based string parsing) and SQL for relational queries inside the same piece of transform code making it your Swiss Army data engineering toolAs you can see although data engineering is not new it has become more complex and includes non-relational data requiring we add non-relation tools to our toolset It has also become the critical first step for enterprise data warehouse and many machine learning and AI activitiesBecause of all this Databricks has become my new favorite data engineering tool
TWzzDqMaRUq5QL57ZHXS9H,"Companies large and small are interested in building machine learning and AI-based systems to help customers and everyday users But little do they realise they have so much data within the organisation to improve day to day activity of every individual within the corporate world""Let's take the example of an IT organisation involved in selling their product & supporting them Each department within this organisation starting from HR finance sales IT engineering & business teams and CXOs use different disparate systems and interact with them every dayNow lets connect these data: Suddenly the organisation has the capability to draw out insights and do more with the data which otherwise isnt more than reportsSales and account executives can get meaningful insights on customers how the systems are used and what more can they do to provide valueThe engineer will have a knowledge base to support the customerCXOs will have access to resource utilisation profit analysis and other corporate insights in real-timeAt HorizonX (http://wwwhorizonxcomau) we work with enterprises and help them realise the data potentials within and outside the organisationOriginally published at mediumcom on September 5 2018"
8Wr3SWYZSsMH4JFBpa4amt,Data is the new oil Its valuable but if unrefined it cannot really be used It has to be changed into gas plastic chemicals etc to create a valuable entity that drives profitable activity; so must data be broken down analyzed for it to have valueClive Humby first coined the term Data is the new oil in 2006 The chaos in data always existed and the struggle from businesses to extract meaningful information from this data is universal With the computing capability to process more data than we can handle 90% of data we receive is unstructured Only 05% of the data we receive is ever analysed and usedData quality issues form a key challenge in the data world Unstructured data acquired from multiple sources often causes a delay in deriving insights or even simple analytics due to data quality issuesMost enterprises set up a Data Quality Framework that defines data quality capability and enforces it as a process by the organisation A data quality framework will benefit data owners data architects business analysts and data scientistsImagine a process run by a business SME to consolidate a report to be sent to a regulatory body but due to data errors introduced at the data entry stage the numbers do not match They then have to get help from other teams to fix the data quality issue before they can rerun the reportThis is illustrated below: Due to the nature of the manual process involved often an enterprise data quality strategy is created There are definitive guidelines but without an automated framework it is going to be costly and more importantly time-consuming not to mention the errors caused by human errorThe StreamSets DataOps Platform helps you build and operate many-to-many data movement architectures Developers design pipelines with a minimum of code and operators get high reliability end-to-end live metrics SLA-based performance and in-stream data protectionStreamSets Data Collector is open source software that lets you easily build continuous data ingestion pipelines that have the following benefits: Griffin is a data quality application that aims to solve the issues we find with data quality at scale Griffin is an open-source solution for validating the quality of data in an environment with distributed data systems such as Hadoop Spark and Storm It creates a unified process to define measure and report quality for the data assets in these systems You can see Griffins source code at its home page on GitHubFollowing are the main components: This is only a part of the solution End-end automation would involve correcting the data quality issue within the stream and publishing to a different Kafka topic This would then be transported to a data warehouse data lake or a number of other consumers for the next step of data validationFor this prototype we used StreamSets & Kafka (with Zookeeper) in a GKE (Google Cloud Kubernetes Engine) clusterand add it as a volumeWell also set up the external libraries in the persistent volumeKafka & Zookeeper are setups using https://githubBecause we need StreamSets to publish we have to create a load balancer service in Kubernetes and update Kafka listeners with external advertised listeners setupWell share a separate blog post on how we setup everything in KubernetesOnce everything is up and running we create a pipeline which reads JSON files from Google Cloud Storage and publishes to KafkaThe next step is to set up a Spark cluster to run the Griffin data quality applicationHere is a brief on the scenario: A Hive datastore contains a customers country information A Kafka stream comes with customer data associated with their company details The Griffin application checks if the name associated with the id from Kakfa streams matches to that of Hive Any anomaly is reported in Elastic Search and visualised in KibanaThe above configuration sets up Kafka streams as a source and uses Hive as a destination to compare with This is the rule: and uses accuracy as a data quality measure Griffin also supports profiling where we would profile an incoming data in stream or batch to ensure it matches the required criteriaSelect Spark 2Copy the envjson and dqjson into the master node of the clusterDownload Griffin source https://archiveapacheorg/dist/griffin/040/griffin-040-source-releasezip build and copy the measure-040jar to the master nodeSSH to the master node and setup a hive table to be used for referenceImport the reference data into the tableChoose the newly created cluster provide the main Application name ie orgapachegriffinmeasureApplicationProvide the configuration files as arguments starting with the env filesAdd the measuring jar file path and create the jobNow upload a JSON file into Google Cloud Storage which would be picked up by StreamSets The file contains JSON string in each line and hence each message is a JSON string Griffin application then consumes the stream and checks for any data quality issues as per the rulesBelow is an example of data where last names between two records mismatch: From Kafka:{id:183email:verdie12@hotmailcomfirst:Angelinalast:Stiedemannaaacompany:Wunsch and Sonscreated_at:2014 12 13T09:16:07Griffin records this as inaccuracy and publishes the results to ElasticSearchThis is just an example of one type of measurement Depending on business need various rules can be added or refined Further customisation to the platform can be done to even fix the data quality in flight and also to apply the cleaning rules to the original ingestion pipeline within StreamSets Data CollectorWe believe in delivering quality data platforms faster and cheaper Let us know and together we could make you a better data-driven organisationWere a team of passionate expert and customer-obsessed practitioners focusing on innovation and invention on our customers behalf We follow a combination of the Lean and Agile methodologies and a transparent approach to deliver real value to our customers We operate as the technical partner for your business working as an extension of your digital teams Talk to us today about your digital and data journeyhttp://horizonxcomau | info@horizonxcom
EFqJD4bBuP8fYKfHhXD6fj,I just got back from my first AWS re:Invent  it was exciting and busy and I learned a whole lot This post is about the key things I learned about new releases other companys experiences and company cultureMy strategy when approaching the re:Invent schedule was to prioritise the hands-on and keep sessions and keynotes for later My favourite experiences at re:Invent were the builders sessions and workshops Getting to talk to the engineers who are working on Redshift and Aurora was so valuableThat being said Ive been tackling sessions I either got to in an Overflow room or put in my playlist for later These are the best of the best of data analytics and leadership sessions from re:Invent 2019Technically announced before the conference this pre:Invent announcement changes the game for Athena usersWith Athena Federated Query users can run SQL queries across data stored in relational non-relational object and custom data sources Prebuilt connectors execute in Lambda and write the result to S3 for further analysisThe first of a series of talks on Redshift the team from Warner Brothers showed how they converted their traditional warehousing architecture into a Lakehouse This new functionality allows users to ingest data into Redshift or operate directly on an S3 lake The best of both worldsThe Warner Bros team talked us through their shift to the Lakehouse model to take advantage of the power of Redshift in combination with Presto and Spark for overnight processing with Lambdas sending data between systemsThe team from Yelp showed how new Redshift functionality and bringing compute closer to storage improves performance They showed how small changes to architecture and the Redshift set up makes a difference to performanceAnnounced pre:Invent Query Priority lets you assign a priority to each Redshift queue We can now prioritise mission critical workloads and deprioritise long running exploratory queries that can afford to wait a bit longerThe team from Nasdaq presented their Big Data architecture and the journey to their current state The team run were hitting Redshifts hard limits longer load times and dealing with unhappy usersMoving to Athena worked in the short term but due to complex queries didnt help with performance They then engaged the AWS Data Lab for a custom solution This highlighted quick fixes and recommended multiple Lakehouses to take advantage of Redshift and S3I attended this session as I had some time at the Venetian It ended up being one of the best sessions of the week and highlights how technology cant succeed without team cultureThe takeaways from the talk related to the need for operational accountability from all levels and the need to create a safe space for operations teamsThis fireside chat was held to discuss how whatever you think about the world influences your modelThe discussion was lively and raised important talking points that everyone involved in building models should be aware of: The Sony team uses streaming technology to evaluate every purchase on their network and prevent fraudulent loginsThe final architecture took the best of batch and streaming processing to reduce time to resolve A sign that batch processing isnt dead yetBatch processing: Stream processing: In this talk Adrian Cockcroft VP Cloud Architecture Strategy at AWS and ex-Netflix distilled what he has learned about leadershipNot all of us can expect to operate like a tech giant but the takeaways were thought provokingIf you want to build a ship dont drum up the men to gather wood divide the work and give orders Instead teach them to yearn for the vast and endless seaGoDaddy went from architecting to BAU of a streaming data platform in just one yearIn this talk they discussed the challenges when integrating Kinesis for integration with other infrastructureAmazon has been in the press recently to celebrate database freedom and the migration of over 7500 databases to AWS I was particularly interested in their methodology as Ive been busy doing database migration myselfThey also faced some of the issues I had when using the Schema Conversion Tool and Database Migration Service This was comforting its not just a me problem Theyve used this to build out the migration playbooks to better support those going through a similar journeyThat sums up what I enjoyed and took away from my re:Invent experience Some interesting learnings  not only in the data and analytics world but in terms of team culture as wellOriginally published at http://wwwhelenandersonco
d7AC6wTL9jtkQ5YirvwVqS,"Nowadays we have a tons of ways to do everything but in my case was very difficult to find a solution for the problem that I was faced on so I want to share what I did to solve mine with you guys I wont explain in details each technology just focus on the solution but I will link with what you need to know with want more information about each one""My problem was a little bit simple just stream changes in a collection from mongoDB and send to S3 But it wasn't so simple as I thought I will explain everything I went throughHere I will use scala but you can do this with others technologies like python egMy sbt code (build tool for scala) configuring all dependencies that I neededAt this point I needed to get changes that occurs on mongoDB so I was faced to the function watch() from mongoDB driver that Opens a change stream cursor on the collection So my code for this: Note: watch() is async so if i just run this code nothing happens because it will register the subscribe and end the programThe onNext() method receive all the information that I needed (token document operationType removedFields updatedFields timestamp) Token is the id of the change with this id you can resume the watcher at that point using watcherresumeAfter(token) The method process() I will explain laterAt this point I really needed something to hold the program runningSo Spark Streaming was my answer but how? Calm down I will explain how I linked spark streaming with subscriberHere my spark configuration note the options for optimize writing to S3I used queue of RDDs as a Stream the code below show how create a session stream based on a memory queue: Here streaming context was configure based on rddQueue so it get data inside the queue iterate over elements and process in my case sendS3(rdd) each 60 secondsThe sendS3(rdd) was responsible for sending RDD to S3 I configured the coalesce(1) the dataframe to generate just one file in S3 (remember that spark works in a distributed way and will save n files without coalesce(1)) and the append mode to dont override the files in the folder I used sparkreadjson(rdd) to make spark infer the schema from json string inside rddAt this point I have the mongoDB subscriber and the streaming context with S3 sender done but separated So how can we link them? Here my ace in the hole""I created a rdd from json string and enqueue in rddQueue in my case I verify if have element in queue if don't I dequeue the rdd and union with the new one to have just one rdd in queue Remember that the process method is called async so each element changed in mongoDB will generate one RDDNow we have everything working together spark streaming watching a queue and sending to S3 every 60 seconds the mongoDB watching for changes and enqueue to spark streaming""So that's the solution I hope it helps you guys"
DjpiSjb2g6uaw2ytFRiuNZ,The world is always changing as are our laws and views Back at the turn of the century immediately following the invention of the digital photograph it was rather jarring to see your picture in the newspaper Instantaneous photographs and newspaper enterprise have invaded the sacred precincts of private and domestic life  Samuel Warren and Louis Brandeis (US Supreme Court Justice) Which led them to propose (legally) the right to be let alone Today we generally accept being observed but rarely accept being identified  this in combination with the realization of how our personal data is being collected and used has led to regulations such as General Data Protection Regulation (GDPR) as well as the California Consumer Protection Act (CCPA) among many And in fact our recent COVID-19 crisis has driven people to think about how to enforce privacy controls for goals like contact-tracing to enable rapid and preemptive quarantineThis is a short story about how those pressures from the real world can have a grave impact on our world of technology In this case how it is turning access control models on their headFirst a quick technology introductionRole-Based Access Control (RBAC) is a model for managing access to objects and most typically tables columns and cells (you may also hear Table Access Control List ACL) As an administrator managing access through RBAC you implicitly predetermine what the users will have access to by adding them to a role Then you explicitly determine the privilege associated with each role Simplifying a bit: you are deciding who belongs in an arbitrary thing in this case the role then you must decide what that thing has access to Almost all modern (and legacy) databases approach access control through the RBAC model primarily because its a simplistic abstraction to avoid manually granting access to individual users one-at-a-time  a simple shortcut if you will In fact open source access control frameworks such as Apache Ranger and Sentry also follow the RBAC modelThe key words for RBAC are implicit and predetermineIn contrast is something called Attribute-Based Access Control (ABAC) The key difference with ABAC is that complex and completely separate rules can evaluate many different attributes As described by the NIST Guide to Attribute Based Access Control (ABAC) Definition and Considerations: At access-request-time the ABAC engine can make an access control decision based on the assigned attributes of the requester the assigned attributes of the object environment conditions and a set of policies that are specified in terms of those attributes and conditions Under this arrangement policies can be created and managed without direct reference to potentially numerous users and objects and users and objects can be provisioned without reference to policySimplifying ABAC: you define your users define your objects and define your rules all independently and let those rules make access control decisions at request-timeThe key words for ABAC are assign and decisionLets break this down in a matrix: To understand this chart better lets start with a simple (real) example from one of our customers: By default everyone can see all rows > 90days oldInsiders can see all rows >90days old + fresh data (<90days old) but only for their regionFor argument sake lets say there are four regions North South East and West With RBAC you would need the following Roles (Ill explain why momentarily): For ABAC youd need to assign attributes to your users (remember these are being assigned to the users based on who they actually are not a role with implicit access to data): The big difference is what we spoke about before with implicit and predetermine for RBAC and assign and decision for ABAC Because with RBAC you need to explicitly predetermine the access for each possible role combination Lets do this with Apache Ranger to demonstrateFive policies…for now Theres a few issues we havent addressed thoughYoure probably wondering why we need roles called role_north_insider and role_south_insider etc instead of having individual roles for each: role_north role_south role_east role_west and lastly role_insider This is because the policy assumes an OR meaning as long as you meet one of the role conditions in the policy it will apply to you For example considering the final Ranger rule in the screenshot which applies to people with role_north OR role_south OR role_east OR role_west it will only need one match for the policy to be triggered against them  this is fine because as long as they dont have the Insider role its the same policy no matter what region However if they do have Insider as we know the policy is different depending on the region which requires us to create a predetermined unique role for each combination to map to the appropriate policySo while ORing the roles saves us time on the non-insiders scenario (the final policy in the Ranger screen shot) it leads to role explosion because of the remainder of the policy What if I had someone that needed access to both regions north and south and was also an insider? You guessed it we would need to create another role for that: role_north_south_insidersNow someone asks you  Hey Nancy started last week we need to get her access to our data Nancy is an Insider and in North and South Do you add her to role_insiders role_north role_north_insiders role_north_south_insiders? That may seem like a simple question but its only simple because you know what the underlying policy for access is tied to those roles  if you didnt know that  how would you know what you need to add Nancy to? Remember there could be hundreds of policies to sift through many of them not created by you Beyond role explosion thats the other problem its impossible to implicitly understand what access you are giving Nancy when you add her to a roleWith ABAC you would objectively assign the attributes to the user (their Region if they are an Insider) and separately would have built a single rule like this: Theres no need for 19 different roles and if a user had both north and south regions they would see data for both regions under 90 days (because both regions would be in the IN statement)As you can see with ABAC youve simply assigned the user the attribute they belong to it is just that an attribute plain and simple there is no under-the-covers access decision implicit to that assignment we are not conflating WHO the user is with WHAT they have access to like we do with a role This is because the rule is kept separate from the assignment and makes a decision at query time for accessAlso consider when the organization decides to add a new region  with ABAC you do not need to change the policy at all you simply start assigning users to that region With RBAC youd need to predetermine a whole set of new roles and policies every time this happens And worse if you forget to do this you will leak dataIn Immuta an ABAC engine for policy authoring decision and enforcement for any database would represent that policy like below; notice its using logical metadata (@columnsTagged) on where to place the policy not literal column and table names and variables (@authorizations) to grab the users Region at query time This provides a level of scalability well beyond Ranger/RBACNow lets talk about our current world of privacy and how its exacerbating this problemThat rule above was more of a business rule but if you start to think about CCPA for example and privacy by design you need to start considering much more granular controls You will need to factor in access decisions on direct identifiers in your data such as names and credit card numbers but you also must consider indirect identifiers such as date of birth and zip code to truly anonymize / psuedonomizeThe masking techniques you will need to use across those different types will and should vary based on the level of utility you need vs the level of privacy you want to maintain  which means you will need many different lenses into a single table Protections that lend some level of utility from a column while also preserving privacy critical for analytical use cases are commonly termed Privacy Enhancing Technologies (PETs) and can be very complex to implement For example we can further expand our original policy in our new privacy world: By default everyone can see all rows > 90days oldInsiders can see all rows >90days old + fresh data (<90days old) but only for their regionAdditionally (this is the new part) all direct identifiers are masked using hashing unless you are doing human resources or billing work All indirect identifiers are open to insiders but are masked using k-anonymization for non-InsidersThis is not a stretch in our CCPA GDPR etc world in fact it will be the norm Theres no way you can do this with RBAC it just doesnt scale and this is a pretty simple example Im sure youve seen this yourself when attempting to manage policies in Apache Ranger IAM Roles on AWS Snowflake Roles that manage both your table access and warehouse access or Databricks Table ACLs  your head starts spinning  this is what we call Role Explosion and/or IAM hellWith ABAC it remains simple because we can assign those attributes (objectively not implicitly) to the users and build decision logic separately and in fact the users environments can change based on what they are doing (human resources or billing) on-the-fly and have that reflected in their accessWith hierarchical RBAC (all roles you possess are rolled up together) environment switching is impossible because you cant switch in and out of roles (not without involving an administrator at least)It is critical to both control and audit under what purpose you are processing your data subjects data For example under CCPA (and every privacy regulation on the planet has a condition similar to this): Business purpose means the use of personal information for the businesss or a service providers operational purposes or other notified purposes provided that the use of personal information shall be reasonably necessary and proportionate to achieve the operational purpose for which the personal information was collected or processed or for another operational purpose that is compatible with the context in which the personal information was collectedIn other words privacy is no longer about controlling how your data is collected  that isnt going to stop  its about protecting how your data is being used Regardless even if we ignore the privacy audit requirement this policy is already too complex for RBAC not to mention Ranger doesnt have any concept of PETs such as k-anonymization as a column-level controlWith flat RBAC (you can only act under one role at a time) you could certainly have separate human resources and billing roles and act under them individually  but then you wont see the data you need because you dont also have InsiderWithout further ado heres the ABAC policy representation: Voila! Also notice it resembles very closely to the plain English version of this policy written aboveIf Im an Insider in the North region doing human resources work Id see all the North region dataThis is what that rule would look like in Immuta a plain English understandable policyPicture for a moment youve done the following in Immuta: Now picture your users executing Spark jobs in their Databricks notebooks and having all that enforcement happening on the fly without ever having to manage a role or table ACL Or having your users acting under the PUBLIC Snowflake role running queries and having all enforcement happen dynamically  they only use Snowflake roles to control warehouse access Its a beautiful thingLets go back to our chartIn our new world of privacy with many highly complex lenses into your data any advantage you had with RBAC is now gone and in fact the Nos are exasperatedFlexibility: There is no such thing as a small or medium sized organization anymore this is because its not about how many employees you have its about how many policies you have The policies are now so complex because of your ethical and legal privacy obligations even the smallest companies have role explosionSimplicity: It is not easy to start anymore because theres nothing simple about your policies which lands you in a role explosion situation out of the gatesSimple rules: See FlexibilityCustomizing permissions: Again there is so much complexity you must separate the WHO (the user attributes) from the WHAT (where they have access) Without that its impossible to implicitly understand what you are giving someone access to when adding them to rolesYou may not have realized it until now but Privacy is what killed RBACImmuta saw this coming 5 years ago and built a platform to allow the enforcement of advanced privacy enhancing technologies and access controls using the ABAC model on any database or compute you choose such as Databricks Snowflake EMR Hive Athena/Presto/Starburst/Synapse Redshift  you name it You no longer have to have your head (or roles) explode Get your access control model under control and welcome to the new world order of ABAC  brought to you by PrivacyRIP RBAC
F4wkKvT34USt8znqLejyK7,We chose Snowflake as our data warehouse around 3 months ago During these 3 months we have been using it in our team Earlier the workflow was this Someone wanted some insight from the data They created a ticket and asked me to get that for them The joke became part of our newsletter tooSince then that has changed Now people dont have to come to me for getting insights from the data They ask for help sometimes with SQL but I am not the sole person who is getting insights from data for everyone It is looking to me like the decision for choosing Snowflake as our data warehouse was the correct one Engineering not being in the middle of data was one of my main goals when choosing a data warehouseIf you want to check how I chose our data warehouse look at this earlier post that I had done earlierI would like to share what was the difference between the before and after What made the after easier And some things that have been a somewhat mixed bagBefore Snowflake I used Apache Spark for getting information from the data I spun up a EMR cluster connected to the zeppelin notebook and started playing around with the data After I got the answers I gave them back Now anyone with an account logs into snowflakes UI writes SQL and get the answers they are looking for People who work on front end use it too to get answers Sometimes product people try it out None of these people were using Apache spark earlier That was too cumbersome Why? Because they had to worry about machines About the exceptions that Spark gave usAnother thing that I am noticing is that using Snowflake to make data based decisions It goes like this Someone says Lets do X Then someone says Why dont we check the data in Snowflake before we decide whether this is a good idea or not? Someone goes into Snowflake and checks the data Then the team is better informed to make that decision That almost never happened earlier A bit more data driven decisions nowThe main thing that helped with this change was that we dont have to worry about machines We still have ETL and ML model training jobs written in Apache Spark When we wrote those jobs we had to decide machine configurations What if we want to process more data? Try different machine configurations What if that new machine configuration fails? Try another Now I am not saying we cannot do it But it takes some time And having a much bigger cluster running is costly What about with Snowflake? We have created 4 warehouses in Snowflake  2 are for automated jobs that run on daily basis One for small ad-hoc analysis One for heavy data requirements We have 4 for budgeting reasons And if someone has to do a bigger work? People increase the warehouse size and decrease it back after the work is complete Quite simpleAnother thing that was unique in Snowflake was its Variant data type Our schema was not fixed We have integrations with 3rd parties They are sending us data Some of it has fixed schema Some of it does not fixed schema Sometimes they do mistakes and go against the schema We need to be able to store and analyze the data Variant data type has been helpful for thatAll in all I would say that it has been a positive experience with Snowflake If you reached till this point do share whether there was anything that you liked about Snowflake I would love to hear your experiences
AiiTN6Ney8YxzMiy3ReET7,In this post I will write down the understanding that I get out of Essence of linear algebra series Videos are great but sometimes we need some notes to refresh our understanding real quick I hope this helps out other people tooLinear Algebra is poorly understood Students understand what they are doing but not what the things representThis series will focuses on the geometrical interpretation of linear algebra to help people get an intuition on linear algebraConvention is to write vectors as a pair of vertical numbers in square bracketsEach vector represents a movement in space When we add vectors we add how much they move us in the directions represented by the numbers That is why vector addition can be considered to be starting from tail of vector A and then considering that tail of vector B sits there and moving along thatMultiplying by a number means that you scale the length of the vector That is why numbers are called a scalars If the number is negative we are also reversing the direction of the vectorMathematicians keep to an abstract understanding because the real value of the 2 interpretations  graphical and numerical is the ease with which people can switch between themThink of the 2 numbers representing a vector as 2 scalars  squishing the vector in 2 directions  x and y These are known as i^ and j^ These 2 scaled vectors are added to create any vector eg 2i^ + 3j^ So all vectors scale these 2 basis vectorsThe choice of these basis vectors is arbitrary All possible coordinate systems can be translated into each other Except when your basis vectors overlap or are zeroThe span of two vectors v and w is the set of all their linear combinations ie av + bw This is basically a way of asking what are all the set of vectors that you can reach by just using these 2 basic operations  addition and multiplicationIf we are thinking about individual vectors think of them as lines If thinking about collection of vectors it is easier to think of them as points with their top sitting at the point and tail at originWhen removing one of the vectors does not affect the span then we can say that the 3rd vector is linearly dependent on other 2 vectors This is same linear combination that was talked about earlierWhen removing one of the vectors does affect the span then we say they are linearly independent vectorsWe are going to understand a way to understand matrix multiplication which does not rely on multiplicationThe transformation in linear transformation is a fancy way of saying function We take in input and get an output But we dont call it a function because transformation is suggestive of movement Think of vector moving from one place to another To understand the transformation think of every point in space moving to another point in spaceA easy way to think is transformations that keep grid lines parallel and evenly spaced But how do we know where do the new vectors corresponding to the all of the original vectors in the plane land after the transformation? Just keep note of where the basis vectors land As long as it is a linear transformation all of the new vectors are still the same linear combination of the basis vectorsSo the 2D transformation can be described in terms of just 4 numbers  the new coordinates of i^ and j^ It is common to package them up into a 2 x 2 matrix where the columns can be interpreted as where i^ and j^ each landMatrix vector multiplication is simply a way of telling what the transformation does to a vectorSometimes we want to have a way to describe a sequence of linear transformations We represent individual transformations as matrices and use matrix multiplication to represent where the basis vectors will land after transformationThis was mainly visuals to show how the transformations described so far in 2D work in 3DMany of the transformations stretch or squish space To understand these transformations it is useful to measure how much a given area has changedIf we focus our attention on the square that sits on the basis vectors and see how much it changes we can tell how much any area in the coordinate space changes This is because grid lines remain parallel and evenly spaced in a linear transformationThe factor by which the area is changed is called the determinant of the transformation Determinant can be negative It would mean we flip space That is called invert the orientation of space In case of 3D space it is still orientation specificMatrices are useful for disciplines of Computer graphics and Robotics But the reason linear algebra is important for technical disciplines generally is because they let us solve a system of linear equationsThe linear equations can be thought of as a matrix vector product producing another vector That makes it easier to think about it We can find the unknowns by simply playing the transformation in reverse This inverse is called inverse of matrixIn case determinant is non-zero then it is almost certain that there is a unique solutionIn case determinant is zero then it is possible to have an inverse but that is not something that a function can give you as you will have to map the squished vectors (which is now single vector/plane) to multiple vectors while function can do one-to-one mappingIn case after transformation all points are on a line then it is rank 1 If everything falls on a plane then it is rank 2The set of all possible outputs of vector after the transformation is called column space of matrix (the one doing the transformation) Basically where the basis vector can be after the transformation This is same as span of the matrixIf the rank of matrix is same as number of columns/basis vectors then it is called full rankIf matrix is full rank then only origin falls on origin If it not full rank then whole line/plane of vectors gets squished to origin This is called the null space/kernel of the matrixIf we have a 3 X 2 matrix then it can be interpreted as a plane cutting through 3D space So a non-square matrix is a transformation that squishes or stretches between dimensionsDot product of 2 vectors is the sum of corresponding elements of the vector Geometrically it is product of the length of 1 vector and length of projection of 2nd vector on the 1st one Due to this interpretation it is easy to understand why perpendicular vectors have zero dot productNumerically cross product is the determinant when v and w are the first and second column of the matrix Graphically it is the vector parallel to the parallelogram with length equal to the area of the parallelogram formed The direction is determined by the order of the vectors as shown aboveI only got as far as Video 11 Will update the below whenever I watch further
VFLR428scjSST4XQ5coPYy,We recently chose a data warehouse after doing a basic POC of some data warehouses  AWS Redshift AWS Athena Snowflake Google BigQuery In this slide I share what were some considerations unique to our business due to which we ended up choosing Snowflake and what were the pros and cons of the various warehousesThis mainly is for sharing the experience that I had while doing POC over the various data warehouses One thing missing from the slides is BigQuery I recently got certified as Google Certified Professional  Data Engineer so I already knew its capabilities and hence I did not do a POC over that Our technology stack was mainly over AWS so going to GCP would have made sense only if there were significant benefits which I did not see while doing the comparison for choosing the data warehouseI will do a follow up post to share the good parts of Snowflake in coming weeks If you have any questions do add to the comments and I will try to answer them as they comeEdit: I finally got around to writing about Experience with Snowflake
YQFJi35rbzd4dryzL9DR7T,Cassandra as a distributed database is affected by the CAP theorem eventual consistency consequence And sometimes eventually means a long long time if you are not taking any action This article is our first telling on our adventures and challenges with Cassandra and how we faced them This one is about Cassandra Repair SystemLet me start with a big loud imperative and truthful statement: While writing or removing data from it the clusters nodes must communicate among themselves to synchronize replicas and ensure consistency This process is what Cassandra calls anti-entropyBesides anti-entropy mechanics two other processes build up Cassandras repair system: hinted handoff and read repair As anti-entropy their goal is to improve Cassandras consistency by taking action on specific occasions; the former is when a node is down for some time and has lost some writes the latter is during some readsYou might be wondering why I have written about subjects that already are present on Cassandras official documentation Bear with meAs you already know  just in case you dont  In Locos main technology is to provide beaconless indoor location intelligence We believe in being able to provide services by anonymously detecting our clients interaction with the world around them The team I work on was built to develop solutions related to this visionBehavior is our first attempt to develop privacy-friendly authentication / authorization products through geolocationOur first authentication product is currently used by a few digital banks in order to accelerate their onboarding process while reviewing user information Through our technology clients addresses documentation turns to be obsolete thus enabling the whole onboarding process to be frictionless for themTo construct this product we adopted Cassandra to anonymously store aggregated devices geolocation data Currently we have a Spark pipeline processing devices daily visits and feeding our inference engineEveryday In Locos integrated devices generate approximately 50 million visits creating new or updating an existing devices frequent locations This is where consistency comes to play; as we have said before inconsistencies happen every time we write to Cassandra although repair systems try to take care of itWhilst analysing a reported issue within our Cassandra data we had a big surprise Two nodes returned a very different set of answers one of which was missing new data Well we knew about Cassandra eventual consistency property but no one in the company ever had a problem with it Until nowJust to be sure we queried both nodes shortly afterThis event taught us about Cassandras read repair… But a bit lateAfter this joyful ride we started reading about Cassandras repair system The documentation has a section dedicated to teaching about when to repair nodes Two of the situations listed are very important to keep in mind: We did not have a routine repair and we certainly had data that wasnt queried frequently enough so read-repair could make its magicA few options were suggested: Although they were simple and doable alternatives they missed a key feature we wanted: a more automatic and less laborious way to repair Cassandra according to a schedule Given that we decided to check out existing projects related to this and find out if they could be a more robust alternativeTwo projects came out quite frequently: Priam is more along the lines of a Cassandra cluster manager It is able to perform token and backup management seed discovery and cluster configuration Cassandra-reaper is a centralized stateful and highly configurable tool for running Apache Cassandra repairs against single or multi-site clustersWe had just found our hero With Cassandra-reaper we could not only get our beloved repair working automatically but also we could check nodes health in a friendly UIIt was very simple to set a kubernetes deployment for it Even if you are not familiar with Kubernetes a similar effort to set up Cassandra-reaper can be accomplished using Docker (docker-compose or a dockerfile) Beware of the storage system you choose for Cassandra-reaper We opted to store within Cassandra as it wraps the whole cycle in a single place so we just have to watch one databaseThe hardest part is to set Cassandras JMX This is the way Cassandra-reaper communicates with the cluster and operates over it For test purposes avoid setting authentication / authorization just make sure JMX_LOCAL=no and you should be good to goWhen all is done you should see this screen when you visit Cassandra-reaper web server We have already added our clusters Any information related to how you can use it can be found in its documentationOne of Cassandra-reapers major features is its simple web UI with quick configuration and very clean layout It is very easy to use and configure any repair and check the clusters health It also comes with an authentication / authorization mechanism which is as simple to set as the deployment itselfThe other one is the split of token ranges into smaller segments This mechanism enables a smoother repair; nodes CPU usage can increase during repair which impacts query latency Be aware that its impact is strongly related to the repair intensity configurationCassandra-reaper has a whole lot of other features and concepts which can be found in its documentation It is now integrated into our system to watch Cassandra status and keep nodes healthy Hopefully we wont have more surprises with inconsistenciesYou can checkout our deployment file hereIf you are interested in building context-aware products through location check out our career page Also wed love to hear from you Leave a comment
hV2xUFXdxQQFMNMvK2tjYa,"Every single tech company that operates at a very large scale will tell you about the importance of knowing how to properly handle data transport and manipulation When providing context-aware location services for 50 million users on mobile phones all over the world we at In Loco need to constantly re-imagine and re-invent our infrastructure such that not only our developers and overall data analysts are able to inspect and extract insights from our data  but also that applications are able to efficiently communicate with very high traffic data volumeWhile designing our data infrastructure our data engineering team worked under two very strong premises that would shape the current architecture: At In Loco we avoid making uninformed decisions at all costs To have the support of a statistical measure or of a key performance indicator it is extremely important for those who have to make a decision  be it at a product level or at a development level Not only marketing and business teams should be able to make informed decisions but also development teamsHaving a centralized data team as a squad responsible for collecting metrics is not scalable either from a business or engineering perspective Also the producer of the data is the person (or team) that will be able to extract the most information and knowledge out of it; and they should have the least amount of barriers to it as possibleJust as employees at a company need to have crisp and clear communication in order to achieve a common goal so do applications We designed our data infrastructure such that applications can easily propagate data without having to worry (much) about cumbersome procedures and protocolsThe following diagram illustrates the main components on our engineering stack We follow with more details of each layer belowWe rely heavily on Kafka to transport data amongst applications in an efficient manner There are several benefits to use a message broker system to transfer data amongst applications: Almost all of our data that goes through Kafka is on Avro Using Avro allows us to easily generate code for data manipulation in many languages (Java Go) without having to write clients or libs by ourselves while also taking advantage of well defined data schemas and efficient (de)serialization""You can't improve what you can't measure! We strongly monitor all of our applications leveraging Prometheus on our Kubernetes' clusters to easily expose metrics We use Grafana to easily create alerts systems  most of which either triggers on-call developers or triggers alerts to Slack channels  and dashboards to keep track of the overall health of our applicationsSome kinds of processing workloads are heavier by nature and require a batch-oriented high IO throughput and very long time frames of data to process (several days)""We use Secor to backup our data from Kafka to S3 Once the data is written in Avro files on S3 we have scheduled Spark jobs that read and convert those files to Parquet and write it back to S3 Spark's DataFrame API has lots of performance improvements when working on Parquet files mainly due to its column-oriented storage which greatly helps also on analytics Spark jobs either reads data directly from S3 or caches it into its own HDFS cluster for higher locality We mostly use Airflow to schedule the Spark jobs and for tasks automationAlthough having Parquet files is very efficient for Spark and other distributed processing workloads it is not easy for non experienced developers to write Scala or Python code for Spark on Zeppelin notebooks To empower our BI team (and analytics-oriented employees along the company) we expose our Parquet data managed on S3 through external tables on Presto After trying to manage several HDFS clusters we have found that the performance gains on data locality by distributed query engines directly on top of HDFS was not worth of the burdensome operation of maintaining Hadoop clustersPresto was our top choice for running interactive ad-hoc queries for several reasons: Finally we expose our Presto analytics clusters on several dashboard tools such as Metabase Redash etc Using dashboards gives more flexibility for BI with query versioning data visualization and enhanced control of its analytics with more independence without the need to action experienced engineers with appropriate know-howFrom a technical standpoint the presented data architecture has allowed us to scale to the throughput of over 200MBps (or over 15TB per day) produced on a daily basis on our Kafka clusters This infrastructure has allowed us to efficiently build both our real time beaconless location-based intelligence services such as fraud detection indoor visits detection and behavior driven mobile engagement and analytics while also powering up our workload pipelinesFrom the business intelligence point of view instead of centralizing the knowledge of our data on a single team we built an infrastructure to make it possible that people are closer to the data and to empower themselves to easily make decisions with better analytics and data support""Are you interested? If you are interested in building location and context-aware intelligent services and products check out our jobs postings! Also we'd love to hear from you in case you have any questions or just want to learn more from us! Let us know what you would hear from us in the upcoming new posts"
PLEDjacUviXmm4xBUvqpHE,ETL as a Service is a dream come true for every pipeline system inside an organization Extract transform and load data quickly and simply without having to rely on other teams to do so The requirements to achieve ETL as a Service are the following: there must be an execution engine to execute the code a scheduler to trigger the execution engine given date and time and a storage engine to retrieve the code fromIf you write any code in Scala Python or whatever languages you want to design your pipeline with (at In Loco it is mostly Scala (Spark) and Python here is a link to an explanation about our data architecture) this piece of code must run somewhere it can be on your computer or in the cloud and there must be a tool that coordinates this execution To coordinate means execute have a retry logic and know if the execution failed or not Nowadays some tools execute pieces of code Tool examples are Luigi Airflow Kubernetes Jobs AWS Lambda Argo Workflows and othersWell the code is written and we know how to execute it but it is an ETL we are not going to alarm a developer/data scientist every hour so the person can click a button to trigger a pipeline people need to sleep There must exist some way to trigger this code based on the current date and time to alarm the responsible if the execution failed and to store the logs generated by the execution so it is possible to debug it Some of the tools like Airflow do the execution and the scheduling Other tools extend the execution engine with a scheduler like Kubernetes CronJobs and AWS Data Pipelines In contrast other tools focus solely on scheduling like Rundeck which we are going to talk about furtherEvery execution needs somewhere to retrieve the code from and regarding this topic we dont have many choices It mostly summarizes between git and AWS S3 Git may seem like the obvious option but if your code produces an output to S3 you may want the code and data to be together It is also easier to deal with S3 than git but keep in mind that you lose code versioning by choosing S3Before thinking about how to make ETLs more simple and self-service to everyone the company pipelines were triggered through Airflow or Data Pipelines and both had an interface to launch Spark Jobs through AWS EMR Both solutions were enough to deal with the products pipeline Still the Business Intelligence team had to rely on having engineers who knew how to implement Spark Jobs to schedule their pipelines Both solutions were not productive since they had no autonomy in running and implementing their aggregationsFor the execution we decided to go with Argo Workflow since we were already testing it for some internal pipelines the docs are impressive and we already had the experience of working with it The workflow definition had a gitsync sidecar app to download the git repository as the first step Because most of the jobs are written in Jupyter notebooks we used Papermill to execute the notebooks and if the job failed the workflow would trigger an HTTP request to Slack to alert the job failure To schedule the workflow execution we used Kubernetes CronJobsThrough Jupyter notebooks we used sparkmagic to communicate with Spark and we implemented a magic on top of the SQL magic to communicate with Presto alsoThis approach got the job done but it had some issues It is not easy or intuitive for non-developers For example data scientists usually dont know Kubernetes and this is not a bad thing They should be able to focus more on the data and not the infrastructure behind the data This problem had an initial workaround that was to search from time to time the git repositories that had notebooks with certain tags saying when to execute it but this still wasnt intuitive for non-developersAfter studying scheduling tools we ended up choosing Rundeck It is a tool that provides a spectacular UI for bash commands scheduling It also offers logs alarms jobs per project structure parameterization of bash commands access control per project is easy to customize and has an open-source version talking like this it looks like it is perfect but it is almost perfectWe needed to add a simple bash script to make Rundeck talk with Argo and also add the S3 log plugin to send execution logs to S3After testing this architecture internally we noticed that the feature of defining Directed Acyclic Graphs with Argo Workflows was not being used which made us stop using Argo since Kubernetes Jobs are good enough for us and Rundeck already is capable of defining a simple pipeline through its UI but this had a minor issue we had to add the Slack notification plugin to enable Slack notifications (this plugin had to be refactored to work using Slack Apps instead of Webhooks)While this model started working fine more and more teams needed to use the platform (we were migrating from AWS Data Pipelines and Airflow) and we decided to create a generic CLI to trigger the Kubernetes Job execution through templates This made Rundeck more generic to everyoneNowadays if anyone wants to schedule an ETL we have two job templates on Rundeck the notebook and the spark job template the user needs to set the notebook path or the jar location set the time and day to run it and set the alarm cases for the job The best part of this architecture is that anyone is able to schedule jobs through the Rundeck UI at any time without having to rely on other teamsWhile our current architecture is elegant and covers our use case we can still see that it is possible to make the execution engine more generic and extend it to ensure better auditing and access control of the executions Luckily the rundeck-plugins repository already has a Kubernetes plugin that is a good starting point for development Another point of improvement is to make more Rundeck-accessing interfaces and one example is a Jupyter plugin so the users can schedule notebooks from within itIf you are interested in building context-aware products using a powerful location technology that genuinely cares about users privacy then check out our opportunities
mm5jHxELJa9yybqhB3jM6K,Gathering intelligence from data is one of the main byproducts of our technological era However it may come with the price of dealing with sensible data from the userSome simple and handy examples of such intelligence are frequency histograms For example suppose you are a business owner and want to know the level of recurrence of your customers ie how many times does a customer come to your business in a week An effective way to display such information is to create a histogram of customers visits as shown in the figure belowComputing these histograms from large amounts of data requires a lot of computational power and memory We need to keep track of a lot of entries to compare and aggregate Also from the point of view of privacy we may encounter a problem since we need some kind of identifier of the entriesWeve come up with a simple but effective idea to perform such task both in a fast and privacy-friendly way But first lets talk about a keystone in such idea: HyperLogLogHyperLogLog (or HLL for the close friends) is a probabilistic data structure introduced by the french computer scientist Phillipe Flajolet The idea of such a structure is to efficiently gather information about a set (a collection of things) With it we can compute the cardinality (number of things without repetition) of the set and perform unions with other HyperLogLogsFor example suppose that you plan to go to some of your friends house and play games You have a pretty large set (in the order of millions) of games your friend also have a lot of them and you two will play together Since of course the plan was to play all the games you both had you want to know previously how many unique games you both together have so you could allocate the right time to arrive at your friends house and still be able to play it all before you need to go (since you have to be at home in time for dinner) HyperLogLog can save you some time in this processBecause of these properties this structure can be (and is) used for tracking large sets of data However another useful and perhaps accidental property is that it makes data pseudo-anonymous In the process of creating the structure elements are converted to the number of leading zeros in the binary representation of their hashes and only the maximum value of those are saved Thereforeit is not easy (and a lot of times not even possible) to backtrack those elementsHowever the intuition behind doing this is not simple so an example helps a lot The following example is totally derived from the blog post of Alessandro Nadalin (and figures totally inspired in xkcd check it!)Imagine that you were invited to the great LogLogParty There is a rule: if you meet someone new you must ask their phone number So you trying to keep track of how many new people you met decide to think of strategies to do so You come up with a list: The first strategy is pretty straight-forward you just need to keep a list of all the people (or phone numbers of people) you met remove duplicates (assuming you met so many people that actually met some twice by mistake) and bam: you have your new friends countThe problem with such an approach is that you are going to have a lot of headache if you meet too many people Removing duplicates and counting becomes a pain in the assThe second strategy is a little more intelligent when it comes to making a lot of new friends You instead of keeping track of all the names of all the new people you have met decided to count the number of leading zeros in each persons phone number and only save the largest youve seen! At first look this strategy seems terrible However lets think about itIf you met someone who has five leading zeros in their phone number this is an event so unlikely that you must have met on average 100000 people (assuming the erroneous but useful hypothesis that phone numbers are uniformly distributed and do not depend on the person who possesses it) So thats the new way to count: just keep track of the maximum number of leading zeros you got and at the end of the party compute 10 to the power of this numberOf course the problems with this method arise immediately: you can only count in powers of ten and worse you can be (un)lucky that in your first new friend you met someone with lets say six leading zeros and automatically have 1 million more friends instead of oneThe problems with the second strategy involved poor precision and high variance of the resultBut keep calm! You do not need to attend a lot of parties Just change a little part of your strategy Instead of keeping track of a single number of zeros lets keep track of the number of zeroes in each bucket! A bucket is a recipient that contains phone numbers that have some kind of similarity For example we could have buckets representing the number on the left of the phone number So numbers starting with 9 belong to bucket 9 and so on Thinking this way when you meet someone new the first thing to do is go to their bucket There you keep track of the maximum number of leading zeros in the rest of the number (excluding the bucket part) For such you now have a lot of buckets each one containing the maximum number of leading zeros of a number that youve seen that belongs to that bucketThe good side in this strategy is that it takes away a great part of the luck factor since you would have been very lucky to be lucky in each of the buckets And also provides a new batch of possible values for cardinality since we are going to take the average of the counting in each bucketYou were just introduced to HyperLogLog Swap people for elements phone numbers for hashes calculate some fancy mathematical bias correction and you can count cardinalities of large sets with low memory and processing with the cost of a little bit of imprecision Of course this example fails to comprehend all the components of HyperLogLog There is a lot of work in both technical and academic perspectives to make it better and more preciseAs I said we developed a way of computing those histograms that we talked about The idea is to use HyperLogLogs (kinda) to do soThe idea behind the strategy is that the number of leading zeros in a hash the value that we track in an HLL can be very representative of a single element Lets say an element has ten leading zeros in its binary representations hash and this is such a rare event (given a reasonable number of users) that we can almost for sure say that if we see ten leading zeros it almost certainly is that elementSo the number of leading zeros is a quasi-ID of a user and the buckets act like a subsampling of all population Why not do the histogram of the leading zeros? Thats precisely itImagine that we have computed HLLs for each day of the week If we look in a specific bucket in day one and see six leading zeros saved and look in day two and see the same six leading zeros count its probably the same person (could have been just luck and were two different people but this is unlikely) So we know that this person has come to this place on two separate daysAn important factor that we need to take in consideration is that the histograms outputted by the algorithm are normalized since looking at the maximum number of leading zeros is equivalent to sub sampling the population However going back to the real histogram is just a question of multiplying each of its entries by the population size that can be estimated with the very HyperLogLogsIn theory the strategy is excellent in practice not so good (mostly because I lied in the theoretical part) Assuming that the number of leading zeros in a bucket uniquely defines a user is valid for the majority of cases but not all and such cases are not negligible Precisely those cases where this is not true make the histogram look skewed The figure below shows itSince we could not compute with precision the histograms associated with a HyperLogLog set a new approach was taken: to save a part of the hash of the user with the maximum number of leading zeroes In such a way its possible to avoid collision with (a lot) more probabilityMinCodeSample does it Its structure is exactly like HyperLogLog except that it saves part of the hashes in each bucket (in fact there is a structure just like this called HyperMinHash although not used for histograms) In this way we can achieve a lot more precision with the cost of saving part of the information that identifies a single user that is with the cost of less privacy This trade-off comes from the fact that the longer the part of the hash that we save the easier it becomes to identify the user that generated that hash At the same time the better and more precise the histogram calculation becomes since we become even more sure of the uniqueness of this sampleAlthough we are aware that cardinality estimators do not preserve privacy we understand that the MinCodeSample represents a step towards keeping our users data safer This safety comes from the fact that: Therefore we see this structure with kind eyes and we are eager to improve it Maybe you can help usIf you are interested in building context-aware products using a powerful location technology that genuinely cares about users privacy then check out our opportunities
SqDzdwhoAJRFHSbagkTojG,Location data is one of the core drivers of the work we do at Inloco It provides us with a way to infer contextual information in the real world which can in turn be used to model the spreading of a virus or build engagement solutionsIn order to provide this kind of information we built a database of Place entities also commonly referred to as Points of Interest (POIs) in scientific literature The planning of our places infrastructure began when our products were still at their infancy and at the time we realized that: This article firstly describes our platform as of the beginning of Q1 2019 and expounds data consistency issues we found during the years We also present our current solutions for these issues and how were planning to improve themFor ease of understanding we define a place entity for this article as follows: A place entity is a direct representation of a physical location in the real world which has well-defined boundaries a purpose a considerable size and is both atemporal and a source of contextThus it is useful to think that in our database places are most often either public sites or establishments This definitions excludes for instance building floors stretches of sand on a beach and landmarks with negligible boundaries eg a statueWith these goals in mind we set out to develop what would become an event based platform which dealt with place records ingestion from both the Web and internal sources exposed them by APIs and web interfaces and was processed on a daily basis by several Spark distributed applications All the while it provided full visibility of what was going onTo arrive at this architecture we went through several iterations which gave us golden learning opportunities While these are not the focus of this blog post we highly encourage taking a look at one of our previous presentations about the topicAs Figure 2 depicts all data we ingested from the web was being sent to Apache Kafka Additionally users were able to manipulate our places database by utilizing our web interface (Places Web) Every action in our DB thus boiled down to a set of INSERT UPDATE DELETE or MERGE Kafka records sent to the places-db-commit-log and then processed by our DB Writer serviceLets say for instance that we ingested a new place entity called Shopping Plaza: if we already had a place entity representing this real-world place in our DB we would just UPDATE its information since its phone number address timetable or other attributes could have changed If however there was no equivalent place entity in the DB we would just INSERT it Similarly manual user operations executed through our web interface would generate operations pertaining to the aforementioned setBy using Inlocos Kafka structure we also gained with little to no effort automatic backup of every action issued to our db However we still needed a way to visualize those actions in real time for debugging and analysis In order to do that we utilized Inlocos Kafka Elasticsearch Injector publishing every action event in a dedicated Elasticsearch cluster with KibanaAnother important aspect of our architecture was the usage of three different data sources First of all we note that our PostgreSQL DB was and still remains our canonical data storage Furthermore while it may seem odd to do that we found out that our clients required certain query types for which there were better toolsFor full text queries for example Elasticsearch offers a full range of query operators and optimizations which we would have to implement by hand with PostgreSQL and would generate code that is hard to maintain hard to improve and very error-proneSimilarly Redis proved to be an amazing tool for simple and efficient in-memory key-value queries including our main use case of geospatial queries which are gracefully performed by the GEORADIUS operatorAll in all we learned that there is no silver bullet when it comes to picking a database to deal with complex records such as places which will be used by multifaceted applications Each one will have pros and cons and it is up to the developer to choose how to combine them to best suit their needsFinally our architecture was also able to handle large-scale batch processing of our whole dataset by distributed algorithms and machine learning models Subsequently we were able to explore and iterate quickly over solutions for problems found in our dataAfter some time using this architecture we started noticing a few issues One of the most important ones was that all synchronization between our three data sources was being done programmatically by us which led to several inconsistencies between the databasesJust like that by ingesting a lot of data from different sources we began to have several data quality issues For instance: we mentioned that whilst receiving a new record we needed to check whether or not it was already present in the database As it turns out this problem is far from easy and is in fact touched upon by different scientific research areas such as Entity Resolution/Record LinkageMeanwhile as we started to acquire more knowledge in dealing with places data we noticed that almost all of the issues approached by us were complex enough to deserve their own blog postSumming them up we highlight: Along the years we have gained significant experience in dealing with all the aforementioned architectural and data issues but more so in the DB synchronization and places deduplication tasksThese tasks are inherently connected since both of them affect our data consistency that is: the former deals with consistency among different data sources while the latter deals with consistency in a single data source Thus we go deeper into these two problems and how we dealt with themSuppose that you are performing queries in two different endpoints of a single API one of them providing full text search and another one providing geospatial queries Among the first querys results you receive a record with an id A which has latitude and longitude coordinates of <-80631229 -348717757> Then you query the geospatial endpoint looking for places in a 100 meter radius from these exact coordinates but record A is nowhere to be seenFrustrating isnt it? This was one of the synchronization issues that was happening in our Places API By manually replicating INSERT UPDATE DELETE and MERGE actions in our DB Writer a failure in executing the given action in one of the DBs and a success in the other ones meant that we were leaving our DBs with inconsistent statesIn order to fix that we could attempt to roll back each successful operation if another DB resulted in a failure but these rollback operations in turn could fail as well Basically it is hard to perform queries in a transactional behavior for three different databases simultaneouslyIt was at that time that we read about the Change Data Capture (CDC) pattern and Debezium A recent post in our blog already mentioned Debezium and the CDC pattern at depth but in short Debezium is able to read every change being applied to a database (in our case PostgreSQL) and stream it to Kafka in an event sourced format similar to our places-db-commit-logBy consuming these records one would be able to not only reconstruct a previous state of the database but also replicate them elsewhere which fortunately was just our use case Hence we configured and started using Debezium to stream data from our canonical PostgreSQL data storage and made use of Kafka sink connectors to read the data and write to our Elasticsearch and Redis databases Figure 3 displays our debezium integrationWhile Confluent has a public Elasticsearch sink connector available we had to create the Redis sink ourselves since no market solution seemed to handle both record deletions and geospatial data as well as wed hoped We intend to make this project public in the futureBy modifying this part of our architecture we consolidated PostgreSQL as our canonical data storage and provided eventual consistency to all our other DBsHaving eventual consistency means that eventually all queries for a specific record will return the same value and in our case eventually boiled down to latencies upwards of tens of milliseconds during normal operation Also we were able to get rid of a great chunk of code in our DB Writer which led to the service being much easier to maintain and less error-proneDeduplicating places is probably the hardest of the issues weve faced and whose resolution would be most relevant to our company Since we consume data from different sources including the web (which is to put it mildly noisy) it is common for us to receive Place entities representing the same real world Place over and over againAs mentioned in our architecture we need not only to solve this problem in an online fashion but also via batch processing of our dataOur first attempts to solve this problem were very domain-specific and manual but resulted in an unacceptable rate of false positive and false negatives
MtRuyiAt49VNHpZRQCZWHf,Here at intermixio we spend all day helping our customers optimize their performance on Amazon Redshift so we get a front-row view of all the ways that Redshift can be used to help businesses manage their data Redshift is a versatile product that can help businesses aggregate store analyze and share their data Weve collected 4 use cases of the multipurpose ways that businesses are using this adaptable toolRedshift started out as a simpler cheaper and faster alternative to legacy on-premise warehouses If you read Jeff Barrs blog post announcing Redshift ( Amazon Redshift  The New AWS Data Warehouse ) the pitch was all about simplicity and priceFast forward to now and the use cases are way more sophisticated than just running a data warehouse in the cloud I recommend looking at the slides from ReInvent 2016  Whats New With Redshift (also see full video of the talk on YouTube)You will find 4 uses cases: Data warehouse technology has been around since Kimball and Inmon What changed with Amazon Redshift was the price at which you can get it  about 20x less than what you had to carve out going with the legacy vendors like Oracle and TeradataThe use case for data warehousing is to unify disparate data sources in a single place and run custom analytics for your businessLets say youre the head of business intelligence for a web property that also has a mobile app The typical categories of data sources are: With a rich ecosystem of data integration vendors its easy to build pipelines to those sources and feed data into Redshift Put a powerful BI / dashboard tool on top and you have a full-blown BI stackA key advantage of Redshift is simplicity It used to take months if not quarters to get a data warehouse up and running And youd need the help of an Accenture or IBM None of that anymore You can spin up a Redshift cluster in less than 15 minutes and build a whole business intelligence stack in a weekendSome examples by teams who built an early analytics stack on Redshift: The combination of price / speed / simplicity expanded the addressable market for data warehousing from large corporations to SMBs However because it is so easy to get going data engineers must make sure to follow the best practices when setting up their cluster and avoid any performance issues they might have data volume and pipeline complexity growsBecause of the cost of storage in previous generations of data warehouses you had to aggregate data It was too expensive to store raw data That changed with Amazon Redshift Because Redshift is cheap its possible to store raw event-level data without getting killed on storage costEvent-level data comes with three key benefitsAnd so that made Amazon Redshift a perfect fit for analyzing machine-generated data like web logs or clickstream data Massive amounts of data that come in at high velocityBecause Redshift is fast and cheap processing machine data data is cost-effective and you can drive the time required for ingest-to-insight (ie the time between pushing data into Redshift and the final analysis output) below the 5 minute mark Not just for basic aggregations but complex 10-way joins across billions (billions with a b) of rows Thats remarkableThe business value comes with exposing that data back into the business For decision making data-driven services etc Here are a few links of tech talks I recommend reading up on they describe in detail how Yelp Lyft and Pinterest use Amazon Redshift to process data and then expose it to services that need itThe business value here goes beyond mere cost savings by migrating your warehouse to the cloud Rather youre enabling new services informed by data These data-driven services are the foundation for better / faster decision making and also new revenue-generating productsThat distinction is key In previous days with the use of basic reporting companies would look at a data warehouse as a cost center Yes data is important but also expensive And so the goal was to keep that cost down as much as possible Limited exposure of data to a limited set of people etcNow it makes sense to increase spend into your data infrastructure Thats because an incremental $ spent on analyzing data can lead to a larger incremental increase in $ revenue generatedThat leads us to the next use case where Redshift drives new revenue as the core engine behind an analytics product ie business applicationsNot all companies have the technical abilities and budget to build and run a custom streaming pipeline with near real-time analyticsBut analytical use cases can be pretty similar across a single industry or vertical That has given rise to analytics-as-a-service vendors They use Redshift under the covers and then offer analytics in a SaaS model to their customersThese vendors either run a single cluster in a multi-tenant model or offer a single cluster to customers in a premium model Take Acquia Lift as an example The charging model then is a subscription fee to the analytics serviceTo give you some back-of-the-envelope math: In a multi-tenant model you can cram data from 10s of customers onto a single node cluster which costs you ~$200 / month Price out the actual analytics service at $500 / month / subscriber and you have a business with some pretty good gross marginsUsing Redshift for mission-critical workloads has emerged in the past few years Here data sitting in Redshift feeds into time-sensitive apps Its key that the database stays up because otherwise the business goes down (quite literally)In some cases eg the NASDAQ that is daily reporting That reporting cant be late or wrong otherwise somebody might quite literally go to jailOther cases include building predictive models on top of Redshift and then embed the results programmatically into another app via a data API An example is automated ad-bidding where bids across certain ad networks are adjusted on a near real-time basis The adjustments are calculated on ROI and what ad types performed best in the last week day and even hourRedshift has driven down the cost of running a data warehouse and as a result expanded the addressable market Because Redshift is cheap it allows to store event-level data which opens up a whole new world of use cases Some of these use cases include data-driven services that create new revenue streams for companiesAnd when data in Amazon Redshift becomes critical for business success its important to make sure your cluster is running efficiently If youre part of a data team thats building mission-critical data pipelines sign-up for a free trial of intermixio to improve your Redshift performanceOriginally published at https://wwwintermixio on November 11 2017
3Z88YLseaj5bS9TMd3TNnP,One of the major propositions of Amazon Redshift is simplicity It only takes minutes to spin up a cluster The time-to-first-report ie the time it takes to go from creating a cluster to seeing the results of their first query can be less than 15 minutes Thats true even for petabyte-scale workloadsBecause its so easy to set-up a cluster it can also be easy to overlook a few housekeeping items when it comes to the set-up That can cause problems with scaling workloads down the road A general complain we often hear is slow queries or slow dashboardsA key configuration to use is the Amazon Redshift Workload Management (WLM) Without using WLM each query gets equal priority The result is that some workloads may end up using excessive cluster resources and block business-critical processesHere are three frequent issues we hearYou can address these challenges with our top 14 performance tuning techniques for Amazon Redshift However odds are youll be able to get some quick performance gains by adjusting your WLMAnd so in this post well recommend a few simple best practices that will help you configure your WLM the right way and avoid these problems Using workload management the right way has a lot of benefits Its the single best way to achieve concurrency scaling for Amazon Redshift Your users will be happy (fast queries) you can scale as your data volume grows and youll spend less time fighting firesAmazon Redshift operates in a queueing model The first step is to define queues for your different workloads Next you need to assign a specific concurrency / memory configuration for each queueAmazon Redshift allows defining up to 8 queues with a total of up to 50 slots In the Amazon Redshift docs youll read to not go above 15 slots By using the techniques in this post though youll be able to use all 50 available slots With clear visibility when and how you need to fine-tune your settingsThe default configuration for Redshift is one queue with a concurrency of 5 If you run more than 5 concurrent queries then your queries wait in the queue Thats when the takes too long goes into effectThe available amount of memory is distributed evenly across each concurrency slot Say that you have a total of 1GB then with a default configuration each of the 5 concurrency slot gets 200MB memoryIf you run a query that needs more than 200MB then it falls back to disk That means it takes longer to execute Disk-based queries also consume a lot of I/O That slows down the entire cluster not just queries in a specific queueUsers then try to scale their way out of contention by adding more nodes That can become an expensive proposition The performance increase is also non-linear as you add more nodesYou can achieve a much better return on your Amazon Redshift investment by fine-tuning your WLM You can fix slow and disk-based queries by configuring Redshift specific to your workloads Because odds are the default WLM configuration of 5 slots will not work for you That includes using the option of Short Query AccelerationYou can read how our customer Udemy managed to go all the way to 50 slots and squeeze every bit of memory and concurrency out of their 32-node cluster following the setup in this blog postWhen the user runs a query WLM assigns the query to the first matching queue and executes rules based on the WLM configurationAnd so the key concept for using the WLM is to isolate your workload patterns from each other You can then create independent queues and each queue supports a different business process eg data loads or dashboard queries With separate queues you can assign the right slot count and memory percentageImage 2 describes the four distinct steps in to configure your WLM And so lets look at the four steps in detailLets look at the four steps in detailStep 1: Set-up individual usersThe first step is to create individual logins for each user A user can be a person an app or a process Anything that can run a querySeparating users may seem obvious but a lot of times logins get shared The problem then is that you cant tell who is driving which workloads Sure with a few users that may be possible But as your organization grows there will be a lot of guessing involvedAlso do not use the default Redshift user for queries For one because it has admin privileges But consider it as your lifeline when you run into serious contention issues  you will still be able to run queries with the default userIf your cluster is already up and running with a few users we recommend doing a reset Delete the old users and assign everybody new loginsStep 2: Define your workloadsThe next step is to categorize all user by their workload type There are three generic types of workloads: Defining users by workload type will allow to both group and separate them from each other What youll find is that workload of the same type share similar usage patternsStep 3: Group users by workload typeWe can use the similarity in workload patterns to our advantage By grouping them well have groups of queries that tend to require similar cluster resources For example loads are often low memory and high frequency Ad-hoc queries on the other hand run less frequent but can be memory-intensiveUse the CREATE GROUP command for creating the three groups load transform and ad_hoc As you can see they match the workload types we defined for our users Use ALTER GROUP to add the users we defined in step #2 to their corresponding groupYou can of course create more granular sub-groups eg for sales marketing or finance That way you can give the users in each group the appropriate access to the data they require But stay within the logic of workload patterns and dont mix different workload groupsStep 4: Define slot count & memory percentageIn the final step we determine what slot count we give each queue and the memory we allocate to each slotWe keep the default queue reserved for the default user and set it to a concurrency of 1 with a memory percentage of 1% The default queue is your insurance in case something goes wrong Consider the 1% of memory as a cost of doing businessFor the other queues slot count and memory will determine if each query has: If both is true thats when you get blazing fast queries and throughput To apply the new settings you need to create a new parameter group with the Redshift consoleWhen you apply the new settings we also recommend activating Short Query Acceleration and Concurrency ScalingEven with proper queue configuration some queries within a queue take longer to execute and may block short running queries during peak volume By using Short Query Acceleration Redshift will route the short queries to a special SQA queue for faster executionConcurrency Scaling for Amazon Redshift gives Redshift clusters additional capacity to handle bursts in query load It works by off-loading queries to new parallel clusters in the background Queries are routed based on WLM configuration and rulesWith your new WLM configuration and SQA and Concurrency Scaling enabled all thats left now is to find the right slot count and memory percentage for your queuesBut that process can feel a little bit like trying to look into a black boxAWS provides a repository of utilities and scripts They involve querying the system tables (STL Tables and STV Tables) The scripts help you to find out eg what the concurrency high-water mark is in a queue Or which queries fall back to diskThere are three potential challenges though with scripts: With our Throughput and Memory Analysis we make finding the right slot count and memory percentage easy You can see the relevant metrics in an intuitive time-series dashboardOur Throughput Analysis shows you if your queues have the right slot count or if queries are stuck in the queue When queries get stuck thats when your users are waiting for their dataWith our Memory Analysis you can see the volume of disk-based queries Some queries will always fall back to disk due to their size or type But we recommend keeping the share of disk-based queries below 10% of total query volume per queueSo if youre ready to implement proper workload management for your Redshift cluster start your free trial with intermixio During the trial well work with you on finding the right configuration for your queuesOriginally published at https://wwwintermixio on June 25 2018
MChpnjDpEFDiP2i6gX7SzS,Amazon Redshift has gotten a lot of news in the past few months Larry Ellison slammed Redshift at OpenWorld in October 2017 the 2nd time after he had already thrown darts in 2016 You know youre doing something right when Larry calls you out on stageWeve not been able to discover the reason why Amazon chose the name Redshift for its data warehousing solution but we suspect it had something to do with hyper expansion Like the universes rapid expansion since the Big Bang enterprise data has also expanded at a rapid rateIn this post well cover the basic concepts behind Amazon Redshift and why its seen so much adoptionLets take the expanding universe analogy a bit further Much of the universe lies beyond our current observational horizon In a similar way lots of enterprise data also lies in the dark  untouched and unanalyzed One motivation for Amazon building Redshift was to ease the path to analyzing this dark data And do it in a way that allows anybody to deploy a data warehouse solution without large setup costs or deep technical expertise Amazon offers a free 60 day trial for data analysts to get their feet wet with Redshift A time-to-first-report is around 15 minutes Thats enough time for analysts to quickly gain some initial business insights into their data and see if it is worthwhile before committing further resources Should they decide to go with Redshift theres no upfront expense The entry price point for the smallest data set is $025/node/hour Thats less than $1000/TB/year an order of magnitude cheaper than current warehouse solutionsWith cost no longer a factor in preventing the democratization of data warehousing lets probe what Redshift is how it works and what the future holdsAmazon Redshift is a relational database system built on PostgreSQL principles Its optimized for performing online analytical processing (OLAP) queries efficiently over petabytes of data The query handling efficiency is achieved through the combination of: Each of these merits its own post In what follows well provide the highlights of the designThe heart of each Redshift deployment is a cluster Each cluster has a leader node and one or more compute nodes all connected by high speed links There are two types of compute nodes each with different CPU RAM and storage characteristicsThe Dense Compute (DC) nodes are meant for speed of query execution with less storage and is best for high performance activities It is implemented with SSD drives The Dense Storage (DS) nodes are for storing and querying big data using typical hard disk drivesYou choose the node type depending on what you wish to achieve For instance you may want to store large amounts of data and run complex queries against it or increase speed of execution for near-real time analytics etc The number of nodes you select depends on the size of your data and the query handling performance you seek Theres a difference in pricing for these options of course And you can adjust these as your needs changeA leader node is the interface to your business intelligence application It offers standard PostgreSQL JDBC/ODBC interfaces for queries and responses It serves as the traffic cop directing queries from customer applications to the appropriate compute nodes and manages the results returned It also distributes ingested data into the compute nodes to build your databasesEach compute node has its storage divided into at least two slices with each slice performing a portion of the nodes workload in parallel A user-specified data distribution plan places ingested data across the various compute nodes and their slices Choose this plan with care so that nodes are not under-utilized and reduce data movement between nodes during query execution There is also an art to selecting the appropriate sort key to make query handling more efficient Well write more on best practices around these topics in future blog postsRedshift clusters run on the Amazon Elastic Compute Cloud (EC2) platform These clusters can either be on a space shared with other AWS users (the so-called classic platform) or cordoned off by your own Virtual Private Cloud (VPC) In either case you assign IP addresses to allow external applications to access you clusters Also you can decide if you want your external data sources to traverse the VPC to populate your clusters Finally you set up your clusters in whats called an availability zone which is an isolated location within an Amazon EC2 regionRedshift data is also replicated in the Amazon Simple Storage Service (S3) Customers can also choose to have another S3 backup in a different regionYou can ingest data into Redshift data from multiple sources including using Amazon S3 Amazon DynamoDB or Amazon EMR as staging entities Or it can come directly from any other data source such as an EC2 instance or a host that supports the Secure Shell (SSH) protocolRedshifts major advantage is that the other Amazon cloud services  S3 for backup EC2 for instances VPC for network isolation  and may other services in the AWS ecosystem  are built into the warehouse deployment and you dont pay for these separately or need to manage their setup Its all done under the hood for you through a simple configuration console when setting up your database and loading your clusters Thus you get the benefits of resilience load balancing monitoring logging and all other such goodness in one managed packageA key advantage of Redshift that we think a lot of people are not aware of is simplicity It used to take months if not quarters to get a data warehouse up and running And youd need the help of an Accenture or IBM None of that anymore You can spin up a Redshift cluster in less than 15 minutes and build a whole business intelligence stack in a weekendA report from June 2017 by Forrester mentions that Redshift has over 5000 deployments with plenty of success stories Thats quite a remarkable adoption considering that Redshift has only been available since November 2012The size of the data warehouses range from a few terabytes to petabytes Looking at its success stories there are four basic types of uses cases for RedshiftDB-Engines provides a ranking for different databases Benchmarking Redshift vs legacy vendor Teradata it becomes clear how much traction has seen in just 5 years of its existenceThe latest addition to Amazons data warehouse toolkit is Redshift Spectrum It is a pay-as-you-use way to run queries against data youve stored in your S3 data lake It uses the same business intelligence applications as well as the same queries that you already use with RedshiftSpectrum is useful when analyzing older or legacy data kept in S3 (incurring only normal S3 storage charges) while newer data is in Redshift Redshift mediates the query towards data in S3 taking care of the hard stuff such as normalizing the data formats It will be a bit slower but older data is not dark any longer (Some enterprises may well have exabytes of such dark data) You only pay $5 per terabyte of data pulled from S3 during the query processingTheres more to Redshift Spectrum of course and we will write more about it in future postsFor now its good to keep the following points about Amazon Redshift in mind: AWS Re:Invent 2017 is coming up and we can expect more announcements on features and use cases that should further drive up adoption
Snif6BFT3yDiicyuVgXvYv,The Amazon Redshift architecture allows to scale up by adding more nodes to a cluster That can cause over-provisioning of nodes to address peak query volume Unlike adding nodes Concurrency Scaling adds more query processing power on an as-needed basisThe typical Concurrency Scaling for Amazon Redshift gives Redshift clusters additional capacity to handle bursts in query load It works by off-loading queries to new parallel clusters in the background Queries are routed based on WLM configuration and rulesPricing for Concurrency Scaling is based on a credit model with a free usage tier and beyond that a charge based on the time that a Concurrency Scaling cluster runs queriesWe tested Concurrency Scaling for one of our internal clusters In this post we present the results of our usage of the feature and information on how to get startedTo be eligible for concurrency scaling an Amazon Redshift cluster must meet the following three requirements: Concurrency scaling does not work on all query types For the first release it handles read-only queries that meet three conditions: For routing to a Concurrency Scaling cluster a query needs to encounter queueing Also queries eligible for SQA (Short Query Acceleration) queue will not run on the Concurrency Scaling clustersQueuing and SQA are a function of a proper set-up of Redshifts workload management (WLM) We recommend first optimizing your WLM because it will reduce the need for Concurrency Scaling And that matters because Concurrency Scaling comes free up to a certain amount of hours In fact AWS claims that Concurrency Scaling will be free for 97% of customers which takes us to pricingSee your data in intermixFor Concurrency Scaling AWS has introduced a credit model Each active Amazon Redshift cluster earns credits on an hourly basis up to one hour of free Concurrency Scaling credits per dayYou only pay when your use of the concurrency scaling clusters exceeds the amount of credits that youve incurredThe fee equals the per-second on-demand rate for a transient cluster used in excess of the free credits  only when its serving your queries  with a one-minute minimum charge each time a Concurrency Scaling cluster is activated Calculating the per-second on-demand rate is based on general Amazon Redshift pricing ie by node type and number of nodes in your clusterConcurrency scaling is enabled on a per-WLM queue basis Go to the AWS Redshift Console and click on Workload Management from the left-side navigation menu Select your clusters WLM parameter group from the subsequent pull-down menuYou should see a new column called Concurrency Scaling Mode next to each queue The default is off Click Edit and youll be able to modify the settings for each queueConcurrency scaling works by routing eligible queries to new dedicated clusters The new clusters have the same size (node type and number) as the main clusterThe number of clusters used for concurrency scaling defaults to one (1) with the option to configure up to ten (10) total clustersThe total number of clusters that should be used for concurrency scaling can be set by the parameter max_concurrency_scaling_clusters  Increasing the value of this parameter provisions additional standby clustersThere are a few additional charts in the AWS Redshift console There is a chart called Max Configured Concurrency Scaling Clusters which plots the value of max_concurrency_scaling_clusters over timeThe number of Active Scaling clusters is also shown in the UI under Concurrency Scaling Activity : The tab in the UI also has a column to show if the query ran on the Main cluster or on the Concurrency Scaling cluster: Whether a particular query ran on the main cluster or via a Concurrency Scaling cluster is stored in stl_queryconcurrency_scaling_status A value of 1 means the query ran on a Concurrency Scaling cluster and other values mean it ran on the main clusterSee your data in intermixConcurrency Scaling info is also stored in some other tables/views eg SVCS_CONCURRENCY_SCALING_USAGE  Theres a list of catalog tables that store concurrency scaling informationNotice: JavaScript is required for this contentWe enabled Concurrency Scaling for a single queue on an internal cluster at approximately 2019 03 29 18:30:00 GMT We changed the max_concurrency_scaling_clusters parameter to 3 at approximately 2019 03 29 20:30:00 To simulate query queuing we lowered the # of slots for the queue from 15 slots to 5 slotsBelow is a chart from the intermixio dashboard showing the running versus queuing queries for this queue after cranking down the number of slotsWe observe that the queueing time for queries went up maxing out at about > 5 minutesHeres the corresponding summary in the AWS console of what happened during that time: Redshift spun up three (3) concurrency scaling clusters as requested It appears that these clusters were not fully utilized even though our cluster had many queries that were queuingThe usage chart correlates closely with the scaling activity chart: After a few hours we checked and it looked like 6 queries ran with concurrency scaling We also spot-checked two queries against the UI We havent checked how this value may be used if multiple concurrency clusters are activeConcurrency Scaling may mitigate queue times during bursts in queriesFrom this basic test it appears that a portion of our query load improved as a result However simply enabling Concurrency Scaling didnt fix all of our concurrency problems The limited impact is likely due to the limitations on the types of queries that can use Concurrency Scaling For example we have a lot of tables with interleaved sort keys and much of our workload is writesWhile concurrency scaling doesnt appear to be a silver bullet solution for WLM tuning in all cases using the feature is easy and transparentWe do recommend enabling the feature on your WLM queues Start with a single concurrency cluster and monitor the peak load via the console to determine whether the new clusters are being fully utilizedAs AWS adds support for additional query/table types Concurrency Scaling should become more and more effectiveOriginally published at https://wwwintermixio on April 3 2019
gXj9SF3fXH6Q3UgLiTpMAg,Properly managing storage utilization is critical to performance and optimizing the cost of your Amazon Redshift cluster Weve talked before about how important it is to keep an eye on your disk-based queries and in this post well discuss in more detail the ways in which Amazon Redshift uses the disk when executing queries and what this means for query performanceAmazon Redshift uses storage in two ways during query execution: Use excessive storage impacts your cluster because: The worst case is (3) when the cluster fills up It can happen for the all nodes in a cluster at once or start with just one node and then propagate through the entire cluster as the image below showsSo lets look into what we can do to fix and prevent this type of situationIf youre not already familiar with how Redshift allocates memory for queries you should first read through our article on configuring your WLMThe gist is that Redshift allows you to set the amount of memory that every query should have available when it runs This value is defined by allocating a percentage of memory to each WLM queue which is then split evenly among the number of concurrency slots you define When a query executes it is allocated the resulting amount of memory regardless of whether it needs more (or less) Hence allocating too much memory is wasteful (since each node in the cluster obviously has finite memory) whereas allocating too little memory can cause queries to spill to diskQueries which overflow their allocated WLM memory are disk-based These queries usually suffer from significantly degraded performance since disk I/O is orders of magnitude slower than memory I/OThere are six types of internal operations that Redshift can spill to disk when executing a query: If any of these operations are processing more rows (ie more bytes) than will fit into allocated memory Redshift has to start swapping data out to disk resulting in a significant slowdown of the queryAWS recommends that you keep the percentage of disk-based queries to under 10% On our own fleet of clusters were usually running well under one percent: Within the intermixio dashboard viewing Recommendations for an individual query will surface the exact touched tables and how to update them as well as how much memory the query used and the amount of memory capacity in the WLM queueRedshift also uses the disks in each node for another type of temporary query data called Intermediate Storage which is conceptually unrelated to the temporary storage used when disk-based queries spill over their memory allocationIntermediate Storage is used when Redshift saves the results of an intermediate operation to disk to use as input for a future operation Intermediate Storage can become important if your query stores a large amount of data between query operations since that storage may cause your cluster to run out of disk space It also introduces additional I/O which can lead to slower execution timesSince intermediate storage is used to carry results from one part of the query execution to another the best way to reduce intermediate storage is to use predicates (eg WHERE clauses JOIN … ON clauses etc) on intermediate steps of your query (subqueries CTEs etc) to ensure that you are not carrying unnecessary data through your query processingFor example consider this query which joins on the results of two CTEs: This query could be re-written as follows to limit the amount of data brought forth into the JOINWe have a great new feature called Query Recommendations that proactively lets you know if your query is using a significant amount of intermediate storageThis value is important when diagnosing spikes in cluster storage utilizationFor example lets assume you see your cluster storage spiking over some time period but dont see a corresponding increase in data transferred (via the COPY command) into your cluster You could search for all queries which have a large Memory to Disk value to identify which queries contributed to your cluster running out of disk spaceHeres a real-world example The following chart shows the actual disk space used in a cluster over a 2 week period broken down by schema This particular chart show consistent storage utilization over time with small variationThe chart of % disk utilization tells a different story On the same cluster over the same period the disk utilization hits 100% quite frequently This is caused by some queries using an extraordinary amount of intermediate storageOne of the cool features we recently released Cluster Recommendations will surface queries with high disk utilization immediatelyMonitoring both Disk-based Queries and Intermediate Storage is crucial to keeping your cluster healthy Keeping on top of this temporary disk utilization prevents your Amazon Redshift disks from filling up due to misbehaved queries resulting in queries being killed and your users being interruptedOriginally published at https://wwwintermixio on August 13 2019
2QwkVCqspsgBmyNTi2My3R,Amazon Redshifts pricing can be a bit confusing since you can choose to pay hourly based on your node usage (on demand) by the number of bytes scanned (spectrum) by the time spent over your free daily credits (concurrency scaling) or by committing to an annual plan (reserved instance)  And as you scale you may accidentally increase the cost of your Redshift account by creating inefficient processes So if you find yourself with ever-increasing Redshift costs try these tips to improve your Redshift performance AND decrease your costs: Amazon Redshift makes it easy to scale Need more computing power or disk space? Simply add a few nodes with a click or two in the AWS Console and after a couple of hours of Read-Only mode your data warehouse will be ready to crunch your valuable and growing dataset faster than everThis ability to scale with minimal effort is also great when it comes to overcoming expected (or unexpected) short-lived spikes in cluster load Need to power through that re-processing of a particularly large dataset? Add a few nodes Need more disk space so you can do a deep copy of your largest table without running out of disk space? Clickity-click doneBut while its just as easy to scale a cluster down as it is to scale a cluster up too often the decision to remove nodes is a harder one to make since it can result in an overloaded cluster Who needs frantic support requests when queries slow to a crawl or your cluster runs out of disk space? Its too easy to throw money at the problem and take the attitude dont fix what aint brokeAnd to make things more time-consuming changing your cluster size requires re-tuning (or at least reviewing) your WLM configuration since the total memory and concurrency available for your queries will changeAs a result many Redshift customers run with over-provisioned clusters simply because they worry that removing nodes will cause their cluster to go down in flames resulting in late hours making data pipeline changes or waiting for the cluster to scale back up Even with affordable Redshift pricing an over-provisioned cluster can get expensiveIn this post well describe how we reduced the spend on one of our own Redshift cluster by 28% Without sacrificing query performance We did so by applying some of the approaches weve shared in our top 14 performance tuning techniques for Amazon RedshiftBefore we look at how to evaluate whether nodes can be removed from your cluster we should first look at how changing the number of nodes in a cluster affects its performance Intuitively we know that adding nodes should do something like increase cluster performanceIn the context of your queries adding nodes to a Redshift cluster does multiple things: See your data in intermixSo how do you optimize your Redshift spend (and maybe decrease Redshift cost) while reducing the risk of bringing your pipeline to a screeching halt by accidentally under-powering your cluster? Measure measure measure (Ok thats measuring three times but measurement really is that important)Since we now understand how adding a node impacts cluster performance we can look at the specific metrics you should measure when considering whether you can remove nodes from your Redshift cluster These are: In the following section well walk through an example analysis of these metrics for one of our own Redshift clusters to see if we can remove some nodes to save moneyThe cluster were going to analyze has seven dc2large nodes and we want to see if we can remove some nodes from itOur first step is to see if we need the nodes simply to store our data Since Amazon Redshifts disk memory and CPU all scale together (in units of nodes) we cant remove a node if we need that node just for data storage However in that case we should consider other solutions to reduce disk utilization so that we can remove a node For example changing compression encodings or pruning our tables  for more detail see our top 14 performance tuning techniquesWe start by looking at the historical disk utilization of the cluster You can view this in the AWS Redshift dashboard under PercentageDiskSpaceUsed or in your Intermix dashboard under Storage Analysis In this case were looking at the last day and a half of disk utilization which we know represents the current typical workload for this cluster (you may want to look at a longer period of time depending on what you consider a representative time period is): We can see that our cluster has used between 30% and 58% of its disk space during this period Since our cluster has seven dc2large nodes the total cluster storage is 112 Tb (160 GB * 7) So were using between 336 Gb (30%) and 650 Gb (58%) of storage at any given timeThis means that if we want to keep our disk utilization no higher than 70 80% (a good rule of thumb to leave headroom for query execution) we can potentially reduce our cluster size from seven to five resulting in a total storage of 5 * 160 Gb = 800 Gb Our peak disk utilization of 650 Gb will be approximately 81% which is acceptable since the peak is short-livedNote that these calculations are all approximate since your cluster may use a different amount of disk for disk-based queries after the resize so it is important to look at the characteristics of your disk-based queries too (see below) However it gives a good starting point for estimating the minimum number of nodes for your clusterSo now that we know we may only need five nodes to support our typical data storage needs for the cluster lets move on to our WLM concurrencyThe next step is to look at the query concurrency in your cluster to see if the cluster is operating without significant queue wait time ie that queries arent sitting around waiting for cluster resources to become available In your Intermixio dashboard you can find this under Throughput Analysis: This example shows that our four WLM queues are operating with very little queue wait time (a peak queue wait time of around 015 sec which is well below what is acceptable for the workload on this cluster) So this means that as long as we can keep sufficient memory in each queue after the resize we should be okSee your data in intermixNext we evaluate whether the cluster has a large percentage of disk-based queries To do that we look at the Memory Analysis page for our cluster in the Intermix dashboard: The left charts show the total amount of memory consumed by the queries running in each WLM queue at any given time and the right charts show the number of disk-based queries By looking at the breakdown of memory usage per queue we can make the following observations: The final metric to look at is the COPY and UNLOAD times since reducing nodes will reduce the number of slices available for parallel data transfers into and out of the cluster What we want to look for here is that our COPYs and UNLOADs are happening quickly enough such that reducing the available parallelism by ~30% isnt likely to have an impact In our case we look at all COPYs and UNLOADs for the cluster and compare the average time to our internal requirement for the workflows running on this cluster For this cluster we need COPYs and UNLOADs to finish in less than 5 minutes on average: Since COPYs and UNLOADs are both finishing in around 6 seconds removing 2 of the 7 nodes shouldnt cause a performance problem for transfersAfter studying the metrics described above we concluded that we can remove two nodes from our cluster without impacting our workloads And lo and behold we did the resize retuned our WLMs and re-reviewed our Storage Throughput and Memory metrics to verify that all is well-and all wasWhen it comes to making infrastructure changes that could impact mission-critical data pipelines the more information you have up-front the more secure youll feel And the more secure you feel the quicker you can iterateOriginally published at https://wwwintermixio on September 18 2018
Eiy5egNA3YKX5n5YvxyuhH,Data is valuable resource powering up analytics predictive models and decision making For a company to make data-driven decisions it first must go through building its data infrastructure And a data warehouse plays a central role in such an infrastructureData warehouses are data storage and processing systems that aggregate data from different sources into a single place With data in one place you can combine and query it across the different sources That includes data coming from users interacting through web and mobile background system logs or third party dataAmazon Redshift is a cloud warehouse solution by Amazon Web Services (AWS) Since AWS first introduced Redshift in 2012 it got everyones attention for its amazing performance and low price point In the following years there were massive improvements from operational and data pipeline perspective Today its the market leader for cloud warehousesSo how does Redshift work and whats been driving its adoption? There are two basic components you need to understand about Amazon Redshift: 2 The economics again a combination of two things: The technology is nothing new Other warehouses use it and there are even open-source data warehouses that are free to use Its the combination of the two and the simplicity that Redshift offers to start with a data warehouseIn this post Ill explain these two components Before that its helpful to understand basic nomenclature and key concepts Well start with the key conceptsIf youve ever googled Redshift you must have read the following Amazon Redshift is a fully managed petabyte-scale data warehouse service in the cloudLets break down what this means and explain a few other key concepts that are helpful for context on how Redshift operatesFully managed AWS takes care of things like warehouse setup operation and redundancy as well as scaling and security All this is automated in the background so the client has a smooth experienceWarehouse service Redshift is a cloud service the customer does not own the physical hardware of the warehouse but can use it through a subscription as a service This has dramatic impact on the procurement model for customersGetting started with Redshift is easy Rather than buying and installing hardware they can spin up a Redshift warehouse upload data and run queries in less than 15 minutes And since AWS manages a fleet of tens of thousands of Redshift clusters customers benefit from automating capabilities that would not make economic sense for any individual on-premise DBAPetabyte There are fifteen zeros in a petabyte 1000x bigger than a terabyte To illustrate this is equivalent to 133 years of HD video Compare that with traditional on-premise data warehouses that operate in the terabyte range Going to petabytes has dramatic impact on the analytical capabilitiesMany companies today already generate a terabyte of data per day In the old on-premise world storing and analyzing that much data would have been cost-prohibitive leading to the famous analysis gap or dark data They may collect data but dont analyze it Redshift solves that problemCluster A cluster is a collection of nodes which perform the actual storing and processing of data Each cluster runs an Amazon Redshift engine distributed and replicated across all nodesNodes AWS offers four different node types for RedshiftDatabases A cluster can have one or more databases A database contains one or more named schemas which in turn contain a set of tables and other objectsWorkloads Youll often hear the term workloads which implies running queries on your Redshift cluster There are three generic types of workloadsUnderstanding and distinguishing the three generic workloads from each other is an important concept for any data warehouse Different Workloads use different amounts of cluster resources and it makes total sense to be able to keep disparate workloads separate to avoid resource contention Redshift manages them with the Workload Manager We explain workloads in detail in our post  4 Simple Steps to Workload Management With the key concept out of the way we can dive into the two components technology and pricing Well see each one of them factors in but its the combination that makes the magic happen and allows Redshift to reach state-of-the-art performance at a low priceOne of the key concepts behind Redshifts exceptional performance is its columnar data storageColumnar storage is a type of Relational Database Management System optimized for heavy-reading tasks Most of the queries you run on your warehouse are scans powering up analytics (?) dashboards or machine learning modelsA common term for these types of queries is OLAP  OnLine Analytical Processing As the name suggests OLAP databases are good for analytics OLAP queries tend to be complex and process a lot of data touching many different tables In fact Redshift is an OLAP database engineCompare this with row-oriented systems which are a better option for write-heavy tasks Its a different data modeling approach used for business applications which process a companys transactions Youll hear the term production database or online transaction processing (OLTP) Most OLTP databases tend to have large amounts of small queries and a high percent of write activity In general OLTP systems load small amounts of data with high frequencyFor a simple example lets take a website like Glassdoor with information on companies their salary levels and reviews by their employeesYou create an account on Glassdoor In your profile you submit the company you work for and your salary That transaction creates a record in Glassdoors production database That same production database also holds records for all other user profilesLets say Glassdoor creates a new marketing report with the average salary across different companies They may even break that down by level or geography and show a historical trend For that information they need to create a query that selects filters and aggregates the records for a corresponding company Its a much larger query compared to the initial write query for each individual user in Glassdoors systemComparing row-oriented vsNow lets see how that query plays out in row-based vs a columnar-oriented format The following table illustrates the difference between the two approachesYou can see that in the column-oriented the data is flipped around Each row now represents a columnAnalytical queries execute fast on tables in columnar format Lets take for example a query give me the average salary per company This query only touches the salary and company columns groups by company and then calculates the averagesCompare that to running the same query on a row-oriented database The query needs to sift through the entire production database To return a result it has to read every single row It runs much longer and is inefficient because its scanning the entire databaseTheres an additional catch here If a query scans less rows it consumes less data As a result the data fits into the RAM and can process in-memory Memory / RAM is much faster than disk Even more since the data is stored column-wise Redshift applies different data encodings per text and numerical columns thus more efficiently compressing the dataColumnar format is nothing new On-premise warehouses like Oracle Teradata and Netezza use it as well The difference here is that they require hardware installation and maintenance It can take weeks and months to get running Modern cloud warehouses like Redshift deliver the database as a service with a cluster up and running in less than 15 minutesRedshift also pairs columnar storage with scaling out the number of nodes which get us to MPP  massively parallel processingMPP is the process of coordinated simultaneous computation of data across multiple nodes in the cluster Each node is using its own operating system and memory also known as a loosely coupled systemAn alternative is tightly coupled or Symmetric Parallel Systems (SMP) Separate processors use a single operating system and memory For warehouse solutions MPP has shown to deliver better resultsThe Redshift architecture uses MPP and consists of a leader node and compute nodes The leader node distributes rows of a table across the different nodes which independently store and process data and queries How you distribute data across the nodes depends on your distribution keyThe leader node coordinates the execution of a query across the different nodes This applies for all types of workloads Once done the leader node combines the results from each node to return the final result of the queryThis process has many advantages By adding nodes you add more storage memory and CPU With each node a clusters processing capacity goes up and scales in a linear fashion That means that a 10-node cluster processes the same query about twice as fast as a 5-node clusterThis concept applies to all three types of generic workloads (loads transforms and ad-hoc queries) For example loading flat files into Redshift is a very efficient process and also takes advantage of parallel processing The leader node spreads the workload across the nodes while reading from multiple files Loading also scales linearly as you add more nodes to your clusterBecause of MPPs simultaneous processing as you add more nodes to your cluster you load data faster execute transform faster and return the results of ad-hoc queries faster And Redshift takes advantage of the elasticity of the cloud Adding nodes is a simple process and takes a few clicks in the AWS consoleWe describe Redshifts linear scalability in our use case story  Intuits Journey to Cloud Analytics The key to linear scaling the correct configuration of your cluster Just like with any other database if you dont configure it right you wont enjoy the performance benefits You can read more in our post  How we configure Amazon Redshift for Performance With traditional MPP databases storage and compute are coupled together That means if you add more nodes you add storage and compute at the same rate It also means youre allocating resources for peak-consumption Thats inefficient as you end up in a situation where you either have too much compute or too much storageTheres a solution to that by using a data lake architectureLets look at how pricing fits into the equation Were starting off with data lakes and Redshift Spectrum Theres a technology component here as well but we can more about the economics and the cost of running queriesCompanies keep their most recent and hot data in a warehouse like Redshift Its the data thats closet to the current businessWhat happens with the rest of it like historical or unused data eg columns and tables that nobody queries? You dont want to delete that data but also not pay the premium for keeping it in your clusterThe answer is to store in your data lake Data Lakes area cheap long-term storage for structured and unstructured data In AWS that product is Amazon S3 The cost for storing data in S3 is about one-tenth of storing it in a Redshift clusterStoring data in S3 implies frequent communication between S3 and Redshift if you still want to query it One approach is to build up an Extract-Transform-Load (ETL) pipeline and load the data into the cluster and then unload it again once analysis is complete Thats complex and not very efficientThe other approach is to query data in S3 and join it with data in the cluster via a feature called  Redshift Spectrum  Spectrum is a serverless query engine Serverless means there isnt any infrastructure to set up or managePoint to your data in S3 define the schema and you can start querying using standard SQL queries Redshift Spectrum runs queries directly on S3 as if they were normal Redshift tables By using S3 and Redshift Spectrum youre separating storage from compute for your cluster To store more data and process theres no need to add more nodes Instead you store in S3 and use Redshift Spectrum to join and query itThis is a huge benefit as you store your data at a lower cost but are still able to query in your data warehouse Separating compute and storage enables customers to scale resources on an as-needed basis rather than pre-allocating resources for peak consumptionRedshift Spectrum queries do run slower but the cost trade-off is worth it increasing the ROI on your data investmentAnd that takes us to pricingAmazon Redshift is priced by node type and the number of nodes youre running There are two categories of nodes in parentheses the price per hour for on-demandAWS offers two billing models for Redshift On-demand or reserved instances Users can choose their desired model depending on their annual usage plans The more upfront you can plan your usage the more you saveYour lowest starting price with Redshift is $1380 per year for one node of dc2 with a 1-year commitment Looking at it from a storage perspective the lowest price comes down to $93420/TB/Year with a 3-year commitment for any Dense Storage clusterIf you want a quick rule of thumb for a 3-year commitment: 4 5 TB is sufficient for most medium-sized companies enterprises may go up to about 80 100TB Very large enterprises that collect massive amounts of data every day go indeed to a Petabyte A good example is the NASDAQ which stores trading data in Redshift But you can get started with as little as a few 100s of GBs and not pay through the noseOverall  these price point are much lower by a factor of about 20x than for on-premise warehouses which makes it so attractive for adoptionThis post introduced Amazon Redshift to help you get started We covered the basic concepts behind the cluster and described its most important featuresYou could summarize it that Redshift is simple fast and cheap Its no wonder that it has found broad adoption in the SMB market and the enterprise alikeWe at intermixio assist organizations to overcome your analytics problems by giving you a unified view to track your data sources queries and user activity for your cluster along with a historical timelineUnlike simple monitoring tools we give you query optimization recommendations and a place to collaborate to improve cluster and query performanceIf youre suffering from slow queries dashboards or ETL pipelines schedule a demo with us or start a free trial Coupled with our Expert Services for Redshift youll spend less time-fighting fires and more time buildingOriginally published at https://wwwintermixio
EiA8xG84SyYN84etgMztMY,At intermixio Elasticsearch is a final destination for data that is processed through our data pipeline Data gets loaded from Amazon Redshift into Elasticsearch via an indexing job Elasticsearch data then gets served to the intermixio dashboard to data engineers giving them a view of the performance and health of their own data pipelinesThe challenge of using Elasticsearch as a serving layer is that migrating data can be painful and requires downtime There are many things that require a data migration: upgrades mapping changes etc The recommended way of handling this is to use snapshot / restores but this is problematic for a real-time application for a number of reasonsIn this post we detail a method to do Elasticsearch migrations with zero downtime We will share the reasons why we need to do migrations and the exact procedures weve used to execute successful Elasticsearch migrations in our environmentThe intermixio data pipeline is quite complex but for this discussion we will focus on the relevant pieces regarding Elasticsearch At a very high-level the pipeline works as follows: Elasticsearch indexing jobs do CDC (change data capture) and only append documents with newer timestamps since the last indexing run Existing documents are never touched and the indexing jobs are idempotent: on each run they query Elasticsearch for the most recent document and use that value to query newer rows from RedshiftThe indexing jobs also use a JSON mapping file which defines the indices fields field types and Redshift source columns that should be indexed This JSON mapping file changes with new releases of our application as we add/remove fields or change data typesOne of the great things about Elasticsearch is how easy it is to get going You can spin up a new cluster and immediately start pushing documents into it Unlike RDBMSs theres no need to think through and define your data schema or mapping up front By default Elasticsearch will automatically try to determine the data type of new fields as you push them in and will set the data type accordinglyThis is great since it reduces the time it takes to get started writing your application code But as with any NoSQL datastore not enforcing a process to manage changes to a data schema can lead to headaches down the road Fortunately Elasticsearch provides a setting called dynamic which controls this default anything goes behavior By setting dynamic to strict we can instruct Elasticsearch to reject any documents which have unknown fields in them This setting can be defined globally in the mapping for your index so that it applies to all documents in the index or can be used within an inner object to override the setting for that object onlySo while you can avoid defining and enforcing your schema up front its generally not a good idea And in cases where you need some dynamic fields Elasticsearch provides a great dynamic templating featureSee your data in intermixSo now that you have your mapping clearly defined and enforced with dynamic=strict you may discover that with Elasticsearchs performance and flexibility come with a few constraints on how data mappings can be modified: To handle both of these cases you need to effectively reindex your data-either by using Elasticsearchs _reindex or _update APIs or by running a job to manually reindex and update documentsAs we started thinking about how to manage Elasticsearch data schema changes we identified the following challenges: Consider the following scenario we faced recently: We wanted to upgrade our Elasticsearch clusters from 53 to 62 copy over all data to the new cluster replace the deprecated _type mapping type field with a new doc_type field in all documents remove several unused fields and then run the clusters in parallel in our pipeline while we compared and validated the new cluster against the old one and tested our application Finally after validation we wanted to hot-swap the new cluster for the oldWith almost 15 billion documents (5 TB) spread across 20 nodes in four environments we needed a system that would allow us to make these changes in a controlled and systematic way allowing us to fully validate the result before making any changes to our production systemTo accomplish our goals we developed a flexible system for making changes to our our Elasticsearch clusters The system adds an extra flow to our data pipeline which we use during migrations: To update our Elasticsearch data schema or upgrade our Elasticsearch clusters we do the following: Steps 1 2 and 4 are managed via a single python script and step 5 is managed via our Elasticsearch cluster comparison scriptDuring migrations we launch a new Target Elasticsearch cluster which is indexed using a new Target index job This Target cluster will eventually replace the original Elasticsearch cluster The Target index job may be running a new version of our indexing code with an updated JSON schema definition for the Target cluster (eg if we are changing the logical structure of the Elasticsearch documents need to make other changes to be compatible with a new version of Elasticsearch etc) and it runs in parallel with the legacy indexing jobSince our index job loads all Redshift rows newer than the most recent document already in the Elasticsearch cluster we first need to copy a single anchor document from each source cluster index into the Target cluster before starting our Target index job Once this is done we can fire up the Target index job Well start seeing all new data indexed into the Target cluster immediately and at this point can start any other software development further up our stack knowing that fresh data is available in the new cluster for developer testingNote we use an anchor document rather than allowing the Target indexing job to start loading all Redshift data for two reasons: 1 We prune data from Redshift so only recent data is available for indexing; and 2 for development and testing purposes it is often more useful for us to have recent data available quickly so that we can start validation and development without having to wait for all historical data to be indexed This is one of the reasons we currently dont use the _reindex Elasticsearch API-if we did we would need to wait for the entire Cluster to be reindexed before we started our Target indexing job since the reindexing order isnt well definedThe final step of the migration is to backfill data from the original Elasticsearch clusters into the Target cluster This is done with a simple idempotent backfill job that copies documents from the source to the Target cluster working backwards in time from the oldest document in each Target cluster The backfill job is also able to apply an optional python function to mutate each document as it copies them (much like the script that Elasticsearchs _reindex API supports)For example we might apply the following function to all documents as theyre copied: This function is called on each source document and the returned mutated document is loaded into the Target cluster Note this migration is also applied when copying the first anchor documentData validation plays a central role in any data pipeline architecture planning we do Without proper validation tools and processes in place development proceeds at a much slower pace since every change requires ad hoc testingFor validating our Elasticsearch data we developed a script which compares documents between two Elasticsearch clusters The script detects missing documents and compares each document on a field-by-field basis It is also able to skip fields we expect to be different and is able to apply a migration function when comparing documents to take into account the differences between the source and Target document structureOnce we have backfilled our new Target cluster validated the data and tested our application against the new cluster we can put it live by simply hot-swapping it out for the old cluster We do this by: Originally published at https://wwwintermixio on July 12 2018
aExAojpBhRjyHMbdcrjTjD,We are glad to announce an online Cloud and Data Engineering Bootcamp for Students across the globe who are graduating out in Fall 2019One can access all the original posts from herePlease fill the form if you want to sign up for this program as student or mentor or hiring managerInstead of having same curriculum it will be customized as we understand the students while running the Bootcamp But at higher level these are the topics that will be coveredThere will be exercises and high quality as well as high volume Proof Of ConceptsHere are the details with respect to Training MethodologyYou can get your questions clarified via Slack You can join the Slack Channel in 2 easy steps to get your questions clarified about the bootcampEven though we do not promise any placements we are planning to engage some of the hiring managers who might hire few of the resources Also I will invite some of my past students who can refer the folks of this bootcamp in their respective companiesClick here to fill the form We will follow up with you shortly You can also join the WhatsApp group to get your questions clarified It is available as part of the formYou can see all the articles on this topic from here
jWaonXEagsQZT5EhzQiUC3,We are glad to announce a live YouTube Series on Linux Fundamentals which every Software Professional should be aware of Before getting into details we are not experts in Linux and we are only trying to share our experience of using handful of Linux commands and ability to write simple shell scripts for most common use casesMain Course link: https://kaizenitversityJoin free live sessions group to get notifications or watch archives by clicking on below linksAt the end of the course your productivity or learning ability will improve for sureWe will understand basic concepts most common commands basic shell scripting automating simple use cases virtualization and more
g2tc4LNkr95MsjW6AzCZo8,Here is the plan to become Data Engineer using Big Data eco system and to fill the gap in the industry we are planning for Data Engineering Immersion bootcampPlease click here for details fill the survey and follow us if you want to know more details about this 150+ hour course: Please add any more information as part of the comments
PGMSwGxa7vKPXqcNfi6yQ9,Yes we can finally sleep…This describes our journey with PostgreSQL at JULO and how thankful we are to have migrated to PostgreSQL 10 It has saved us from many sleepless nightsAs one the most used RDBMS out there PostgreSQL is very popular PostgreSQL announced their support in Native Partitioning since version 10 We liked this functionality but held off on migration for a long time in favor of stability Last year when version 10 became more stable and mature we finally decided to give to move from 95 to 106 because of the many benefits it offeredThe story began in 2018 when we had a scheduler to delete data in a table that is already more than 1 year old The scheduler executed a DELETE statement on a log table where age > 1 year old Nothing was wrong with this until our customer base started increasing from 500 a day to around 4000 a day BOOM! the size of table and the number of rows is now 8 times bigger The DELETE statement which initially ran within 20 30 minutes now exploded and took at least 4 hours to execute Not to mention we had to run VACUUM to reclaim the size of disk that was left by dead rows in PostgreSQLThe downside of doing a DELETE statement here is that it can also partially lock the transaction that happened at that time So we started searching for a better way to solve this problem because we needed to keep a close eye whenever the scheduler was about to run Of course we set the scheduler to execute in the early hours of dawn where the number of incoming users is low: AKA sleeplessInspiration always comes suddenly so after we moved to PostgreSQL 10 we started to do research on how to implement this The basic concept is that you will have a parent table and children tables that will inherit from the parent The children tables will each store values only with the condition that we have declaredSay for example that you want to create a sales table like this: Now lets say you want to group the sales per month because you want to query it faster per month and you want to delete data more than 6 months All you need to do is to just add a little bit of declarationAfter the table gets populated you can now see that the data is separated by month and you can create a scheduler to automatically DROP the table that is more than 6 months old When you DROP the table you automatically reclaim the size on the disk without having to VACUUM itWe learned the following three lessons: With these lessons learned we made a script to automatically generate the SQL statement since you need to create it manually And its not nice if you need to create 100 child tables from the parentSo heres a script that maybe can help you to automatically create these tedious tasks for you The script is only generating the RANGE partition because this is what we use You can index as many columns as you want in the tablealthough I dont really recommend that You can just change whatever you need to change in the script The output is a sql file that you can execute into the PostgreSQLThe result has been very good for us
h2ocdp8HiNrWY2x5Q9TzCe,The mission of the Data Pipelines team at JW Player is to collect process and surface data from the worlds largest network independent video platform Because our customers span a wide range of industries and use cases the tools we develop have to be scalable flexible and reliable for both our internal and external customersBatch processing is an integral part of our data processing pipelines The other half of our pipeline is via real time processing with Apache Flink As our scale continues to grow so does the need for a flexible stable and scalable batch processing platform The focus of this blogpost is how we built a platform on top of Apache Airflow called Alpaca Alpaca has been a huge improvement to how we process batch data at JW Player allowing us to create new datasets and batch ETL jobs faster than ever beforeOur data is mostly an event stream of Pings A single Ping represents an event that happened on a specific JW Player This event can range from playing a video clicking on an ad or if that video was recommended Since we have tens of thousands of instances of JW Player across the open web and millions of viewers we see around 3 billion of these Pings per dayUsage-Mini is the name of our previous batch processing platform Its a custom application built off Luigi Using Luigi meant that a lot of the difficult work of operating a data pipeline scheduling and state management had to be developed and maintained by us: While Luigi was great for our current needs at the time we began to quickly outgrow its usefulness A big pain point we had with Usage-Mini was the speed at which we could create or modify datasets Simple asks like adding columns to a table could take over a week to do on our largest tablesThe other pain point was not having an easy way to manage the state of jobs We didnt have an easy way to reprocess backfill or scale up jobs other than manually intervening in the backend databaseRefactoring our Luigi based system wouldnt fix the fundamental problems that we had with it Over the past summer our Data Science team started adopting Airflow for their data processing needs and we decided to do a proof of concept ourselves This is what we love about Airflow: Usage-Mini our Luigi based orchestrator is an application Alpaca is a platform built on top of Airflow We provide a shared set of tooling and utilities that allows our developers to quickly create new datasets from modular componentsAlpaca is: We want Alpaca to be as flexible and composable as possible To accomplish this we provide a range of base operator images These images allow us to run Airflow tasks that create pyspark-emr jobs run database queries and execute Python scriptsOn top of these images developers specify what their task does via a flavor A flavor can be thought of as an implementation of a specific base image In most cases flavors are completely config driven We provide all of the code in the base image and then let the config specify the use caseWe see this as a best of both worlds approach Configuration files are great for specifying job related parameters such as which database to run a query on Configs are also easy to develop for the end user and we can enforce the config schema with tools like Yamale We also dont want end users to have to touch the internal operator code However using configuration as code doesnt work well when we want to define DAGs and complex logic We offload the code to the base image and DAGs definition and use configs for the restWe have a unified platform called the Deployment System for managing Kubernetes deployments Additionally we have an API called Gantry which is used to launch and manage these deployments In order to launch Airflow tasks on the Deployment System our Data Science team created the GantryJobOperator Built off Airflows BaseOperator it interfaces with the Gantry API allowing Alpaca to launch Airflow Tasks as Kubernetes Jobs All the operator needs is a reference to the DockerHub image for the operator along with configuration optionsA single operator with a uniform interface means that users have to learn a lot less about Airflow to make a new DAG The developer just has to worry about their job specific code and not how multiple different Airflow operators work or how to fix them if things go wrong Theyre even free to test their job code before even making a DAG Check out Bluecores blog post for further explanation on why using a single operator is a great ideaWe want to load data from AWS S3 into Snowflake a cloud-based data warehouse This would be a DAG composed of a S3 Prefix Sensor followed by our GantryJobOperator with the snowflake-loader image and the example_flavor flavorOnce the Airflow Scheduler triggers the job the S3 Prefix sensor will begin to poll the S3 bucket When the files land the sensor will complete and trigger the GantryJobOperator (GJO) The GJO will: With Alpaca weve built a platform on top of Airflow that allows us to quickly build new jobs from config driven batteries included custom operators We didnt build these with just the Data Pipelines team in mind but with the goal that anyone should be able to create their own Airflow DAGs without having specific data engineering knowledge We built Flink as a Service at JW Player to allow any user the ability to create real time data streams using Apache Flink and now we want to do the same with Airflow While weve had users already create Alpaca DAGs we can go even further in building an even easier to use platform on top of what weve already createdIn the next post well be taking a deeper dive into how our custom operators work what they can do and how we developed them to be easy for anyone to create new jobs
95hhCr2vVHvn4TaUspEaaE,Apache Airflow is an open source platform for programmatically authoring scheduling and monitoring workflowsWith Airflow engineers build directed acyclic graphs (DAGs) to define workflows One DAG contains a set of tasks and each task is one instance of an operator Airflow provides a lot of built-in operators: BashOperator to run bash scripts PythonOperator to run any Python code S3FileTransformOperator to move data in S3 etc Once a DAG is defined the Airflow scheduler executes those tasks on an array of workers while following the specified dependencies Moreover Airflow has a rich user interface that makes it easy to visualize pipelines running in production monitor progress and troubleshoot issues when neededSound familiar? I mean any ETL tool nowadays usually provides such functionalityAt Kabbage we process data from a wide range of data sources to support analytics automate decisioning (our systems and processes responsible for evaluating our users credit worthiness) and run machine learning models Historically we have used SQL Server Integration Services (SSIS) to run our ETL jobs It worked fairly well for a while but as our business grew we were faced with several new challenges: Airflow addresses the above issues pretty well The design of Airflow supports horizontal scaling natively We can simply add more Airflow workers to increase processing power Airflow also provides a solid suite of default modules for common tasks such as moving data to and from S3 and creating Spark jobs in EMR and Databricks among others Even when there is no built-in support we can usually find Python libraries to build our customized airflow operators For example we used the simple-salesforce library to develop our own operator for Salesforce data movementBesides those advantages the most unique feature of Airflow compared with traditional ETL tools like SSIS Talend and Pentaho is that Airflow is purely Python code meaning it is the most developer friendly It is much easier to do code reviews write unit tests set up a CI/CD pipeline for jobs etcWe made the decision to use Airflow last year and since the first job was deployed we have seen quick adoption of the Airflow platform internally More than one hundred jobs/DAGs are running in our production cluster Some of them were migrated over from SSIS but many of them are brand newWe built a customized Airflow docker image and use that as the baseline of both development and deployment Developers use docker-compose to run Airflow instances locally for testing Development and production clusters use the exact same docker image triggered by Bamboo to deploy new releasesThe cluster infrastructure is shown in the above figure Since all of our cloud infrastructure is hosted in AWS we leverage AWS RDS services for meta storage and AWS Elasticache for our scheduling queue Note that we use AWS Storage Gateway as shared storage to save things like DAGs configurations and intermediate data Most people use AWS EFS for that purpose but weve found that Storage Gateway works pretty well and is much cheaper The Storage Gateway also plays a role of migrating existing Windows file share storage to S3 at Kabbage Currently all Airflow docker instances including the web server workers and scheduler are running on a DC/OS cluster So far the infrastructure fits our needs very well and we havent experienced any outages of our Airflow cluster for more than one yearWe organize the Airflow DAGs development code as follows The dags folder contains both common library folders and all jobs definitions Each job folder could have one or multiple DAGs The conf and sql folders have the same structure The conf folder is used for environment specific variables and the sql folder is used for SQL templates (based on Jinja template engine) The consistent structure allows each DAG to read the corresponding configuration and sql statements more easily (convention over configuration)Weve developed a set of common Kabbage operators One example is that we developed a generic file movement operator supporting local file system sftp ftps Google Drive and S3 with advanced pattern matching criteria built-in Another example is that we built a customized operator to trigger Databricks notebooks which makes CI much easier (We will write a separate article in the future to discuss more about Databricks development) Besides operators we also provide several template Airflow DAGs to demonstrate the typical uses we need: moving data from and to sftp/S3 triggering a Databricks job in spark etcIn terms of testing we enforce unit tests for common libraries We also do smoke/validation tests to load our DAGs to make sure they are properly constructed We also apply rules to smoke tests For example a notification email address must be included in our DAG configurationThe goal of all of the above work is to simplify the Airflow job development process Any engineer even those without much experience with Python can add new DAGs by themselves Weve observed a high adoption rate throughout our organization including engineers with just a SQL or NET backgroundWe have hundreds of SSIS jobs running in production today Migrating all of them is not a simple task The main challenge is the complexity of the job dependenciesWe adhere to the following rules to perform migrations: We have migrated a significant amount of jobs and in general weve seen that those jobs are more stable and easier to troubleshoot than their SSIS predecessorsMany engineering teams want to put more and more jobs into our Airflow platform Based on the DevOps philosophy you build it you run it we need better multi-tenancy support and role based access control so that teams can manage their jobs more easily and independently without risking impacting other teams jobs There is a very recent blog post from Lyft talking about this topic in detailTerraform + Kubernetes has been adopted as a standard for infrastructure management at Kabbage It helps us to automate environment creation and testing We plan to migrate our current Airflow docker orchestration infrastructure from DC/OS to Kubernetes as well using Terraform to automate the provisioning It will not only consolidate the tools we use but also simplify the work to bring up and shutdown a new environmentWe look forward to sharing the outcomes of those improvements in the near futureIn this article we shared the data processing challenges Kabbage faces as well as the reason we chose Airflow as our primary workload orchestration platform We also shared some details of Airflow development and deployment at KabbageAirflow is a great open source platform for developing modern ETL applications However there is no silver bullet Data engineers still need to apply well-known practices idempotency for example to develop robust data processing jobs no matter what tools we use
RHvjppEuiHvBN8oKxLWo5e,At Kapten we are proud to write that in about a year we developed a pretty stable and mature data lake Our architecture mainly revolves around daily batch data ingestions and processing jobs handled by Airflow We host all our workers and services on Google Cloud Platform and deploy pretty much everything on their awesome Kubernetes Engine We manage around 50To of data and 400 jobs (aka DAGs in Airflow) We also developed a custom framework on top of Airflow so that mere mortals with SQL knowledge can create their own data ingestion and processing with a pull request of a few lines of configuration At Kapten data-engineers do not write custom ETLs anymore its the future user of this ETL that writes it mainly data-analysts Delivering this architecture was a great milestone in the life of our data-engineering team But with the growing data volumes and new business needs comes a new challenge: making our data lake real timeApart from the fact that stream processing is an innovative field in data engineering we have other valid reasons to make the shift to real time These reasons come from technical and business contexts our company is currently inAll company data is updated in our data lake every day It means that every night we ingest into our data lake all changes made the day before on production databases It might represent quite a lot of data and extracting this new and cumulated data has a cost Transferring it to our data lake create spike loads on production databases due to our activity and it can take several hours for our ingestion jobs to finish As the company and data grow these long ingestions delay the BI dashboards availability in the morning and late data is not good for businessIf we acquire the ability to process new data as its created we could smooth out our ingestion process along the day forgetting the need to wait for midnight to ingest this new dataData lifecycle in production databases and in our data lake is not the same On a production database a document can be updated several times a day However changes may occasionally be missed by our ingestion jobs as we only get the state of the document at midnight It means in some case our data lake only has coarse grained snapshots of the data Real time ingestion could allow us to go beyond this limit By capturing all changes made in a database in real time we would ingest all mutations made in a document transforming data update as event and not only snapshots It offers a finer data resolution and enables new data mining capabilitiesIn our current architecture data is refreshed once a dayReporting and dashboard are made available with yesterdays data to analysts and business teams around 8 am Everyone arrives around 9 at the office and all decisions made in the day are made upon figures that have been computed and updated according to yesterdays platform activity You might think its fine and the data is fresh enough to make accurate assumptions about our market In most case it is true but new needs that require more frequent data updates are arisingAt Kapten as a ride hailing market place we have two types of customers riders and drivers We have various communication channels to manage our communication with them in an automated way All these systems work on trigger logics : send a templated sms to a driver when he has made more than X rides in the last X days or send a coupon to a rider when he has not made rides since X daysEfficiency of these automated operation is multiplied when the right email is sent at the right time And the right time is probably not the time when the ETL jobs end in the early morning Making customer data ingestion realtime will enable more fluid interaction with our customers and empower our CRM teamsAt Kapten operation teams are involved with daily interactions with drivers in each city They often need to ensure everything is going fine in the market by reading some macro metrics or by investigating specific driver cases Hence they would welcome an access to real time data For instance when we open a new city it is crucial to follow minute by minute the supply and demand evolution and act upon these figures so that every problem can be fixed in a matter of hours and not daysData science and more specifically prediction models are all about predicting an outcome from a context When you ship a model into an app the current context is the one your user is in So for accurate prediction you need accurate context and view on customer data It is not relevant to make a prediction about a current behavior if all your features are not computed from the latest customer action in your appIn a nutshell for performant data science models in the wild (in production) you need to feed them with lively computed features otherwise your data scientists will become frustratedWe could and we did After all in production databases the data is always up to date not every morningBut running analysis on top of production databases is what we called pre-history data-engineering at KaptenFor several important reasons : GDPR : Business teams must not access personal data and data not related to their expertiseLoad and tooling : Running some analysis and computation might requires a lot of computing resources that are not available in our production database Production databases are made to serve our app and micro services with data Dedicated computing infrastructure and different technology stacks need to be deployed to perform analysis This is why we use BigQuery as a highly scalable and performant query engineDSL : Our production databases run on MongoDB which DSL and core structure is not made for analytical purposes BigQuery offers a SQL DSL which is a basic commodity for analystsCentralized data : One of the biggest advantages of our data lake is that it offers a clean registry of our business data A lot of work has been built upon the raw data provided by production databases Our data cleanings and standardization allows a convenient access to the data for business operationalAll these reasons explain the purpose of having a data lake at Kapten and that using our production databases as a real time datasource is not conceivableWe are currently working on this real time shift and will write a follow up article to walk you through the technical process that implies shift to a real-time data lake
CLdJxuoYgumq9EB6sTBbbs,We have a lot to share in todays longread: well retrieve data on ad campaigns from Vkontakte (widely popular social network in Russia and CIS countries) and compare them to Google Analytics data in Redash This time we dont need to create a server as our data will be transferred to Google Docs via Google Sheets APIGetting an Access TokenWe need to create an app to receive our access token Follow this link https://vkcom/apps?act=manage and click Create app on the developers page Choose a name for your app and check it as a Standalone app Then click Settings in the left menu and save your app IDCopy this link: And change YourClientID to your app ID this will allow you to get information about your advertising account Open this link in your browser and you will be redirected to another page which URL address holds your generated access tokenYou will also need your advertising account ID to make API requests It can be found via this link just copy it: https://vkAccess token expires in 86400 seconds or 24 hours If you want to generate a token with an unlimited lifetime period just pass scope to the offline parameter In case if you need to generate a new token  change your password account or terminate all active sessions in security settingsUsing APIs to collect dataLets write a script that would allow us to retrieve information on all users ad campaigns: number of impressions сlicks and costs The script will pass this data to a DataFrame and send it to Google DocsWe have several constant variables: access token advertising account ID and Vkontakte API Version Here we are using the most recent API version which is 5103To get advertising stats you need to use the adsgetStatistics method and pass your ad campaign ID to it Since we dont run any advertisements yet well use the adsgetAds method that returns IDs of ads and campaignsUse the requests library to send a request and convert the response to JSONWe have a familiar list of dictionaries returned similar to the one we have reviewed in the previous article Analysing data on Facebook Ad Campaigns with RedashFill in the ad_campaign_dict dictionary as follows: specify ad ID as a key and campaign ID as a value where this ad belongs toHaving ID for every ad needed we can invoke the adsgetStatistics method to collect data on the number of impressions clicks costs and dates for a particular ad so create several empty lists in advanceWe need to invoke the getStatistics method for each ad separately lets refer to the ad_campaign_dict and iterate our requests Retrieve all-time data by calling the period method with the overall value Some ads may not have impression or clicks if they havent been launched yet this may cause a KeyError Lets recall to the try  except approach to handle this errorExporting Data to Google DocsWell need a Google API access token navigate to https://consoledevelopersgooglecom and create one Choose any name you like then go to your Dashboard and click Enable APIs and Services Choose Google Drive API from the list enable it and do exactly the same for Google Sheets APIAfter activation you will be redirected to the API control panel Click Credentials  Create Credentials click choose data type and create an account Choosing a role is optional just proceed and specify JSON as a key typeAfter these steps you can download a JSON file with your credentials well rename it to «credentialsjson» On the main page youll find the email field  copy your email addressGo to https://docsgooglecom/spreadsheets and create a new file named data well pass data from our DataFrame to it Put the credentialsjson file in one directory with the script and continue coding Add these links to the scope list: We will use the ServiceAccountCredentialsfrom_json_keyfile_name and gspreadauthorize methods available in the oauth2client and gspread libraries for authenticaion process Specify your file name and the scope variable in the ServiceAccountCredentialsfrom_json_keyfile_name method The sheet variable will allow us to send requests to our file in Google DocsApply the update_cell method to enter new value in a table cell Its worth mentioning that the indexing starts at 0 not 1 With the first loop well move the column names of our DataFrame And with the following loops well move the rest of our data points The default limits allow us to make 100 loops for 100 seconds These restrictions may cause errors and stop our script thats why we need to use timesleep and make the script sleep for 1 second after each loopSee how you can connect Google Analytics to Redash in this article «How to connect Google Analytics to Redash?»Having a table with Google Analytics and ad campaigns from Vkontakte exported we can compare them by writing the following query: ga_source  the traffic source from which a user was redirected Use the CASE method to combine everything that contains vk in one column called «vkcom» With the help of JOIN operator we can add the table with the data on ad campaigns merging by date Lets take the day of the last ad campaign and a couple of days after this will result in the following output: TakeawaysNow we have a table that reflects how much were spent in ad costs on a certain day the number of users who viewed this ad were engaged and redirected to our website and then completed the sign-up process
NcYmng3ahJfk53KRz3rG8C,In todays article we are going to use public data from vkcom to interpret and classify users attitudes about the 2020 amendments to the Constitution of RussiaFirst off we need to receive data using the newsfeedsearch method this method allows us to get up to one thousand of the latest posts from the news feed by keywordThe response data contains different fields like post ids user or community ids text data likes count comments apps geolocation and many more We are only needed ids and text dataSome expanded information about the author will also be useful for our analysis this includes city gender age and can be received with the usersget methodThe received data should be stored somewhere we chose to use ClickHouse an open-source column-oriented DBMS Lets create two tables to store users and their posts The first table will be populated with ids and text data the second one will hold user data such as their ids age and city The ReplacingMergeTree () engine will remove duplicates in our tablesLets get to writing our script import the libraries and create several variables with constant values: If you dont have an access token yet and want to create one refer to this step by step guide: Collecting Data on Ad Campaigns from VKDefine the get_and_insert_info_by_user function that will receive a list of user ids and expanded information about them and send it to the vk_users table Since the user_ids parameter takes a list as a string object we need to change the structure and omit the square bracketsMost users prefer to conceal their gender age and city In such cases we need to use Nullable values To obtain user age we need to subtract the birth year from the current year if the birth year is missing we can check it using the regular expressionOur script will work in a while loop to constantly update data as we can only receive a thousand of the latest data pointsThe newsfeedsearch method returns 200 posts per call so we need to invoke it five times to collect all the postsThe data we received can be parsed VK users always have a positive id while for communities its negative We need only users data for our analysis where from_id > 0 The next step is to check whether a post contains any text data or not Finally we will collect and store unique entries by user id Pause the script after each iteration for 180 seconds to wait for new user posts and not violate the VK API rulesFor one week our script collected almost 20000 posts from VK users that mention the keyword constitution (or конституция in Russian) Its time to write our second script for data analysis and visualization First create a DataFrame with the data received and evaluate the sentiment of each post identifying whether its positive negative or neutral We are going to use the Dostoevsky library to analyze the emotion behind a textAssign all the contents of our table to the vk_posts variable with a simple query Iterate through all the posts select those with text data and populate our DataFrameInstantiate our model and iterate through the posts to evaluate the sentiment of each entryAdd several boolean columns to our DataFrame that will reflect whether its a positive negative or neutral postThats how the DataFrame looks now: Lets examine the most negative posts: Now lets add data about the authors of these posts by merging two tables together on the id columnLets find the percentage of posts for each group: positive negative neutral Iterate through these three columns and calculate the values more than zero for each data point Then do the same for different age categories and genderAccording to our chart 45% of recent user posts relevant to the keyword constitution have a negative meaning while the other 52% are neutral Later itll be known how different the Internet opinions from the voting resultsIts noticeable that among the men audience the proportion of positive posts is less than 2% while for women its 35% However the number of negative posts for each group is almost the same 47% and 43% respectivelyAccording to our analysis posts made by younger audiences between 18 25 years have more positive sentiment which is 6% While users under 18 years leave mostly negative posts this may be because most users under the age of 18 prefer to hide their real age this makes it difficult to obtain accurate data for such a groupThe proportion of negative posts is almost equal for all groups and accounts for 44%As you can see the data is distributed equally in all three charts This means that half of all posts relevant to the keyword constitution and made by VK users over the past week mostly have a negative sentiment
ciMKTBTt87xvWSqVLKGLGW,Whilst working on a mainframe application migration to Amazon Web Services for an organisation part of their migration strategy included migrating data for their applicationThe legacy application is hosted on a iSeries mainframe running Z/OS a Customer Information Control System (CICS) interface to perform Create Read Update Delete (CRUD) operations and DB2 to store the dataThe organisation is in the process of a digital migration implementing an event-driven micro-services architecture and is comfortable with eventually consistent data across systems The organisation in question is also delivering software using Continuous Delivery practicesAn on premises transaction log scraper and an API hosted in Microsoft Internet Information Services 100 exists as part of the legacy application stack and provides the development team with the means to access the data housed on the mainframeAs part of ensuring data consistency between both systems some form of change data capture is needed in order to consume data in an event driven manner and update data in the new systemAs stated on Wikipedia change data capture (CDC) is a set of software design patterns used to determine (and track) the data that has changed so that action can be taken using the changed dataIn this case the mainframe based application is still the source of truth for the application data whilst the new application is being builtTooling such as Attunity Replicate https://wwwattunitycom was being trialled in the organisation but the solution was not production ready and hence could not be used to fulfil the change data capture requirementsThe team made a decision to use the technology available and augment an existing solution with A SNS Topic A SQS Queue and a Lambda to facilitate the data migration requirementsThe testing overhead that is incurred to validate the migration processParallel Run of the migrationDifferent source systems different coding and a unique dataPerformance of legacy system can dictate how the solution scales (this is especially true if the legacy application is mainframe based and uses batch networks) Data migrations then have to be performed when batch networks are not running and the mainframe has MIPS that can be used to perform ETL (Extract Transform Load) type workIt should be noted that the on premises API acts an anti-corruption layer and enforces specific business logic The legacy on premises API outputs data in JSON format which allows the data to be migrated whilst both solutions are run in parallelDue to both APIs being written in C# different source systems and different encoding can largely be negated Both APIs have a suite of unit tests thereby negating the need to unit test ETL scripts The testing of data transformation can be fully automated at this pointThe 2 APIs in question have validation logic and therefore act as an anti-corruption layer for the data migration process and the new API being developed acts as gateway and data sanitization mechanism to the new system The new API uses a landing table to store the data in a raw JSON format so that data capture can be replayed at any point in timeThe APIs can be monitored using modern tooling such as new relic cloud watch metrics and cloud watch logs Failures of any sort can be detected early and members of the development team can investigate any errors that appear in the log stream of the logging dashboardsThe performance of legacy systems can dictate how the solution scales The horizontal scaling of the Lambda functions used in the solution can be used to provide performance on demand scaling up or down as required by the amount of data flowing from the legacy system to the new systemThe delivery of the entire technology stack can be automated using a build pipeline AWS provides Cloud Formation to automate the deployment of the solution to AWS New ETL stacks can be deployed as needed as the data migration needs of the organisation changeAre highly available durable secure fully managed pub/sub messaging service that enables you to decouple micro-services distributed systems and server-less applicationsAmazon SNS provides topics for high-throughput push-based many-to-many messagingUsing Amazon SNS topics your publisher systems can fan out messages to a large number of subscriber endpoints for parallel processingBecause SNS topics are used in the solution other systems that are interested in the data can listen for downstream messages being published allowing other systems to hydrate or augment their data with the data being published during the replication processSQS Queues also have the characteristic of back pressure If the Lambda function processing the queued items does not de-queue in a timely manner the queue can end up backing up and there can be a long lead time till the data migration has completedA file dump of the data can be used to populate the new system This mechanism can be error prone as it relies on the ETL scripts that need to be verified and tested If the data in the file dump can have data consistency errors that can go undetected due to encoding and floating point differences between 2 different systemsThe architecture of Attunity Replicate is comprised of three domains: the source database the replication server and the target database End users interact with these domains through the Attunity Click- 2-Replicate designer and the Attunity Replicate Console This product architecture reflects Attunitys commitment to unlocking information assets through high performance scaleable and easy-to-use solutionsSource:https://blogqlikIn summary there are a multitude of ways to solve the challenge of migrating data whilst running 2 systems in parallel the pros and cons of each will depend on your organisations own unique context At Lexicon we appreciate this and tailor solutions to meet your business needsI hope this post gives you the basis to start thinking about how to derive different ways to address data replication and some insight into some of the challenges were helping to solve for our clients
4s8mvJe6Dp75kRJQNevqox,In Mapan we strive in providing network to thousands of lower income communities with better access to valuable products and services using technology To achieve our goal we are taking care of many fields in businesses like communities (online and offline) order management customer service and supply chain Those things are intertwined and producing a large number of data in various data formatsThe Data & Business Intelligence team in Mapan is responsible in producing insight and analytics based on available data We have data stored in many sources like MySQL PostgreSQL Couchbase third-party system and even Google Spreadsheet It is a big challenge to consume data from various sources since the data from each source has different structure and some data is unstructuredDue to complex nature of our business rapidly changing and complex business rules are inevitable We frequently change the business logic in our ETL system within limited time Sometimes we are overwhelmed because we need to take care the business logic and ETL infrastructure at the same time We want to focus on the things that really matter the business rules instead of managing ETL infrastructureGoogle Cloud Functions is a serverless execution environment for building and connecting cloud services With Cloud Functions you write simple single-purpose functions that are attached to events emitted from your cloud infrastructure and services Your function is triggered when an event being watched is fired Your code executes in a fully managed environment There is no need to provision any infrastructure or worry about managing any serversIn this post I will demonstrate how we use Google Cloud Function for data transformation using the data from Zendesk API as an example I will not explain about the data extraction here since the data extraction system is separated from data transformation The result of data transformation will be loaded to Google BigQuerySuppose we want to track the history of assignee changes of Zendesk tickets We can use the data from Zendesk Ticket Events API because the information of ticket assignee changes can be retrieved from there Since we already have a job that copying the data from Zendesk API to our data lake in Google Cloud Storage we just need to use the data from Google Cloud Storage Here is the example of Zendesk Ticket Events data: The Ticket Events data doesnt have consistent schema because the child events contains various kind of events and every event has different structure Hence we need to create a function to preprocess this data before loading it into Google BigQuery for further analysisIn this case we will use GCS trigger for the Google Cloud function So whenever a new data is loaded to GCS the function will be executed automaticallyThe function will read the data from Google Cloud Storage transforming the data and load the result into Google BigQueryThe deployment is very easy We can deploy our function just with this commandEach trigger has different option for deployment You can read the documentation of Google Cloud Function from hereAfter the function has been deployed it will transform the data and load it into Google BigQuery whenever there is a new data in Google Cloud Storage By doing data transformation with serverless approach we can greatly reduce our time in maintaining data infrastructure and enable us to focus on the business logic Furthermore the event-driven approach enables us to deliver insight in real-time Last but not least whatever tools we use for data transformation make sure that we can deliver insight to the businessThis article was written by Muhammad Redho AyassaIf you are interested in knowing more on how your skill in data engineering or data science can help society send your latest CV to recruitment@rumaco
hsP3NxmMfQT9nUTsKBNkKY,Since recent years as different organizations strive hard to be more transparent more efficient and more data-driven there is a surge in the need of a stable analytic platform for internal departmentsTrust me in those days your engineers hate these requests a lot they may struggle to dump a full month of production data then for some reason the connection drops and they have to do export again After all of those things oops the business team come back to say : Well in fact we need to export another table as there is something missing…Thankfully now in each organization you alway have a dedicated analytics database for those job ( hopefully) Depend on the technologies you have chosen for those dedicated analytics databases you may call it Data Warehouse or Data Lake or Cemetery of Data These effectively are dedicated data servers which replicated production data applied a lot of Extract-Transform-Load  which serve as Analytical Database (OLAP- Online Analytical ProcessingAfter working with different companies  ranging from startups to corporations working with different technologies these companies have chosen as the back-bone data warehouse system I think it worths to note down my reflection on those technologiesAt first the company itself and notably the tech-team is so excited about the technologies  But in the end all of those nice pieces of technologies have to serve the organizations in some way Lets be honest business is about Minimum effort Maximum Efficiency you need to choose the best technologies to get the job done and those best choices are sometimes not always the cool onesThus I start these small series to share my experience in different Data Warehouse technologies I have been working withIf you have ever spent time in Analytical Teams in big organizations then definitely at some point you have used Apache Hive (https://hiveapacheAfter the publication of the paper MapReduce: Simplified Data Processing on Large Clusters by Google Research Lab MapReduce became the famous trend  and with the arrival the opensource Apache Hadoop ecosystem opensource MapReduce officially opened the so-called big-data age and organizations quickly adopt this new piece of technologyApache Hive itself it not a database for the commoners it is just a tool which translate your normal SQL query to MapReduce jobs that can be run on distributed Hadoop File System (HDFS) However in a not-so-strict sense Apache Hive serves its purpose just as any analytical database can do So lets try to consider it in that senseAny database management system consists of the following componentsIn any database management system you interact with its driver (WebUI driver connections…etc) to send your query The query is then sent to query parser/validation and at this level the system will draft out an execution plan for your queryWhen the execution plan is formed it is sent to the executors which actually launch the workload At the lowest level the data is pulled out from physical files stored on the disk or on the memory to retrieve the data The data is then returned in the readable format for consumptionLets describe what happen when you want to query something from your fileImagine you have a file called clientscsv  The file look like: JohnDoe120 jefferson stRiverside NJ 08075JackMcGinnis220 hobo AvPhila PA09119John Da ManRepici120 Jefferson StThen when you says to Hive: Lets trace the flow of this query: If you can see Hive does not have its proper Executors or does not even manage the data file directly but relies totally on Hadoop for those tasks Thus Hive itself is not a proper database management systemEvery times I have to use HIVE I keep asking myself those questions In fact it seems that if your organization has moved most of its analytical data to Hadoop File Systems then you have a few choices: AB Write SQL query and forget it take your coffee and just wait for resultsClearly if you ask your analytical teams to a bunch of complicated code just to get the data out of your Hadoop File System they are not going to do it Or even worst they are going to write for once just to data from your Hadoop File System to CSV and they will just work with plain CSV (here we go again…So in short Apache Hive is an extension on Hadoop a friendly arm which enables everyone in the team to happily work with data stored on HadoopThe problem arises from the fact that everything said in HIVE depends on MapReduce jobs and Hadoop itselfSince MapReduce Hadoop ecosystem itself is Java-based a single query has to go through a bunch of transformation translation and then to be distributively executed on Java Virtual MachineWhats wrong with that ? You may ask… But as a guy who has been there done that I can tell you that its a big pain Name it: A NullPointerException was thrown at line number 120045689 of planet Scorpion Alpha X67Y  generated by machine Tetra 76 failed at task 24…Its pretty like that how you are going to debug your code even sometimes you write only some extremely simple SQL JOIN of 2 tablesIn most case I figure out myself that technically the answer is NO Since in most organizations I have been working with most of them (not all) do not actually need MapReduce at all May be it was the trend of Big Data Era that drives the adoption of this MapReduce leverages the jobs running on huge dataset in a very cheap (but not necessarily fast) way However the fact is that most of the organizations that are using it do not have that huge data setSometimes I have seen case that people put just 500 000 lines of data to HDFS and run HIVE SQL query on it Technically 500 000 lines of data is no where to my definition of BIGTo be fair indeed I have seen in a few organizations they truly have huge datasets which is really costly to run extensive query on it and HIVE becomes very handy you have have to wait 30 minutes for the execution but at least it gets the job done Instead of having nothing you have something Its a step aheadApache HIVE and its brother Apache Hadoop is listed in my category of Bad guyBut remember sometimes  you still need the Bad guy to get the job done
2xPWPabd87yyQN3wB5hrmD,Today I am going to share an interesting issue that many of us see pretty frequentlyAs you work with data it is a very common task to extract some badly formatted CSV file and load it somewhere Let say to a databaseAs a sane-engineer would do instead of do it manually which you could but that would hurt your engineers pride so much so that you decide to write a script to handle the jobAnd then our story begins There is one thing you forget to check  It turns out that the your CSV file is not a normal csv but its a huge one dumped in a harshly manner from a department you have never known that they exist The CSV file contains hundred thousands or even hundred thousands of lineTrust me he may have spent his weekend clicking on retry/try again just to get the CSV file for youAnd now your script takes 8 hours to execute and still does not complete Out of sudden  something happens  and you have to execute your pipeline againNow most of the times your code look as simple as thisThe catch here is that you are using a single thread to do itIn fact the rows in your csv are independent which mean line number • 00000 can be processed before line number 1 and it does not change the nature of the jobYou realize that reading from CSV is amazingly fast it is the database writes that is I/O time consuming so instead of write each file separately you spawn a new process to handle the write operation That means multi rows can be processed/written at the same time into the databaseBravo Welcome to the world of concurrency programmingFor this task lets use Golang as this language is designed to handle such thingsThe worker interface has only one method as well as one single purpose: to execute a task In our case for simplicity we give a stringNext we create a WorkerPool This Pool is the combination of available workers up for processing a taskWhen given a job the WorkerPool is going to pool out a random worker in its pool and execute the job in a separate goroutineThis means your heavy work load  whatever it is  is going to be executed in a goroutine without blocking each other  This is the secret sauce that accelerate your programNow depending on the type of work you want to do you may create different type of worker Just remind you that WorkerPool stores pool as array of pointer to Work interface so that as long as your Worker satisfies Work interface you should be fineHere I create just a simple Worker to print out the message It implements the executeTask method of Work interfaceAs mentioned above your task is executed in a separate goroutine so some how you must have a mechanism to notify the termination of the execution process To make it happen you have 3 choice in Golang: mutex chanel and waitGroupIn this example for every task about to execute I increment the waitgroup by 1And for every task complete I notify the waitGroup about its terminationhttps://mediumPutting things together we can execute the code In this example I create an array contains 3 words alpha  beta gamma to represent the work loadYou may choose the number of workers as you wish The number of workers should be reasonably big enough to make the difference but sometimes too many is just too many Note that the gain in time is not a linear function So you have to figure it out based on experimentsGolang gives you the power to use worker pattern in async programming This pattern in Golang is very powerful just like Golang itself Without setting up a complex Queue engine you have the capacity to lever up your ETL scripts and data processingLets thanks this cute little guy for his contribution
JngeVfunxUkNKx9GQhYMLs,The business is changing constantly so objectives change every month processes are lagging behind necessities and the organization changes a lot every six monthsNew people are joining the team every month and they are rushing to learn how the company operates how the tools work They dont have an internal network so communication can breakAt Liv Up our business analytics team has the goal of fostering data-informed decisions across the organizationPeople at the business analytics team dont make the final business decisions we empower decision-makers through data analyses and recommendations To successfully impact decision-making we have to get over two issues that are aggravated by hypergrowthTo create a high-performance team in the middle of such change and uncertainty we have to guarantee that analysts have real autonomyCommunication is expensive and many times inefficient Business analysts are in the middle of decision-makers and technical staff such as data engineers If they cant understand the business context by themselves and cant develop their analyses without creating an issue in the engineering backlog they will face paralysis and as time goes get frustratedIn hypergrowth we have to trust people to make fast well-informed decisions If we need to sync the entire company to make each decision or pair data engineers and business analysts for each project we are losing precious timeTo achieve autonomy we have to guarantee that analysts have proper business context and are independent to consume data and develop their analysesLiv Up is a vertically integrated company This means that we control and operate all the value chain from raw materials sourcing production distribution sales product development customer support and many other business facetsThis model has several advantages such as agility to develop new products as we own development and production higher margins as we remove intermediaries and many othersHowever this also means more complexity We have dozens of processes to control and nobody has the bandwidth to understand what is happening at the entire company all the timeTo solve the complexity problem we have to specialize Our business analysts work inside multidisciplinary squads they live their processes face their challenges and create relationships They sit side-by-side with chefs product managers and logistics specialists to gather business context and develop better analysesThis model radically reduces communication overhead as many things simply dont need to be said or written If the analyst lives the day-to-day of that squad she knows what is happening and what is importantOf course there are several trade-offs with this model We have to align processes and tools inside the business analytics chapter career progression is much blurred in this system and the analyst needs to be independent enough to discuss ideas with decision-makers without the support of a technical leader Nevertheless the pros outweigh the cons in our caseOKRs are another powerful tool to inform business context to analystsWell defined OKRs targets focus on what matters If a decision-maker demands analysis of projects or metrics disconnected from OKRs the analyst is much more inclined to challenge the decision-maker and suggest analyses focused on the squads objectivesIt is common in the analytics space in general to end a dashboard project or a presentation with a so what? Analyses should be actionable and OKRs help with this focusOur business analytics team has a mix of technical-oriented and domain-oriented people It is much easier to explain food product development for a food engineer than to a software engineerPeople with domain knowledge mainly in specialized areas such as logistics or food product development accelerate business context gathering and generally generate good deep analyses However it is important to master the technical skills and a profile mix of technical and domain experts enables knowledge flow inside the chapterOur challenge is to empower analysts with high-quality data and good analysis tools without creating data engineering dependancyTo overcome this challenge we leverage our modern data stack combining powerful technologies that guarantee scalability productivity and efficiency without compromising autonomyIn addition to good technology we have defined responsibilities between software engineers data engineers and business analystsOur architecture is heavily based on Google BigQuery and other services of the Google Cloud Platform We have 4 components in our architectureIngestion Layer Here is where all data is ingested Following an Extract Load Transform (ELT) process We first load all our data in Google BigQuery before processing anything Besides MongoDB (our main transactional database) we ingest data from 10+ sources using Stitch BigQuery Data Transfer Service and custom Python scriptsTransformation Layer Here the data is cleaned integrated between sources and transformed to create our master dataset (we have one table per entity) Leveraging the power of serverless BigQuery with simple SQL select statements and control of Dataform our analysts own this processThis model has several advantages Business analysts are independent to create new tables and columns they dont need a data engineer to adapt our data model to changes in the business Besides these new tables and columns can be used by other analysts (and data scientists) improving productivity If something awkward happens in the visualizations or analyses they know the entire pipeline and are empowered to fix the problemsThere are potential challenges with such freedom Development of circular dependencies wrong transformations leading to duplication of rows and out of date data To solve this problem we are using Dataform that provides a suite of scheduling testing and version controlling for SQL transformationsModelling Layer After the data is transformed into our master dataset we use LookML to model dimensions and metrics and to create explores to consume data Business analysts own this process too They can modify business metrics as we evolve and create derived tables if they need specific dataConsumption Layer The final destination is to extract value from data Leveraging previous transformation and modeling analysts can user Lookers point-and-click interface to create dashboards analyses and presentations with maximum productivity Besides we also use Jupyter Notebooks for specific use cases but most of our analyses are in LookerTo achieve autonomy the responsibilities over the above architecture must be clear At Liv Up business analysts are responsible for a large part of the pipelinesSoftware engineers They are responsible for data capture Engineers set trackers to send events to our Snowplow pipeline guarantee the accuracy of production collections and support business analysts to understand the schemaData engineers In the business analytics context they are responsible for the technologies used in the architecture maintaining cloud infrastructure and developing custom extractions or transformations for specific use cases that demand Python programming They provide guidelines for SQL and LookML development Besides that they provide great support for data scientists (content for another post)Business analysts (BA) As you can see BAs have two important responsibilities: to transform and model data and to generate business analyses They are responsible for every SQL code inside Dataform and all LookML code inside Looker After the data is in the data warehouse they have the autonomy to use it in the best possible way following the squads needsOne challenge presented in the first paragraph is that business analysts have to ramp up fast especially during hypergrowthLooking at our architecture it seems that they need to learn a lot of things to become productiveSQL Analysts must master SQL They write SQL code every day and it is the main interface between analysis and data There are plenty of good online resources on SQL programming and it is the industry default so there are many people in the market that know SQLGit Both Dataform and Looker provide version control Still analysts must know git only conceptually as these platforms automate the process in the graphical user interface so they dont need to interact with the command lineLookML The Looker modeling language is also mandatory on our stack The language is pretty simple there are good online courses and the documentation is awesomeDimensional modeling and denormalization Although there are entire books written about these topics analysts must know the basic concepts to start modeling data Facts dimensions metrics and star-schema They can also learn by example with the modeled tables in Looker / BigQueryIn our experience new analysts take 1 ~ 2 months to become highly productiveSome things that help to scale the teamAutonomy is mandatory for any company growing fast that values independent thinkers and high performers It is the job of everyone to design systems processes and the organization to allow team members to collaborate in the best way possibleAt Liv Up we have several initiatives to empower people with autonomy from microservices in the engineering team data pipeline ownership in business analytics team to end-to-end food development projects in the product teamIf want to build the food company of our time and share our company style follow us on https://wwwlinkedincom/company/liv-up/In the next post we will talk more about the data products team (responsible for data science)
iMyghsMkBkSj9GFogKV7vt,A problem that many companies face these days is data governance What most people dont know is the impact of this problem Not having the minimum amount of governance enhances local team autonomy but overall autonomy drops drastically when teams depend on each other Regarding data analysis consistency not only CAP consistent but also structurally is key to be productive and effectiveBusiness structures are constantly changing and it should be very well communicated between teams Communication is the most difficult part of this story teams change their concept of a business entity and no one is ever informed or just let it pass because no one truly knows where it would impact This is a common situation because whole systems are too complex for people to keep trackDecentralized systems leave teams with full autonomy to do whatever they need but in the real world many systems depend on the data generated by other systems This requires a heavy team and data management effort to keep track of all changes and making sure that making sure that things are not breakingWhen a team couples itself to another teams database (using directly or copying data) or to third party schema (CRM) they lose their autonomy to grow Since they are locked in a schema level neither team can make schema changes without impacting the other or having to duplicate the dataIn the scenario where a team duplicates data there is also another huge problem which is the consistency (now the CAP consistency) between the databases Managing multiple databases is hard given that in order to keep then in sync would involve complex distributed transactions solutions Those solutions however are often ignoredThe replication scenario becomes even harder to solve when there is a third party tool (eg CRM marketing applications) to be updated or worse still when there are multiple third party tools for the consistency reason and the coupling that it usually causesThis leads to companies having multiple schemas of the same data and managing all those schemas without breaking anything requires a lot of energyTo avoid this scenario interfaces must be well built by not allowing any breaking changes and well documented so that its easy for people to use Changes are usually slow and normally demands data migration All teams are responsible for their application for the generated data and also exposing this data for the required payloads This is how a well designed system with a well defined interface and purpose should workBut in a real world this is way too slow for teams on a fast growing business in which applications are meant to prove their worth before being fully implementedThis leads to teams having a centralized database in which many applications rely on or copy data from Everything a team does will automatically reflect on all applications that depend on this central source of information without having to update services interfaces or documentation This centralized database is often not exactly a database but some third party application like a CRMSyncing databases and managing schemas are hard to solve problems and should be avoided if possible because it makes the coordination between teams (or applications) a huge management issue and waste of energy dropping their productivity drasticallyThe first thing to do is defining who owns the data Setting the accountability of a portion of the data makes communication easier between teams This portion of data could be divided (and should if possible) regarding business domain modelSo for instance all marketing funnel data is the responsibility of the team responsible for the marketing application After the conversion of a lead the generated data should be the responsibility of Sales team which controls all sales flow After selling the Customer Success application handles and generates more data for that customer so this applications team is the data owner Having this owners well known makes easier to track problems in general because no matter what a system does it normally manipulates data So if data is missing corrupted or wrong whoever is responsible for that data chunk should investigateThis will not solve the problem with systems interface or data being duplicated but it is a good way to startInterface enforcement is also a big problem to solve which makes team communication much easier but it usually makes teams productivity drop because it demands too much technical and non-technical effortTo give a simple example suppose there is a Service (A) which is consumed by 3 other services (B C D) These services are written in 2 different languagesHaving a well designed and implemented service normally means having a well documented interface and clients for the most used languages In this case we would have 2 client libraries for our serviceIf there is a need to change something in the interface the following tasks would also need to be done: The task list gets even larger if the change breaks the old interface (ie changing field name removing some field etc…) because services often need to be backward compatibleThis effort is well paid in the future but in early age applications this is often an overhead to the development process Normally only the interface is changed causing documentation and clients to be outdated or non-existentAnother approach is to centralize the definitions of the data that is shared between the systems All data would be defined as metadata very close to what SQL databases do and be distributed to all applications that cares about this dataThis approach enables teams to change the definition of its business entities and distribute it so other teams can consumeFor instance in a Car Sale system the team responsible for selling the ads have their ad definition as: The team responsible for creating the Marketing product in which ads will be listed and leads would be generated can consume ads data with the up-to-date definition of what an Ad isThere are several tools like Confluent Schema Registry which addresses this Schema Management problemHaving a centralized and standardized way to define business entities solves the interface enforcement problem which is normally the bigger issue Storing this data is also a huge headache which slows teams down So enforcing this definition will only partially solve the productivity problemTo solve this matter its possible to have a centralized data storage which uses the distributed schema to persist and validate data This way teams would only be responsible for the definitions of its own business domain models and applications rules Problems like security governance replication performance would be completely outside of those teams scope of workAside the productivity gains other benefits can be achieved by implementing this approach: Many applications called Data Platforms (like Looker or Tableau) try to address the complexity of multiple data sources for analysis They are meant to unify access to all databases and cross their dataBut consistency (regarding schema or data) is not addressed by those tools They map to external databases and the analyst who uses the data will have to deal with the complexity of gluing all of those data sources togetherIn other words those data platforms will not solve any of the problems described above but the concept of one place to look at data is somewhat near the concept described above of centralized data storage There is another kind of plataform solution trying to solve those issues: Customer Data PlatformsThey are defined according to the CDP Institute as a packaged software that creates a persistent unified customer database that is accessible to other systemsA CDP is basically a standardized way to define centralized business models and storage so that every application can consume business data which is a great way to solve the problems described aboveMany products are already built around this concept those tools have their own definition of customers and relations requiring applications to abide by those rules Those tools are mostly built for marketing purposes They have many built-in tools to help marketing improve results and allows integrations with other applications like CRM and other marketing platformsThe problem with those tools arises whenever there is a need to customize their definitions or to allow other entities besides customer If there is a need to customize their schema it would be necessary to implement most of the code to manage schema and persistence layer which is almost the entire solutionThere are many complexities involved when talking about crossing multi tenant and multi operational workflows data Building a CDP solution from the ground up enables the implementation of all of those complex concepts but using a market solution is normally much faster and usually can solve most of the problemsIn a next post I will describe how we at Loft are solving all of those issues
79f97MD7sQpE9eCERU9jP7,TL; DR Apache Spark is a lot to digest; running it on YARN even more so I hope this post helps a bitThis article is an introductory reference to understanding Apache Spark on YARN Since our data platform at Logistimo runs on this infrastructure it is imperative you (my fellow engineer) have an understanding about it before you can contribute to it This article assumes basic familiarity with Apache Spark concepts and will not linger on discussing themYARN is a generic resource-management framework for distributed workloads; in other words a cluster-level operating system Although part of the Hadoop ecosystem YARN can support a lot of varied compute-frameworks (such as Tez and Spark) in addition to MapReduceThe central theme of YARN is the division of resource-management functionalities into a global ResourceManager (RM) and per-application ApplicationMaster (AM) An application is the unit of scheduling on a YARN cluster; it is either a single job or a DAG of jobs (jobs here could mean a Spark job an Hive query or any similar constructs)The ResourceManager and the NodeManager form the data-computation framework The ResourceManager is the ultimate authority that arbitrates resources among all the applications in the system
DbivcYurae2MSBwwYrBnMD,A friend asked me this week what the difference is between using Hadoop and its related ecosystem for data storage and analysis and using a traditional Data WarehouseA Data Warehouse is a structured relational database where you aim to collect together all the interesting data from multiple systems When putting data into a warehouse you need to clean and structure it at the point of insertion This cleaning and structuring process is usually called ETL  Extract Transform and Load The data warehouse approach is helpful because then your data looks clean and simple but it is also very limiting: youre now unable to change the questions you want to ask later since the datas already been pre-processed or to correct for errors in the ETL process It can also get very expensive as enterprise data warehouses are typically built on specialised infrastructure which becomes very pricey for large datasets Theres a nice diagram of a full data warehouse structure on the Wikipedia article hereThe Hadoop ecosystem (could also be called the Big Data approach) starts from the same aim of wanting to collect together as much interesting data as possible from different systems but approaches it in a radically better way With this approach you dump all data of interest into a big data store (usually HDFS  Hadoop Distributed File System) This is often in cloud storage  cloud storage is good for the task because its cheap and flexible and because it puts the data close to cheap cloud computing power You can still then do ETL and create a data warehouse using tools like Hive if you want but more importantly you also still have all of the raw data available so you can also define new questions and do complex analyses over all of the raw historical data if you wish The Hadoop toolset allows great flexibility and power of analysis since it does big computation by splitting a task over large numbers of cheap commodity machines letting you perform much more powerful speculative and rapid analyses than is possible in a traditional warehouseOne of the most confusing things in this field is naming  one mans data warehouse is another mans data store  so Im sure Ill get some corrections from people who use different terminology from that above However I hope this does untangle things at least a little bitNB This blogpost was originally published by Francine Bennett on MastodonCcom on 20/08/18
VfknN9kkcwp4Qm8ushaZn6,Mastodon C advertises itself as a Clojure shop when talking to clients and partners but lets delve a bit deeper into what this means and why we believe its a key advantage for us Ultimately it comes down to a simple idea: Simplicity is hard work But theres a huge payoff The person who has a genuinely simpler system  a system made out of genuinely simple parts is going to be able to affect the greatest change with the least work Hes going to kick your ass Hes gonna spend more time simplifying things up front and in the long haul hes gonna wipe the plate with you because hell have that ability to change things when youre struggling to push elephants mastodons aroundClojure is a computer programming language that promotes simplicity whilst simultaneously being incredibly powerful and broad It hails from a family of languages known as LISPs (List Processor) which are famous for their distinctive fully parenthesized Polish prefix notationIf youve ever seen code from other languages such as JavaScript or Python youll notice immediately that this is aesthetically very different However like any programming languages once youve learnt the nuances and the idioms writing code that is concise yet expressive becomes quite straight-forward Clojure is especially geared toward a style of programming known as declarative whereby a programmer rather than solve problems imperatively (do this then this) is more descriptive (change this data to look like this) allowing the underlying language and/platform (the JVM in Clojures case) to do the heavy-liftingClojure is predominantly used as a server language which means that its usually the workhorse at the bottom of a technology stack doing all of the business logic logging management and logistics This suits us quite nicely as this is where most of the Mastodon C magic tends to happen  reading in data crunching lots of numbers managing distributed computations across the cloud Having all of our solutions written in Clojure means that we significantly reduce the effect of silo-ing people into specific projects One person can easily move between jobs and this is something we actually mandate as part of our support processRecently there has been a big movement to bring Clojure to more platforms Where once it was almost exclusively big beefy servers Clojure is now also available in your browser via ClojureScript  a near-perfect reproduction of the Clojure language which transpiles to JavaScript This opens up a vast amount of opportunities for development across browser mobile and desktop as well as allowing seamless integration with lots of powerful and popular JavaScript libraries  such as React jQuery D3 and many more Its completely possibly now to write entire client-side web applications using ClojureScript  and many haveConsidering Mastodon C is already bursting with experienced Clojure developers using ClojureScript for our frontend work is a total no-brainer and of course comes with the added benefit that  once again  no one is left-behind when it comes to rolling up their sleeves and contributing to the code There is no server team and frontend team as is so typical at most polyglot organisations  everyone does everythingIts important to note that although our preference lies firmly in Clojure and ClojureScript we arent scared of cutting our teeth on other languages especially if they are designed for and excel at a specific task  R is a good example of this
YxTTrJjtPSLBpvgVTJCFXD,Imagine you enter a supermarket and all the products on the shelves arent classified you spend hours trying to find a simple cereal box and dont care what brand it isNow put this thought into lunchtime and you need to be quick thus you can buy the cereal and go back to your work in timeWhen you have an unimaginable income of data and your architecture is not healthy this is what happened You spent hours trying to find a simple table to work with and insights will not come as wellYou may think that all databases can process any data that you want However it is not as simple as that It can process numerous files formats but do not be attached to this statement Many files formats have no hierarchical structure and that makes it harder for any database to process it properlyA good example of file format that is light but not well processed is the CSV(Comma Separated Value) most found on Google Sheet files or Microsoft Excel exports It is considered one of the most compact file formats the csv can be represented as an array with commas separating each information on a row That makes it small although not able to show the data properly A better format and most used is JSON(JavaScript Object Notation) JSON is less compact than CSV though it shows better hierarchical and other relational data that means it has a lot of scalability in terms of adding and editing content It makes files json much more readable by the machineThat makes the cost of processing a csv only much higher than a database with only json and if you multiply your number of files for example a thousand files a day it will cost even moreBefore continuing this discussion it is important to understand this topicDont worry because it will not be a theoretical lecture with a bunch of technical termsInstead we will see some images to illustrate the whole architecture: Above there is a simplistic architecture model but it can illustrate the realityFirst you have your data source for example in a store each product bought becomes information in that way the cash register becomes a source and it sends raw data to the database It receives that features in a format like csv and the ETL (Extract Transform and Load) machine transforming this into whatever you need to better understand it For example ETL can change file format for better process then your csv will turn into json that we saw above is better for the machine to understand as well as turning some big tables into small ones with some calculations to create an even better visualization in the end The ETL in its end exports the processed data to a visualizer so you can see charts maps and all that glamour of a good dashboardTons of data means a lot of headaches without an organized and well built architectureImagine in the operation above if your report gets stuck into the database because you didnt process its format and you dont have space anymore The machine that your architecture is in will need an upgrade and it costs thousands of dollars and months of work and neither of that is what you wantTherefore to keep your environment healthy and at a minimum cost as possible some expertise is required to do such a framework But understanding its principles is mostly relevant when you are hiring someone to do this for youIn conclusion there is a lot of documentation inside each platform website that can help you decide what is the best for your company In addition think that data is the new oil its value is immeasurable and companies like Google make a fortune out of it
nAvRLG9FdAaRrmxZibaswH,Below we use the term AI system By this we mean any software system that incorporates at least one predictive model leveraging machine learning statistical modeling or any other AI techniques A few examples: An automatic fraud detection system a recommendation system an image classification system and a sentiment analysis of social media postsWith AI rapidly expanding to critical business use cases across verticals the pressure is on for data teams These teams are accountable for producing and maintaining high-quality predictive models and to continuously improve these modelsBeing accountable data scientists and data engineers are losing sleep Everywhere we go we hear concerns about a variety of issues which are inherent to AI systems What worked well in the research stage may not work well in production and good performance today does not guarantee good performance over timeBy design predictive models have biases Model biases sometimes lead to underperformance for a sub-segment of the data Sometimes these biases are driven by the training data being weighted differently from the data flowing in production Biases could go undetected for some time if performance is not monitored at a granular levelAlso  by design AI systems are susceptible to data changes over time Predictive quality degrades when a concept drift occurs (nice and simple definition here)Additionally data integrity issues in any part of a complex system containing models may cause major performance mishaps For example a 3rd party adjusting a data sources format or an engineer introducing a bug in a pre-processing pipeline may cause severe model underperformance and/or unexpected model behaviorSo how would data teams keep up? They need to monitor and continuously analyze model performance and take corrective actions Such actions include generating or acquiring better training data retraining models or redirecting business applications to more suitable modelsIn the spring of 2020 amid a global pandemic and an economic crisis data teams are on high alert and expect data changes in the real world to throw off their models However having built-in automated transparency into model behavior is even more critical in normal times when nobody expects the issues to ariseProduction monitoring is a well established discipline in software In fact the practice is so entrenched in modern IT that some refer to monitoring as the most fundamental need of a production system Naturally there is a healthy industry of production monitoring solutions with well established cloud providers such as the publicly traded Datadog and New Relic Yet data teams find themselves searching for more tailored solutions addressing the unique challenges of AI monitoringFrom one perspective AI systems are like Lego structures The building blocks are gradually becoming standardized but no two systems across two organizations are quite the same Data teams design unique AI systems to fit the needs of their business Therefore AI monitoring has to be highly configurable by the user The user has to be able to define/edit what is monitored what constitutes relevant anomalous behavior and when to get alerted on such behaviorFrom another perspective AI systems are like road networks Problems could originate in one place and manifest in many others Hence it is insufficient for AI monitoring to just alert on anomalies Automatic root-cause-analysis is integral to the monitoring Furthermore predictive models should not be monitored in a vacuum but rather in the context of the full AI system Perhaps quality degradation in one model is the result of a bug in a data processing stage that happens elsewhere in the systemFinally and as mentioned above AI systems have hidden biases that manifest in model underperformance for subsegments of the data When these biases explode into the surface they can cause business catastrophes (customer rage reputational hit revenue loss unexpected expenses of restaffing a team to investigate and fix) Therefore data teams cant settle for tracking average performanceWhat is clear is that when AI systems are in maintenance mode the risk of issues going unnoticed for some time is greater and the importance of an expert monitoring platform becomes even greaterThese characteristics imply that AI performance requires more sophisticated statistical modelingFirst the metrics Most commonly teams would track and review model precision and recall However what if there is no ground truth? No problem There is still much to track in order to assess model behavior over time For example the features and outputs of models We found that the highest performing AI teams track a whole spectrum of additional indicators starting with confidence intervals result counters (eg number of categories depth and breadth) and other descriptive metrics of the result and feature spaces (eg density of clusters correlations between classes)Second the aggregation and analysis We believe that best-in-class monitoring has to include looking into performance (for which the metrics are the proxy) in granular data segments For example to detect that a sentiment analysis model performs worse on shorter texts or in different parts of the country (with different slang and expressions)We believe that in order to implement AI monitoring successfully much could be borrowed from Application Performance Monitoring (APM)  a well established solution space In principle we would like to make it as easy for data scientists to monitor AI systems as it is for engineers to monitor infrastructure and applicationsSimilar to APMs  we believe strongly that production monitoring should be standalone tech stack agnostic and put practitioners in control (to plan and execute their own monitoring)  the latter is a fairly recent evolution in APMNevertheless AI monitoring should also deviate from APM practices primarily because AI monitoring requires more sophisticated statistical modeling To enable that AI monitoring requires more granular data collection and more sophisticated aggregation schemesSo weve built Mona to be a best-in-class AI monitoring platform that works with any development and deployment stack (and any model type or AI technique) is easy to integrate and configure and yet gives data teams an engineering arm  to be in full control of the monitoring strategy and executionThe founders of Mona hail from Google Trends and McKinsey & Company and we have begun to build a team of brilliant and passionate big data engineers and data scientists to bring the vision to lifeAbout a year and a half ago we ventured on a mission to empower data teams to make predictive models transparent and trustworthy and raise the collective confidence of business and technology leaders in their ability to make the most out of AI We are grateful for the support of our early customers our investors and advisors and are excited to continue the journey to fulfill this missionSpecial thanks to David Magerman and Ori Cohen for providing editorial feedback on this post
adR6ZqEePeEvZqjtdo2JKy,How we created a real-time reporting database from multiple sources leveraging cost-effective AWS services DMS and LambdaMoving data is nothing new However as the focus on data escalates so do the demands that businesses have on using data One such demand that is difficult to meet is real-time reporting We are talking real-time like within a couple of seconds Not all companies can throw teams of data engineers and devops engineers at it We couldnt; we needed a high bang for your buck solution with the resources we hadSpecifically here at Mynd we needed to find a solution to achieve the following: At Mynd we have built our own internal system on which we run all our operations This system named Otto is built on a services architecture within AWS Currently we are streaming a mix of roughly 20 different Aurora MySQL and DynamoDB data stores into a single aggregated Aurora PostgreSQL database The replication time generally occurs within milliseconds and during a large spike of new or changing data it can go up to a few seconds We achieve this by using Data Migration Services (DMS) and Lambda functionsWe chose Aurora PostgreSQL as our target database The key reasons are: Postgress ability to quickly ingest data and make changes to existing data (data being streamed is not immutable) perform complex queries across many different schemas quickly and its general usefulness to take on data warehouse-like workloads Additionally our domains are complex and require many joins and analytic (window) functions across schemas to produce the reporting needed by our operations Lastly we needed a database that is performant and can scale The AWS version of PostgreSQL with Aurora adds some nice performance enhancements and their management of replicas has made scaling easy for us If you go with Aurora PostgreSQL we recommend the following: Note: Aurora PostgreSQL isnt being proposed as an all in one data warehouse Focus on streaming the data which is time-sensitive and move other big data data lake and batched analytic functions to the more specialized data warehouses (Redshift BigQuery Snowflake etc) Typically with those workloads you can delay data and youre solving a different problemFor our service applications that use Aurora MySQL we leverage Data Migration Services DMS can be used for a one-time migration from A to B where A and B can be the same or different types of databases and it can be used for continuous migrations (streams) DMS works with many different types of structured databases We have used DMS for other flavors of SQL migrations outside of this specific use case with similar successConfiguration is relatively simple: These features of DMS have made it a great tool for us As our source changes our target stays in sync without having to maintain additional migrations and we leave the sensitive data behind since its not needed for general reporting usageFor the times when DMS does have a problem you need monitoring Since DMS is an AWS service it naturally integrates well with Cloud Watch For the metrics that arent available with CW we augment the logging with Datadog by using Python to look at DMS stats through the CLI Here are some helpful configurations and monitors weve found: We like DMS; however it does not handle unstructured data like DynamoDB Lucky for us we have data stored in DynamoDB which needs to be queried in real-time alongside our structured data We looked for a tool we could use similar to DMS which would sync our DynamoDB data stores into our aggregated Aurora PostgreSQL reporting database in real-time but didnt find anything suitable so we built our own lightweight tool To do this we used Lambda functions to move data from DynamoDB into our targetDynamoDB like many NoSQL databases has an event stream Additionally Lambda can receive these streams and trigger a process which takes all of these table events and moves them into our target database The stream data received from DynamoDB is ordered comprehensive (all table data) and specifies INSERT UPDATE and DELETE commands You can think of it as a very stripped down version of KafkaHeres some of the logic we used for this (which should work generically for most NoSQL to SQL streaming): Just like with DMS this needs monitoring to ensure your data is up to date and the homegrown solution is working Since we are using Lambda it again works well with CW and Datadog These two metrics are ones we watch carefully: With both DMS and Lambda we have a real-time single source of truth for reporting with minimal ongoing effort or cost Additionally we can granularly tune the performance of DMS and Lambda for speed vs cost when needed The schemas are automatically updated and the only coordination between teams is notifying when SQL functions need to be updated based on the new logic If a new data point is needed for a report an engineer typically only needs to add one line of code to the SQL function because the data is already available Currently we have multiple services using DMS with a latency of milliseconds and Lambda syncs with a latency of 1 to 3 seconds This low-cost low-maintenance solution is performing well for us with indications that it will continue to scaleKen Ostner is the VP of Business Intelligence at Mynd Property ManagementContact him at ken@mynd
ThXi93btJSqqi4T6odJmdq,"A data warehouse is constructed by integrating data from multiple heterogeneous sources that support analytical reporting structured and/or ad hoc queries and decision makingData sync between systems is very important when you have a lot of dependency between the main app 3rd party apps or the CRM tools Here in NestAway we use Salesforce to handle operations and we needed the data into our data-warehouse to do a lot of analytics and machine learningThere are many 3rd party tools available in the market StitchData Alooma Heroku etcWe already had a data pipeline to sync out databases to BQ based using Airflow Hence we developed our own data-pipeline using Airflow to get the data sync between Salesforce to BigQueryLets deep dive into the architecture to understand it betterSpring boot application responsible for pulling data from both MySql/PSql DB and Salesforce Apex HTTP APIs in a multi-threaded environment which stores and then upload it to GCS bucket in JSON file (new line delimited format) • Crane (A loader agent to BQ): Rails application responsible for loading operation from GCS bucket into BQ dataset by taking care of real-time changing schema for tables and handling multiple loading request • Apache Airflow: We use Apache Airflow to orchestrate our batch processes run regular schedule/cron jobs ML and data pipelinesSalesforce uses basic OAuth 20 authentication which enables secured login We used Forcecom REST API connector https://githubcom/jesperfj/force-rest-api as a dependency to connect and fetch data Using this you need not worry about the auth token generation or mostly refreshing the token after expiration it handles internallyhttps://companyvisualforcecom/services/data/v430/services/data/v43""To fetch let's say 1000 records from this we might need to hit this HTTP call probably 1000 times Insane!?💀 A better way would be to partition the data by date-time and fetch using Query formathttps://companyvisualforcecom/services/data/v43where Salesforce gives the data in a bunch of 200 records per request along with a URL of the next 200 recordsQuestion is now where you get the query params or the column names which has to be added to a query You can get it in describe call and get the required columns to be fetched""Let's say there are 10 SObjects to be fetched each having 1000 records Hippogriff runs in a multi-threaded environment where each thread is responsible to handle each SObject/table (see the architecture above for more info)Files are uploaded to a temp folder in GCS to avoid network latency and bulk upload failures Once uploaded to a temp folder they are moved to active_jobs in the same bucketUsing Airflow DAG we run our first task as KubernetesPodOperator to do this kind of resource intensive task which spawns up a k8s pod based on Docker imageMust read for this: https://mediumAirflow runs this task every 15min where it brings up the k8s pod (hippogriff poller agent) pulls the data from SF and records are further stored into JSON file with the new line as delimiter and uploaded to Google Cloud storageOnce uploaded the next task of Airflow DAG would be to call Crane (loader agent) which further starts loading files from one folder to Google BigQueryOnce data is present in GCS Data is loaded to BQ in multiple phases as below: 1) Making BQ equivalent schema based on SF-BQ datatypes mapping • Update the schema of tables in BQ if any schema change happened in salesforce SObjects • Use BQ load job service to load files to temporary tables in BQ by passing the GCS file URL and the updated BQ schema(You might wonder why copy data to temp tables in BQ instead of directly copying to the source table in BQ?)BQ Load Job service guarantees atomicity only at the file level not on a batch of files So assume we have a batch of 2 files belonging to 2 tablesWe always want all(in this case 2 tables) our tables to be up-to-date in BQ not just some of them • Copy all the temporary tables to source tables in BQ (eg: temp_lead to lead) and then drop the temp tables • Do any post upload tasks if needed (like sending batch success/failure notification creating views in BQ validation of data between BQ and source DB etcWe faced a lot of challenges during the process with the infra resources network etc since we wanted to migrate data between systems which is a very resource intensive task and to keep the pipeline running every 15min Following are some of them : Having data sync for Salesforce into BQ is very much useful for analytics and machine learning purpose Data pipelines sound easy but as you see it is pretty complex to develop Airflow is one of the tools which helped us in rapid development and made life easier for our day to day operationsAlso the core members of Data-Engineering team at NestAway: Rohit Pai and Sonali DaveHappy coding"
CQUZpiWbynAGrC4qV6SvHF,Humanitys fight against Coronavirus depends on the speed and precision with which we can take defensive action like social distancing as well as fight the disease through medical advancement Data can help on both fronts and a great example is this New York Times article that studied the origin and spread of the virus using location dataAt Nexla we are committed to helping for this cause We have been providing our data integration tools as well as our expertise to teams that are contributing to the global effort to fight COVID-19 In addition we are trying to help make public data more accessible and easier to use for researchers Please contact us if you need help with your research on COVID-19 We are providing pro bono help to qualified researchers in the form of our software by committing a portion of our compute capacity and teams timeAs in a typical data science or analytics process researchers are getting data from various public and non-public resources The first step for them in using this data is to create a pipeline from each data source into the data system (Spark Athena Snowflake BigQuery etc) where they will run their analysis models and algorithms The pipeline often includes significant and unavoidable work such as changing formats data structures handling errors and delivering clean data To keep the process running continuously for unreliable or changing data proves as it comes over time is even harderAn example of highly valuable research data being made available is the COVID-19 Open Research Dataset (CORD-19) provided by Allen Institute for AI One of the ways we are helping researchers is by making such public data available in a variety of different formats making it ready to use in the data applications of their choiceFill the form here to download* the CORD-19 data in Parquet Avro or JSONL formats *Please be sure to read the terms and conditions of the data providerHead here to fill a quick form and get access to this data: Continue reading if you would like to learn more or need help with additional data …Source Data and Data StructureThe source data here has the complete contents from medical research papers on Coronavirus The entire dataset is nearly 30000 files New files are being added periodically The data providers here have done the heavy lifting of converting information in research papers into structured data making it a gold-mine for researchers who can now apply various data tools to analyze Each research paper is expressed as a single JSON record contained in a single file This has the benefit of presenting the entire contents of the paper in an object structure with fields such as authors keywords sections references etc As a result all the information within each paper is well organized for analysis and cross-referenceFormat Conversion BenefitsSource data here is a conversion of unstructured data in the form of research papers into structured data as a JSON Objects Here are some of the benefits of the format conversion we have provided: We are here to helpWe will continue to proactively find and help with accessibility of public datasets If you are a researcher working on COVID-19 let us know how we can help you We are providing pro bono help to qualified researchers in the form of our software by committing a portion of our compute capacity and teams time Please contact us at support@nexlacomThis story was originally published on the Nexla blog: https://wwwnexla
VSR3SXz5C4DcWX2PB497cc,This article targets beginning to intermediate-level Angular developers and covers a wide variety of topics that arise in production applications While centered around the concept of editing tabular data these techniques may be used in a variety of other Angular applicationsFor anyone who has read at least one of my articles you should understand that my background is applied mathematics and scientific computing So this article continues the trend of exploring the use of Angular in scientific and business (analytics) applicationsWorking with time-series data is a fundamental concept in numerous business and engineering sectors In this context front-end development is largely concerned with minor transformations and display of data Concepts such as data grids tabular display and visualization with charts are quite familiar to front-end devs What is likely to be less familiar is the need to edit one or more values in a time seriesData often comes from physical instruments that have some degree of fallibility and/or manual entry that is subject to typical human error So at some point during your FE career it may be necessary to develop components that facilitate both the display and editing of tabular data Only the latter is discussed in this articleBefore continuing point your friendly neighborhood browser to this Github so that you can follow along with the project deconstructionThe data used in this sample project comes from an actual historical dataset on used-car sales from the book Machine Learning in R by Lantz For tutorial purposes suppose that all data in the table comes from reliable sources except mileage which is hand-entered in another application The code provided with this article simulates a use-case where someone with edit and/or approval authority visually examines a series of data to search for outliers That data is displayed in a table which contains an Input field in one column to support editing that particular item To make the demo more realistic the original data was hand-edited to insert a number of outliersAnd it would not be a project if we did not have some requirements! Every one of the following requirements was taken from an actual client application Ive worked on in the past • Display the data in a table with headers and data returned from a service • One and only one column is editable the car mileage This is hardcoded into the application and will not change • The table should be paged The number of initial rows and allowable rows for padding will be provided Allow sorting by date of manufacture as older cars should generally have more mileage • A user may tab between rows but indication of an edited value is via pressing Return Ive also been required to add a small button to the side of the input in actual projects but thats not required for this demo • User inputs are validated while typing Only numerical integer inputs (with no minus sign) are allowed If the user enters an incorrect character reset the input field value to its value when the user first focused on the field (or the most recently edited and valid value) • Inputs fields have a small grey border by default (color to be provided and not changeable) When the user successfully edits a mileage value replace the border with a green color (to be provided and not changeable) • Whenever the user navigates to a new page the input borders should be reset to the default value • Whenever a user clicks on a row whether they edit a value or not record that click and store the number of clicks on each car id to be returned to the server I actually had a client who wanted to do this to capture interest in a particular row of data ie they believed the click was indicative of interest in the data whether the user actually edited the data or not Okay well as long as the moneys there … I dont care :) • Capture whenever the user moves from one page to another so that we can potentially take action in the future Yes folks thats a common one … people want to do something but they wont know what it is until well into the future • Add a Save button Clicking on this button will send a record of all edited data to the server For tutorial purposes the button will be implemented but the handler only logs the edited data to the consoleIn an actual application a person with edit authority would perform the data editing and then after saving the data a person with approval authority would be responsible for viewing all data and approving the modifications This article is only concerned with the edit portion of the processEnough has been written on the use of Material and Material Table in particular that there is little benefit from adding a lot of explanation in this article Suffice to say that I personally prefer using ngContainer to create templates for each column The most important column in the layout provided below is mileage and there is a Material Input field that allows editing of the mileage valuesNote the inclusion of the Material paginator near the end of the layout/src/app/features/materialmodule/src/app/features/table-edit/table-editmoduleThis allows the table-edit functionality to be easily imported into any project including ours in /src/app/appmoduleAll data models (interfaces) for the application are in the /src/app/models/modelsts fileWhen the user edits car mileage it is necessary to record the id of the edited vehicle and the new mileage value which are stored in an IEditedData instanceThe main app component /src/app/appcomponentFrom this point the remainder of the application is handled by the table edit componentThe table edit component (/src/app/features/table-edit/table-edit/table-editcomponentts) employs an InputSelectorDirective to select individual Input fields/src/app/features/table-edit/table-edit/table-editcomponentA ViewChild of this Directive provides a direct reference to a single instance of that Directive applied to an Input field with the class editable This application however requires references to all such input fields in the current table page This is where ViewChildren and QueryList are used/src/app/features/table-edit/table-edit/table-editcomponentThe QueryList provides a reference to the InputSelectorDirective for all Input fields in the current pageSome programmatic support is required to interface with the Material table specifically a data source reference to the MatPaginator (paginator) and MatSort (sorting)That concludes the basic setup for this component In terms of logic a summary of relevant class methods follows to aid in your deconstruction of the applicationThis method is called whenever the mileage data is edited It first checks the argument and event id and then stores the edited data in the class edited-data RecordThis method is called whenever a user clicks on a table row which is taken as an indication of interest in that data whether it is edited or not Yes Ive actually had to implement this for a client in a real applicationThis is a placeholder for you to implement a service call to store the edited data should you wish to modify the code for use in a production environment The edited data is logged to the console to help visualize formatting of the edited-data RecordThis is another placeholder method in case you want to modify the application to perform some function whenever the user pages to another set of table dataThis method is called to validate a number while typing It defers the validation to the library method ValidationcheckNumber() which is useful for numeric entry of physical properties that must be greater than or equal to zeroThis method is executed whenever the QueryList of Input fields changes (ie on page change) The methods primary action is to reset the border color on all new fields Modify for additional functionality as you see fitSince the QueryList of InputSelectorDirective instances changes every time the user navigates to a new page of the table it is necessary to subscribe to changes in that listThe use of this method and onPage() provides a natural separation of the primary focus on Input field changes with any other activities that may be requested on page change The result is better focus on single responsibility between methodsThis Directive provides a collection of Output and event handlers to facilitate editing the mileage data in the table/src/app/features/table-edit/directives/input-selectordirectiveA single HostBinding to the border-color style facilitates changing the border color of each Input field based on whether that element is initially displayed or in an edited stateThere are two host listeners one for the focus event and the other for keyup When an Input field receives focus it is necessary to capture the current value and the id associated with that mileage value The former is used to re-populate the field with the initial value in the event a typing error is detected The id must be emitted along with the edited value in order to associate the edited value with a specific record of car dataThe keyup listener performs basic validation on the current numerical input for the mileage value A valid value on clicking Return causes the Input field to be colored green Input errors while typing cause the field to be repopulated with the last-known-good valueThis has been a long and somewhat involved deconstruction Scientific engineering and business-analytic applications often expose a much higher degree of interactivity to FE devs I hope this article and the supporting code has helped beginning- and intermediate-level Angular developers with their understanding of the platformEnterpriseNG is a two-day conference from the ng-conf folks coming on November 19th and 20th Check it out at ng-conf
2Tf3hUhToG3GvsvPyUzorM,Last April I joined the Data Engineering team at the NYC Department of City Planning Almost a year later we celebrated our 2nd birthday and welcomed a fourth team member on board For me the Data Engineering team is a tech hub full of excitements where we embrace open-sourced technologies and work to create transparent and reproducible workflows A big part of our work focuses on designing and building data products to meet the business needs of different internal and external data usersThe first data product I helped to build is the Facilities Database which includes all the facilities owned operated or licensed by a city state or federal agency with geometries and categories created by us The City Planning Facilities database has been around for over 20 years and it is used for neighborhood planning fair share analysis and facility sitingFor a very long time it was a big headache for City Planning to maintain this data product for several reasonsWhen the Data Engineering team set out to refactor the Facilities Database we spent several months brainstorming and iterating the workflow Finally we successfully addressed all the above inefficiencies and published the December 2019 version of the Facilities Database on Bytes of the Big Apple In this latest update we applied a brand new data infrastructure that is 100% cloud-based as well as improved the workflow of several processes namely data loading and geocoding This upgrade is undoubtedly successful since now we are able to produce the Facilities Database within five minutes on any machine at any timeLets break down what weve done to make this happenKnowing how troublesome it is to intake 50+ data sources manually each time we set the goal to prioritize using open data when we started refactoring the Facilities Database If input datasets were not published or up to date we would connect actively with various data owners/city agencies and ask them to update their datasets on NYC Open data or other open data platforms on a reliable frequency with a consistent URL provided In the end we were able to extract 40 datasets through open data URLs without manually maintaining them It might require a lot of communications and long waiting at the beginning but in the long run we believe it will foster engagement within the open data community as well as save us a lot of overhead in accessing source dataAfter intaking all the datasets we are ready to do some data munging In order to do so we just need to ingest all datasets in a building environment develop ETL pipelines for each of them and get an output table It doesnt sound very complicated right? But the reality is much tougher than this In order to refactor the Facilities Database we needed to iterate the building process hundreds of times to get the perfect output table we all satisfied with During the time one of the source schema or URL might change which would undermine the downstream ETL completely and take up a lot of time to fix Consequently the iteration process got to hold upTo address this problem we created a PostgreSQL server on Digital Ocean a user-friendly cloud service provider to store all relevant source data before doing any data munging We call this PostgreSQL database Recipe where we store the current and historical versions of different source datasets InRecipe each dataset is given a designated schema where the name of the table reflects the date the dataset was inputted into the database On top of that each schema has a view called latest which points to the most recent version that got loaded in With this data buffer Recipe we can queue up all the source data in a centralized place before running the ETL pipelines which allows us to constantly iterate the building process even if one of the source data changesTo make this happen we data engineering team applied an open-sourced library called GDAL which allows us to load datasets in different kinds of formats such as CSV shapefile and GeoJson into a PostgreSQL database at high speeds In addition the column names are normalized during this process so that standard naming conventions can be expected in downstream processesLater on all the source data are loaded from the Recipeinto another PostgreSQL database Build Engine where we process the data and create the final output In the end the output datasets are exported to a third PostgreSQL database Publishing where other teams can get the data and use it in their applications or publish it on our open data platform Bytes of the Big AppleNow weve figured out a more efficient way to store datasets and transfer them from one database to another but we still needed to develop a solution to make this workflow reproducible In the previous update one of the biggest roadblocks we encountered is that we couldnt install all the required dependencies we used in the past due to unavailable old versions or different computer environment settings This could diminish the whole data pipeline weve built For this reason we brought in Docker container a virtual machine where you can run your ETL scripts whenever and wherever you wantLearning how to use it is much easier than you may think Lets say you want to load a dataset from its source URL to the Recipe the database that contains all of our archived source data In this case you can either install all the required dependencies such as gdal and postgresql-client-10 to your computer or you can simply download a Docker imagecook which has all the dependencies pre-installed somewhere in the cloud With this Docker image on your computer you can spin up a virtual machine (technically a Docker container) by executing one command-line in which you can replicate the same data loading process at any time You can read the following article if you would like to learn more about the Docker images and Docker containersOur data engineering team uses Github Repository to maintain different types of Docker images where we specify all the dependencies that need to be installed in a Docker file This file is used to create a docker image on Docker hub where anyone can access itWe also took advantage of Docker containers to automate our geocoding process GeoSupport allows you to get the geographic information of any address in New York City as long as it has a valid house number street name and borough code (or zipcode) This software is very reliable and fast and we use it to geocode all of our data products such as the Facilities Database More precisely we utilize python-geosupport the python bindings built on the GeoSupport Desktop which enables you to call this software using a python script To make it more accessible we built a Docker image python-geosupport based on it After applying this Docker image as well as leveraging the multiprocessing technology in our workflow we are now able to geocode one million records in 10 minutes You should check out the following article if you are interested in using python-geosupport to do your own geocoding projectsSo far weve utilized the above data infrastructure and workflow on the majority of our data products such as Facilities Database and PLUTO We look forward to keep improving this framework and we are about to incorporate Github Actions a continuous integration tool to streamline the workflows of all our data products
QoepgodLn4Cy4kL67P4DJC,At NYC City Planning I was looking for a tool to automate a data pipeline process we had built When I discovered Airflow I thought it might solve my problem but I wasnt sure I couldnt find a clear explanation of what it was that didnt include technical language This is my attempt at a simplified primer of AirflowAirflow can sound more complicated than it isFundamentally Airflow is an orchestrator of a sequence of tasks It was designed primarily for tasks that move analyze and transform data Most tutorials will assume you will be using it for this purpose But it could also schedule a tweet to your mom on her birthday and or call random pay phones Airflow doesnt careAirflow does this by giving you a language and templates to define sequences of tasks These tasks can be any number of things a computer can do running a script querying a database sending an email waiting for a file to appear on a server Airflow manages when these sequences should run what order to run the tasks in each sequence and what to do if a task fails It also manages the resources necessary to run these tasks scheduling tasks as computing resources are made availableAirflows primary responsibilities are: Its helpful to think of Airflow like an air traffic controllerAn air traffic controller manages a finite amount of resources (airspace and runways) orchestrating sequences of tasks (flight paths takeoffs landings and taxiing) that depend on whether other tasks have completed successfullyAirflow like an air traffic controller keeps detailed logs of each of these sequences: what command was given how that command was executed and how long it took And if tragically there is an error it knows if it needs to try again or to notify someone else for helpWell come back to this metaphor laterAirflow uses several terms that can be unclear or jargony to new users but are actually straightforward concepts The words can be strange at first but well flesh them outThe four key terms to understand are: a DAG an Operator a DAG Run and a Task InstanceLets continue with our air traffic metaphorAir France flight 11 between New York and Paris is a flight route defined by Air France It is in essence a sequence of tasks that gets an airplane from JFK to CDG Air France defines how often this sequence should be run In the case of AF11 its run dailyA flight route is made up of many different tasks: taxiing to a runway taking off raising the landing gear navigating to waypoints etc Some of these tasks can be run in parallel some in sequence and some depend on others before happeningAn Airflow DAG is like a flight route It defines a sequence of operations and how often they should be runIn Airflow a DAG is made up of Operators Operators define the individual tasks that need to be done They can trigger bash commands run SQL on a database transfer data between systems listen for changes on a server or even send an email or Slack message Airflow comes packaged with several built-in Operators and even more are available through open source librariesTo recap: a DAG is made up of Operators and together they form the blueprint of a work flow The DAG defines the sequence and schedule of operations the Operators define discrete tasks that need to take place within this sequenceSo what happens when a DAG is executed? It becomes a DAG RunA DAG Run is what Airflow calls an executed instance of a DAG Airflow triggers this execution based on the schedule defined in the DAG (hourly daily weekly etc) Once triggered Airflow orchestrates the execution of the Operators in the correct order assigning available computing power to any outstanding tasks that need to be completedAirflow calls an executed Operator a Task Instance (This is an example of where Airflows naming conventions could be much clearer) A Task Instance is the most granular concept in Airflow It represents an attempted operation at a certain time with certain parametersReturning to our air traffic controller metaphor Air France Flight 11 on Monday September 25 2017 took off at 10:05pm EDT and landed in Paris the next day This flight followed a sequence of operations defined by the airline to get a plane from JFK to CDG AF11 on Sept 25 is the DAG Run to the daily AF11 flight routes DAGThe four core concepts in Airflow are: An Airflow DAG is a defined sequence of operations An Airflow DAG is a Python script that defines what should be run how often in what order (in sequence or in parallel) and what to do if an error might occur (If youre interest in the maths behind DAGs theyve been showing up a lot these days)Operators are the blueprints for individual operations A DAG is made up of one or many operators These define the individual tasks that make up a DAG Airflow defines three types of Operators actions transfers and sensors and provides many built-in operator classes to interact with common databases and other systemsA DAG Run is an executed run of tasks defined by a DAG A DAG Run is logged including when the run began exited and if any errors occurred along the wayA Task Instance is an executed Operator Its log contains the exact command that was given to a server at what time and the detailed output of what occurredAirflows primary strength is that it is language and technology agnosticIt doesnt care what you trigger This means you can have scripts written in whatever languages make the most sense for your workflow All that matters is that these scripts are granular (they only do one thing) and that they return a success or failureAnother strength is Airflows configuration as codeBecause Airflows task definitions DAGs and Operators are just Python it gives tremendous flexibility in how to define a workflow For example it allows a DAG to dynamically add new Operators based on files in a directory This makes it possible to define a DAG that dynamically creates new tasks when additional scripts are added to a folder or for several DAGs to import a collection of Operators defined in another Python fileAirflow is a fast-moving project No doubt some of these concepts may change or be renamed in the coming years Confusing terminology aside its flexibility and active open-source community have proven to be very valuable Within a few weeks I was able to convert an existing data generation workflow to use Airflow
NfDkXQzjsQmuqmmr6V9zTT,NYC Plannings digital teams provide data and digital tools to planners and the public We build and maintain foundational datasets used to understand the state of the citys built environment The software we design makes this data accessible and understandable bringing insight into a growing city Now were growing tooWell gather clean and regularly update hundreds of datasets collected from a variety of agencies on topics from transportation parks pollution and schools Well build data and application infrastructures that are maintainable and adaptable Well sit with city planners analysts and technical experts understanding their work and translating their needs into wireframesJoin us
6W2AiAxBGwyCyZrdMNQary,"My team recently migrated our data warehouse at Omio from AWS Redshift to Google BigQueryThere were several compelling reasonsOur data pipelines have grown significantly in the last few years But this has also created an unwanted amount of legacy code through our data infrastructureChallenges notwithstanding we were able to successfully complete the migration in two months The remainder of this post describes our approach and learnings in detailWe decided that a key step in this process was to first move all the tables from Redshift to BigQuery in order to allow our end users to migrate their downstream jobs/queries seamlesslyOne of our engineering teams had earlier built an excellent tool in Golang called Blueshift to create incremental replicas of tables from Redshift to BigQuery Blueshift takes a yaml configuration file with details of the source and destination tables and triggers a Google DataFlow job to copy data Blueshift also watermarks the data it has copied - this enables it to copy data incrementally in subsequent runsSince there were 100s of tables to be copied we wrote another small Python tool called Shifty to automate the generation of Blueshift config files for all our tables The copy jobs were orchestrated in Apache Airflow each morning when the latest data got updated in RedshiftThis approach allowed us to make all the tables available in BigQuery even though our batch jobs were still running on RedshiftThis is where the fun began To migrate the batch jobs we needed to port the SQL code from Redshift to BigQuery This sounds easy but there are several differences between the SQL syntax and functions supported by Redshift and BigQuery So each query needed to be carefully rewritten without breaking anything or introducing bugsLets take an exampleLets imagine a simple batch process with 2 transformation steps SQL-1 and SQL-2 SQL-1 reads Table-1 and Table-2 transforms and writes the output to Table-3 The next step SQL-2 then reads from Table-3 Table-4 and Table-5 and produces Table-6 as the final result²Notice that in this pipeline Tables 124 and 5 are read-only Tables 3 and 6 are written (updated) by the transformation steps This means that in order to test this pipeline we need to test data in Tables 3 and 6 against expected valuesEach of these functions can be considered as a separate step that can be developed and tested in isolation Whats more these steps can be developed in parallel Lets see howFor the first transformation step SQL-1 we can proceed to rewrite it in BigQuery by reading its inputs Table-1 and Table-2 from Production schema (which are copies of Redshift tables See Step-1) and writing to a Development schemaIf the new output (DevTable-3) exactly matches its Redshift cousin (ProdTable-3) then we can be confident that our migrated code in SQL-1 is working fine This is exactly the idea behind characterization test as proposed by Michael Feathers and others as a way to safely change legacy codeWe decided to strictly adopt a lift and shift strategy in order to minimize the vector of change This meant that we avoided the urge to refactor or optimize the SQL logic even in cases where we suspected it to be redundant or unoptimized It was important that we focussed on one problem at a timeNotice here that for this step Table3 is read-only and therefore we use ProdTable3 and not DevTable-3 This is key in isolating these transformation steps so they can be unit testedBy isolating the 2 steps this way both can be developed and tested in parallelOnce the individual SQLs are unit-tested its time to integrate themNotice again that this time we are now using DevTable-3 as input to SQL-2 The final output of DevTable-6 should still be compared with the Prod version to make sure everything looks goodAt this point if the new data exactly matches the Prod data we are ready to replace Dev* tables with Prod* tables in the pipeline The migration of this pipeline is complete""To make sure we didnt break anything in the migration process thorough data testing was needed We wrote a small tool in Python called wasserwaage ( German for a carpenter's spirit level) which was essentially a wrapper around Python's excellent datacompy module This tool helped us compare snapshots of data between Prod and Dev tables and printed even the smallest differences that it found between two datasetsIt was also important to test our migrated pipeline for a certain period of time and continuously compare the output with the baseline We relied on Redash for this and built a set of dashboards to compare daily aggregated metrics from the new pipelines with the corresponding production valuesSince we had already copied the tables (Step-1) we were able to simply replace these copies with the new versions in production without impacting downstream users³ as soon as an ETL was ready to be deployed This allowed us to continuously rollout finished ETL jobs whenever they were ready"
GnwfRYhz6vCyEdwAaPzbfN,How can we prepare for disaster? If a server crashes a technician unplugs a machine or a meteor hits a datacenter how can the business recover crucial data? In this tutorial we demonstrate how to backup the Oracle Cloud Database with Oracle Database Cloud Backup Service RMAN (Recovery Manager) and Object Storage Here Oracle Database Cloud Backup Service enables connection and communication with Oracle Cloud Infrastructure (OCI) while RMAN executes backups of your Oracle Cloud Database to OCI Object StorageNote: there are many different ways to backup an Oracle Cloud Database this is meant to represent one optionNote: given the diversity of customer environments and the ever-changing landscape of Oracle Cloud Infrastructure please take the commands and their explanations listed below with a grain of saltBefore we get started here is a link to all the pre-requisites needed to use the Oracle Cloud Backup Service I will try to provide as much context to each pre-requisite and provide helpful links and examplesNote: If you have downloaded the Oracle Cloud Backup Service on a different machine than the one which hosts your database you can use this command to copy your key from your local machine to your database host instance: Note: To SSH to the virtual machine which hosts your database: I am doing this a bit backwards and providing a list of commands for installing the oci_installjar file before detailing the pre-requisites for the command I am doing this because I wanted to provide a nice flow of commands If you like more detail about the commands or are running into issues simply reference the pre-reqs listed in the next sectionNote: If you have executed some the commands listed above no need to repeat themNote: depending on the version of the database you are installing you may need to copy files from one directory to another within the database instance To do this I recommend accessing the root user copying files and changing permissions for a file: Note: another option is to log in as the instances Oracle user and run the commands above To do this you can enter the following command after you have SSHd into the instance: To execute the jar file that will install the Oracle Cloud Backup Database Service we need to gather some information from our Oracle Cloud Tenancy: Note: OCID is a unique value which identifies specific components of OCI like user compartments tenancy and services Learn more about OCIDs hereNote: simply type the following if you want to create a folder in your current working directory: Note: simply type the following if you want to create a folder in your current working directoryHere is the GENERIC command that installs the Oracle Cloud Backup Database Service: Here is an EXAMPLE command: Note: Ive spaced out the commands so that it is easier to understand but when executing these commands in linux they should all be in one line separated by a spaceNote: if you are getting errors here is some documentation that helps diagnose some common issuesWe can validate successfully installing the jar file by checking if wallet and lib folders were created and if we see a config file which contains the OPC_HOST OPC_WALLET OPC_CONTAINER OPC_COMPARTMENT and OPC_AUTH_SCHEME valuesNote: To open and read linux files I recommend installing nano by typing the following (assuming you are in a directory with access to the config file): Now that we have successfully installed Oracle Database Cloud Backup Service and established a connection to OCI lets setup RMAN (Oracle Recovery Manager) to manage our database backup to object storageNow we are ready to configure RMAN for backing up the Oracle Cloud Database to object storage Simply enter this command to set/authorize RMAN connection to the database: By this point you should see a lingering RMAN at the bottom of the terminal and a message above stating that you have successfully connected to the databaseNote: Here is some additional documentation and another source that is helpful for logging into the database using RMANNext we need to configure some RMAN parameters before backing up the database: Some additional commands: Note: details about the commands listed above and other options can be found hereFinally: At this point the command should result in the database being backed up into the object storage defined earlier To confirm the backup is successful simply check object storage for database content and metadata files along with other related objectsAs an aside to connect to the database tables here is a link to some documentation More information about connecting to a database Below is a command that accesses the database tables through SQL PlusThere are other ways of backing up an Oracle Cloud Database why choose this option? I believe this provides find-grained control and the ability to set a cron job for scheduling the backup With object storage involved the database can be backed up to another region where the database is not running to provide high availability during a disaster Nevertheless there are automatic backups available in the console the Oracle OCI Python SDK and a number of other options to backup your database I recommend considering the use case RPO RTO and other factors before choosing this backup solution as it is tedious and time consuming to implement
3J6oTpvfWVG2wZcz3jpnmx,Have you ever found a doctor on your health insurers website only to call their office and be told that they dont actually take your insurance or they no longer practice at that office? You might think that posting accurate information should be as easy as uploading a phone book-esque spreadsheet but even after decades of publishing their doctor directories health insurers still struggle to do it wellSearching for a doctor is a universal pain point in interacting with our health care system When you search for a pediatrician an urgent care clinic or an orthopedic surgeon near you youre searching the doctor directory An insurance companys doctor directory is the list of all the doctors (and places like urgent care clinics and pharmacies) in its network along with details like their office locations phone numbers specialties and credentialsThe doctor directory is one of Oscars core data assets In addition to the search tools our members use when trying to find a doctor on our website or mobile app it gives our Concierge Teams the details they need to help members when they call with questions It feeds our claims system so we know which primary care doctor to pay to cover a members annual physical Finally it informs how we construct our networks so that our members can see top-notch doctors across all the specialties they might needBuilding and maintaining an accurate current and useful directory is a complex data and technology problem that Oscar has worked to solve Below I discuss some of the technical aspects of how we build our directory and what makes it challenging to do wellThe data feeding the directory comes from many sources: health systems and doctor groups; national groups of facilities and labs and urgent care clinics; and third-party vendors who curate specific data like credentials and certifications Every week we receive 50 100 different data files across all of these partners Currently these feeds come mostly from our partners in our three 2017 network states (New York California and Texas) As we expand in 2018 into Tennessee Ohio New Jersey and new areas of Texas and California our data integrations will grow accordingly These integrations which we call rosters (yes we think of our doctors as akin to star athletes!) contain basic information about each doctor like their name ID number specialties and office locations as well as more detailed attributes like what languages they speak their board certifications and licensing and occasionally even their headshots This data most often arrives to us as a CSV file but its common for us to receive this data in Excel (*xlsx) spreadsheets over email or one of a couple special provider data formats (eg the fixed-width PNDS format) Sometimes the data included in these files represents a simple change to the system (eg add these providers to the network) but often it represents a snapshot view of the world (eg here are all the providers we have) The CSVs are small relatively speaking usually in the hundreds to thousands of rowsAt this point most would be thinking Integrating small CSVs of doctors? Seems pretty easy But of course its not easy Some high-level complexities are: There is no single standard data format for provider data All components including delivery (file REST) form (CSV fixed width Avro etc) schema data formats (MM/DD/YY or YYYY-MM-DD?) and even semantic codes (specialties languages) come in different shapes and sizes Sometimes important data like the ID number last or first name TIN (Taxpayer Identification Numbers) or specialties is missing for no clear reason Sometimes its missing for a good reason; for example it might be understood that all doctors in the roster have the same TIN or specialty Other times while all the necessary information is includedthe data file is formatted incorrectly causing fun hiccups like a column of numeric fields being interpreted as numbers which results in a truncation of its leading zeros thus invalidating the data We get thousands of 4-digit zip codes and 8-digit SSNs every weekOther common issues include: If we built a hospital systems data infrastructure from the ground up today many of these issues would be easy to solve (at Oscar were big fans of opinionated data formats like Thrift Avro and Protobufs) But many of the systems of record are decades old and thus difficult to change; they have many many dependencies that span many groups throughout a hospital organization so necessary changes require tinkering with the entire infrastructure that must remain stable In an ideal world there would be a single standard format for provider rosters but standards are often hard to wrangle We are moving in that direction but its a long way off as many different parties have to coordinate to get thereIn-network doctors have contracts with health insurance companies to offer their services to at lower pre-negotiated rates Plans like Oscars will only cover care received with in-network doctors and facilities  -except in emergencies  -so its critical for Oscar members to have accurate and up-to-date information on whether a doctor is in-networkEven if Oscar did receive squeaky-clean data in a beautiful fully-specified form wed still have to determine network status for each doctor But the status isnt strictly defined on a doctor-by-doctor basis Network status is also determined by a combination of many things including each doctors group and practice affiliations specialties office and TINTo further complicate this assessment we also need to know a doctors past and future network status When we get a claim today for an appointment from January 15 2017 we need to know whether that doctor was in network on January 15 regardless of whether theyre in network now This network status needs to be bitemporal and it is determined by the contractual agreements we have with our providers These contracts are different between every health system and insurance company so there will never be a single source of truth for our systems to use to pull network statusLets say we have a pediatrician who used to be in network in 2016 but is no longer in network in 2017 We obviously dont want to show that doctor when members search for pediatricians near them But if a member messages their Concierge Team we need our team to clearly see that the doctor is explicitly out-of-network (rather than just missing) so they can better help the member find an in-network pediatricianWe may get data about a doctor from many different sources Maybe they are employed by one hospital system and have admitting privileges at another (so we see them on both hospital systems rosters) We may get their board certifications from one vendor and their licenses from a different vendor We need to determine first whether these four data points represent the same doctor and then merge them together to present a single consistent provider to the outside world This is an entity resolution problemThe NPI a unique identification number for covered health care providers gets us most of the way there given that it has a Luhn checksum and thus makes transcription errors detectable But different NPIs exist for individual and group insurance which causes confusion Sometimes systems will accidentally give the group NPI in place of the individual Other times theres a transcription error (eg a leading zero gets dropped somewhere along the way) Its even more tricky for networks of facilities like urgent care centers which may share the same NPI because they belong to the same corporate entityBut lets say through the NPI were able to determine that provider A from one source is the same as provider B from another Awesome! Now we get to resolve all of the child entities (billing group offices specialties education etc) of A with all of those with B Is office 1 from A the same as office 1 from B? We use multiple address normalization and geocoding APIs to help us with this task but ultimately 123 Main St Suite 10A is a different address from 123 Main St and even geocoding algorithms can have some drift such that an addresss geocode from a year ago may be very slightly different than that of the same address today 123 Main St Suite 9A may have the same geocode as 123 Main St Suite 10A but they are clearly different offices Or are they? Maybe its just a typo Offices are the hardest of all the entities we reconcile for this reasonWere far from perfect but over the past few years weve developed a set of internal standards to make our Doctor Directory as robust and accurate as possible A few approaches we take are: We have a number of plans to improve our provider data management system over the next 12 months including: Weve learned the challenges of managing provider data through many many hard knocks over the last four years Its difficult But thats what makes it fun Its also solvable This needs to be a collective effort meaning it wont be any one format library team or company that solves this problem and it wont happen in the next year  maybe not even the next five But collectively through a lot of hard work well get there together and our dysfunctional health care system will run a little bit better
ivPA9aCvSyqEudJ85CekwC,How to deal with the new big goal in the software worldHuman is the only animal that trips twice on the same rock After years of talking about breaking the monolith in services we have done the same thing again: data monoliths (aka data lakes and data warehouses)I have been working on many projects (in different companies) for the last years and I have seen that the problems the data monolith causes are similar: Besides that data monolith is in fact is a monolith and we understand a lot of bad things (and some good one) this involves; so we dont need to explain how this stuff (coupled services problems releasing new features hindering continuous deployment messy test strategy etc…) could be a problemThe actual way is to put the focus in our services and then by an ETL get the data the services/sources produce and after some transformations put the data wrangled in a place where later it will be used/served (data marts API BigQuery model pipelines etc) These processes are made inside data pipelines and as we said before we can difference three clear parts (ETL): The idea of this decoupling is to break this pipeline in some pipelines by domains or well by services in the same way that we do with the code servicesSo what if instead of having only one pipeline each service had its own (or more) that would process its own data? So each service should be responsible for clean transform and share their own data in immutable datasets as productsWhat can we win with this change? First of all we don t have to forget that there are a lot of changes in the different services Here is when the problems start because or we forget to coordinate with the data people about the changes or this coordination is not possible at this momentSo in summary each domain prepares its data put it as a product in a messaging broker from where other services streaming tools governance analytics data scientists tools (as jupyter notebook) machine learning pipelines a global storage and much more can achieve themBeyond this I am sure that if you now you are doing nothing with machine learning you will do So think about it is it possible to make a good model without having good data? If you want to do a good job in ML you probably need a model pipeline also by domain and it will need this data pipeline If you want more information about this topic you can read this another post I wrote: continuous deployment in machine learning systemsSo we are going to break the data monolith and we are going to have this by domain but actually teams have no time to create each its own data infrastructure To resolve this problem companies have to put the focus on creating a common data infrastructureIn the data mesh article from Martin Fowlers blog (written by my ex fellow in Thoughtworks Zhamak Dehghani) they write some capabilities this infrastructure should have some of them: scalable polyglot dig data storage data versioning data schema data lineage data monitoring/alerting/log data product quality metrics data governance security and compute and data localityWith this data platform teams (developers and data scientist) only need to take care about what things they want to do with the data they produce and how are they going to serve this data to other teams like now we are doing with CircleCi or Jenkins (teams only need think about how they want to make its CI not the infrastructure) For instance in this image Metaflow explains the infrastructure from the data scientists point of view: And the next image (from the data mesh post of Martin Fowlers blog) could be a possible final status In this schema you can see the different domains with their own pipelines and the transversal data platformWell first of all we love code and for years we have been improving the best practices in code and the best methodologies and all the techniques that XP contains So lets use code also in data and all the best practices we knowWe could use a lot of tools and frameworks to make our data pipelines with code In the Engineering team of Packlink we love GCP so in this post I am going to explain this stuff with Airflow because the Google platform has a click and play and fully managed workflow orchestration built in Airflow: Cloud ComposerRemember that the important points (the same as in services is the same if you use CircleCI Jenkins etc) are the ideas the tool is only a way to achieve our aims Other good tools/frameworks are Metaflow Luigi Prefect Pinball At this moment every company is building its own framework and all of them different but insisting the point is the idea and that each of them is code In the next sections I am going through: I am going to assume that you have a data monolith right now If not if you have a greenfield maybe this chapter is not interesting to you but sorry we are talking about break the monolithWe have the data monolith Which is the best strategy? Not seem that make a waterfall and put the whole infrastructure at once is a good idea so to achieve our objective we are going to use the Strangler pattern This is a typical way to break the monolith in legacy systems and as I said before we have learned a lot and we want to reuse this knowledgeThis pattern is good to use because; we have a lot of things running well and we want to do this work in a useful and agile mindset in these three steps: So to start we can choose the most simple domain and build a new data pipeline then make a lazy migration co-living both systems and when we are secure that everything is running well delete the legacy partFirst of all we going to continue trending everything as code so data pipeline tooWith the strangler approach and in the first iteration the logical is reuse part of the logic we have done before and we are going to do it with TDD as XP and our common sense sayIf you are thinking about how I am going to explain a simple method to achieve it in a fast way Besides that you must decide your strategy in the long termIf you know Airflow you know that there are multiple operators (python_operator http_operator bash_operator mysql_operator and a long etcetera) depending on the kind of code you want to put in every DAG These operators have some problems The worse to me is the complex and ugly way of test them furthermore if you are using Python you will have listened to something about dependency mess With these operators you need to install all the dependencies in all the project doesnt need a different version of them and use the same version of PythonMy solution is to use while we are doing the strangler pattern the docker operator kubernetes_pod_operator If you use Google Composer you should use the Kubernetes one and never the GKEContainerOperator as this can lead to resource competition With these operators the code that will be executed in each DAG will be into a container (there can have python code scala java Beam processes Spark processes Kafka streams summarising whatever you can imagine…) so you can use your TDD in the language you want and you could reuse some of your legacy codeWe still need to write the own Airflows code so we will do TDD also to do it It will be the definition testing part included in the unit testing of the Airflow part Before showing the code I want to thanks Chandu Kavar and Sarang Shinde for the how-to posts about testing strategies in airflowSo like we do TDD first the unit tests: This is the pipelines production code that we have generated with TDD: But unit testing is not enough in Packlink Engineering know that tests are the most important part of the code So lets go with more testingNow is time for integration tests in the airflow part (the containers have their own unit and integration tests) With this approach the integration tests here are related to the Airflow config and XComs which in summary are the way in airflow we have to share information between tasks in a DAG In our example we need to test that the info saved in packlink_hello_task is the one taken by stream_data_task: The last part of pyramid testing is e2e tests In the pyramid e2e tests are always on the smaller side because they are very expensive In Airflow maybe even more You can see a good example here: https://githubcom/chandulal/airflow-testing I couldnt have done it betterThis brings me to the end of the post
G8u439F5e6EVNgX7GPADk8,If you have visited Palantirs website recently you have learned that our platforms let organizations manage data like software engineers manage code Collaboration tools like Git play a major role in the software engineering tool chain: Git commits establish code versions and Git branches isolate developers and features into their own sandboxes In this blog post we are going to explore why dataset versioning and sandboxing are critical to collaborative data engineering in full analogy the importance of Git workflows for software engineersThe starting point of our discussion is a simple request that we frequently get from our customers: can we export our datasets from Palantir Foundry to our existing data lake or S3 bucket? While this is of course possible it is important to understand that such exported datasets lack precisely those versioning and sandboxing features that make Foundry a great tool for collaborative data engineering We will see through the Git analogy that the (counter-factual) absence of these features can easily lead to software bugs in the software engineering case or data inconsistencies in the data engineering caseToday any non-trivial software code base is hosted in a source management system for instance Git In a nutshell Git provides to developers a versioned file system with virtual sandboxes: different Git commits correspond to different versions of the code base (ie the source code files) and Git branches allow developers to implement their features or bug fixes without interfering with other developers workNow in analogy to the export Foundry datasets to S3 request mentioned above lets consider what it would mean to export a Git repository to a standard file system without Git functionality For instance we could run a periodic script (maybe triggered by a Continuous Integration workflow or by a Git hook) that checks out the latest commit on the master branch and then copies its source code files to some other directory say /home/rfink/git-export/It is immediately clear that such an exported Git repository cannot support branching workflows simply because a standard file system does not support branching More subtly the exported repository suffers from write/write conflicts in case multiple users try to modify files concurrently For instance if two instances of the export script were to perform two concurrent exports wrt two different commits then the export directory will likely represent a non-deterministic combination of the two code versions The exported repository is thus strictly a single-user environment as far as write workflows are concernedMaybe most surprisingly the exported code directory does not even represent the source code faithfully for read-only workflows: if a user tries to compile the source code while the export script is running (and thus while the exported snapshot of the source code is being updated) then the compiler will likely see an inconsistent mixture of multiple code versions In other words the export is not atomic In best case the compiler will fail (maybe with a syntax error) but in a much worse case the compiler may succeed but produce an incorrect program (say with a race condition that vaporizes money) as a result of compiling semantically incompatible source code filesIn order to use the exported directory safely we need to synchronize all access with a read-write lock in particular preventing concurrent access of one writer with one or more readers or writers (Multiple concurrent readers are OK as long as there is no concurrent writer) File systems typically dont provide such locking mechanisms across files and directories and thus the exported code directory isnt actually all that useful compared to the Git repository It is then not very surprising that standard code export mechanisms for instance Java source JARs exported to artifact repositories (eg JCenter) store immutable artifacts: they get exported and written once named uniquely by commit hash and then wont ever change again This is one way to implement a read/write lock Now lets turn our attention to data engineering workflows and apply what we learned above Just like Git versions code Foundry versions data: Foundry datasets (and thus all files contained in such datasets) are versioned and support branching as a sandboxing mechanism This means that Foundry datasets are safe for concurrent access: since each dataset version is immutable (just like Git commits are immutable) different users can read a dataset at a particular fixed version while some other user updates the same dataset; the changes performed by that user (eg adding or removing data changing the schema etc) are stored as a new version¹ and thus dont interfere with the read workflows of other users Different users can even write to the dataset concurrently if they each write to their own branchThe versioning and branching functionality is particularly important for data pipelines A data pipeline is a sequence of data transformation steps that turn some input datasets into one or more refined/filtered/enriched derived datasets Since the output datasets are versioned we can safely schedule such transformations jobs to update datasets across the data platform without risk of data corruptionNow what does it mean to export a Foundry dataset to S3? Well just like Git repositories can be exported to a plain file system Foundry datasets can of course be exported to an S3 bucket (In fact behind the scenes Foundry datasets are typically already stored in an S3 bucket just like Git repositories are usually already stored in a file system) However by exporting a dataset from a versioned file system (Foundry) to an non-versioned file system (S3²) the exported dataset suffers from the same anomalies we described above in the case of Git: Just like in the source code case we can solve the concurrency problem by exporting immutable snapshots of the dataset for instance into timestamped directories in the S3 bucket Alternatively we could write an atomic LOCK file whose existence tells consumers that the directory is not currently safe to read Both approaches require the reader to understand and observe the mechanism: in the first case the reader needs to continually update its source folder reference to the latest available timestamp (ie it is no longer a simple flat export) in the second case the reader would have to observe the LOCK file when reading³A more sensible approach is to use Foundrys Hadoop FileSystem implementation in lieu of the standard Hadoop one and then interact with S3 or Hadoop-backed Foundry datasets directly Our FileSystem implementation is a drop-in replacement for the standard Hadoop JAR that provides transparent versioning and branching support⁴ and additionally avoids the latency and storage cost of maintaining export copies of datasets Note that this is exactly the same approach that Azure and AWS took with their respective Hadoop FileSystem implementations that serve data stored in their block storage backends as virtual Hadoop files and folders; in other words from a programming and API perspective reading or writing Foundry datasets through the Foundry FileSystem implementation is equivalent to reading or writing S3-backed datasets through the S3 FileSystem implementationFoundrys data management layer enables collaborative data engineering workflows by extending software engineering best practices such as code versioning and sandboxing to data and code; we call this concept co-versioning of data and code Large-scale data platforms based on Foundry like Skywise power the collaboration on data assets data models business logic and data-driven applications between more than 10000 data engineers and analystsMark and Robert are the founding engineers of the Palantir Foundry data platform¹ To avoid excessive storage cost Foundry stores a mixture of snapshots and diffs just like Git This is particularly useful for append-only datasets of immutable records such as system logs or sensor readings which are often among the largest (and fastest-growing) datasets our customers use Our versioning scheme has no storage overhead for such datasets: behind the scenes we effectively store each diff in a separate folder in the backing file system (eg datasetA/diff1 datasetA/diff2 …) so that the whole dataset is simply represented by datasetA/* (Its a bit more complicated than this because users can selectively delete files from those diffs but you get the idea of zero storage overhead for this case) ² While S3 supports per-object versioning a dataset typically comprises multiple files (eg Parquet files) Atomic versioning of a whole S3-backed Hadoop subdirectory is not supported by S3 ³ The devil is in the details though … how would a reader monitor the continuous non-existence of this LOCK file for the duration of the read? ⁴ In addition to versioning and branching our implementation also provides client-side/end-to-end encryption
mcVekh6goVAxtTzewMwFi2,Some of us here at Pandera Labs recently attended Snowflakes Unite the Data Nation conference in Chicago After seeing the product demo understanding Snowflakes capabilities and seeing the real world customer demos I got excited to try out Snowflake on my ownI went to the Snowflake page and discovered that they offered a demonstration account with $400 of credits So I clicked the Start for Free link and I was on my wayThe Snowflake documentation contains step-by-step instructions for getting started including complementary videos Following the instructions I was up and running in no time I looked over the Snowflake in 20 Minutes tutorial and decided to modify the instructions to use our own AWS S3 account and sample data with a goal of loading data from our S3 account into this new Snowflake accountI used the web interface menus to create my objects and load the table but also captured the SQL for every step: 12Aggregated to 2700 rows in 17 seconds • I completed all this work in less than 20 minutes I found the interface very intuitive and the database to perform very fast even with the smallest instanceIm also not a big fan of using scripting languages and writing lots of code to develop data warehouses We were looking for a ETL/ELT tool to complement Snowflake and Matillion has some desirable features: I went to the Matillion website and signed up for a demo The demo was impressive and we signed up for a two week trial Like Snowflake Matillion also had an easy to follow step by step getting started guide Their documentation also has tons of examples and videosI wanted to try out two common test cases with Matillion The first test case is to test a full file extract and the second to test an incremental loadFor the full file extract I will load a file from an AWS S3 bucket into a staging table then compare the staged data to the target table to determine what changes need to be applied to the target table The last step applies the changes to the targetFor the incremental load we will load a incremental file from our AWS S3 bucket The source system does not provide an indicator of whether the record is new or changed so we will be performing an upsertI found both products intuitive and easy to use The learning curve is not steep for anyone who has used a relational database and ELT/ETL tool in the past: armed with only the documentation from the products websites I was able learn both products and successfully write real-world examples in a matter of hours I even reached out to support and found both vendors to be very responsive and helpful The combination of these two tools is both very cost-effective and powerful
Pe3CuKHQKSGdj2ftU5uZuo,This blog post is intended for users who are familiar with Apache Impala If youd like to learn about Apache Impala read more hereA common problem encountered with Apache Impala is resource management Everyone wants to use as many resources (ie memory) as they can to try to increase speed and/or hide query inefficiency However its not fair to others and it can be detrimental to queries supporting important business processes What we see at a lot of clients is that there are plenty of resources when their clusters are freshly built and the initial use cases are being onboarded There isnt a concern about resources until you continue to add more use cases data scientists and business units running ad-hoc queries that consume enough resources to prevent those original use cases from completing on time This leads to query failures which can be frustrating for users and problematic for existing use casesIn order to effectively manage resources for Apache Impala we recommend using the Admission Control feature With Admission Control we can set up resource pools for Impala This means limiting the number of queries the amount of memory and enforcing settings for each query in a resource pool There are many settings to Admission Control which can be daunting at first We will focus on the memory settings that are fundamental for a cluster that already has dozens of active users and applications runningThe first challenge with Admission Control is manually gathering metrics about individual users and the queries they have run to try and define the memory settings for resource pools You could manually use the Apache Impala queries pane and chart builder in Cloudera Manager to go through each users queries to gather up some stats but thats very time consuming and tedious to re-evaluate at a later date In order to make informed and accurate decisions on how to allocate resources for various users and applications we need to gather detailed metrics Weve written a Python script to streamline this processOur script can be found on GitHub: https://githubcom/phdata/blog-2019-10-impala-admcontrolThe script generates a csv report and does not make any changes Please review the readme and run the script in your environmentThe csv report includes overall and per-user stats for: Every workload on every cluster is going to be different and have a wide range of requirements As you go through the report there are a few high priority items to look forFirst are users running queries missing stats? (count_missing_stats column) If you see queries running without stats we suggest that you investigate which tables are missing stats and make sure that computing stats is a standard procedure in your environmentSecond compare max to 99th columns With the 99th columns we are trying to account for the majority of their queries (99%) This will allow us to account for bad or errant queries if any of the max columns are more than 10 20% higher than the 99th investigate that users highest queries to see if they were bad queries or if those few queries could be improved to better utilize resourcesThe settings we are going to define based on this report are: Well walk you through how to determine each of these settings for the necessary resource pools Once that is determined well use the Create Resource Pool wizard in CM to create each pool as shown in the image belowTo really gauge this we would need to have a separate report that took query start times and durations to track the average 99th percentile and max concurrency for each user For this setting we would suggest that you keep this as low as possible based on the use case because it ultimately affects the Max Memory you want this user or group of users to be able to consume To make things simple for the number of queued queries we would set this to the same number we set for max running queriesThis is the maximum amount of memory that we want to give a query per node The safest entry for this setting is per_node_max column from our report An exception to this is if you have investigated the users highest memory usage queries and found that the per_node_99th is a better representation of good queries from the user then use per_node_99thThis is calculated from (Default Query Memory Limit * 20 (number of Impala hosts) * Max Running Queries) For example if we want a resource pool to have a 4GiB max per node query limit and be able to run 5 queries at a time this comes out to 400GiB of max memoryThis setting is determined by concurrency duration and the SLA of queries If you have a query that has to run within 30 seconds and its tuned to run in 20 seconds if it sits in the queue for more than 10 seconds it will violate the SLA Third-party applications running against Apache Impala may have their own query timeouts that this may interfere with that we would prefer to return an immediate error For long-running ETL workloads that may end up with data skew increasing query duration you can extend these timeouts to ensure that all of the queries are queued and ranLike Clouderas Admission Control Sample Scenario our cluster has 20 nodes with 128gb of memory for Impala on each node (2560 GiB total for Impala)Immediately we can see that three users (svc_account3 user1 and user4) need to be followed up with to see if their memory stats can be improved with compute stats or if several of their queries were just poorly written We should also look into svc_account1 because their _99th and _max numbers are so far apartDefault resource pool for users: This is our general pool for anyone on the platform that does not have a justified use case for additional resources Were setting aside 25% of the cluster resourcesDefault resource pool for service accounts: This is a general resource pool for standard workloads being generated by applications or scheduled processesPower Users resource pool: This is the resource pool for users who require more resources user3 may be the only one that qualifies for the Power Users resource poolsvc_account2 resource pool: Out of the service accounts this is the only one that we found that really needs a dedicated resource poolWe recommend creating dedicated resource pools for each service account to ensure that resources are protected and not consumed by standard usersAfter implementing the guard rails of Admission Control our customers have much more reliability and consistency in their workloads There is some care and feeding involved however In some scenarios new use cases go through a process of needing to request and justify resources above and beyond defaults As a reminder every workload on each cluster is unique and it may take some trial and error to fully implement Admission Control Our hope is that this blog post gives you a headstart in being able to implement Apache Impala Admission Control in your environment Please reach out to phData for additional information or assistance in getting Admission Control implemented
5cQKZr6MkEFCyuNV7zuVdB,Data centralization without careful metadata implementation is like stocking a warehouse without sorting and labeling all the boxes Yes you may have everything you need in there; but your end users will be wandering around lostIn this post well be discussing how to implement metadata governance as a key pillar of data management including: The first step of course is to decide which metadata categories to add and to agree on a common data vocabulary and taxonomy However reaching consensus is usually easier said than done Its all too easy for this step to spiral out of control as you suss out intra-departmental variations and custom usages into hundreds of hours of discussion and debate amongst various parts of the organizationAt phData we recommend starting simple Define the most fundamental metadata requirements implement them and iterate from there Build from the bottom-up rather than from the top-down; end users will educate the data product teams on whats valuable versus what isntHere are the very basics we recommend starting with: Remember: zeroing in on the right level of metadata for your organization is an iterative process Dont hold up progress trying to figure everything out on day one Instead start with the essentials Then using the process outlined below you can continuously tune metadata to the organizations needsUltimately end users data journeys should start by using a standard metadata repository to search and find data When using the Cloudera platform this would be either Navigator (CDH) or Atlas (CDP) Both of these tools allow end users to search for data and retrieve its metadataFor example imagine an end user searching for the MARA table to retrieve a description (ie General Material Data) and the date it was last updated From there the user would know where to find material data that was refreshed in early September Theres a number of ways to define this metadata and keep it updatedAt phData wed recommend defining a simple table and using that as the source of truth for metadata This table would be Kudu ideally so you could efficiently do inserts updates and deletes  though it could be any normal database table as well It would look something like this: This table is owned by the data platform team that gathers the information from the data owners The data owners are responsible for defining the data The data platform team then owns the process of updating it in the systemOnce the metadata is in a tabular format we recommend integrating it with Navigator and/or Atlas for end users to search and refine Both tools offer APIs making this easy to accomplish In the example below well be using the Navigator API to upload the dataThe first step is to get the entity identification for the column you want to add metadata to This is done using a query on the table name (eg table1) and database name (eg default)This will return a JSON object with a lot of information in it Below is an abbreviated example Look for the identity column which is the unique identifier for the column you want to add metadata toNow that you have the identity you will use it to upload your table metadata for the column Here is the command for using the Navigator API to upload this dataYou can then query the column and see the newly added metadataFinally you can use Navigator search to search the governed data you added In this example we are pulling back all the columns that have personal data in your data platformNOTE: The up_ prefix on the search expression is to indicate you want user-provided data (ie the metadata you just uploaded)If youd like to try this yourself phData provides a base application that reads from a Kudu table and then uses the Navigator APIs to upload this data into Navigator This allows you to simply define metadata in a table and then automate the process of getting it into a searchable dashboard in Navigator You can find it here: https://githubcom/phdata/kudu-navigator-utilityAdding metadata governance is critical to ensuring that your centralized data platform doesnt turn into a data swamp and that your end users dont find themselves floundering End users need clear descriptive names for tables and columns; compliance officers need simple efficient ways to search for personal dataHere are three simple rules for ensuring success with your governance: Following these three simple rules will ensure quick wins in metadata governance  and a solid foundation for your data platform to build on as it grows and evolves
a9AUVpAQzvdTbG9Mx3JYU7,In mid-2017 we were working with one of the worlds largest healthcare companies to put a new data application into production The customer had grown through acquisition and in order to maintain compliance with the FDA they needed to aggregate data in real-time from dozens of different divisions of the company The consumers of this application did not care how we built the data pipeline However they were sure to tell us that if it broke and the end-users did not get their data they would be out of compliance and could be subject to massive finesThe data pipeline was built primarily on the Cloudera platform using Spark Streaming Kudu and Impala however there were components that relied upon automation built in Bash and Python as well Having supported data products in the past we knew that data applications need robust support beyond strong up-front architecture and development Specifically we began thinking about how we could ensure that errors do not go unnoticed If any part of our pipeline has issues we need the ability to act proactivelyThere are many log aggregation and alerting tools on the market Out of the box Elasticsearch provides the same log searching functionality as Solr and Kibana provides a very good web user interface In our case though managing an Elasticsearch cluster is work and our customers are already using Cloudera Search so an additional cluster was not worth the extra overhead In addition security and alerting is provided only with the paid version of ElasticsearchOther solutions were prohibitively expensive For example one of the options we evaluated was priced based on the volume of data ingested and aggregated Having been down this road in the past we knew that you end up spending a lot of time adjusting log verbosity and making sure only important things are logged This is low-value work and storage is cheapGiven that the application had PHI and HIPAA data we also wanted a solution that included role-based access control (RBAC) The Sentry integration with Solr Cloud provides out of the box role-based access control so our customer is able to use their existing Sentry roles to protect their log data against unauthorized access using their existing processesPulse is an application log aggregation search and alert framework that runs on Cloudera CDH Pulse builds on Hadoop/Spark technologies to add log aggregation to your existing infrastructure and integrates closely with Cloudera through a Cloudera CSD and ParcelWhen running an application its important to be able to: Pulse stores logs in Apache Solr which gives full text search over all log data Apache Sentry which handles role-based access control works with Apache Solr so its easy to control access to logs Pulse itself adds functionality like log lifecycle management so logs are kept only as long as needed It includes log appenders for multiple languages that make it easy to index and search logs in a central location Pulse also has an alert engine so you can create alerts that will notify your team when things go wrongPulse runs on your existing infrastructure and is not a cloud-based service Pulse is packaged into a Cloudera Custom Service Descriptor (CSD) It is installed via Cloudera Manager In addition to making the install of Pulse painless on your Cloudera Hadoop cluster Cloudera Manager acts as a supervising process for the Pulse application providing access to process logging and monitoringPulse consists of four components: Logs stored in Solr can be visualized using existing tools including Hue Search Arcadia or Banana Each log record stored in Pulse contains the original log message timestamp making it easy to create time-series visualizations of your log dataThe Log Collector is an HTTP REST service that listens for log events batches them and writes them to Solr Cloud Because the Log Collector is just a REST API its easy to configure applications in any language to use it Its on the Pulse roadmap to flip the log-collector around and read log events from a Kafka topic instead of just listening for messagesLog appenders are the application-specific code and configuration used to write log messages to the Log Collector Weve built clients that write to the Log Collector for Java (using log4j) Python and BashFor Java you can attach a custom log4j appender using only changes to your log4jproperties To use the Python appender you will need to import a Python library that extends the Python logging framework The Bash appender is a simple script you can source that makes logging from Bash applications to the Pulse framework easyThe Collection Roller will define applications in Pulse and handle log lifecycle The image below describes how the Collection Roller works with collection aliases There are two collection aliases: the write alias and the read aliasThe write alias (suffixed with _latest internally in Pulse) will always point at the most recently created collection Its used by the Log Collector so the log collector doesnt have to know about specific collectionsThe read alias (suffixed with _all internally in Pulse) will always point at all log collections for a single application It is used by visualization or search tools wanting to access all log dataEvery day a new collection is created while the oldest collection is deleted The image below describes this processUsers interact with Pulse through the Alert Engine and visualization toolsBecause Pulse uses Solr Cloud behind the scenes any visualization tool that works with Solr Cloud can be used including Arcadia Data Hue Search or BananaThe Pulse Alert Engine is a daemon that continually monitors logs and will alert users when something goes wrong The alert engine has rules that can use the full power of the Solr/Lucene query language
Sz7ZKcdY2vZ5fUEoVZwTfV,When we talk about data the number of technologies available on the market is overwhelming and staying up to date is a key challenge both for businesses and for engineersOne of the reasons why I recently joined Photobox was to be in a data-driven company with the challenge of building a new data platform using some of the most cutting edge technologies available  AWS (Amazon Web Services) Snowflake and LookerI spent the last four years of my career mainly working on GCP (Google Cloud Platform) leading the development of The Telegraph data platform After that I wanted to get out of my comfort zone and undertake a new challenge starting to learn a new stack of technologiesWith this article I would like to share the joys and pains of this learning journey and I will try to bring an unbiased comparison between GCP and AWS offeringsPlease keep in mind that this is from the point of view of a data engineer that has just recently started playing with AWS and Snowflake For this reason it would be impossible for me to give a full comparison of the services offered by Amazon and Google so I will limit this article to the areas that I touched while building and extending the new data platform in PhotoboxThe aim of this article is not to discuss in detail Photoboxs data platform architecture but I will try to give enough context to justify the decisions that we took during our journeyThis article is divided into three main sections that cover the flow of the data in our platform from Ingestion to Warehouse: In each section I will describe (at a high level) what we are building in Photobox and to provide a comparison I will do an exercise to design the same on GCP sideOne of the first things you have to figure out when building a new data platform is how you will ingest the dataIn Photobox we built a self-service platform that can ingest both real-time events and batch data Multiple internal and external clients can push events to the platform If an event conforms with the expected schema then it is ingested otherwise it is discardedThe event ingestion process in Photobox mainly uses the following AWS services: External events are sent through API Gateway and validated If the payload is accepted then the event is published to Kinesis Stream that propagates it to Kinesis Firehose Firehose through a Lambda function anonymises any PII data that might be present in the payload (based on JSON naming conventions) and stores the anonymised dataset into S3 from where Snowflake ingests the dataInternal events are acquired in a similar manner but using SNS topics that our data platform subscribes toTo design the equivalent process using GCP I would probably use the following Services: In GCP external events are handled first using Cloud endpoints and an application deployed in Cloud functions This application will publish every event into a Pub/Sub topic Dataflow will subscribe to the topic and consume the data applying an anonymisation function to each record then it will store the data in micro-batch into Cloud StorageInternal events are acquired in a similar manner events are published in multiple Pub/Sub topics then anonymised using Dataflow and stored in Cloud storageThis is obviously only one solution to the problem Im sure that discussing with AWS or GCP experts the same result can be achieved in multiple different waysIf we start to compare the two solutions from the external events ingestion branch we can see that on one side we have Amazon API Gateway while on the other side we are using two components Cloud endpoints and Cloud functions to perform the same taskFor those not familiar with AWS Amazon API Gateway is a service for creating publishing maintaining monitoring and securing REST and WebSocket APIs at any scale Routing requests from an API gateway into Kinesis is easy and doesnt require additional components to connect the two servicesFig-3 AWS API Gateway POST /event endpoint used to collect external events in Photobox data platformLooking at API Gateway limits we can see that by default it can manage a stunning amount of throughput  10000 requests per second (RPS) with a maximum payload size of 10Mb (see here for details on soft and hard limits of the service) During peak time at Photobox the number of external events per second that we collect might exceed that quota in the future but luckily this is a soft limit that eventually can be increased if neededOn the GCP side Cloud Endpoints is an NGINX distributed proxy-as-a-service that provides access control to your APIs and validates REST request This layer can be easily integrated with an application deployed in Cloud functions (or another GCP service like GKE) to efficiently route events into Pub/SubGoogle Cloud Functions is a serverless execution environment for single-purpose functions that are attached to events emitted from GCP cloud infrastructure and services It plays a role similar to Lambda functions in AWS Cloud Functions can handle up to 100000000 requests every 100 seconds with a maximum payload size of 10MB (see here for detailed limits and quotas)Both of the GCP and AWS services for event collection are serverless fully scalable and able to handle a high volume of events The main difference that I can see between the two solutions is the number of components used to achieve the same result  one on AWS vs two on GCP Ive always been a fan of simple designs since having fewer components to maintain and monitor generally makes life easierComparing the streaming and anonymisation part in Fig-1 and Fig-2 we can see that in AWS Kinesis Stream and Kinesis Firehose (with a Lambda function) are used while in GCP Pub/Sub and Dataflow perform the same taskAWS Kinesis Datastream is a scalable and durable real-time data streaming service In our platform it is used in front of Kinesis Firehose to provide seven days of data retention and read throttling (if needed) Data records are managed in shards It is possible to scale shards up and down based on the needs Because resharding (increasing or decreasing the number of Kinesis shards) is not automatic Kinesis requires constant monitoring through CloudWatch to optimise the number of shards to efficiently handle the volume of dataResharding supports two operations: two shards can be merged into one or a shard can be split in two Each shard in Kinesis Stream can manage up to 1000 PUT operations per second with a maximum data blob size of 1MBKinesis Firehose on the other hand is fully managed and scales automatically This technology is used in our platform to automatically write data into S3 buckets prior to the application of an anonymisation Lambda function to each set of records This ensures that no PII data are ingested in Photoboxs data platform to comply with GDPROn GCP Pub/Sub plays the role of Kinesis Stream This technology is scalable durable event ingestion and delivery system used for stream analytics pipelines It provides a many-to-many asynchronous messaging that decouples senders and receivers It doesnt require any management and can handle 1000MB/s publishing throughput and 2000MB/s subscription throughput with a maximum message size of 10MB Detailed quotas and limits are available hereDataflow (Apache Beam) plays the same role as Kinesis Firehose with an anonymisation Lambda It consumes messages from Pub/Sub applies the anonymisation logic and writes in micro-batch into a Storage bucketIn this case GCP uses fewer components than AWS to achieve the same result Also Pub/Sub (unlike Kinesis Stream) doesnt require any management to pre-allocate shards or to scale the process based on a monitoring systemOn the other side Kinesis Firehose handles micro-batch partitioning into S3 automatically and supports inline a function defined in a Lambda This seems more convenient than deploying a Dataflow pipeline in GCP and writing your own logic to micro-batch records into Cloud StorageIn the internal events ingestion branch on the AWS side Photobox internal clients are relying on SNS topics Records are published into Kinesis Stream through a Lambda function and lastly Firehose applies the anonymisation function and writes into S3On GCP side the same process would use two components One or more clients can publish on a Pub/Sub topic(s) and a dataflow pipeline can consume anonymise and write the records into StorageThis second approach has fewer moving parts to be monitored therefore it seems simpler to maintainAnother key point when you design a data platform is which tool(s) you use to transform the data and how you orchestrate multiple data pipelinesThe transformation tool Im really enthusiastic about is Data Build Tool (DBT) and all the ELT (Extract Load Transform) in Photoboxs data platform relies on it If you want to know more about my previous experience with DBT have a look at this article I wrote about DBT at The TelegraphFig-4 How DBT pipelines are orchestrated in Photobox data platformAs you can see from Fig-4 Apache Airflow is the scheduler of choice in Photobox and it is used to orchestrate all our data pipelines Airflow is deployed in Amazon ECS using multiple Fargate workers As a part of Airflow deployment AWS RDS is used as scheduler metadata storage and ElasticCache supports the queueing serviceAirflow has a pool of lightweight workers that dont perform any computationally intensive jobs; their only purpose is launching ECS Task Definitions which are the actual data pipelines The transformation logic is written in DBT and wrapped in a multi-purpose Python application that allows the running of any operations that might be not supported in DBTA DBT pipeline defines transformations steps as models (SQL queries) which are linked in DAGs (Directed Acyclic Graphs  not to be confused with airflow dags) A DBT DAG ensures that a set of queries is performed in the correct sequence to transform the data as desired Once the fargate worker that runs DBT has executed the task then it is released freeing up unneeded computational resources This ensures that the only application constantly running in our ECS cluster is Airflow while all the data pipelines run on ephemeral workers that are alive just long enough to perform the taskFig-5 How the same solution can be designed in GCPIf we design the same process using GCP then Cloud composer can be used as a replacement for a custom Airflow deploymentCloud composer is nothing more than a managed version of Airflow hosted by Google that normally requires minimal tuning to be usable for most common use cases On the other hand if you are used to having full control of your Airflow this product may be too restrictiveTo run data pipelines instead of ECS Google Kubernetes Engine (GKE) can be used and Kubernetes Jobs can take the place of ECS Task definitions(Let me open a small parenthesis on this Google normally suggests running your data transformations using Dataflow I personally find Dataflow really useful with streams of data whereas if your team speaks SQL Im of the opinion that DBT is a better tool to run batch transformations DBT can fully unleash the power of your cloud DW and is designed to handle dependencies among models running your logic in a reliable wayComparing the two solutions we can see that AWS does not offer a managed version of Airflow while GCP does This is not a deal-breaker but personally I dont like to invest data engineers time to manage and deploy infrastructure if not strictly needed I think it is important to know how to do data ops but its also more important to know how to avoid doing it This can free up resources that can be used to deliver real value to the businessOn the other hand I was pleasantly surprised by ECS Provisioning and de-provisioning Fargate workers is fast It takes roughly one minute to have a worker ready to run your data pipeline and another minute to kill it You might hit an AWS quota limit running multiple Fargate workers in parallel but this is a soft limit that can be circumvented by asking AWS for a quota increaseOn GCP side in my experience if a node in the GKE cluster can allocate the desired resources then creating a Kubernetes Job is really fast but if the GKE cluster doesnt have a node available it will have to auto-scale This operation will normally require a few minutes delaying the data pipeline executionDepending on the usage that you are expecting and time constraints that you might have when running your data pipelines one technology might suit better your needs than anotherI think in this specific context you rarely see the difference until you have a data pipeline that has a short predetermined time slot to run and requires high computational resources In that case you might have to pre-allocate a node in your GKE cluster to be sure the resources are going to be ready when requested or have a process that will force your cluster to auto-scale a few minutes before your pipeline is supposed to runUsing ECS you can specify the hardware requirements for the task in the task definition and the Fargate worker provisioning time is likely to be similarThis section could easily become an article itself For this reason I will try to be concise highlighting only the main characteristics of Snowflake (the DW that we are using in Photobox) and BigQuery its GCP equivalentSnowflake doesnt sit under the large AWS umbrella but it seemed to us the best option as a first building block for our platformThe reason why we adopted Snowflake was mostly to overcome limitations that you might find in other products like Athena and Redshift Athena is an interactive query service that at the moment of testing didnt support any updates on existing tables An essential feature like CTAS (Create table as select) was introduced at the end of 2018 and the allowed number of concurrent queries was not a fit for the number of tasks that we run in parallel in our data platform With Athena we also struggled to handle high volumes of nested JSON recordsRedshift itself doesnt support schema-on-readYou can access the Glue catalog from redshift or use Spectrum to access data stored on S3; nevertheless joins between spectrum or Athena tables with inner Redshift tables happen in Redshift therefore query performances depend on the size of the Redshift clusterFor all the reasons above we decided to opt for Snowflake having a better decoupling between storage and computeSnowflake is Software-as-a-Service (SaaS) and uses a new SQL database engine with a unique architecture designed specifically for the cloud that allows processing petabyte-scale-data with unbelievable speed It has no limit on concurrent queries assuming you have enough resources available to handle them and requires minimal knowledge of the infrastructure that is under the hoodThe DW provides a web UI (Fig-6) and everything from the permission management to increase the performance of a Warehouse can be handled in SQL statements Snowflake supports schema-on-read capability managed through views and stages which allows smooth JSON schema changes in the ingestion layer With Snowflake raw data can be stored in S3 and accessed through external tablesOn GCP side BigQuery is Software-as-a-Service (SaaS) and doesnt require any infrastructure management Like Snowflake it can be accessed through a web UI and Cloud Storage can be used to host raw data with BigQuery also using external tables to access the dataSpeed-wise its not easy to judge It seems to me that Snowflake is faster than BigQuery in a head-to-head test and this fact seems supported by different benchmarksOn the other hand you might find engineers mentioning that in some situations BigQuery outperforms SnowflakeThe truth is that in different contexts you might get different performances and it really depends on: Regarding pricing models with Snowflake you pay credits/hour for each virtual warehouse plus the data storage cost which is normally negligible and aligned with your cloud provider costs Basically you pay only when you have machines up and running executing your queries and the total cost mostly depends on your usage pattern and the fact that your virtual warehouses are suspended when not in useLuckily Snowflake provides an auto-suspension option to switch off a virtual warehouse when no queries are executed for a certain amount of timeWith Bigquery you pay for the amount of data you read during your query ($5/TB) plus the cost of the storage (currently $002/GB/month) When possible the UI provides an estimation of how much data you are going to query so the pricing is usually fairly transparentGoogle also offers a flat-rate pricing plan as an alternative to the pay-per-query modelI can see the pros and cons of both models and it really depends on what usage patterns you are expecting to do on your data warehouseSnowflake could become really expensive if you accidentally leave a virtual warehouse on or need to have it constantly available On the other hand with BigQuery having a team of analysts querying high volumes of data you could end up with a big bill pretty fastIn both cases education is the key If you dont want to lose control of your costs it is important to educate engineers analysts and any other consumer to use these cloud data warehouse technologies in the proper way Data has to be wisely modelled and optimised for consumption and in some cases role-based restrictions might come in handy This can help to keep control of who is accessing your DW (and how) and prevent data from being consumed in the wrong wayAside from performances and price where I think Snowflake shines compared to BigQuery is the learning curve and how friendly (SQL-wise) it isWhen I started using BigQuery a few years ago I had to learn BigQuery (Legacy) SQL and shortly after BigQuery Standard SQL In both cases I had to dig into the documentation to understand how to perform a simple cast or query a partition I was simply unable to start to use the product without doing the proper reading Google had to write guides on how to migrate codebases from Legacy SQL to Standard SQL since direct equivalents of some functions were not availableThe first time I got access to Snowflake I started writing SQL statements without reading any documentation I didnt know the proper syntax so I wrote it as though querying a MySQL database I knew that my query would probably raise dozens of errors (and actually I did get a few) but fixing my syntax was trivial and it took me only a few moments to have a running queryI do really appreciate the effort that the Snowflakes team put in making their data warehouse easily accessible without the need to take a deep dive into documentation when Im not using any function that is Snowflake specificAfter my first four months in Photobox I can say that I deeply appreciate some tools that AWS offers and I totally fell in love with Snowflake At the same time its hard not to compare with what GCP offers and sometimes I find myself thinking about how it would be to have some of GCPs tools on AWS sideIt is obviously impossible to have everything and what is really better depends on the use cases that you have and on which tools your engineering team feels more confident using One tool might seem great in a certain situation and less good in another For this reason its hard to say one cloud provider is overall better than anotherWhat is for sure is that Im glad of the opportunity to work with the great team of engineers that we have in Photobox and the freedom that we have to try new solutions and learn new technologies Our mission to build one of the most cutting edge cloud-based data platforms is only beginning but it makes me proud every time we add a new piece to the puzzleStefano Solimito is a Principal Data Engineer at Photobox You can follow him on LinkedIn
HUMfkRBsDY4Jsdgr4aiGwu,At Pinterest we use Kafka as the central message transporter for data ingestion streaming processing and more With a growing user base of +175 million people and an ever expanding graph of +100 billion Pins we currently run >1000 Kafka brokers in the cloudAt this scale we encounter Kafka broker failures every week and sometimes multiple failures in a day When a broker fails the on-call engineer needs to replace the dead broker on time to minimize the risk of data loss due to other node failures We often need to move workloads among brokers to balance the workload Replacing brokers and re-balancing workloads requires carefully creating and editing the partition reassignment files and manually executing Kafka script commands These operations add significant overhead to the teamTo scale up the Kafka service operation we built DoctorKafka a service for Kafka cluster auto-healing and workload balancing DoctorKafka can detect broker failures and automatically reassign the workload from failed brokers to healthy ones It can also perform workload balancing based on the settings Today were releasing DoctorKafka as an open source project on GitHub In this post well cover its architecture and how it can be useful to youFigure 1 shows the high-level overview of DoctorKafka Its composed of three parts: Note that DoctorKafka only takes confident actions It will send alerts if its not confident which actions to takeThe metrics collector runs on each broker and collects Kafka broker metrics on inbound and outbound network traffic as well as the stats of each replica Figure 4 shows part of the broker stats collected by a metrics collector Topic partition reassignment usually incurs extra network traffic and distorts the metrics even with replication quota setting (available in Kafka 0101) Because of this the metrics collector explicitly reports if a topic partition is involved in reassignment when the metric is collectedWhen the central DoctorKafka service starts it first reads the broker stats from the past 24 48 hours With this DoctorKafka infers the resource requirement for each replicas workload As Kafka workload is mostly network bounded DoctorKafka focuses only on replica network bandwidth usageAfter it starts DoctorKafka periodically checks the status of each cluster When a broker failure is detected it reassigns the workload of the failed broker to others with available bandwidth If theres not enough resources in the cluster to re-assign the workload it will send an alert Similarly when DoctorKafka does workload balancing it identifies the brokers whose network traffic exceeds the settings moves the workload to others with less traffic or performs preferred leader election to move the traffic We ignore the broker stats collected during partition reassignment and network traffic cool-down period to get more precise workload informationDoctorKafka has been in production at Pinterest for months and helps us manage >1000 Kafka brokers Were happy to release it to the community and you can find its source code and design documentation on GitHubOpen source is a big priority for engineers at Pinterest Companies like YouTube Google Tinder Snap and more use our open source technology to power app persistence image downloading and more For all our open source projects check out our GitHubAcknowledgements: Thanks Jon Parise for helping me through the open source process
AS82yzEsfS2hxWVBtiRDKn,"As a data-driven company many critical business decisions are made at Pinterest based on insights from data These insights are powered by the Big Data Platform team which enables others within the company to process petabytes of data to find answers to their questionsAnalysis of data is a critical function at Pinterest not just to answer business questions but also to debug engineering issues prioritize features identify most common issues faced by users and see usage trends As such these analytics capabilities are needed by engineers and non-engineers equally at Pinterest SQL and its variants have proven to provide a level ground for employees to express their computational needs or analysis effectively It also provides a great abstraction between user code / query and underlying compute infrastructure enabling the infrastructure to evolve without affecting usersTo provide employees with the critical need of interactive querying weve worked with Presto an open-source distributed SQL query engine over the years Operating Presto at Pinterests scale has involved resolving quite a few challenges In this post we share our journeyFigure 1 below gives an overview of Presto deployment at Pinterest Our infrastructure is built on top of Amazon Web Services (AWS) EC2 and we leverage AWS S3 for storing our data This separates compute and storage layers and allows multiple compute clusters to share the S3 data We have multiple Presto clusters that serve different use cases These clusters can be long or short-lived Two major ones are ad-hoc and scheduled cluster: the former serves ad-hoc queries and the latter serves scheduled queries Keeping ad-hoc queries separate from scheduled queries enables us to provide better SLA for scheduled queries and also brings more predictability in resource demand on the scheduled cluster""Pinterests analytical need was served by a more conventional data warehouse that didnt scale with Pinterest's data size until 2016 which was then replaced by Presto Running Presto at Pinterests scale came with its own challenges In the early days of onboarding Presto we frequently saw issues including Presto coordinator crashes and cluster getting stuck with close to zero worker parallelism Later in this blog we explain the reasons for these issues and discuss how we solved themWe have hundreds of petabytes of data and tens of thousands of Hive tables Our Presto clusters are comprised of a fleet of 450 r48xl EC2 instances Presto clusters together have over 100 TBs of memory and 14K vcpu cores Within Pinterest we have close to more than 1000 monthly active users (out of total 1600+ Pinterest employees) using Presto who run about 400K queries on these clusters per monthPresto is well known for its capability to query from various systems however only the Hive connector is currently used at Pinterest Hive and Presto both share the same Hive Metastore Service Its very common for our users to use Hive for writing data and Presto for read-only analysis In addition we recently started allowing Presto to create tables and insert data primarily due to the following reasonsEach Presto cluster at Pinterest has workers on a mix of dedicated AWS EC2 instances and Kubernetes pods Presto deployment at Pinterest should look very similar to any large scale Presto deployments There are a couple of in-house pieces ie Presto Controller and Presto Gateway that we talk about in the next subsectionsPresto controller is a service built in-house that is critical to our Presto deployments Following are some of the major functionalities served by the controller as of todayPresto gateway is a service that sits between clients and Presto clusters It essentially is smart http proxy server"
HBi3GhBGmfSvgpJnGfWKUp,At Pinterest we run thousands of experiments every day and rely mostly on daily metrics to evaluate performance The daily pipelines can take 10+ hours to run and sometimes are delayed which has created some inconvenience in verifying the setup of the experiment the correctness of triggering and the expected performance of the experiment This is especially a problem when there are bugs in the code which might take several days to catch and cause bigger damage to user experience and top line metricsAs a solution we developed a near real-time experimentation platform for fresher experiment metrics to help in catching these issues as soon as possible Some examples of issues that may come up are: The dashboard above shows the volume (ie number of actions) and propensity (ie number of unique users) of the control and treatment groups of an experiment for a selected event The counts are aggregated for three days after an experiment gets launched or ramped up If a re-ramp (increase in user allocation of control and treatment groups) occurs after the three days the counts accumulate again from zero for another three daysWe then perform several statistical tests to ensure the results and comparisons between control and treatment are statistically valid Since metrics are delivered in real time we do these tests every time a new record is received in a sequential fashion We use different methods from traditional fixed horizon tests so as to not bring high false positive rate Several sequential testing methods have been considered including Gamblers Ruin Bayesian A/B test and Alpha-Spending Method For the sake of numerical stability we started from t-test + Bonferroni Correction (treat our case as multiple testing) with the number of tests pre-determined for our initial implementationThe realtime experimentation pipeline consists of the following main components: To filter the activations this job uses Flinks Broadcast State pattern The CSV published by the recently ramped experiment groups job is checked every 10 seconds for changes and published to every partition of a KeyedBroadcastProcessFunction that also consumes activationsBy joining the broadcasted CSV with activation stream the KeyedBroadcastProcessFunction filters out those activation records for experiments that are not ramped up within the last 3 days Additionally the group-ramp-up-time is added to the activation record and it is inserted into the filtered_experiment_activations kafka topicBefore we dive into the real-time experiments job its worth looking to the objects that are inserted into the intermediate Kafka topics SimpleEvent objects are inserted into the filtered events topic and ExperimentActivationWithRampedUpTime objects are inserted into the filtered_experiment_activations topicAbove is a high level overview of the real time aggregation Flink job Some of the operators are covered here briefly while some are described in detail in later sections The source operators read data from Kafka while the sinks write to our internal Analytics store using a REST interfaceDe-duplicate events → This is implemented as a KeyedProcessFunction that is keyed by (eventuser_id eventevent_type eventtimestamp) The idea is that if events from the same user for the same event-type have the same timestamps they are duplicate events The first such event is sent downstream but is also cached in state for five minutes Any subsequent events are discarded After five minutes a timer runs and clears the state The assumption is that all duplicate events are within this amount of time of each otherFind first trigger time → This is a Flink KeyedProcessFunction keyed by (experiment_hash experiment_group user_id) The assumption is that the first experiment activation record received for a user is also the activation with the first trigger time The first activation received is sent downstream and saved as state for the next three days since an experiment ramp-up (we aggregate counts for 3 days since experiment group got ramped up) A timer clears the state after three days of ramp time • minute processing time tumbling windows → Both Numerator Computer and Denominator computer aggregate counts when events come in and send results downstream These are millions of records but we dont need to send results so frequently to the Analytics store We accomplish this more efficiently by having a 15 minute Flink tumbling window that runs on processing time In the case of Numerator Computer this window is keyed by (experiment_hash experiment_group event_type timestamp) When the window fires after 15 minutes the record with the max_users is taken and sent downstream the Analytics Store sinkWe implement the stream-stream join with Flinks IntervalJoin operator IntervalJoin buffers the single activation record per user for the next three days and all matching events are sent downstream with additional experiment metadata from the activation recordWeve looked into Flinks IntervalJoin source code It does buffer activations for three days into a left-buffer However events will be deleted immediately Currently it looks like there is no way to change this behaviour via configuration We are looking into implementing this activation-event join using Flinks coprocess function which is a more general purpose function for stream-stream joins We can buffer events for X minutes so that even if activation stream get delayed for X minutes the pipeline can handle the delay without undercounting This will help us avoid double joins for the same user and can result in a more dynamic pipeline that is immediately aware of re-ramps of experiment groups and support more dynamic behaviour like automatic extension of coverage of aggregations in case of re-ramps of groupsThe Join Results Deduplicator is a Flink KeyedProcessFunction that is keyed by experiment_hash experiment_group event_type user_id The primary purpose of this operator is to insert a user_first_time_seen flag when sending records downstream  this flag is used by the downstream Numerator Computer to compute propensity numbers (# unique users) without using a set data structureThis operator stores state till the last-ramp-time + three days after which the state is clearedThe Numerator Computer is a KeyedProcessFunction that is keyed by experiment_hash experiment_group event_type It maintains rolling 15 minute buckets for the last two hours which are updated every time a new record comes inFor volume every action counts so for every event action counts are incrementedFor propensity numbers (unique users)  it depends upon the first_time_seen flag (increment only if true)The buckets roll/rotate as time passes The buckets data is flushed downstream to the 15 minute tumbling windows every time a new event comes inA three day timer (from ramp-time →three days) that clears all state upon firing effectively resetting/clearing counts to zero after three days since ramp-upIn order to make our streaming pipeline fault-tolerant Flinks incremental checkpoint & RocksDB statebackend were used for saving application checkpoints One of the interesting challenges we faced was checkpoint failure The issue appeared to be that checkpointing process takes an extremely long time and it eventually reaches timeout We also noticed that typically when checkpoint failure happens there is also high back-pressureAfter taking a closer look inside the checkpoint failure we found that the timeout was caused by some subtasks not sending acknowledgment to the checkpoint coordinator and the whole checkpoint process was stuck as shown belowSeveral debugging steps were then applied to root cause the failure: It turned out the subtask was functioning normally and it was just too busy processing messages As a result this specific subtask had high back-pressure which prevented barriers flowing through Without recipient of barriers checkpoint process could not move forwardAfter further checking Flink metrics for all subtasks we found that one of them was producing 100x more messages than its peers Since messages were partitioned by user_id across subtasks this indicates that there are some users producing much more messages than others and that leads to a conclusion of spamming This result was also confirmed by ad hoc querying our spam_adjusted data setsIn order to mitigate the problem we applied a capping rule in Filter Events Job: if for a user within one hour we see more than X messages we only send the first X messages We were glad to see there was no checkpoint failure anymore after we applied the capping ruleData accuracy could not be more important for computing experiment metrics In order to ensure our real time experiment pipeline behaves as expected and always delivers accurate metrics we launched a separate daily workflow that performs the same computation as the streaming jobs does but in an ad-hoc way Developers will be alerted if the streaming job results violate any of the following conditions: By querying experiment metadata we run the validation on experiments under 3 cases respectively: This workflow can be visualized as belowIn this section we present some basic stats to show the scale of real-time experiment pipeline: 2 100G checkpoint • 200~300 experiment groups • 8 masters 50 workers with each being ec2 c5dReal-time Experiment Analytics is the first Flink-based application in production at Pinterest Huge thanks to our Big Data Platform team (special thanks to Steven Bairos-Novak Jooseong Kim and Ang Zhang) for building out the Flink platform and provide it as a serviceWere building the worlds first visual discovery engine More than 250 million people around the world use Pinterest to dream about plan and prepare for things they want to do in life
c6Nay6nYFmVSNMGYcA6637,To provide a bit of context Ill explain the basics of VWO Its a platform through which people can run targeted campaigns of various types on their websites: perform A/B experiments track visitors and conversions do funnel analysis render heatmaps and play visitor recordingsThe real power however lies in the reporting of the platform All of the above mentioned features are connected together And for Enterprise customers ingesting all this vast data would be useless without providing a powerful platform that can deliver those insightsUsing the platform you can make arbitrarily complex queries on large data sets Here is a simplistic example: Notice the boolean operators Those are provided in the query interface for customers so they can make arbitrarily complex queries to slice and dice the dataThe customer in question was trying to do something which should be reasonably fast if you think about it intuitively: This was a site that got huge amount of traffic and we had more than a million unique URLs stored for them alone And they wanted to search for a fairly common URL pattern that relates to their business modelLets dive into what was happening in the database The following was the original slow SQL query: Here are the timings: It ran on around 150k rows The query planner showed some interesting details but no immediate bottlenecks jumped outLets examine the query a bit further As it can be seen the query does a JOIN on three tables: Also notice that we already have all of our tables partitioned by account_id So there is no possible scenario of one large account overwhelming other accounts and causing slowdownUpon close inspection we see that there is something different in this particular query The following line needs to be examined: Initially I thought that perhaps the ILIKE operation on all those long URLs  we have more than 14 million unique URLs captured for this account  might be causing the slowdownThe pattern matching query itself is taking only 5 seconds independently Matching millions of unique URLs is clearly not a problemThe next suspect on the list is the multiple JOIN statements Perhaps the excessive JOINs have contributed to the slowdown? They are usually the most obvious contender when it comes to query slowdown but Ive not really believed that to be the typical caseAnd it wasnt the case The JOINs are quite fastI was ready to start tweaking the query to achieve any possible perf gains possible My team and I brainstormed 2 tactics to try out: Yep The subquery when wrapped with EXISTS makes the entire thing super-fastBut this was still extremely slowThere was this one little thing that Id been looking at and dismissing all along Since nothing else was working I decided to take a look at it anyway It was the && operator While EXISTS was the performance booster that made things faster it seemed like the && was the only remaining common factor in all versions of the slow query
LKwxq6TuaTTMdqr32xwtwo,ETL is an important aspect of the Data Engineering field One such tool to accomplish the same is SAP Data Services\u200b It acts as an excellent Pipeline builder to transport the data from source to destination with effective cleansing and modification as per the requirements The article shows the explanation for all the transforms in the tool for Integrator and Platform and how to extract and provide quality data for further usageVarious transforms available in the SAP BODS Transform section helps us to manipulate the data from the source as per the requirement and pass it on to the staging or the target tableAIt helps in moving the data from Source to the Target or from another transform as it is if there are no modifications to be done on the data Full Push down operation enables the data to be flown even if the Source and Target are in different Data sources Query Transform helps in passing the data as well but gives performance issues Data Transform will help pass huge chunks of data in a shorter time Data Transform practices the Full Push down operation which helps in reducing the execution time and helps in faster transfer rateRefer to the blog by Dirk Venken: https://blogssapEffective Date Transform works like a previous row operation It takes the date of the next row as the end date of the current row This transform uses the primary key information to identify the sequence of the rows and assign the appropriate dates Use the example of an employee leaving a certain department and use start and end dateIn the above example a Row Generator has been used for the sequential Id generation and the Date Generation transform generates the dates which we are going to be used as the Start_Date ParameterWe add the columns Customer_Id and Start_Date and assign the parameters accordingly and pass it on to the effective date transformIn the above diagram Fig: 21 the highlighted part shows the default date being provided when the row count gets finishedThe hierarchy flattening format provides the parent child relationship in the table Each row will have a parent child relation More than one column can be attributed to the Parent and Child relationship We can display the hierarchy in the Vertical and Horizontal structure The below example shows the Organization structure between Managers and Employees and the Fig 32 shows the way it would be represented in the tabular formatIn the above example ie Fig 33 I have used the input table shown in Fig 32 and used 2 Hierarchy flattening transforms for Horizontal and Vertical representation The Horizontal Output shows the output in levels and the Vertical representation shows the output in depthPoints to remember: 1 Specify correctly the Parent and Child column • Use the appropriate number in Maximum Depth field in Horizontal representation else might result in an error • Check in the Maximum Length paths in Vertical representationThis transform produces a series of dates in a new column in the incremental order based on start and end date provided and the sequence of increment provided This transform can repeat the rows (if used with an existing table) if the sequence of dates is too long and hence needs to be used conservativelyHistory Preserving transform is used to maintain the Historical records in the SCD Type 2 format (Refer to https://wwwdatawarehouse4uinfo/SCD-Slowly-Changing-Dimensionshtml) SCD Type 2 shows the history of a certain row and as the name suggests it preserves the entire history of the table irrespective of any modifications madeKey Generation and Table Comparison go in tandem with the History Preserving transform Key Generation generates a sequence of numbers for the row provided in the same with the table linked to itAs shown in the above example an update has been made for the field businessentityid =1 which is in the input table It shows 2 records for the same as an update has been to the jobtitle record and the cur_flag has been changed accordingly which indicates properly the correct record (Refer Fig: 51) An id field is generated at the start of the table with the logic provided from the Key Generation transform The above example is an SCD Type 3 operation as all the 3 operations have been used together SCD Type 1 uses only Table Comparison SCD Type 2 uses Table Comparison and History PreservingThe picture below shows the fields that needs to be modified to achieve the objective We can set the newly created cur_flag=Y or N indicating the latest recordTable Comparison is used to compare two data sets and produces the difference with rows flagged as INSERT UPDATE and DELETE in the target tableThere are 3 types of process in Table Comparison: 1In the row-by-row mode the Table Comparison Transform executes a select statement for every single input row to lookup the value in the compare table This results in a relatively slower operationIn this method the input is sorted according to the Primary key column in the ascending format and then the SQL is run only once for the entire data The time is consumed in the sorting partIn cached mode this transform is just collecting the input data and indexing it in memory to later lookup the rows inside the cachePivoting transform generates a separate row for every column provided in the Pivot Columns option It converts a column into multiple rows The column provided in the Non-Pivot Columns is set as the base for the filtering of the rows provided in the Pivot ColumnsIn the above diagram we have clearly identified the Non-Pivot and Pivot fields and the result below shows the id field setting the base for the jobtitle and birthdate fieldsReverse Pivot converts multiple rows to a single column without losing out on the data It helps in summarizing the data into a single column Below example shows the way the reverse pivoting works The Axis Value sets the base for the formation of the columnsThis transform is used to process large XML files into small batches and push them to the output table During execution Data Services pushes operations of the XML_Pipeline transform to the XML source https://wwwyoutubecom/watch?v=4q7iRchpOKw  Follow this link for the usage of XML Pipeline transform excellent explanation hereCreate a Nested Schema with the XML definition with the xsd file and use the same in the DataflowAfter we drag the XML Nested schema to the designer open the object and mention the XML path as shown in the picture belowAlso Un-Nest the components in the Query transform in the Query stage as not doing so will result in an error An Output can be attached to the Query transform and then run the Job The XML data flows to the Output in an Un-Nested formatMap CDC Operation is used to support changes in the records of the source database As the name suggests Changed Data Capture(CDC) the DML operations performed on the data is reflected on the target end Another thing to note in this operation is while creating the Datastore the Database type must be set to Native CDC This transform reads the source DML changes ie INSERT/UPDATE/DELETE and reflects the same in the target system So the transform maps all the source rows logged as insert to an insert all updates to update and the deletes to delete in the target table This transform can be used in place of Case transform Table Comparison and Map Operation put together The only constraint is the data should be the CDC type and cannot be used otherwiseThis can also be used in place of Case transform and Map Operation transform put togetherBWhen we transfer the records via other transforms to the target it usually does the Normal mapping ie the records passing through are inserted in to the Target as it is Map Operation can control this behavior and it provides 4 statuses which are: Normal Insert Update DeleteTypes of Map operation provided: Normal: This operation pushes the rows from source to target as it is with no modifications It doesnt check for primary keysInsert: The only difference between Normal and Insert operation is that in the Insert command the primary key columns are checked on the target side and the duplicates are being prevented from being inserted from the Source sideUpdate: Update Operation checks the Primary Key on the Source and updates the corresponding non-Primary key columns in the Target from the Source Table If there is any changeDelete: This operation deletes the rows provided in the source from the target and does not pass the row if not present in the target tableMerge transform helps in merging the columns from various Query or Map Operations and put it to a single output This works only if the number of columns is the same and dont vary in any wayCase transform is used to filter out the rows and then send them to the target table that we want Same rows can be sent to different targets as wellThis transform is a combination of Case and Query transformAs per the below condition provided in the Validation segment the rows satisfying the condition will go the Pass case and provide the records The conditions which are not being met will go the Fail Case The Action on Fail option determines the course of action that needs to be taken on the conditions specifiedThis transform generates rows serially as per the count provided in the transform This is a very simple transform for serial generation of keys The difference in aspect to the Key Generation transform is you cannot link tables internally in the transformIn this transform we can write the SQL instead of using the table and write complex queries as we want After writing the query we must click on Update Schema which shows all the columns that are going to be displayed We can also select the Datastore from where we are writing the query for a specific tableXML_Map transform is like a query transform but it helps in nesting the data and putting the data into the Nested XML target It doesnt matter what the source type is The output will be in the XML formatIn the below example we have used 2 tables and joined them internally We can link tables inside the XML_Map transform as shown in fig below We can click on the respective columns of different tables in the output side and put in the conditionsThis transform helps in hiding the characters of a certain field This is useful in hiding confidential information like Credit Card Number Contact Information etcIn the below figure In the Mapped Input Field we can set the field that we want to Mask In the Unmasked length we have specified 0 This implies that all the characters need to be masked We can modify it as and when we want If we specify the value as 4  then only 4 characters will be unmasked depending on the Starting Position in the field above Unmasked LengthA Query transform works like a SQL query and helps in pushing the data as per the conditions provided This is the simplest of all the transforms and most widely usedAdditional Information: a The difference between Key Generation and Row Generation is the Row Generation doesnt need an input table associated with it and will generate keys nevertheless Key Generation will need a table associated with it for it to generate keysb Date generation just generates dates by adding a new column and Effective Date acts like a previous row operationc Map CDC is used for reducing the dependency of various transforms used together but the data should be of CDC typed Between XML_Pipeline and XML_Map transforms XML_Pipeline is used to convert the XML files to any format that we want it goes the vice versa for XML_Map
7P27eYkLu9WNDCv3BzJmMM,"""In my series of BigData Architecture we have seen the internal working of Sqoop Now as part of this article we'll see the process behind the execution of Sqoop incremental load in a production environment via various scheduling tools of a Sqoop JobThe process is as follows • Sqoop metastore: Metastore is the place where the metadata of each action performed is loggedIn this case Sqoop determines the last value of the records to be imported in order to perform incremental load • Execute the Sqoop Job • You can see at the end of the sqoop job execution details are written to the metastore • Lets check the last value counter in sqoop metastore after sqoop job execution""67 Execute sqoop job again and you will see only the new record being imported into the sqoop customer table""8Now that we have seen how the incremental update takes you can debug easily incase of any issues during incremental load of a Sqoop job via sqoop metastore and also reset the last value accordinglyNote: The commands that were executed related to this post are added as part of my GIT accountSimilarly you can also read about my BigData Architecture here: If you would like too you can connect with me on LinkedIn  Jayvardhan ReddyIf you liked this article dont forget to clap and share!"
SQpGKDcV6fnm4RVodQYgkK,Bangkok Irsyad  In an article about social media mining the reader probably expects a chapter about Facebook Launched in 2004 and initially limited to Harvard University today Facebook is a multibillion-dollar company with nearly 2 billion monthly active users Its popularity makes it an extremely interesting playground for data mining In this series we will discuss mining posts from an authenticated user; mining Facebook Pages visualizing posts and measuring engagement building a word cloud from a set of postsThe Facebook Graph API is at the core of the Facebook platform and its one the main components that enable the integration of third parties with Facebook As the name suggests it offers a consistent graph-like view of data representing objects and the connections between them The different platform components allow developers to access Facebook data and integrate Facebook functionalities into third-party applicationsIn terms of data mining opportunities there was a major shift during 2017 with the release of version 20 of the API One of the main objects of interest in data analysis is the social graph that is the list of connections (friendship) between users Since version 20 of the Graph API applications that want to access this information must explicitly ask for the user_friends permission but the API will only return the list of friends who are also users of the given app Effectively this choice has transformed what used to be a goldmine for data analysis This section discusses the creation of a Facebook app in Python and the basics of the interaction with the Facebook Graph APIThe access to the Facebook API is offered through a registered app Developers have to register their app in order to obtain the credentials needed to consume the Graph API As a Facebook user you explicitly have to register as a developer in order to create apps and your account has to be verified via either by mobile phone or credit card From the Facebook Developers website (https://developersfacebookcom/) the procedure is straightforward: clicking on the Add a New App link under the My Apps menu will open the dialogue window as shown in below where we provide Social Media Mining as the display name for our sample app (there is a 32-character limit): Once we have selected a name and category for our app we can click on Create App ID to confirm its creationIn order to access users profile information as well as the information about their interactions with other objects (for example Pages places and so on) your app must obtain an access token with the appropriate permissions A token is unique to the user-app combination and handles the permissions that the user has granted to the application Generating an access token requires user interaction which means that the users have to conform to Facebook (usually via a dialogue window) that they are granting the required permissions to the applicationFor testing purposes another way to obtain an access token is to use the Graph API Explorer (https://developersfacebookcom/tools/explorer) a tool developed by Facebook to provide developers with a convenient interface to interact with the Graph API The figure below showcases the use of the Graph API Explorer after we have selected our app from the list of available applications we can click on Get User Access Token from the Get Token menu: This action will open a dialogue window like the one in Figure below that we can use to specify what kind of permissions we want to include in the access token Confirming the permissions for the app will generate the alphanumeric string for the access token which will be valid for two hours from the time of its creation Clicking on the small information icon next to the token will show this informationAs we can see from above figure the permissions for a Facebook app are particularly fine-grained In this way the users who install your app will have full visibility of the kind of data they want to share with your appOnce the app details are defined we can programmatically access the Facebook Graph API via PythonFacebook doesnt provide an official client for Python Implementing our own client using the requests library could be an interesting exercise in order to understand the peculiarities of the API but fortunately there are already some options out there to simplify the processFor our examples were going to use facebook-sdk also based on the requests library which provides an easy-to-use interface to get data from and to a web service At the time of writing the latest version of the library available on PyPI is 100 and it fully supports Python 3 The previous versions have shown some hiccups in terms of Python 3 compatibility We can install the library using pip from our virtual environment as follows: After we have obtained a temporary token following the steps described in the previous section we can immediately test the library The following script facebook_my_profile
GFY3s63LQVSXVevbZrwNQH,Prophecyio is a high-performance zero compromise cloud native data engineering product powered by Spark & Kubernetes for Enterprise Data Engineering teams Prophecy also provides a highly automated replacement for legacy ETL products to accelerate the journey to open source and cloud computingStarting in systems engineering and excited by the potential of GPUs I moved from Microsoft Visual Studio team to become an early engineer in the CUDA team at NVIDIA and played a key role in compiler optimizations to get high performance on GPUs Its a delight to see how far CUDA as come in powering deep learning and bitcoin mining Passionate about building a startup I moved to learn Product Marketing & Product Management leading both functions for ClustrixDB through a hard pivot to a limited repeatable product-market fitAt Hortonworks I product managed Apache Hive through the IPO It was not fun to be in front of customers and see them struggle with Data Engineering The Hortonworks team put in a massive effort to make Hive better & simpler  fewer configurations faster performance a real cost based optimizer and a simplified stack in Hive 20 and beyondMy co-founder Rohit Bakhshi brings strong product expertise with the go-to-market experience from Hadoop GraphQL Apollo and Kafka - scaling multiple Enterprise Data Engineering platform companies to unicornsWere delighted to raise our seed round from SignalFire with Ilya Kirnos joining the board & Andrew Ng working closely with usLarge Enterprises have 10s of thousands of ETL workflows in production on premise in a legacy ETL format and theyre paying through the nose There are compelling reasons to move to Open Source and Apache Spark - freedom agility talent and cost - that are well understood by the leadership of these EnterprisesWe think Legacy ETL workflows or dataflows are too low a level of abstraction to focus onLets look at some specific solutions: Apache Spark is ubiquitous in public and private clouds with managed services in public cloudsTranspilers (drawn as dragons since the dragon book) are our compiler based products to convert Legacy ETL assets into Prophecy with restructuring support This includes workflows configurations and datasets which we transpile to an open source technology stackAs Enterprises are moving to public clouds and multiple data centers hybrid cloud is the new-normal state: In these cases keeping a single control plane while distributing the data plane provides significant simplicity and cost savings The motivation for distributing data plane might be based on regulation performance or reliability This requires the right abstractions in the Data Engineering product for development and productionAs we talk to Enterprise teams were finding that with different roles come different preferences: Clearly current interfaces are not meeting these needs and as we look to design the right interface lets review the strengths of each: We believe there is a much better way of doing this! With compiler magic we have a Unified Visual & Code Interface that provides Interactive ExecutionThere are a few other unique features that Ill talk about in subsequent posts such as user defined componentsPS Were working on hard problems and looking for top engineers to help us get there! If youre interested reach out to me at rajbains@prophecy
VErFH6bWmFceqmezeZXCd7,At Prophecy were building a Data Engineering product to replace Legacy ETL bringing modern software engineering practices to data We have a unique take on the metadata system merging traditional metadata code and big data metadata into a unified system Now that the foundation of the system is strong wed like to share our learningsAs Enterprises abandon legacy ETL products to adopt modern data engineering theyre running into the challenges for which bay area companies have developed AirBnbs Dataportal Ubers Databook Netflixs Metacat Lyfts Amundsen Googles Data Catalog and LinkedIns DataHubOur metadata system represents persons teams projects workflows datasets scheduled graphs runtime environments clusters jobs It supports our Code=Visual IDE and Column Level Lineage We have designed the system with a focus on certain aspects that make us different: We liked the concept of modeling metadata as entities and aspects by LinkedIns DataHub and built on this It has the following characteristics: Now if we want to add business metadata such as column-level lineage we just decorate the datasets with the lineage aspect This allows us to develop new features without any changes to existing code pathsOn premise there are Hadoop clusters for test staging and production environments and in the public cloud the Spark clusters are often ephemeral In our systems Fabrics represent such physical or virtual environment Also the same workflow needs to read or write a dataset (logical dataset) that might be stored needs to read and write different physical locations So we have Physical Datasets for the same Logical Dataset on each FabricOur metadata storage is a completely functional git repository Our customer integrate Jenkins and CI/CD with it Metadata contains much more beyond Git thoughFor many Hadoop based systems Hive Metastore is a challenge It stores the schema physical layout and not much else Neither will it suffice for your needs for a rich metadata system nor can you do away with itComing from systems with background in databases and compilers having a REST interface for metadata made little sense due to high surface areaInitially with REST we ended up with too many endpoints no type safety and interface changes requiring much co-ordination This would be equivalent of having a SQL database and adding a new JDBC endpoint for every query We quickly abandoned it in favor of GraphQLFor GraphQL implementation we use Apollo client in the user interface to work with React and for our services we have written our own Scala client but we could have as easily added Scala plugin to GraphQL Code Generator (using javascript) Our services and crawlers use this interface On the server side we use Sangria with GraphiQL for testingStorage uses a Git client and for SQL we use Slick the functional-relational mapping is intuitive and terse The Entity graph is small and stored in Postgres Aspects are stored as Json documents also in Postgres We store metadata for multiple Hive Metastores in Postgres as wellApart from the incremental work of moving to represent consumption side with reports dashboards business definitions and business user comments the roadmap features that were critical considerations for the design are: Our users define new types of Aspects and decorate Entities with themEnterprises often have a multi-cloud strategy that we have designed for We have the concept of multiple Fabrics so that one data plane can be on Azure Databricks and the other an AWS EMR Secondly the metadata will be visible across both locations via shared storage made possible via geo-distributed Postgres compatible databases such as CockroachDBWe will implement text and facet search across all stored metadata soon including searching the codebase Its an essential part of any metadata system Well add relevance to show recent and important datasets for discoveryWere quite happy with how our metadata system has turned out and think it will serve us well for quite some time If you have ideas on improving it or want to discuss the system reach out to me at rajbains@prophecyio
HCdmtpGwNBJ2HRrY8hbCEa,As a startup focused on building a product for Data Engineers one of the core skills in our team is Parsers and Functional Programming in Scala Previously weve worked on compilers for CUDA/Graphics at NVIDIA or for Visual Studio at Microsoft One the other hand weve worked on a DSL for Contract Definition Language (to express computable insurance & re-insurance contracts for catastrophic risk modeling) where we wrote the parser in F#The constraints for these two classes of compilers tend to be quite different A hardware compiler typically takes a large team years to build and often Lex/Yacc or Bison thats LALR(*) is used for parsing Recently ANTLR thats LL(*) is the parser of choice for many Java developers These parsers quickly become complex and inscrutableAs a startup we develop at high speed with a lean team and for we really liked Parser Combinators  since theyre simple intuitive and fast to develop We didnt find comprehensive examples of Parser Combinators on the internet so were adding one that introduces them and addresses practical issues when developing themFull code on Github is here Well cover the following topics in the rest of the blog: Lets start with a motivating example where well write a parser that can parse this simple function in pseudo-code generating the Lexical Tokens and Abstract Syntax Tree (AST) shown below: This requires a Lexer (also based on ParserCombinators) that generates these tokens: and a Parser that consumes the tokens and produces the AST: Parser combinators allow you to create tiny parsers and combine them to build very powerful parsers Lets say we want to use regular expressions to create a tokenizer We can write one large expression and soon thatll get complicated to comprehend With parser combinators each different token can have a regular expression parser that can be composed into one parser for the program Lets do this by example  well start by trying to write a simple Lexer that reads the code string and breaks it down into tokens based on regular expressions: Here we note that there are 4 parsers the identifier parser parses the regular expression and on successful matching applies the function after ^^ that returns the IDENTIFIER case class (struct in Scala)Compositions of parsers can be done in a multiple ways Or Composition as shown above with rep1(literal | identifier) that matches one or more repetitions of two parsers ORed together This means that first the literal parser is applied at the current input position and on failure the identifier parser is applied so this gives us ordering of rulesSequential composition matches one parser after the other in sequenceRecursive descent parsers including Parser combinators do not support left recursive grammars This is often solved by modifying grammars to remove left recursion which is a painful processTherefore if you write a rule such as: youll get a stack overflow exception since youre just recursing back into the same rule However Packrat parsers use lazy evaluation inbuilt in Scala to delay the evaluation till the result is required allowing us to write such left recursive constructsLazy evaluation also provides memoization that makes sure that the matched rules are not applied again (by different paths) leading to linear time parsing at the expense of some memory This also gives us infinite lookahead and supports a really large class of grammars For the people whore interested in knowing more see Fords paper of Packrat parsing and Manohars implementation in ScalaOperator precedence does not often lend itself to a clean solution If you implement the parser simply youd do it like thus: You run this and quickly find the problem with precedence: the + incorrectly binds tighter than the *In recursive descent parsers the standard solution to this is shown to be a scheme like this: This only works for a very simple case but when you look at a real operator precedence such as the one for Java you see that there are 16 levels of unary and binary expressions that are to be interspersed with all the other expressions (eg if-else function calls) at each levelFor this blog the parser we added shows 4 levels We broke the overall structure of parsers using class inheritance: Now we create multiple levels of expression parsers each overriding binary operator parser per precedence levelOne thing that doesnt get talked about much is debugging of parsers Sometimes a small typo might lead to a rule not being applied where you thought it shouldve been Here is the debug output that shows the rules being tried which ones failed and which ones matchedWeve shown how to very quickly develop parsers for real world DSLs using parser combinators
ZcatnsyZuEoLskwkPQgJRK,Spark has become the default Data Engineering platform in the cloud With Databricks AWS EMR AWS Glue Azure Databricks Google Dataproc and Cloudera one can rely on Spark being ubiquitously availableAs we work with Enterprises to move legacy ETL to Spark weve been focusing on building the right replacement We find that the current interfaces fall short and defining a new interfaceDriven by Visual Drag-n-Drop interfaces these have a vast number of Enterprise developers adept at using them Actually its quite nice to get a visual overview of how the data is flowing but over time the clicking is exhaustingNow legacy ETL products claim to support Spark On the ground this means developing workflows in their proprietary format that sits in their legacy store generating unmodifiable crappy code There is no longer an appetite for these boxed solutions in the EnterpriseA lot of technology companies especially in the bay area choose to write code instead One can use notebooks but without ordering and standard structure there is a consensus that these are no way to write production codeIDEs give a vast canvas to paint code on but with the power and flexibility different teams paint differently They end up with different ways of structuring code and managing configurations In the worst case this means long Spark scripts where it is a nightmare to understand how the data is flowing by doing variable chasing across instructions Its no joy for a production support team to find errors under time pressureWe looked at various roles in Data Engineering including Architects Engineers QA and Support and Engineers with different preferences and thought hard about how to make everyone successfulWe believe the way forward is to use Git as the source of truth and Visual Graph as a view on the code Our Code=Visual (code-equals-visual) interface provides: Above you can see that the edits made in Code Editor are instantaneously visible in the Visual Editor Note that this is standard Spark code where every component is an object with apply function that is DataFrame in and out We focus on the DataFrame API right now that is equivalent to SparkSQLYou can edit the code add expressions local variables newlines comments  as long as the AST reduces to the defined sequence of DataFrame operatorsBelow youll see that you can make edits in the visual editor and they will show up in the code Also note that the structure and comments added in the code editor previously are preservedAs weve built this interface were finding that it has much more far reaching consequences than we had initially thought: Were quite excited to share what were building and get feedback so we can build the right interface for Spark Well dive into different areas in follow up posts: As we try to build the best interface we are sure other engineers have ideas on how we can improve on this wed love to hear from you Reach out to me at rajbains@prophecyio with feedback or request a demo here
AmFRabYH6aEw2hS8nrWGtv,Over the last decade the role of the data engineer has evolved drastically Industry shifts such as the emergence of scalable processing and cloud data warehouses mean that todays data is collected more frequently is far less siloed and more accessible to everyone across an organisation than ever before The role of the engineer has become integral as these systems require increasingly sophisticated infrastructureThis growth in significance responsibilities and available tools means that today it can sometimes be difficult to define the data engineer role and many companies actually differ in what they expect from their engineering talentWith this in mind we have gathered our top pieces of advice for data engineers to help them keep pace with an ever-changing role charting how the position has changed and how to work alongside data science teams to deploy the best standards of practiceTen years ago data engineering consisted of data warehousing business intelligence and ETL (tooling to move data from place to place) There was limited development of pipelines for analytical models but for the majority of industries these often remained in the proof of concept phase due to no clear path to productionIn order to remain competitive let alone stay ahead of the game todays companies are eager to invest in analytics and productionise models The sheer size and quantity of data has also increased giving scalability added importanceWith the changing attitudes and priorities of businesses there are a couple of key focuses for todays data engineers to thrive in this modern landscape: ● Best practices in the software development lifecycle (SDLC)  These include proper use of version control release management and automated devOps pipelines● Information security  With the ever-increasing threat of hacks data engineers need to understand cloud security best practices and be vigilant in handling data This includes managing data privacy in an evolving regulatory landscape (eg GDPR etc)● Data architecture principles  These have always been important in data engineering and include separation of concerns degree of logical groupings traceability lineage and well-defined data models● Business domain knowledge  Domain expertise is increasingly required to draw insights from dataAlongside the above areas that are key for all data engineers to understand we believe it is vital for them to be aware of the industry trends that have helped shape the role over the last decade Understanding these developments in analytics will help engineers explain the value of their work in conversations with organisations who may not be aware of these revolutionary advancementsCloud has finally reached a tipping point where even institutions such as finance and government that have historically shied away are embracing it In the last 4 years alone the market for cloud computing has doubled from ~$114B to ~$236B Amazon Web Services has led the market over the past several years (currently at 33% market share) but Microsoft Azure (13%) and Google Cloud Platform (6%) are catching upData engineering used to be dominated by closed-source proprietary tools Now we are seeing a growth of open source tools and in many cases a preference for these tools in data organizations Open source libraries such as Spark and Tensorflow have become widespread and many organisations are seeking to minimise vendor or product lock-in This was a driving factor in open sourcing QuantumBlacks very own Python library KedroCompanies simply have more data at their disposal now more than ever before which makes it more important for data engineers to understand how to scale More than 90% of the worlds data was created in the last few years Data engineers need proficiency in tools that can help to quickly organize and assess this massive amount of dataBefore working closely with an organisation data engineers need to assess the existing infrastructure for analytics to see what level of onboarding will be required For example how you approach a complex project with a company inexperienced with data science and data engineering will be completely different from an organisation with an existing analytics platformAt QuantumBlack we find that clients often fit one of four archetypes in regard to their data engineering capabilities These are: In all of these archetypes data engineering plays a critical role; it is often the make or break factor in organisations achieving their North Star in analytics It is widely cited that 80% of data science consists of data engineering However even in companies that see analytics as a competitive differentiator and have it on their agenda we often see ineffective internal data organisation This leads to a conflict between upper management who wish to see value from analytics and the reality on the ground mired in a challenging and technical data environmentFor analytics teams to work efficiently with organisations and convince them to invest in data engineering no matter the existing analytics capability it is important to show the long term value that engineering talent can bring to the table Data engineers can unlock data science and analytics in an organisation as well as build well curated accessible data foundationsAt QuantumBlack we believe that data engineering and data science should work hand in hand That was part of the inspiration for Kedro our Python library that creates a shared project platform for data science and data engineering Rather than staying in separate siloes we have seen improved performance in clients that fully integrate the two teamsIn short companies must consciously invest in developing their data engineering capability in order to have a truly successful analytics program This includes setting a solid foundation with data governance  identifying gaps and quality issues while improving data collection The organisations that thrive in the years ahead will be those that not only acknowledge the challenges of developing and productionising models but actively invest in engineer talent in order to maximise value from dataIf you are interested in joining our data engineering team please see here for roles currently available and contact us at careers@quantumbackcom
72YYBFD8YQZngfzB85PdGp,A key tenet of Raybeams mission whenever we start at a new client is to deliver value quickly This value often takes the form of marketing or product insights that we derive by combing through large amounts of data As Raybeam is a software engineering firm specializing in data and analytics we tend to work on the data backend and the analytical frontend simultaneouslyThe most furious pace of engineering work tends to happen at the beginning of a project as we update or migrate the data and processing architecture As data quality improves were able to use it to deliver more and more insights While we do pride ourselves on delivering sustained quality at a quick pace not everything can happen at onceModern data pipelines often consist of hundreds of external sources sending millions or billions of events per day A good pipeline can collect clean process and serve that data to users quickly and on a regular schedule When a customer marketing manager checks their dashboard in the morning it should reflect the current state of customer marketing There should be no surprises If the data is meant to be less than 2 hours old it should beBut what happens if one source of data hasnt been updated? It could be one of a million reasons; an API token expired the source was late or devops had an unscheduled outage Anyone in a data organization has had this happen to them probably multiple timesTrust in data like all trust takes a long time to build and a moment to break Once broken you must start from scratch Broken trust doesnt just mean the data is wrong It means the data is wrong and no one was notified Its an important distinction Its not possible to control all of your upstream sources so that data is never wrong or out-of-date But it is possible to notify users they make a decision based on incorrect or outdated dataWe wanted to create something that is simple for analysts or data engineers to incorporate into their workflow but also effective for reporting status to those colleagues that might not be as familiar with the underlying data processThats why weve created the rb_status_plugin for AirflowFor most business users it should answer the question Is my data ready? quickly If the answer is no it should give the user some way to follow upFor analysts or engineers it should fit seamlessly into their Airflow data system and be quick and easy to set upOur plugin installs like any other Airflow plugin There are detailed instructions for a variety of Airflow setups in the READMEOnce installed the rb_status_plugin is incredibly easy to use It allows you to choose a set of Airflow tasks and assign their result to a report That report can be scheduled the same way an Airflow DAG is scheduled In fact thats exactly what it is The plugin will create a DAG to run its checks based on the tasks you care aboutThe tasks can be anything but weve found it works best with data quality checks We tend to run many Pass/Fail tests against our data pipeline during and after its execution In a very simple example we could have a set of tasks that verifies: We could bundle these tasks into a report and schedule it for every morning at 5AM Now say this dataset contains all of the data used in the marketing acquisition teams dashboards We could make sure all of the marketing managers on that team were subscribed to this report Each morning when they log on theyll see an email showing them the status of the data that day It would presumably be green the majority of the time For the days where there was a problem however they would see that the data is not ready and have a quick way to follow upOur primary goal was to make it both simple to create reports and simple to use the reports Our philosophy is to make small simple tools that work well Obviously there are many other parts that go into providing confidence including scheduling data quality testing logging and alerting just to name a few We build tools to handle those parts of the process as well Were working on a plan to make it easy for all of these tools to work together For now though we think weve created a simple tool that will make our lives and our clients lives just a little easier Maybe it will help you tooWere going to walk through a couple of use cases to show how were using this pluginFirst lets assume were working with a social media marketing manager They handle all social channels and their overriding goal is to make sure theyre getting the best ROI for their budgetIve just made this up but you can see that Im loading data from some social channels joining it running a few tests and then building some downstream data setsFrom the green outlines you can tell theyre all passing right nowNow lets pretend Im an analyst on the social media marketing team and I want to create a report so the people I support can feel confident about the data without asking me every dayThe rb_status_plugin adds a menu item in Airflow named Status You have two options in the drop down You can view all current statuses in the Status Page or you can go to the current report listWere going to click into the Status Page in this exampleWe havent created any reports yet so well be greeted with some helpful options and instructionsWere taken to a straightforward page for adding a reportYou can name and describe the report so that anyone receiving it knows what the status relates toThe owner and owner email will automatically be added to the list of subscribers Emails for the main audience of this status report should be added in the Subscribers fields In our example that would be anyone in the social media marketing teamFor scheduling the options follow Airflows scheduling options You can choose None if youd like to manually trigger the report This is good for when youre first testing it Otherwise you can choose daily weekly or for more customized cases cron notationNow were going to choose the tests that will combine to determine the status of the report Tests in our case could be any task in Airflow A lot of times that will be data quality tests However the rb_status_plugin library can use any Airflow taskYoull notice we try to make this easier by providing a type-ahead search and dropdownSomething to note if a task has not yet been run it wont be in Airflows database so it wont show up in the dropdown That means that you need to run the specific task at least once manually or run the entire DAG that its in at least onceIf you go back to the status page youll see a warningReports are actually DAGs behind the scenes so they need to runSince we set the schedule to None its not going to run on its own Well need to manually trigger itYou could trigger it like any other DAG by clicking the play button in the DAG list We also provide a run and delete option in the report list UINote that all DAGs are created in the Off state for Airflow We at Raybeam like to follow conventions of the underlying tool as much as possible so reports will also be Off after creation Youll need to click on the On/Off toggle either in the report list or DAG list to turn the report on and run itOnce the report is On and youve run it you will need to wait until its picked up and run by Airflow It should finish quickly since its mainly checking Airflows own underlying databaseIt looks like all tests have passed and we have a successful statusClicking on details will take the user back to the Airflow status pageNow is a good time to go back and schedule this report to go out on a regular basis That way you can keep your users up to date automaticallyLets create a second report For this one Im going to pretend that Im on the data engineering team I dont know exactly how the business is using every piece of data but I do know that all of my external partners need to be loaded correctly every day on timeFor this example Im going to have our Facebook load fail Maybe someone didnt check and the API key expiredIn this particular case weve set up the DAG to continue even if a particular job fails Thats because weve set up other tests throughout that DAG that tell us if the final data set is validMaybe for the social marketing team they can go one day without new Facebook data Yeah yeah I know pretty unlikely but its an exampleFor this report Im going to include all of the load tasks In an actual Airflow instance maybe I have load tasks scattered throughout a bunch of DAGs Thats fine! The rb_status_plugin views everything at a task level You can add tasks to a report no matter which DAG theyre inNow you can see our report in the report list Well need to turn it on and run itWell as you can see our status report failed because the Facebook loading job failedYoull also notice that the social marketing report is still listed We include a summary note at the top that Some tests are failing If its red you know to check the specific reports If its green you know the entire system is good to goThe failed task is listed in the details and it links back to the job log for that task run You can easily use the status report to click back into the task and find the underlying problemAnd once again any subscribers will receive an email with the current status of the reportBy now youve been thoroughly introduced to the new rb_status_plugin We discussed the need for it and have gone through a couple of use casesTry it out and give us feedbackYou may have noticed that our screenshots look a little different than your Airflow setup Thats because we use Astronomer when we have the choice Astronomer is the easiest way to run develop and deploy Airflow that weve foundWe use their docker image for local development Scaling to an enterprise capable deployment can be done in minutes Give them a try if you havent yet
KkZ9nCy2B74WnPbUF9zPkJ,Even before I joined Postman my colleagues here were dealing at a scale of handling 6 million users all around the world The amount of data which needed to be processed to get some meaningful insight was huge Here is a taste of its scale: Some months were in for me at Postman I got assigned to a project which needs to handle millions of rows in service logs These spans over GBs of dataWithout a doubt we could have processed these logs in vanilla code and maybe use some libraries too But this would incur more cost for operating and maintaining the code base Moreover libraries which are not tested rigorously increases the surface area for bugs All these added libraries and infrastructure results in increased human hours Then we decided to look for something elseWith prior experience in distributed systems the team and I knew of its advantage and limitations Keeping that in mind the next step for us was to look for solutions in distributed processing And dont forget the Map-Reduce functionalityPostman believes in a philosophy that human hours are the most valuable resources We always strive for a managed solution as much as possible Maintained solution handles upgrades of software and hardware by itself (3rd party) We need to focus only on logic not anything else around itWe were at AWS Community Day 2018 Bengaluru If you want to check out the photos visit hereDeveloping any project in a vanilla code of any programming languageORusing libraries available in the Apache Spark ecosystemAn advantage of vanilla code is you are familiar with basic concepts You could have skills and tricks to do something in a way which makes development faster for you The caveat here is could you propagate the same learnings to your team members? You might want to have better control over what you want to achieveOne can argue about a similar set of knowledge in the ecosystem of toolsAccording to my viewpoint • support of a community2 better documentation of any methodology3 system paradigms4PS: With Apache Spark being an open source tool you also dont lose your controlIn my early years of programming one of my colleague beautifully put a thought in my mind I can say this actually transformed the way I write code I wont put any effort into explaining this casually putting it hereCode is like poetryI can not put more emphasis on how difficult it is to modularise any code Also then maintain the same if you are not the only developer on the project A single developer project enables you to put your thoughts (read opinions) on how you structure the code It might be good or maybe not The real test of skills happens when there are more than one contributors to the project Your modularisation should be consumable without you explaining to each one of themWith tools such as Apache Spark the contributed code is very small Why? Because all the nitty-gritty of core functionality is hidden away in library code What you have is a very simple small liner of the ETL processFor egThere is a lot of support for simple readers and writers from the Apache Spark community This enables you to easily modularise your code and also stick to one paradigm So what do you get finally? You get a beautiful structure code which everyone can understand and contribute toPS: You can always extend the default readers and writers to perfectly match with your requirementsI have a very strong belief in the power of community For me community boils down to something like a league of superheroes fighting for a common goal Here is a quote to put my thought process in very simple wordsTwo is better than one if two act as oneWhy I believe in that is because if one fails the other one will help him There can be a debate using this proverbwhich means that if there are many people involved in doing the same thing the final result will not be good But this happens when things happen behind closed doors in a kitchenOpen source community has solved this problem very beautifully and proved its mettle Quality and quantity of open source projects and community are proofs for it Apache Spark is one of them This means that its quality maintainability and ease of use is much better At least better than few chefs trying to build vanilla products behind closed doorsPS: Active open source community means there are a lot of people already maintaining and actually doing the work for youTime to development is one thing and actually deploying the code in production is another Most of the times projects stay in PoC mode and never come out of it I have seen some companies use the same PoC in production server These PoCs dont give much thought on whether they will be able to handle the current traffic and rate at which load increasesInherent nature of distributed systems such as Apache Spark makes supporting large scale a cakewalk These systems are designed solely for the very same reason to handle scaleWhile building a vanilla system most of the times it is designed to handle current load presuming it wont increase This might be true in an ideal world but we dont live in onePS: While I try to vertically scale most of the times but I have done horizontal scaling as well which is buttery smooth in distributed systemsMonitoring and ObservabilityI wont go into details of these terms as there is a lot of content around it already For production deployments you might hope to do it and forget it in an ideal world but again we dont live in it Deploying anything in production brings a lot of itsy-bitsy or larger problems Systems in production should be continuously monitored and be designed for observability Dont forget the alerts too which could subside under monitoring Apache Spark with its web UI and added support from AWS makes it a much better alternative than building custom solutions in vanilla codePS: Probably you dont I am not judgingI usually write about Data Find more posts from me on medium or on the devto community
aiyqCXPG2RLfCJmp3ZgG7y,Ive worked with data for 20 years and still get amazed by the amount of reliance companies have on Excel Spreadsheets Im not talking about the normal uses of Excel; Im talking about the big horrible spreadsheets Probably created by some consultant or part time techie 10 years ago who loved a bit of VBA and has since moved on to greater things But long after theyve moved on their work is still fundamental to the business and no-one really understands how its just a big lump of risk that no-one can afford to fix I shouldnt really be surprised; Excel is a brilliant and very powerful tool which pretty much everyone is comfortable using And whats the alternative anyway? Corporate IT departments dont typically allow business users access to buy or create software I dont blame them; Ive seen greater monstrosities created in MS Access which fewer people could supportSo were left with Excel Spreadsheets flying around the business storing large amounts of critical data In recent years SharePoint and Office 365 have improved the governance and reduced the amount of emailing of these documents But were in an age where companies are trying to maximise their use of data they want it consolidated aggregated and analysed The CEO wants (on his mobile) up to the minute stats about how the business is performing but how can he get that when the information is spread over countless files This is the job of the Data EngineerIts not a new problem weve been extracting data from Excel for years but it never just worked Just as you thought you had a tried and trusted methodology you found out the customer was running an ancient version of office never mind challenges over 32bit Vs 64bitEven if the versions aligned there was still another problem most techniques involved having some Office components (If not full Office then at least Microsoft Access Database Engine 2010 Redistributable) running on the computer performing the data extraction Whilst the adoption of SharePoint ( O365 or Teams ) has meant that the Spreadsheets are no longer sitting on file shares or individual computers we have lost a natural host for these componentsAlthough Microsoft Graph has been around for a few years Ive been too busy getting on with tried and tested architectures to look up from the weeds and find out about this new technology As is often the case it takes a different scale of problem which cant be solved by any existing techniques to make us go and learn about how we can adopt something new Thats exactly what happened with a recent customer and caused me to discover and love Microsoft GraphMicrosoft Graph is the gateway to data and intelligence in Microsoft 365Thats how Microsoft describe it all delivered through single a REST API endpoint Providing access to SharePoint / Teams / Outlook / Azure Active Directory and much more besidesWhilst some of the data is read only there is much that can be written to as well Im not going to try and describe all its capabilities here since theres plenty of documentation already available (plus API reference) Rather I wanted to share an example of how its helped me solve a real customer problem and become one of my go-to technologiesThe recent challenge which put Microsoft Graph firmly into my toolkit was with a customer who had built a whole application in Excel and then created several thousand copies of it These Spreadsheets were used by a large number of people in the business; each planning and manage their individual area The problem was that the company wasnt able to see what was happening across the organisation; they wanted to be able to compare each areas performance back to a baseline A significant amount of functionality was built into these Spreadsheets plus the users relied on so much that Excel just provided out of the box which meant the option of rewriting them into a modern application was just too costly for the benefit it would deliverMy first thought for interacting with documents in SharePoint was to use Flow for it has connectors for Excel Online Alas the Excel Online connectors in Flow were unable to interact with the Spreadsheets because of Unsupported Features ie they were full of Macros and saved as a binary files (xlsb) to help performance Whilst trying to figure this out I stumbled across one of John Lius Blogs and then his walkthrough using the Graph API and this changed everythingWhat I learnt was the ability of the SharePoint API and the Graph API to: With a few steps I could iterate through all the files in the document library checking for the existence of worksheet I needed and extract the data to a csv The MS Graph API didnt care that my documents were full of macros or the format they were saved in Nor did it base the structure of the table on the first 8 rows (another favourite feature of the Microsoft Jet OLE DB 40 Provider we once used extract data from Excel) It just pulled the data straight out of the cellsYou may notice that my first couple of calls were made to the SharePoint API whilst the later were to the MS Graph API They key difference between these seem to be how you authenticate (Flow simplifies this) and (more importantly) that the MS Graph requires the premium Flow connector HTTP with Azure AD / Invoke an HTTP request Using a Premium connector can significantly change what licence youd need and if youre not sure about that then Id recommend reading Matts blogWhilst I was very happy with this architecture my customer wasnt and so I found myself looking for alternativesData Factory is great at moving files between lots of cloud sources however there isnt an out of the box connector (linked service) for a SharePoint document libraryWhat there is is a linked service for REST APIs which brings us nicely back to my first experiments with the new MS Graph API Interacting with the Graph API follows a common pattern of interacting with many other REST API We request a token using a set of credentials and then send that token into each request The token is valid for a period of time but needs to be submitted every time because by its very nature the Graph API is RESTful and holds no stateThe key difference in the development experience between Flow and Data Factory is that Flow simplified the authentication experience whilst in Data Factory we have to get our hands dirty and manage this ourselves There is a nice blog from Lars Bouwens which help me get started and the Microsoft documentation helped fill in the gapsTo get this working we need to: Were authenticating to the MS Graph API through a Service Principal If youre unfamiliar with a Service Principal think service account or daemon The idea being that it is just an account that an application uses to access resources without being tied to or logged on as an interactive userPlainly there is security aspect here which needs to be considered MS graph API is incredibly powerful and can literally drill into information around your data estate on OneDrive on SharePoint online amongst other things So there needs to be control This is achieved again through the Service Principal by only allowing it to access sections of the MS Graph scopeIn this example we need to grant our Service Principal that is calling the MS Graph API access to the SharePoint Online folder where our Excel files are going to be stored This needs to be done by someone with Azure AD Administrator Permissions If you need to only read information then the permissions that the Service principal needs are: If you need to add or update lists and their contents then you need : Once these permissions have been requested you then need to have them approved by the Azure AD administratorSo whilst Data Factory can absolutely be used to invoke the Graph API this would have introduced a new service into our architecture My preference is always to limit the number of technologies used within a solution to help with ongoing support and maintenance So before implementing this solution I thought Id have one more tryWe already had an instance of Databricks running in our solution and current thinking seems to be that there is no data challenge which Databricks cant solve so this was the obvious next candidate to provide a solutionFirst I wanted to see how well Python (key languages of Databricks) could shred the Excel documents The go-to library for manipulating data in Python is pandas and because Excel is so prevalent this has a method for reading Excel files into a DataFrame (the data structure used to hold a table)Alas whilst read_excel supports both xls and xlsx file types it doesnt support xlsb But Python is full of open source libraries and of course Im not the first person with this problem so eventually I found an Excel 2007 2010 Binary Workbook (xlsb) parser for Python in pyxlsbSo Databricks could perform the shredding of the xlsb file into a usable format but my solution is not yet complete The Excel files are managed within a O365 SharePoint document library so Id first need to move them onto the Databricks cluster but they are large files and I didnt like the idea of continually copying these files around to extract such a small amount of informationAny programming language worth its salt these days is able to interact with REST APIs as they are the lingua franca of the Internet in terms of integrating systems Databricks support Scala and Python amongst other languages and seeing as Python is a commonly adopted language for data scientists and analysts well look at an example in PythonWe first need to get a token which we can pass through to the Graph API This is very simple In our notebook we can declare the following function: The requests library allows us to communicate with the REST service json allows us to manipulate the REST call results easily and adal is to allow us to use the AuthenticationContext for Azure AD Our service client secrets are stored in Azure KeyVaultWe create the REST session (not a server side session) then use the acquire_token_with_client_credentials method of the Authentication Context object to obtain a token for subsequent calls This returns us a token_reponse object from which we can then construct the correct authentication header for each REST call we makeFrom this point on its easy to drill into the MS Graph API as we see fitThats it Once you have a grasp of the Service Principal and the hierarchy of the Graph API its simply a matter of playing with the syntax to drill to the data you wantOf course not everything is smelling of roses because Excel doesnt know the difference between formatting and data Itll still helpfully remove leading zeros from your phone numbers and convert what should be a code 5E2 into the number 500 but we can only fix one thing at a time Unfortunately the end users will still ask can you load cells coloured green but theres just no helping some peopleAs is often the case what started out as an interesting requirement has lead to a exploration and reevaluation of the latest technologies Ive discovered a new architecture to add to my toolkit So when a customer now talks to me about their problems with Excel documents Im thinking about the opportunities to use the Graph API rather that the fear of what I might uncoverI just wish Id found the Graph API sooner
gT9izEJxruZxrd2CbNkZeB,Over the years of building data platforms Ive used lots of technologies to load and transform data but the data warehouse itself remained pretty constant Most of the time hosting it on SQL Server then subsequently on Azure SQL database which I saw mostly as a means to improve the cost of ownership and simplify disaster recoveryWhen Microsoft first converted their Parallel Data Warehouse product to be a cloud offering (originally called Azure SQL Data Warehouse) I didnt switch acrossBut some the features of whats now called Azure Synapse Analytics have at last drawn me in: the ability of Polybase to integrate with the data lake combined with the separation of compute costs from storage costs With support for database projects added in Visual Studio 2019 Synapse has now become my default DW hosting optionTheres still the occasional technical challenge but they seem more surmountable Like making the most of the platform how to make the data flow fastest for the smallest running costTheres two standard approaches to handling increasing scale: buy bigger (scale up) or buy more (scale out)Hosting in the cloud means that scaling up is only a financial challenge theres no need to replatform you just need to slide the lever upScaling out means distributing (sharding) your data so different fractions of your data resides on different databases (eg split by customer) This allows the processing load to be shared out but typically this isnt trivial to set up or maintainWith Synapse you can take advantage of both approaches Choosing a good distribution for your data gives you the benefit of scaling out whilst the ability to scale pause and resume means you only pay for the compute you needTo understand whats limiting the ability of the database engine to execute queries would traditionally start with looking at the slow queries and understand what indexes support the query With Synapse Analytics there can also be problems with skew in table distribution but we should also consider how much compute resources (ie memory) are allocated to the queryThis is because unlike in Azure SQL we need to tell Synapse what types of queries were expecting to execute on the connection since it will allocate the resources to the command before attempting to execute itTrade-off between available memory and concurrencyEffectively we need to make the Scale Up Vs Out decision for each of our connections We tell Synapse Analytics this by allocating each user to a resource class these resource classes indicate what percentage of memory is allocated to executing each querySo for a Synapse Analytics instance running with Service Level DW400c the memory allocations for the dynamic resource classes areThere is a temptation to think that in a data warehouse the queries are churning lots of data and therefore will need lots of resources so they should be assigned to xlargerc But with 70% of the resources allocated to each query that effectively makes the workload single threaded Even setting lowering this down to largerc means only 4 concurrent connections can execute in parallelTo get the most data through the system we first need to understandUnfortunately the answer is as often with performance it dependsFortunately the monitoring built into the Azure Portal helps usHere you can see multiple queries being thrown at Synapse running with a performance level of DW400c Ive set the grain of the charts to 1 hour and with many of my queries taking over a minute to run I dont need to worry about counting queries that start and finish within the minute The connections are logged on with a user assigned to the largerc using Azure Data Factory to execute many queries in parallel Despite this the top right chart shows that we only get 4 concurrent queries briefly 5 when a simple query was executed by smallrc userThis shows that when we try and push more data through the platform the queries just start queuing up Whats more our use of DWU (Data Warehouse Units) is maxed out at 230 despite us paying for 400 This suggests we should reduce the memory allocated to our connection to allow greater concurrency Or even use mediumrc connection for smaller queries to increase concurrency and largerc for those which demand more resourceAs youre beginning to see reaching the optimum tuning requires lots of patience to rerun the workload and analyse the improvement Whilst I havent got a go-fast button hopefully this has helped you to start understanding how your workload is being throttled To really tune it youll need to invest some time to understand the inbuilt Dynamic Management Views
YSfoLmhyMvf2WjYm7LzrCn,As with creating new functionality in any programming language the first task is always make sure the code produces the expected output (the unit test) Next is to clear down the system and check it will still work when the new component its incorporated into the whole (the integration test)At this point the temptation is always to push the change into user testing and move onto delivering the next piece of functionality Often forgotten is to review the solution and understand how it behaves at scale; when a serious amount of data is thrown into it This may not even be a requirement for the business as usual operation maybe just during the initial data migration But invariable if its not considered in advance the system is bound to fail at the worst possible momentAzure Data Factory is built for scale but this can be limited by neglecting non-functional requirements and choosing the wrong coding patternOne important place to check in the For Each activityThe For Each activity has a subtle but critical configuration Sequential By default this is set to false meaning that ADF will try and perform all the iterations at the same timeThis will be good for you if you your code expects it and allows for itConsider the simple requirement use ADF to move files to a directory which must be looked-up in a configuration databaseAn initial approach may be to loop over the list of files call a procedure to determine where the file needs to go set the result to couple of variables and perform the copy activityFor unit testing set the Sequential flag to monitor how it all worksThis will work in unit testing a few files but try and move hundreds of files it will take a awfully long time (as you might well expect)Alas the behavior is not as some might expect What you need to appreciate is the scope of the variables They are not scoped to within the For Each loop rather there is only a single instance of them for the entire pipeline execution This means that when the pipeline is executing with multiple For Each items running in parallel the setting of the variables will be happening out of sync with the file copy activityWhat this means is that the files will likely go to the wrong place and thats likely to make you unhappyIn this scenario we can see that the problem is in the use of variables Whilst Id typically encourage using variables to make code clearer to monitor and maintain in this case its preventing the solution from scalingSo we need to replace the setting of variables with expressions within the subsequent tasks ieSo a few subtly changes can make a big difference to how your solution performs Yes some of your expressions will become more complex and you may need to repeat blocks of logic in multiple places But its better to have a well performing system than just beautiful codeRemember that Programming languages are all different and a pattern that serves you well in one can cause you serious headaches in anotherHopefully reading this will have removed one such headache for you
VybDeTQTuvW7fY7PZ46XnW,Here at Riskified we analyze millions of online transactions per day to detect and prevent eCommerce fraud Our AI platform crunches these orders to predict their risk level and returns a final approve or decline decision As Riskifieds R&D we use our own machine learning-based pipeline to select data train the models validate and replace them in production We use several different models according to industry and other parameters To make sure were always using the right model for each case our data science department validates the model using its predict component which simulates our online flowIts not just our data science teams that use the models predictions to check their accuracy Our business operations teams also use it to forecast performance when onboarding new clients As developers its our job to provide those users with an internal tool that they can use to select the data perform a prediction and get results from the modelRecently due to our increase in volume of data we reconstructed our ML infrastructure Our main goals were to have an infrastructure that could support our need for accuracy scale and stability In the end we chose Apache Spark as the main computing engine that most of our platform is based onIn this blog alongside Hen Ben Hemo well talk about our migration to Spark and the specific challenges that arose from using the CatBoost algorithm for validationOur offline flow for predicting the risk level of an order (meaning giving it a score from the model) includes the flowing process: In the past when running the process above on Redshift we performed the following steps: So basically this is the code we used for this process (pseudocode): In our case we needed to process a huge amount of data (over 5 terabytes) that contains the information for orders along with their features (calculated by our data science department) Each row of orders can contain hundreds of features meaning a lot of columns (around 800 more or less) Also we wanted to be able to make predictions using several CatBoost models meaning our basic logic should be: Each prediction returns the score provided by the model for every order based on the trained modelAs we grow not only is the number of users taking advantage of this tool rising our sample size for prediction is increasing as well We now have over one billion records of data that we want to classify We wanted to be able to predict more than one million orders for each batch using several modelsAs you can see our previous orchestration wasnt built to the scale were aiming for: We chose Apache Spark as our main data processing platform Our main reason for choosing it is that Spark uses an in-memory computation engine which makes it much faster than its competitors Its support of Master-Slave Architecture makes it scalable when needed (by simply scaling up Spark executors as necessary)Additional reasons for choosing Spark for Riskified: We use the CatBoost library as the main machine learning algorithm in our analysis flow Over the last six months weve upgraded our ML analysis platform pipeline such that our core structure is based on Apache Spark but when we began rewriting our pipeline we found that the current CatBoost library (024) still doesnt have Spark integrationIn order to create a single instance of our models in the Spark executor we load the models in each executormapPartitions() Spark transformation is similar to map but runs separately on each partition (block) of the RDD so func must be of type Iterator<T> => Iterator<U> when running on an RDD of type TAfter loading the data (in our case orderFeaturesDataset) we use mapPartitions for distribution so that each executor works on a different part of the dataset Now when we are inside mapPartitions we can run an atomic code inside the individual executorsEach executor has an iterator (a part of the dataset) We then load all of the models using our CatBoostModelLoader and iterate the rows in that executor using flatMap Next we can call the evaluate function (that performs the prediction) of each model that we loaded in order to get its score: In this post we went over our solution for using Spark as the main computation engine for our prediction component in our machine learning platform pipeline We discussed the issues we encountered while using Spark & CatBoost and how we dealt with them eventually succeeding to process over 1 million records in just 6 to 7 minutes! (for comparison with our previous tool the time measure was around 30 40 minutes for each run) This gave us the ability to classify dozens of models a day (scale  yay!) compare them and decide which one gives us better results
Frf3UmcinQRyhGV6NMWd6i,At Ro we have a significant number of data access permutations (the usual financial + operations + pci levels plus a variety of HIPAA buckets) We also want to minimize the amount of time it takes any Looker user/analyst at Ro to disseminate useful information to the entirety of the companyRather than maintain a large number of parallel models dashboards and look spaces weve opted for column-by-column permissionsThis means that all of our Looker users can see all explores and the names of all possible dimensions and measures However if they try to access a dimension or measure in an explore for which they do not have access they will simply get no data If they are trying to view a visualization/look that someone else created that has information that they are not authorized to access they will see that the visualization exists but they will not be able to see any data
aWzX6yAmjikN2yYrmceCXw,The open-source repo for this tool can be found hereCreating pivot tables is a relatively common need as seen by its popularity in software like Microsoft Excel and Google Sheets We built a small tool to create pivoted database tables It can be used as a standalone tool or integrated into ETL pipelinesAt Ro our business and operations are growing quickly in terms of both scale and complexity We launched at the end of October 2017 with a single brand (Roman) treating a single condition (erectile dysfunction) but in the last few weeks weve added a new brand (Zero for smoking cessation) and several conditions under Roman (hair loss cold sores genital herpes and premature ejaculation) with more on the wayFor the data team this means our data warehouse needs to support business units and analysts who want to look at data through a variety of lenses They may want to focus on the overall company on a particular medical condition a particular brand or some cross-cutting category (all prescription medicines vs OTC products) Pivot tables are one important tool for making it but easy and efficient for them to ask and answer the questions theyre interested inFor context our data warehouse is in Amazon Redshift and our solution is currently Redshift-specific (though it would be straightforward to adapt it to most other databases)At the database level the most common way to write pivot queries is probably to hand-write chains of CASE statements This is unsatisfactory because its labor-intensive error-prone and needs to be updated by hand every time a new column is added or a new case becomes possibleAlternatively pivoting may be done outside the database using a spreadsheet program or a business intelligence tool like Looker This doesnt help if its important to have the pivoted data in the database whether for performance reasons or because its used further in your data pipelineWe built a simple way to generate pivot tables (or just pivot queries) which weve posted on GitHub here The README discusses how to use it including some examplesThe implementation strategy is to take the list of pivot columns find every existing combination of values in those columns and then generate appropriate pivoted columns An aggregation function and default value can be specified individually for each pivoted column or the script will attempt to guess reasonable defaults based on the columns name (ie SUM will be used for a column whose name starts with `total_`)The result is that we can declaratively specify pivot tables and have them automatically regenerated The changes necessary to support new columns are either non-existent or minimal and localizedSpreadsheets and business intelligence tools can create pivot tables but thats not always a replacement for having an in-database pivot tableSome databases include related functionality (like PostgreSQLs crosstab table function or Microsoft SQL Servers PIVOT feature)The popular tool dbt also includes a macro for creating pivot tables in the dbt-utils package
Ls2kqEtDmG785evQFX7oka,In February we published Automatically create a pivot table in Redshift This is an updated version of that article now that weve switched to Snowflake where we use the same techniqueWeve open sourced the solution described in this article hereCreating pivot tables is a relatively common need as seen by its popularity in software like Microsoft Excel and Google Sheets Some databases (including Snowflake) include a pivot function but theyre too static  you have to provide an explicit list of values rather than deriving them dynamically from the data We built a small tool to create pivoted database tables It can be used as a standalone tool or integrated into ETL pipelinesAt Ro our business and operations are growing quickly in terms of both scale and complexity We launched at the end of October 2017 with a single brand (Roman) treating a single condition (erectile dysfunction) but since then weve added two new brands (Rory for womens health and Zero for smoking cessation) and several conditions under Roman (hair loss cold sores genital herpes and premature ejaculation) with more on the wayFor the data team this means our data warehouse needs to support business units and analysts who want to look at data through a variety of lenses They may want to focus on the overall company on a particular medical condition a particular brand or some cross-cutting category (all prescription medicines vs OTC products) Pivot tables are one important tool for making it but easy and efficient for them to ask and answer the questions theyre interested inFor context our data warehouse is in Snowflake and our solution supports either Snowflake or Redshift (which we used previously) It would be straightforward to adapt it to most other databasesAt the database level the most common way to write pivot queries is probably to hand-write chains of CASE statements This is unsatisfactory because its labor-intensive error-prone and needs to be updated by hand every time a new column is added or a new case becomes possibleAlternatively pivoting may be done outside the database using a spreadsheet program or a business intelligence tool like Looker This doesnt help if its important to have the pivoted data in the database whether for performance reasons or because its used further in your data pipelineWe built a simple way to generate pivot tables (or just pivot queries) which weve posted on GitHub here The README discusses how to use it including some examplesThe implementation strategy is to take the list of pivot columns find every existing combination of values in those columns and then generate appropriate pivoted columns An aggregation function and default value can be specified individually for each pivoted column or the script will attempt to guess reasonable defaults based on the columns name (ie SUM will be used for a column whose name starts with `total_`)The result is that we can declaratively specify pivot tables and have them automatically regenerated The changes necessary to support new columns are either non-existent or minimal and localizedSnowflake has a `pivot` function but it requires a static list of values whereas our solution pulls them from the database dynamically as compilation timeSpreadsheets and business intelligence tools can create pivot tables but thats not always a replacement for having an in-database pivot tableSome databases include related functionality (like PostgreSQLs crosstab table function or Microsoft SQL Servers PIVOT feature)The popular tool dbt also includes a macro for creating pivot tables in the dbt-utils package
jpB37CUhGi2apKnQYEoUKa,The open-source repo for this tool can be found hereWhen building a data warehouse or datamart you often want to define derived tables in terms of SELECT statements preferably without needing explicit DDL (ie CREATE TABLE) but without giving up the ability to use primary keys foreign keys or important database-specific features like Amazon Redshifts DISTSTYLE DISTKEY and SORTKEY We built a small tool at Ro that makes this easy to do without needing to pull in a larger data pipeline-management toolAt Ro our business and operations are growing quickly in terms of both scale and complexity We launched at the end of October 2017 with a single brand (Roman) treating a single condition (erectile dysfunction) but in the last few weeks weve added a new brand (Zero for smoking cessation) and several conditions under Roman (hair loss cold sores genital herpes and premature ejaculation) with more on the wayFor the data team this means we need to be able to react quickly to changes in the underlying products and business For instance if half a dozen columns are added to a table in the production database we want to minimize the amount of time and work it takes to add those columns to appropriate derived tables while also minimizing the chances that the additions will break anythingFor context our data warehouse is in Amazon Redshift and our solution is currently Redshift-specific (though it would be straightforward to adapt it to most other databases)If youre defining your derived tables with standard CREATE TABLE and SELECT statements then youll need to add the new columns in both places If you skip the CREATE TABLE and use SELECT INTO (or its moral equivalent like CREATE TABLE AS) instead youre down to one place to change but now you have a different problem  you cant define primary or foreign keys or take advantage of Redshifts DISTSTYLE DISTKEY and SORTKEY featuresWe built a simple way to get the advantages of SELECT INTO without losing the ability to specify keys which weve posted on GitHub hereThe README discusses how to use it plus a couple notable limitationsThe implementation strategy is to first run a variant of the query with SELECT INTO and LIMIT 10 creating a temporary table with a small sample of rows
HC3wXVMzkh3mY6DEcFGxb6,We published a small tool you can use to import semi-structured data from Google Sheets to Snowflake taking advantage of Snowflakes variant typeThere are a variety of cases where you might want to import data provided by business users into your data warehouse  for example inventory cost information or logs of marketing spend These probably wont come directly from your application or from a third-party integration but rather from the operations team the finance team the growth team etcIf your organization uses Google Sheets a lot thats probably a fairly convenient way for business users and input and manage the dataSo why not just pull the information directly from a sheet? One possible challenge is that these sheets are likely to change over time with columns being added and removed We can take advantage of Snowflakes support for semi-structured data in particular the variant type to keep things flexiblelooks something like this: Using this module the result of importing the sheet above would look like this: By default all items are imported as strings but a   coercions argument is supported to specify fields that should be interpreted as numbers dates and timestampsYou can then use Snowflakes support for querying semi-structured data to work with the data object convenientlyThe repository has additional information on initial setup and supported optionsThe most important limitation is that this tool replaces the entire table every time so if historical information is removed from the sheet it will also be removed from the databaseWhile this is a convenient technique it wont be appropriate for every situation We also import data into traditional tables import from CSVs and import similarly but in an append-only fashion some or all of which well likely expand on in future commits and articlesIn terms of implementation strategy you could export CSVs to say S3 and then import them from there Or use an ETL/sync tool some of which support reading from Google Sheets or files stored in Google Drive or an SFTP server Yet another option would be to use something like AirtableWhichever approach you take we hope this is an interesting example of dealing with semi-structured data from heterogeneous sources while taking advantage of non-relational features in databases
dqKW8eLMYiKm5apidYwuKa,In Redshift the type of LISTAGG is varchar(65535) which can causelarge aggregations using it to consume a lot of memory and spill to disk during processingLISTAGG is a convenient way to aggregate a group of values by concatenatingthem together It supports DISTINCT a delimiter and a within-group orderingWell go through a few examples of using LISTAGG and then talk about asituation we ran into where it resulted in much higher memory usage than you might expect without knowing any of its implementation detailsGiven this example table: Here are a few examples using LISTAGG: We dont use LISTAGG very often   concatenated strings are usually not afriendly format to work with in the context of a relational database   but we do use them sometimes when theyre the most useful way to aggregate data and present it to a business user Its definitely worth knowing about but it uses a lot of memory and therefore probably isnt appropriate for largeaggregationsThe type of LISTAGG is varchar(65535) In our case the results of ourLISTAGG expressions were around 10-50 characters long   nowhere near 65535 characters The good news is that Redshift optimizes varchar columns when storing them by omitting the implicit trailing blanks In other words having over-large varchar columns wont necessarily blow up your disk usageHowever during processing Redshift needs the full width in memory and this is where we ran into a problemIn our case we had a pivot query that returned 1091055 rows andoriginally included two LISTAGG expressions At one byte per character thatwould be 2 LISTAGG expressions times 1091055 rows times 65535 bytes which is 143004578850 bytes or just over 143 gigabytesIn the best case the query exhausted the available memory started spillingdata to disk exhausted the disk and then failed with a Disk Full error Atworst it contributed to the cluster freezing and becoming unresponsive untilrebootedTheres no LISTAGG but without the high memory usage to point to so thebest alternatives will really depend on the circumstances Possibilities couldinclude performing the aggregation on demand for smaller result sets or in a BI toolWell continue to use LISTAGG from time to time but well keep the expectedsize of the result set in mind when deciding whether its appropriate
dXRaHFwkGim3fc4yKxrfYC,At Ro we have a development workflow that uses two Looker servers: staging and production Although LookML code can be migrated and managed across multiple Looker servers it is tough to do the same for the user defined Looks and Dashboards The gzr utility was a step in the right direction to manage Looks and Dashboards between our two Looker servers but after experimenting with various gzr commands we found that they were geared towards managing individual Looks or Dashboards and not tailored for bulk migration or managementWe have built Ro-Gazer as an easy-to-use script to bulk view download and upload Looks and Dashboards between multiple Looker serversWe have found Ro-Gazer especially useful for:   Reducing downtime of content on the production environment as users can prepare their Looks and Dashboards on staging (accounting for changes that will make it to the production environment) and instantly migrate them when those changes go live  Creating local backups of all user defined content We found it reassuring to have local backups of all Looks and Dashboards so that we could have a record of how they were built We also use these backups in conjunction with Lookers Content Validator to accurately rebuild content that may no longer be running correctly after data changes  For Looks or Dashboards that have a lot of content changing fields can often take a lot of time to process on the Looker server Weve found that sometimes it is more efficient to download the json file from the Looker server make changes in a text editor and re-upload it Note that in this case you need to know exactly the names of the fields you are changing where to change them and what youre changing them toIf you would like to read more about how to use Ro-Gazer please visit our GitHub Repository
ERtEGVdzonxj5JbP9a3mM8,This winter we switched from Redshift to Snowflake for our data analytics warehouse Our top drivers in switching to Snowflake were: Real-time data Snowflakes architecture separates storage from compute meaning that reading and writing can occur in complete parallel without interfering with each other With Snowflake there is no performance impact if we have real-time data syncing - all data in our warehouse is current to within 30 minutesHandling concurrent queries A Looker user refreshing a dashboard might generate 15 25 queries at once Snowflake can automatically spin up more computing resources and run all of the concurrent queries in parallel with zero drag on the execution speed of any individual queryDatabase administration Snowflakes knobs are directly tied to our performance needs and extremely simple Higher concurrency? Just increase the cluster count limit More power? Just increase the cluster size The amount of database admin required with Snowflake is negligibleEasily providing extra computing power to certain users: Certain data users of the company are running lots of heavier queries  we wanted to have the option of being able to easily crank the power dial specifically for them should query run time become an issueNote that our switch to Snowflake was coupled with introducing new data syncing vendor We will describe our trials with this particular piece of infrastructure in later postsThe main competitor to Snowflake that was briefly considered in the switch was Google BigQuery BigQuery is geared toward append-only paradigms; this was a non-starter given that much of our data model currently involves mutable fieldsSo far Snowflake has been everything that was promised and then some We are quite pleased with the outcome of our switchIn addition to having the afore-mentioned core needs fully met we also found out that some of what we originally considered to be Snowflake bells and whistles are actually quite useful especially JSON features and the variant data typeImportant note: No one from Snowflake was involved with this article in any way We really wrote this simply because we are pleased with our new setup and want to share what weve learned
hnKjBeMtojjkAMBZFZrgCB,Fynd is an online to offline (O2O) fashion e-commerce portal that brings in-store fashion products from retail brands to an online audience Fynd pulls real-time streams of inventory data from over 9000 stores in India to provide its 17 million customers up-to-date information on the latest offers and trends in fashion Data and technology are at the heart of Fynds businessAs a retail e-commerce company Fynds business is predicated upon its ability to respond to consumer behavior as it happens Fynd is constantly monitoring transactions and activity on its platform to uncover issues and trends in orders inventory management and security Fynd only has a very short amount of time in which to identify these situations before the opportunity to respond is lostFynd works in concert with their retail brand partners to put on limited-time sales that could last a week several days or even minutes Fynd experiences significant traffic during these sales A 2-minute sale could see a million concurrent users on Fynds platform and Fynd needs to know everything about the sale while it is going onFynds marketing team is an analytics powerhouse and asks a host of questions about their sales How many orders are coming in? What are the top-selling brands products and price ranges? Are there geographic areas that are outperforming others? In which demographics is the sale performing best? And they need answers in real time to adjust their marketing tactics to optimize Fynds sales performanceLive metrics are also very important to the team in assessing where they stand relative to sales targets A retail brand may have predetermined a certain volume of product they wish to sell for a discount for example and Fynd needs to react to sales conditions in real time in light of these targetsFrom an operations perspective Fynd tracks metrics like the number of visitors on the platform orders coming from different channels and the response times of critical systems constantly refreshing live dashboards with these metrics Fynd has to immediately detect unusual events Is there an issue with the site that is causing a problem for the consumer or is there is a consumer on the site causing a problem for Fynd? Fynd needs to know if the number of orders coming in is abnormally high or low for instance which could be symptomatic of fraud or a problem with the payments backend respectivelyTo power their business Fynd collects data on many types of events from its mobile and web applications During campaigns Fynds users could generate 30 million events per day and all the data that is produced is streamed into KafkaFynd would prepare the data and load it into one of several analytics platforms in the cloud so that it could be queried to support marketing decisions But that process required a minimum of 30 minutes-too long for an online business like Fynd Any consumer behavior discovered through this flow would be long gone before Fynd could respondFynds technical team turned to Rockset to reduce the time it took from data to insight Instead of loading the data periodically from Kafka Rockset connects to Kafka to continuously sync new dataFynds real-time JSON event streams are automatically ingested and schematized without any manual intervention so Fynd can perform SQL queries right away in Rockset Another difference is the improved performance Fynd experiences on their queries as Rockset fully indexes all of Fynds data to deliver millisecond-latency SQLWith Rockset as part of the data flow Fynd developed a serverless microservice to keep tabs on their key metrics Using AWS Lambda functions in conjunction with Rocksets client libraries the technical team created a feature that fires off a query to Rockset whenever an endpoint is called Fynd can now refresh metrics and live dashboards multiple times a minute in a lightweight serverless mannerBy using Rockset on the critical path Fynd can now obtain immediate insight into what consumers are doing on their platform And they can react more quickly and more effectively making better decisions to maximize campaign results than beforeThe new flow also eliminates much of the management and monitoring of the data platform There are no servers to provision when building on Rockset no infrastructure or data warehouse administration and no requirement to prepare and load data as Rockset continuously ingests new data This frees up the technical team to work on tasks with more direct revenue impactWe need to carefully monitor our growth in real-time Is a certain product suddenly selling more? Is there a fraudulent transaction? We easily generate 20 30 million events per day all captured in Kafka streams Our applications query the data every few seconds By sending our raw event data directly from Kafka to Rockset we save a lot of time and energyIn an attempt to get to the data more quickly some marketing queries are bypassing the analytical systems and hitting the operational databases today which is not ideal Amboj intends to offload these queries to Rockset which is better suited for such workloads and track even more metrics using Rockset in the near future Amboj also looks forward to scaling Fynds data platform with Rockset to support Fynds growthOriginally published at https://rocksetcom on March 19 2019
8XXbEnfTRz2wNihoabzugf,A fast-moving technology field where new tools technologies and platforms are introduced very frequently and where it is very hard to keep up with new trends I could be describing either the VR space or Data Engineering but in fact this post is about the intersection of bothI work as a Data Engineer at a leading company in the VR space with a mission to capture and transmit reality in perfect fidelity Our content varies from on-demand experiences to live events like NBA games comedy shows and music concerts The content is distributed through both our app for most of the VR headsets in the market and also via Oculus VenuesFrom a content streaming perspective our use case is not very different from any other streaming platform We deliver video content through the Internet; users can open our app and browse through different channels and select which content they want to watch But that is where the similarities end; from the moment users put their headsets on we get their full attention In a traditional streaming application the content can be streaming in the device but there is no way to know if the user is actually paying attention or even looking at the device In VR we know exactly when a user is actively consuming contentOne integral part of our immersive experience offering is live events The main difference with traditional video-on-demand content is that these experiences are streamed live only for the duration of the event For example we stream live NBA games to most VR headsets in the market Live events bring a different set of challenges in both the technical aspects (cameras video compression encoding) and the data they generate from user behaviorEvery user interaction in our app generates a user event that is sent to our servers: app opening scrolling through the content selecting a specific content to check the description and title opening content and starting to watch stopping content fast-forwarding exiting the app Even while watching content the app generates a beacon event every few seconds This raw data from the devices needs to be enriched with content metadata and geolocation information before it can be processed and analyzedVR is an immersive platform so users cannot just look away when a specific piece of content is not interesting to them; they can either keep watching switch to different content or-in the worst-case scenario-even remove their headsets Knowing what content generates the most engaging behavior from the users is critical for content generation and marketing purposes For example when a user enters our application we want to know what drives their attentionFrom a data engineering perspective this is a classic scenario of clickstream data with a VR headset instead of a mouse Large amounts of data from user behavior are generated from the VR device serialized in JSON format and routed to our backend systems where data is enriched pre-processed and analyzed in both real time and batch We want to know what is going on in our platform at this very moment and we also want to know the different trends and statistics from this week last month or the current year for exampleThe clickstream data scenario has some well-defined patterns with proven options for data ingestion: streaming and messaging systems like Kafka and Pulsar data routing and transformation with Apache NiFi data processing with Spark Flink or Kafka Streams For the data analysis part things are quite differentThere are several different options for storing and analyzing data but our use case has very specific requirements: real-time low-latency analytics with fast queries on data without a fixed schema using SQL as the query language Our traditional data warehouse solution gives us good results for our reporting analytics but does not scale very well for real-time analytics We need to get information and make decisions in real time: what is the content our users find more engaging from what parts of the world are they watching how long do they stay in a specific piece of content how do they react to advertisements A/B testing and more All this information can help us drive an even more engaging platform for VR usersA better explanation of our use case is given by Dhruba Borthakur in his six propositions of Operational Analytics: Our queries for live dashboards and real time analytics are very complex involving joins subqueries and aggregations Since we need the information in real time low data latency and low query latency are critical We refer to this as operational analytics and such a system must support all these requirementsAn additional challenge that probably most other small companies face is the way data engineering and data analysis teams spend their time and resources There are a lot of awesome open-source projects in the data management market  especially databases and analytics engines  but as data engineers we want to work with data not spend our time doing DevOps installing clusters setting up Zookeeper and monitoring tens of VMs and Kubernetes clusters The right balance between in-house development and managed services helps companies focus on revenue-generating tasks instead of maintaining infrastructureHow are our users reacting to specific content? Is this advertisement too invasive that users stop watching the content? Are users from a specific geography consuming more content today? What platforms are leading the content consumption now? All these questions can be answered by operational analytics Good operational analytics would allow us to analyze the current trends in our platform and act accordingly as in the following instances: Is this content getting less traction in specific geographies? We can add a promotional banner on our app targeted to that specific geographyIs this advertisement so invasive that is causing users to stop watching our content? We can limit the appearance rate or change the size of the advertisement on the flyIs there a significant number of old devices accessing our platform for a specific content? We can add content with lower definition to give those users a better experienceThese use cases have something in common: the need for a low-latency operational analytics engine All those questions must be answered in a range from milliseconds to a few secondsIn addition to this our use model requires multiple concurrent queries Different strategic and operational areas need different answers Marketing departments would be more interested in numbers of users per platform or region; engineering would want to know how a specific encoding affects the video quality for live events Executives would want to see how many users are in our platform at a specific point in time during a live event and content partners would be interested in the share of users consuming their content through our platform All these queries must run concurrently querying the data in different formats creating different aggregations and supporting multiple different real-time dashboards Each role-based dashboard will present a different perspective on the same set of data: operational strategic marketingIn order to get the data to the operational analytics system quickly our ideal architecture would spend as little time as possible munging and cleaning data The data come from the devices in JSON format with a few IDs identifying the device brand and model the content being watched the event timestamp the event type (beacon event scroll clicks app exit) and the originating IP All data is anonymous and only identifies devices not the person using it The event stream is ingested into our system in a publish/subscribe system (Kafka Pulsar) in a specific topic for raw incoming data The data comes with an IP address but with no location data We run a quick data enrichment process that attaches geolocation data to our event and publishes to another topic for enriched data The fast enrichment-only stage does not clean any data since we want this data to be ingested fast into the operational analytics engine This enrichment can be performed using specialized tools like Apache NiFi or even stream processing frameworks like Spark Flink or Kafka Streams In this stage it is also possible to sessionize the event data using windowing with timeouts establishing whether a specific user is still in the platform based on the frequency (or absence) of the beacon eventsA second ingestion path comes from the content metadata database The event data must be joined with the content metadata to convert IDs into meaningful information: content type title and duration The decision to join the metadata in the operational analytics engine instead of during the data enrichment process comes from two factors: the need to process the events as fast as possible and to offload the metadata database from the constant point queries needed for getting the metadata for a specific content By using the change data capture from the original content metadata database and replicating the data in the operational analytics engine we achieve two goals: maintain a separation between the operational and analytical operations in our system and also use the operational analytics engine as a query endpoint for our APIsOnce the data is loaded in the operational analytics engine we use visualization tools like Tableau Superset or Redash to create interactive real-time dashboards These dashboards are updated by querying the operational analytics engine using SQL and refreshed every few seconds to help visualize the changes and trends from our live event stream dataThe insights obtained from the real-time analytics help make decisions on how to make the viewing experience better for our users We can decide what content to promote at a specific point in time directed to specific users in specific regions using a specific headset model We can determine what content is more engaging by inspecting the average session time for that content We can include different visualizations in our app perform A/B testing and get results in real timeOperational analytics allows business to make decisions in real time based on a current stream of events This kind of continuous analytics is key to understanding user behavior in platforms like VR content streaming at a global scale where decisions can be made in real time on information like user geolocation headset maker and model connection speed and content engagementOriginally published at https://rocksetcom on September 6 2019
YqEfmbsKz8V3H7BuFtiocP,"This post offers a how-to guide to real-time analytics using SQL on streaming data with Apache Kafka and Rockset using the Rockset Kafka Connector a Kafka Connect SinkKafka is commonly used by many organizations to handle their real-time data streams We will show how Rockset integrates with Kafka to ingest and index our fast-moving event data enabling us to build operational apps and live dashboards on top of Rockset We will use a simulated event stream of orders on an e-commerce site for this exampleWell provide all the steps youll need to connect Kafka and Rockset and run some simple ad hoc queries and visualizationsIf you already have a Kafka cluster ready and data flowing through it then you can skip this portion of the guide Otherwise set up a Kafka cluster and verify it is running A Kafka quickstart tutorial can be found here A single-node Kafka cluster is sufficient for the example in this blog although you may want a multi-node cluster for further work with Kafka and RocksetAll the code used in this blog is available under kafka-rockset-integration in the Rockset recipes repository The Python code provided will simulate e-commerce order events and write them to Kafka The following steps will guide you in downloading the code and setting up the Python environmentCreate and activate Python virtual environment rockset-kafka-demo and install all the Python dependenciesOpen configpy in your favorite editor and update the following configuration parameters You will need to enter your Rockset API keyWe will use the rock CLI tool to manage and query our data in RocksetInstalling the rock CLI tool has already been done in as part of the pip install -r requirementstxt step above Alternatively you can install the rock CLI tool with the pip3 install rockset commandConfigure the rock CLI client with your Rockset API keyCreate a Rockset collection named orders""The --event-time-field=InvoiceDate option instructs Rockset to treat a document's InvoiceDate as its _event_time a special field used to handle time-series data efficientlyRead more about special fields in Rockset and working with event data Users working with event data in Rockset can set time-based retention policies on their dataKafka Connect an open-source component of Apache Kafka is a framework for connecting Kafka with external systems such as databases key-value stores search indexes and file systems Rockset provides Kafka Connect for Rockset a Kafka Connect Sink that helps load data from Kafka into a Rockset collectionKafka Connect for Rockset can be run in standalone or distributed mode For the scope of this blog the following steps explain how to set up Kafka Connect for Rockset in standalone modeThis will build the jar in the /target directory"
GSJ6pCBfqT49CbRYxTyj8J,One of the great things about working for Red Ventures is the investment they make in the development of their employees Engineers are provided with plenty of resources for learning and development as well as the opportunity to attend conferences I was lucky enough to attend the OReilly Strata conference in New York last week and came back with some great insights into the current state of the world of data engineeringIf you havent been to the conference before or have never heard of it until now Strata NYC is an annual conference on all things data With over 180 sessions occurring over the course of two days there was an incredible amount of content to absorb The sessions I attended were centered around a data-engineering track with a focus on data warehousing and architecture streaming systems and cloud Of all the incredible sessions keynotes and vendor demonstrations there were some common themes that paint a picture of the current state of data engineering as well as point to where the industry is going In this post Ill go over some of these themes and the related sessions as well as provide some resources for further readingCloud was one of the threads that tied together nearly every session I attended and is becoming the foundation of modern data engineering infrastructures Any startup founded in the last five years has almost certainly adopted a cloud-first architecture and large established enterprises are quickly migrating major workloads to the cloud John Hitchingham the director or performance engineering at the Financial Industry Regulation Authority (FINRA) described their transition from an on-premise data lake to a cloud data lake on AWS The driving force behind FINRAs move to the cloud was the high cost of scaling the infrastructure to handle peak data volumes AWS (and other cloud platforms) provide some key advantages over on-premise architectures most notable of which include the following: One of the more recent trends in cloud computing is the adoption of serverless technologies Ben Snively a solutions architect at Amazon gave a session on how AWS can be used to implement a serverless architecture for data storage and processing In his talk he first described the evolution of cloud architecture from virtualized servers to managed servers and finally to serverless As its name implies serverless tools do not require the provisioning or management of server resources and one of the key benefits is that the user does not pay for time when the tools are not being used AWS has extensive support for serverless analytic workloads including object storage in S3 streaming in Kinesis and compute using Lambda functionsOne of the most intriguing applications of serverless patterns to a data engineering workloads is in ETL Numerous tools have been created by the major cloud providers that take the pain out of ETL Amazon recently released Glue which is a fully managed service for running ETL jobs in the AWS ecosystemOne of the more subtle but profound themes of the conference was the fact that after over 40 years SQL is still the dominant tool for data analysis A recent post by Timescale CEO Ajay Kulkarni described the recent resurgence of SQL in a world where it seemed that NoSQL systems were destined to take over His argument was centered around some of the key limitations of NoSQL namely the lack of a standardized cross-platform language This trend was extremely evident at StrataOne particularly interesting trend is the use of SQL in analyzing streaming data Tyler Akidau a software engineer at Google and co-author of the OReilly book Streaming Systems gave a fascinating talk on the foundations of streaming SQL In the talk he provided an excellent overview on the difference between querying tables and streams The key point was that tables represent data at rest while streams represent data in motion While we can use the same SQL constructs to query both tables and streams there is a fundamental adjustment that must be made when querying streaming data Akidau distilled the difference between querying streams and tables quite nicely: Tables capture a point in time snapshot of a time-varying relation Streams capture the evolution of a time-varying relation over timeThe application of SQL to these new paradigms demonstrate just how flexible SQL isAs far as trending technologies in the data ecosystem goes streaming is arguably the most prevalent Kafka the open source stream processing platform is used by numerous companies to ingest billions of events per day Michelle Ufford a data engineer at Netflix spoke on how Netflix drives automation in data engineering workflows In her talk she said that Netflix uses Kafka to process hundreds of billions of events every day Obviously not every company has Netflix-sized data and there are plenty of use cases for stream processing that make sense for smaller data sets For example one increasingly common implementation of stream processing is as a replacement for traditional GUI-based ETL tools in an effort to achieve near-real time analyticsStreaming systems have applications beyond analytic workloads and are often used for core business operations Gwen Shapira an architect at Confluent gave an excellent talk titled The Three Realities of Modern Programming: The Cloud Microservices and the Explosion of Data In the talk she described how modern infrastructures should be built using microservices that communicate with one-another using a stream of events Often referred to as event sourcing this pattern enables an application to generate an immutable stream of events that can be used for in-depth historical analysis Although implementing a streaming architecture has its advantages there are high management and operational overhead costs which is why Shapira recommended utilizing managed services when possibleFurther reading: Data engineering is an incredibly exciting field to be in right now and spending a few days learning about how experts in the field are solving problems and leveraging the latest technologies was invaluable While many aspects of the industry are evolving some things are staying exactly the same ETL is one of the main areas undergoing momentous change as complexity and overhead has decreased significantly with managed cloud resources and serverless architectures Although the nature of data is expanding from data in tables at-rest (both RDBMS and NoSQL) to dynamic streaming data SQL is still the dominant language for data analysis over 40 years after its inceptionIn addition to the sessions both days of keynotes were excellent and there were some great networking opportunities as well To top it all off Cloudera sponsored a party at a midtown Manhattan rooftop garden that had a dance floor and an open bar
EKpKYxYntFEbwhkD9DZEyZ,To celebrate the two-year anniversary of the Scout24 Data Landscape Manifesto we would like to formally present it to the world in this blog postOver the last two years weve used the manifesto to drive Scout24s internal effort to empower all employees to assume responsibility for their data and to become more data-driven Weve discussed it far and wide including in conference presentations in Germany Norway Spain the UK the USA and Australia but weve never put ink to paper and made it publicly available in written formAs a tech team Scout24 Data Engineering started where most tech teams start: with the tech We built a modern cloud-based data lake that would scale indefinitely along with the companyHowever we initially failed to realise that our goal is for the company to become truly data-driven not only to have a modern cloud-based data lake In fact the technology is the easier part The hard part is changing the company culture to go along with the technological changeBecoming data-driven requires technical organisational and most importantly cultural changeFrom this disappointment and soul-searching came a vision not of the technical architecture but about how the company should view data and how the data team could enable this change of valuesConsequently together with the rest of the Data & Analytics team we wrote the Scout24 Data Landscape Manifesto to articulate this vision It represents the teams strong opinion of the roles responsibilities and values necessary for building a data-driven company at scaleThe manifesto which follows consists of seven principles each with an introductory statementRoles responsibilities and values for a data-driven company at scaleWe believe that collecting and analysing data is crucial to understand our business our customers and the market in order to provide the right services and products#1 Data is a key asset of our companyWe therefore believe that everyone in the company must have easy access to the data available and it must be easy to publish data which can be used by others This requires a solid Data Platform: easy-to-use tools reliable infrastructure and simple guidelines for publishing and consuming data in a secure and privacy-aware way#2 We Data & Analytics are responsible for providing the Data Platform and we provide support and training for itWe believe that exhaustive centralised data management does not allow us to scale to the level of data creation and consumption we aspire as a company because it creates a bottleneck and introduces accidental indirect dependencies Instead we believe that data autonomy is the only way for data usage to scale across the company However for data autonomy to not become data anarchy there has to be a clear set of basic rules and responsibilities#3 Data autonomy puts data producers and data consumers in control of their data and of their metrics and thereby allows us to be data-driven at scale but this comes with responsibilityWe believe that extensive data availability data discoverability and data usability are crucial and that  at scale  no one else can ensure this other than the one controlling the source where the data is originally generated#4 Data producers are responsible for publishing data to the central Data Lake for the datas quality and for publishing metadata that makes it easy to find and consume the dataWe believe that the stakeholder of a metric has to be the single owner of that metric and its definition and has to drive its implementation Without a single source of truth about what a metric means we risk that multiple diverging and possibly contradicting understandings and implementations develop over time#5 Data consumers are responsible for the definition and visualisation of metrics and for driving the implementation and maintenance of these metricsWe believe that a minimum level of company-wide comparability and reliability of core KPIs is crucial for leading the company into the right direction and can only be achieved with coherent core data Our executive leadership team is the owner of these core KPIs and the data group represents the executive leadership team in terms of metric ownership#6 We Data & Analytics take the full ownership and responsibility of the company-wide core KPIs and making the coherent core data entities from which they are derived accessibleWe believe that transparency is crucial for understanding what the meaning of a metric is If month-to-month comparability must never break there is no way to continuously improve metrics and their transparency based on new insights#7 We value data transparency over data continuity which means we may break metric comparability if it is for the cause of enabling better insightsUltimately we believe that together these principles will lay the foundation for an inclusive and highly scalable data landscapeWe wish to build a federal landscape of data producers and consumers with just enough rules to ensure seamless co-operation without severely impeding autonomyIt is now two years since we introduced the manifesto at Scout24 and began using it to guide the development of our data platform So far the reception has been mixed Some teams welcomed the autonomy it provides while other teams objected to the new responsibilityTo switch from being a passive to active participant in the data landscape of a company is a difficult cultural change It requires news skills and knowledge but mostly it requires a shift in mindset: no longer will data and insights be delivered in a nice clean package Instead each employee is expected to generate their own insights based on basic data literacy Although this sounds like a burden we believe it is liberating Who knows more about your business problems than you? You are in the best position to use data to answer the questions that you want answeredWe still believe focusing our efforts on building a self-service data platform instead of data preparation and analysis leaves our internal users in control and empowered to make fast data-driven decisions However so far this argument has not been completely convincing especially because some teams lack the technical background or engineering resources to properly assume responsibility for their data In these situations we take a practical stance and assume surrogate responsibility until the team has developed the skills internally In addition we are developing training materials and improving the usability of our tools to lower the bar of technical knowledgeDespite the non-universal acceptance the manifesto successfully set the tone for our evolution into a true data-driven company The data processes weve built not only show that we are able to scale our data analytics activities but also set the foundation for our next journey towards exploiting AI and ML across all our product offeringsSincere thanks to the Scout24 Data & Analytics team members involved in the original writing of the manifesto Although it was a group effort Arif Wider from ThoughtWorks and Sebastian Herold now at Zalando Tech deserve special thanks for their initial contributionsAlso thanks to all those at Scout24 who pushed the organisation to understand the benefits of distributed data responsibility We know it wasnt always easy but the long-term benefits of a scalable data culture will continue to bare fruit for many years to come
2ufgxh96j3ok5TTFVgTxkk,Read the original article on Sicaras blog hereAirflow is a platform used to programmatically declare ETL workflows Learn how to leverage hooks for uploading a file to AWS S3 with itThis article is a step-by-step tutorial that will show you how to upload a file to an S3 bucket thanks to an Airflow ETL (Extract Transform Load) pipeline ETL pipelines are defined by a set of interdependent tasksA task might be download data from an API or upload data to a database for example A dependency would be wait for the data to be downloaded before uploading it to the database After an introduction to ETL tools you will discover how to upload a file to S3 thanks to boto3Airflow is a platform composed of a web interface and a Python library This project has been initiated by AirBnB in January 2015 and incubated by The Apache Software Foundation since March 2018 (version 18) The Airflow community is really active and counts more than 690 contributors for a 10k stars repositoryAlso The Apache Software foundation recently announced Airflow as a top-level project This gives us a measure of the community and project management health so farAs for every Python project create a folder for your project and a virtual environmentYou also need to export an additional environment variable as mentioned in the 21st of November announcementEventually run the commands of the Getting Started part of the documentation that are pasted belowCongratulations! You now have access to the Airflow UI at http://localhost:8080 and you are all set to begin this tutorialNote: Airflow home folder will be used to store important files (configuration logs database among others)A DAG is a Directed Acyclic Graph that represents the tasks chaining of your workflow Here is the first DAG you are going to build in this tutorialOn this schematic we see that task upload_file_to_S3 may be executed only once dummy_start has been successfulAs you can see in $AIRFLOW_HOME/airlowcfg the value of the dags_folder entry indicates that your DAG must be declared in folder $AIRFLOW_HOME/dags  Also we will call upload_file_to_S3py the file in which we are going to implement our DAG: First import the required operators from airflowoperators Then declare two tasks attach them to your DAG my_dag thanks to the parameter dag Using the context manager allows you not to duplicate the parameter dag in each operator Finally set a dependency between them with >>Now that we have the spine of our DAG lets make it useful To do so we will write a helper that uploads a file from your machine to an S3 bucket thanks to boto3boto3 is a Python library allowing you to communicate with AWS In our tutorial we will use it to upload a file from our local computer to your S3 bucketRead the full article on Sicaras blog here
FL5jyAkfZHbnK26Mv65kK3,Read the original article on Sicaras blog hereManaging several Raspberry Pi can be a lot of work This article will teach you how Kubernetes and Docker will make your life easierI own 4 Raspberry Pi and I got interested in Kubernetes when I was tired of managing my Raspberry Pi and keeping track of what was installed and running on which machineUsing Docker containers allows me to make sure my applications are packaged with their dependencies Kubernetes is a platform that manages containers on hosts It allocates the containers on the available Raspberry Pi I can pull out one of them to do something else and when Im done I add it back into the clusterFor this step I wont reinvent the wheel A tutorial has been made to install Kubernetes on the Raspberry Pi This gist is updated on a regular basis to keep up with any breaking changesThis can be tedious because you have to repeat some steps (like burning the SD cards install Docker etc) for each Raspberry Pi It took around an hour for four machinesRead carefully some steps are to be performed on the master node only and others are to be performed on all the nodes (or all the slave nodes)The only thing that didnt work as mentioned in the tutorial is getting the 3/3 Running on the kube-dns pods It only showed 3/3 Running after I launched the commandTake your time to get it working Ill wait for youIf you dont have a cluster of Raspberry Pi available but you still want to try Kubernetes on one machine you can use MinikubeTake a look at the functionyml file There are two files in the file You created a service which maps the port 8080 of your pod to the port 31118 of your cluster You also created a deployment of one pod from the Docker image functions/markdownrender:latest-armhf exposing an API on port 8080 Your API is now available on port 31118 from outside of the clusterYou interact with Kubernetes with the kubectl command Here you read a configuration file and create the objects described in the filePods are the base unit of Kubernetes It can contain containers and volumes All containers within a pod share the same port space and IP address (they can find each other on localhost)Deployments are a way to manage multiple pods together (for example replications of the same pod) They are the preferred way of managing the state of the cluster (eg I want 3 replicas of this pod from that image)Services are the network interfaces of the pods It allows mapping the port of the pods to external port They also act as load balancers when you use replicasThere are a lot of different objects corresponding to different functionalities (like jobs configuration replication etc)I use my cluster as a home server to host applications For example it serves as a backend for my humidity and temperature monitoring sensors following a water damage in my apartment I log my data in InfluxDB and plot them with Grafana Kubernetes answers my problem because: I also have deployed some applications like my burndown chart app that I use to track my goals Before that I was using Heroku on the free tier but the application was slow to start and it was public Now: My other use case with my cluster is the experimentation with distributed systemsYou can deploy an image in a few lines of configuration and set up your experimentsOverall Kubernetes answers my needs: I can host my applications without having to manage individual machines There are also some bonuses: Its a bit over-engineered but as its designed to work on a huge number of hosts with a huge number of pods it works really well on my small setup
PTbKEysM7GtCh2TAn5NNTv,Read the full article on Sicaras website hereIn this article you will see how to write a Serverless plugin that checks your local Git branch before deployment so as to avoid deploying bad codeIf you are curious about the Serverless framework you can read this excellent article explaning how to deploy a rest api with Serverless in 15 minutesWe use the amazing Serverless toolkit to deploy to AWS We managed the different environments using environment variables as explained in the Serverless documentationTo deploy to the desired environment we simply run: where TARGET_ENV is the target environment eg development staging or productionA development process typically includes the following environments: When deploying local files are zipped and uploaded directly to AWSThere is no control of the local files before there are uploaded which is fine when deploying to the development environment but wrong for staging or production environmentIn the screenshot above the git branch sprint18/feature919/awesome-feature is deployed to the staging environment This could be wrong for several reasons: To prevent us from deploying the wrong Git branch to the wrong environment we added a hook to our deployment by implementing a Serverless pluginYou can learn how to write you own Serverless plugins in this blog postHere is the code: … Read the full article on Sicaras website here
aRRs8EVTSxpoVbkDfMNkpq,Read the original article on Sicaras blog hereAs part of my last project I had to deploy a web app to Cloud FoundryThis article tells the story of the many challenges we had to solveFirst I briefly sketch the app at stakeIts a modern web app offering data visualization and model building for predictionThe front-end fetches its data from the Node server through http andwebsocketTraditionally the app is deployed to an infrastructure as a service (IaaS) providing virtual machines and abstracting away physical resources (machine network)The app source code looks like the following: Before deploying the app you needed to provision a server with all binary dependencies ie Nginx (reverse proxy and static files) Node+Npm Mongo ZeroMQ C lib Python 3Then you can deploy the app by copying the source code to your serverand by starting all services (Nginx/Mongo/Node/…)Cloud Foundry is an open source cloud platform as a service(Paas) While IaaS provide low-level features (virtual machines / networks) PaaS allow you to work directly with web-app containers and managed services eg message queues databases or job launchersMany advantages stem from the higher level of abstraction of PaaSYou may install Cloud Foundry yourself in your own servers or on your favorite IaaS (Amazon Google …); it will take *at least 14 machines* for a basic usage thoughAll you need to do is push your code to the cloud and thats it Cloud Foundry creates a container for running your code You dont know exactly where and thats ok: the only thing that matters is that your code is running somewhereWhen you push a piece of code Cloud Foundry detects the language and provision the container accordingly For instance the presence of a packagejson file triggers the NodeJs buildpack that will install Node/Npm binaries and Node modules in accordance with itCloud Foundry has inherited from Heroku the concept of buildpacks together with a bunch of buildpacks for most common languages to choose fromCloud Foundry has a healthcheck mechanism to ensure your app stays up If your app goes down Cloud Foundry automatically restarts it You can monitor / start / stop your apps by calling the Cloud Foundry APIor through the UI your Cloud Foundry provider has built for youNeed to handle more traffic? Add more instances of your app The cloud controller (CC) includes a load balancer dispatching traffic among your instancesYou dont want to scale your app manually? Use an autoscaler service either from the Cloud Foundry community (eg this one) or from your Cloud Foundry provider catalog (eg the Pivotal autoscaler)All benefits from Cloud Foundry come at a price: the app has to follow somebasic guidelines to fit the framework and work well in the cloud In the following Ill give my recommendationsOur app is monolithicEven though the app have separate components (front-end / back-end / workers) it is meant to be deployed as a single block of code In order to fit the Cloud Foundry paradigm we need to push small independent components and make them work together in the cloudOur app is clearly not statelessFor instance the Node app reads and writes the local filesystem all the timeThis is a major issue as Cloud Foundry does not persist app container filesystem If Cloud Foundry decides to restart the Node app  and it does for plenty of reasons  the filesystem is wiped outOur app is hugePython dependencies weight ~15GB which is way above the 1GB Cloud Foundry upload limitAnyways were positive we dont want to push ~15GB each time we deployPython dependencies should be downloaded and cached in the cloudHopefully there are plenty of buildpacks that do the jobTraditionally Nginx serves the front-end files We keep it the exact same way in the Cloud by pushing the entire webapp folderand using the staticfile-buildpackTo do so we create an application manifest file at the root of your app describing how and what to push to the CloudHere is our manifestfront-endyml file: The manifest is self-explicit and quite intuitive: Thats it! you are ready to deploy to the cloud: first be sure to have the cf command line client installed and the run the following: cf push -f manifestfront-endFor the sake of simplicity we choose to push the front-end source codetogether with vendor files (bower) This means we have to build locally the front-end app ie install vendor files before pushing This is fine as long as the source code and vendor files together remains lightweight (~50MB in our case)
ak7pijy2VSS2UQi56YGiiw,This article aims to prepare you for the Databricks Spark Developer Certification: register train and succeed based on my recent experienceSpark is this technology everyone is talking about when dealing with Big Data analytics It is closely related to Hadoop and makes distributed computing accessibleYet in my opinion it is a wonderful opportunity to build scalable jobs and tackle problems that were up to now reserved for massive computersAnd what would Spark be without Databricks (the company created by the founders of Apache Spark)? More than 30% of the current committers are directly related to the company that provides support and visibility to Apache SparkYou can choose between the Scala and Python (aka pyspark) developer certification I took the Python oneDatabricks uses a service provider named Kryterion to organize their exams You will have to register here I chose the online monitored version finding it comfier especially when knowing it lasts 3 hours The exam itself takes place in an application called Sentinel Secure that among other things prevent you from accessing other resources on your computer during the examNote 1: the online proctored exam is supported by standard English keyboards only But all questions are Multiple Choices Questions so dont let it destabilize you Fun story I took this typing pattern thing too seriously Since Im in France Im using a French keyboard And because Sentinel detects your identity through typing pattern I tried to artificially alter my writing pattern to anticipate the difficulty I would have with the English keyboard during the exam Result? I had to authenticate 10 times on the exam day Not the best way to start the examNote 2: A detached web camera is required for the online proctored exam Your internal cam will do the job no need to buy an external one But also note that someone will watch you all your exam long No paper no pen no leaving your chair… 3 hours its longRead the full article on Sicaras blog here
k2dcUK56tUcU7XDYJpfP2R,Amazon AWS is a service where you can borrow a machine (sometimes for free) to do whatever you want We are going to learn how to set up a server to run a real time Chat App with FeathersJS FeathersJS is an open source REST and real time API frameworkWe will need to set up a database and Feathers We will launch those 2 parts on your Amazon AWS service This is a simple tutorial where we will learn how to run your real time app on your Amazon AWSSelect Ubuntu server and choose the Free Tier Eligible T2We need 2 network rules the ssh rule on port 22 source custom and the custom tcp port range 3000 source anywhere then launch your instance ! This step is important to reveal the port 3000 to the world on tcp and the port 22 on sshThis step is extremely important to connect to your EC2 machine ! Choose to create a new key pair name your key pair webApp and download it then view instance Congratulations you have launched your Amazon AWS instance The next step is to try to connect to this instanceThis step is for linux / mac users if you have a windows please follow this link Create a directory called webApp and put the webApppem in it (if you are a mac user you may have downloaded a webApppemtxt rename it webApppem) navigate inside this directory using bash then run : to modify the rights on the keyYou can find the right command by connecting to the Amazon AWS website and click on the connect button A pop-up will appear with the right ssh command For me for instance it is going to be : Congratulations most of the dirty work is done by now ! The aim is to be pretty simple and help you set up your machineNow we need to install Docker on your ec2 machine Docker runs kind of virtual machine which are called images but are way lighter than virtual machines Docker will allow you to run all the bricks of your app with a simple command line ! The good thing about Docker is that it encapsulates all the software you may need This is precisely what we want to do when we run an app we wrap the different components of our architecture in containersTo install docker you can type on your ec2 terminal the following commands : The last command is to make sure that docker is installedTo install Feathers and download the app we will simply pull a Docker image That is where the magic resides There is already a Docker image with Feathers installed and the real time chat app coded We will use this ready to use image and deploy it to amazon You can then quit your shell and the AWS instance will still be running your app ! Lets tryRun the following command : Now we need to create a folder and bind it to the docker container For instance navigate to your webapp folder and run : You can notice that the folder is empty but the app is running! You can copy the content of the docker container to your host machine with this command: We now run in a detached mode npm start you can use yarn start as well This means that in the background we have a process keeping our chat app alive! We can now access to our EC2 Amazon machine through HTTP Give it a try copy the public DNS to your browser and add the port :3000 in the end For me it looks like this : http://ec2-54-187-133-189us-west-2computeamazonawsYou should have the same picture as this oneYou can signup login and test your real time chat app by creating two accounts and exchange messagesEnjoy agility with an environment iso-staging where you can deploy in one command lineToday you learned to deploy a real time application to Amazon Real time is easy to set up with FeathersJS Docker can help you encapsulate all FeathersJS environment and push it to Amazon Docker can even help you launch your application on Amazon and restart it when it crashes but it is not a production ready environment You can now start coding and enriching your real time application and deploy it to the cloudIn the next articles we will deploy deep learning / big data applicationsDo you need agile Data Science / Web / Mobile services for your business? Do you want to apply for a job at Sicara? Feel free to contact us We will be glad to welcome you at our Paris offices
cF9MTrB73PdBZ4PM9ZpCzu,Read the original article on Sicaras blog hereThis article will share an approach on how to make the data injection flow more transparent in the data lake due to monitoring of custom logs in KibanaThe previous project I was working on was dedicated to the construction of a data lake Its purpose was to inject gigabytes of data from various sources into the system and make it available for multiple users within the organization As it turned out it was not always easy to certify if all the data was successfully inserted and even if the problem was already evident it required hours to identify its cause Hence there was no doubt that the system needed fixingFrom a technical perspective the solution that we proposed might be divided into three principal blocks: In software development logging is a means to decrypt the black box of a running application When the app is growing in its complexity it starts to be trickier to figure out what is going on inside and here is where the logs are getting more influent Who could benefit from them? Both developers and software users! Thanks to logs the developer can restore the path the program is passing through and get a signal of potential bug location while the user can obtain the necessary information regarding the program and its output: such as time of execution the data about processed files etcIn order to improve the robustness of the application the logs should fulfill the two standards: we wanted them to be customized so that they contain only the data we are interested in Hence it is important to think of what really values in the application: it may be the name of a script or an environment time of execution the name of the file containing an error etc The logs should be human-readable so that the problem could be detected as fast as possible regardless of the processed data volumeThe first sub-goal is to prepare the logs that can be easily parsed by Logstash and Elasticsearch For that reason we are keeping the logs messages as a multi-line JSON that contains the information we would like to display: log message timestamp script name environment (prod or dev) log level (debug info warning error) stack traceThe code below can help you to create your customized logs in JSON format for a mock application which consists of the following parts: the application body is written in mainpy script the logger object is defined in logging_servicepy its parameters are described in logging_configurationyml To add the specific fields into the logging statement we have written CustomJsonFormatter class that overwrites add_fields method of its superclass imported from pythonjsonlogger package The function get_logger from logging_servicepy returns the new logger with the desired configurations Note: the best practice is to define the logger at the top of every module of your applicationTo create filelog run the code above placing the files in the same folder and running the following command from your terminal: To promote the readability of logs we were using the ELK stack: the combination of the three open-sourced projects Elasticsearch Logstash Kibana There exist multiple articles that can give you insights about what it is and its pros and consRead the full article on Sicaras blog here
gmobStTcS8gpCYCYQ7uHNe,Read the original article on Sicaras blog hereA data lake makes structured and unstructured data massively available within a company Spark connectors allow users to retrieve data from the data lakeA data lake is a reliable collection of transformed data coming from and at destination of various businesses But a data lake is worth nothing if users cannot query the data it holds Users want to get the data crunch it and visualize it That is why connectors from Spark to data visualization formats  such as Tableau  are a necessary step in data engineeringConnectors in Spark are interfaces used to write Resilient Distributed Databases (aka RDDs aka Sparks distributed dataframes) to external storage systemsSpark comes with a lot of pre-packaged connectors within the Spark SQL API For instance write connector makes it very easy to write a dataframe to CSV using a single line of codeOther supported standard formats are: In some cases however you will be compelled to work with more exotic formats Tableau Alteryx Microsoft: major data software companies developed their own formats for big data Tableaus coorporate solution Tableau Server for instance uses either Tableau Data Extract (tde) or  more recently  Hyper (hyper) as storage formats for its data tablesThe proper way to convert a dataframe to your target format is to proceed partition by partition RDDs are distributed in partitions which are not directly accessible to clusters driver where Python code is runningFor each partition of the RDD collect data within that partition to the driver You can then go through the collected partition and insert it to your target file row by rowThe function convert_and_insert will be provided by data vendors For Tableau formats for instance you can refer to Tableau SDK (for tde) or Tableau API 20 (for hyper) that both have C++ Java and Python APIsThis method supposes that you have full control: Once data is converted to the proper format it can be exported to the cloud and made available to users For instance I use Tableau REST API to publish Tableau files to Tableau Server Every vendor provides developers with dedicated APIs to publish data to the cloudNot all APIs however are well-documented so my advice is to directly clone the project and dive in the code to see if the one feature you need is already implemented If features are still missing you will even be able to submit a pull requestI built a command-line interface in Python on the top of my connector The user can choose a source environment and a target environment (as represented in the figure 1 above) These options are parsed mapped to a configuration file and both services described above in part 1(convert) and part 2 (export) are sequentially triggered to publish formatted data to the cloudConnectors are another piece in the puzzle of data lakes They allow your users to visualize fresh data in real-time without technical knowledge
BgeZoun6Ekch6Mu7bf9xEm,Docker is the best platform to easily install Tensorflow with a GPU This tutorial aims demonstrate this and test it on a real-time object recognition applicationDocker is a tool which allows us to pull predefined images The image we will pull contains TensorFlow and nvidia tools as well as OpenCV The idea is to package all the necessary tools for image processing With that we want to be able to run any image processing algorithm within minutesFirst of all we need to install DockerAfter that we will need to install nvidia-docker if we want to use GPU: At some point this installation may fail if nvidia-modprobe is not installed you can try to run (GPU only): Eventually you can run this command to test your installation Hopefully you will get the following output (GPU only): You probably are familiar with Jupyter Notebook Jupyter Notebook documents are both human-readable documents containing the analysis description and the results (figures tables etc) as well as executable documents which can be run to perform data analysis Jupyter Notebook can also run distributed algorithms with GPUTo run a jupyter notebook with TensorFlow powered by GPU and OpenCv launch: If you just want to run a jupyter notebook with TensorFlow powered by CPU and OpenCV you can run the following command: You will get the following result out of your terminalAnd eventually you will get the following result You can therefore test your installation by running the jupyter notebooksThe first link is a hello TensorFlow notebook to get more familiar with this tool TensorFlow is an open-source software library for dataflow programming across a range of tasks It is principally used to build deep neural networks The third link gives an example of using TensorFlow to build a simple fully connected neural network You can find here a TensorFlow implementation of a convolutionnal neural network I highly recommand using GPU to train CNN / RNN / LSTM networksNow it is time to test our configuration and spend some time with our machine learning algorithms The following code helps us track objects over frames with our webcam It is a sample of code taken from the internet you can find the github repository at the end of the articleFirst of all we need to open the access to the xserver to our docker image There are different ways of doing so The first one opens an access to your xserver to anyone Other methods are described in the links at the end of the articleThen we will bash to our Docker image using this command: We will need to clone the github repository which is a real-time object detector: Finally you can launch the python code: The code that we are using uses OpenCV It is know as one of the most used libraries for image processing and available for C++ as well as PythonYou should see the following output OpenCV will open your webcam and render a video OpenCV will also find any object in the frame and print the label of the predicted objectI showed how one can use Docker to get your computer ready for image processing This image contains OpenCV and TensorFlow with either GPU or CPU We tested our installation through a real-time object detector I hope it convinced you that most of what you need to process images is contained in this Docker image Thank you for following my tutorialIf you want to be notified when the next article comes out feel free to click on follow just below
ASGfNgqrgRTWkNjnhgZQC5,No digital transformation program is complete without a data-based initiative With some speculating that artificial intelligence will create a winner takes all game in certain industries the need to capture and process data that will feed into artificial intelligence algorithms has never been more urgent In fact a recent Gartner survey found that leading organizations are expected to triple the number of artificial intelligence projects they have in place by the year 2022While artificial intelligences potential is exciting the outlook for big data initiatives  one of the cornerstones of any artificial intelligence process  is less positive According to Gartner Analyst Nick Heudecker approximately 85% of big data projects are slated to fail Among the primary causes of failure cited are the difficulties inherent in integrating big data with existing business processes and applications management resistance as well as security and governance challengesConsider the following scenario: A data analyst working with the sales team would like to track new metrics in the daily sales reports However the data analyst realizes that the underlying data to these metrics does not currently exist in the database that feeds into the reports The analyst then proceeds to contact the data engineers that own the data warehouse to add these new metrics to reporting tablesTasked with this new request the data engineers check the database schema and realize that the data elements required to calculate these new metrics are not in the data warehouse In this scenario the data engineers must: This entire process could take weeks before the data analyst is able to report the new metrics in the daily sales reports Meanwhile the sales team have lost the opportunity to act on potentially valuable insights earlier In addition the long operational lead time to go from data acquisition to insight creates a high marginal cost to ask the next business questionThe challenges that plague traditional data analytics call for an improvement on existing methodologies to increase project success rates Fortunately there is a modern approach to data analytics that aims to accelerate the ability of analytics teams to deliver insights to usersDataOps is an approach to data analytics that refers to the union of data experts and IT operations It simplifies and reduces the end-to-end data analytics lifecycle from the origin of ideas to the creation of reports and models that add value While DataOps aims to achieve with data analytics what DevOps has achieved with software development DataOps also focuses on the diverse set of people processes and tools that exist in data environments DataOps achieves focus by leveraging agile principles the DevOps principles of continuous integration and delivery and lean manufacturing These tenets are in place to minimize redundancy foster a culture of continuous improvement and where possible reduce the number of steps in a data lifecycle through automationAt a manufacturing plant assembly managers coordinate the activities of the assembly line workers to ensure that work is being done efficiently while still producing high-quality products Within DataOps data pipelines play a similar role Specifically they manage the full lifecycle of data: scheduling jobs executing workflows and coordinating dependencies to process data from source systems to downstream applications With countless accessible data sources available for enterprises and even more downstream endpoints for data consumption data pipelines need to be robust and scalable to route that data correctly without losing integrity For example within an enterprise customer data may need to be filtered and sent through a rigorous security scanning process while at the same time streaming location data is being combined with sales data and utilized downstream to train a machine learning model Modern data pipelines will minimize the amount of code changes required to handle these different types of scenarios as they need to adapt to the changes in incoming data at the speed and scale required The most efficient way to enable this flexibility and scalability is to use parameterized metadata-driven pipelinesParameterization allows users to successfully run the same process using different inputs Following up on our manufacturing example it would mean a clothing factory could change from producing red scarves to blue scarves by changing the color dye that they use For data pipelines it means that pipeline code must be re-usable and elastic enough to accommodate all kinds of data inputs with few constraints on schema or type In data-driven organizations these inputs are always changing Any new data source schema change or other analytics improvement requires an update to the data pipeline Having to re-engineer a data model every time a new requirement comes in can take up valuable resource time that could be better spent elsewhere Instead by parameterizing workflows within the pipeline a set of metadata inputs and conditions (ie information about the data source) could be used to determine how that new data source might be sent through the pipeline These metadata inputs and conditions enable teams to quickly bring together siloed data sources without major code changes or resource requirements which is crucial to lowering cycle times and analytics delivery By emphasizing parameterization the time taken to get data from point A to point B is minimizedBuilding a metadata-driven pipeline that can handle all analytics scenarios in a large organization is daunting Even with a robust data governance policy poor data quality can creep in Fortunately there are tools and services available to make this process easier Open source tools such as Apache Airflow Luigi and Azkaban can help organizations accelerate the data lifecycle at a low cost There are also managed services available that can handle the back-end infrastructure to minimize any upfront work required Azure Data Factory (ADF) is a managed service within Microsoft Azure that we have used recently for our clients ADF is an intuitive data pipeline orchestrator in Azure with drag-and-drop capabilities a huge variety of connectors to bring in any number of datasets and a large number of activities Its intuitive user interface and integration with other Azure services makes it a great option for organizations hosted in AzureRecently we used ADF to build robust pipelines to bring data from key sources in the organization into a centralized data lake which was then used to curate and consolidate downstream analytics Following the DataOps approach of ensuring code reusability we parameterized our entire pipeline to ensure that the same pipeline could be used for any future data source that our clients might decide to bring in We developed JSON files to store information about the data sources (Salesforce Azure SQL Databases Marketing Platforms etc) and used ADF to orchestrate and process that data through the pipeline Now when the organization wants to bring in a new data source they can simply add that source to the metadata JSON files During the next scheduled pipeline run the new data source will be automatically run through the pipeline where it is processed checked for data quality and loaded into the data lake where it is available for analytical use Instead of taking weeks to determine requirements modify code and get the approvals required their data engineers are now able to add a new source in minutes by using already-approved code and incorporate it into the analytics lifecycleSpeed to market is crucial to the success of big data initiatives within organizations and thus finding ways to improve business processes and the data lifecycle are essential While changes to existing processes may face the inertias that stem from the established practices of an organization building metadata-driven pipelines is one key step towards reducing the time spent deploying analytics to production and planting the seeds of a DataOps-enabled modern culture of data
nR2ET9fSsEV98KaGacuSQJ,At any point in time you are about two or three clicks away from reading an article about data science machine learning or artificial intelligence As an analytics architect these topics are broached in nearly every meeting that I attend with my clients or prospective clients In fact a recent Gartner survey measured that 59% of survey respondents had some type of artificial intelligence deployedI am excited about the prospects of this; artificial intelligence and machine learning are having a transformational effect on how companies can service their customers However I am also cautious The same Gartner survey shared that 34% of respondents claimed that data quality and scope were the top challenges for their artificial intelligence initiatives Dirty data fed into algorithms can do more harm than goodThere is a very specific kind of dirty data that happens as a result of data engineering that I dont believe gets enough attention: balance Balancing your data engineering pipelines refers to making sure that your data transformations output an expected number of rows The expected number of rows during initial data pipelines will typically be dictated by the number of rows in the source system (The data engineer wants to be able to represent the source system with high fidelity before it is altered by downstream business logic) Its my firm belief that by ensuring that your pipelines are balanced data engineers will avoid most downstream data quality issues leading to better analytical outcomesA data engineer wants to take a snapshot for the Transactions table in an operational database and load the snapshots into the raw zone of a data lakeThe data engineer can do this with a high level of confidence that this stage of the pipeline will be balanced Each data lake snapshot file will have the same row count as the table that it was taken from Business users can query the most recent snapshot of the data and have comfort that the dataset has high fidelity However it is not always best to take snapshots of tables; what if the table is extremely large? This will force storage costs to increase exponentiallyTo account for this the data engineer implements incremental logic when loading data from the Transactions table (ie load only the rows that were added since the last pipeline run) This makes the pipeline far more efficient and the associated costs of the pipeline go downBut there is a problem: several weeks after the pipeline was implemented business users begin reporting that their analytics are not matching their expectations The data engineer needs to think about where things could be going awry In order to investigate the engineer takes a row count of the Transactions table in the source and then the combined row count of all incremental filesThe data engineer knows that something about the data pipeline is off but has no idea when the problem occurred When our team builds data pipelines we make sure that we measure the row counts of the source and the sources matching data lake entity then store that information into a balance table The balance table should look at the current rows for a database table at the time the pipeline was run and compare that to the number of active rows in the data lake entity This process is explained further in depth belowThe incremental logic employed by the data engineer looks at loading rows that have a system modified date of after the last time the pipeline was run and before the pipeline was triggered The figure below visualizes two incremental jobs on the source table called Transactions Additionally the data engineer has learned the error of their ways and implemented processes to store row counts of the source and the data lakes active row counts for the Transactions table after each pipeline run in a balance table Data lake entity rows are counted through a tool like Hive or Presto In the example below the incremental data pipeline has been run twice resulting in two batches of data loaded to the data lakeEverything should be balanced in this situation because the pipeline should only have to append data into the data lake entity The data engineer can look at the balance table and feel comfortable that they are maintaining data integrity in the data lake However what happens when there is an update to the data? Below shows that there was an edit to Transaction Number 3 that changed the Amount field then loaded to the data lake in a third incremental batchTransaction 3 has an updated modified time and was included in the third pipeline run In a typical data lake scenario that only loads files into data lake storage there will be one more record in the table than in the source when a business analyst queries this dataDelta Lake can help correct both the balance measurements and how the business queries the data with minimal effortDelta Lake is an open source library made by Databricks (the same folks that open sourced Apache Spark) that takes the benefits of a database (ACID transactions DML statements to upsert data) with the benefits of a data lake (schema flexibility scalability) Delta Lake allows you to create Delta tables to treat groups of similar data lake files as one entity (similar to other frameworks like Hive Presto and SparkSQL) However unlike the other frameworks being able to perform Update Insert and Delete statements on these Delta tables is a game changer! Lets explore how Delta Lake allows us to gain comfort that our pipelines are balancedUsing Delta Lake the data engineer creates a Delta table around the Transactions files in the data lake allowing for ACID and DML-enabled transactions to occur The Delta table can also be queried with SQL which will be familiar to business analystsThe data engineer can capture this change in Transaction 3 while also feeling comfortable about our balance by treating our Delta table similar to a Type II Slowly Changing Dimension Instead of simply loading batches of data into data lake file storage the data engineer can perform upsert statements on Delta tables we add a current flag to each row and turn off and on rows as they receive changes Here would be a look at our Delta table at the end of the third pipeline run: When the data engineer takes the balance of this table to compare with the source they will only include rows where current equals TRUE This will return six rows to match the six rows from the source allowing the data engineer to rest easy knowing that their data pipeline is balancedThe above example is prevalent in any data lake scenario; however there are many different reasons why your source table would not match the active rows in the data lake entity· Tables are joined during the incremental query Joins can cause duplicate rows when at least one tables join column is not unique causing higher target row countsUsing a balance table helps to surface these problems early and will speed up the development process overallOur team is looking for ways to extend balancing using open source tools One tool that is of interest is Great Expectations Great Expectations is an open source python library that allows data engineers to apply business and data quality rules to batches of data then automatically create documentation from those rules Great Expectations could be used to apply a business rule that source row counts must match target row counts If met Great Expectations can send a Slack notification noting the success of the pipeline
Cc9e7f5KFhsDTUnAqivHo4,Big DataSnapTravel is a startup headquartered in Toronto with a strong data-driven culture Aside from business decisions data plays a large role within the organization from hiring decisions all the way down to the frequency of in-person standups As an Engineering Intern at SnapTravel I was immersed in this culture starting from Day 1My project that unbeknownst to me would take three months to complete seemed rather simple: Design and Implement SnapTravels Supply ETLs: Three ETLs how bad could it be? Oh how wrong was ITo be honest I had no idea where to begin as I was not clear on the exact requirements nor the business use cases for this data In fact it was quite surprising to discover that there was no business owner for this project considering it would be our BizOps team who would be consuming this data As any good engineer would my first instinct was to create a design specification that would outline the requirements and key results we expected from these ETLs in addition to the technical details of how we would engineer themBelow I explain my thought process both when making technical design decisions (related to these ETLs) in addition to critically analyzing why SnapTravel chose to build their data infrastructure a certain wayThese ETLs would need to consume user search request data and the corresponding rates provided by SnapTravel The raw data exceeded 500 GB in our data warehouse Snowflake requiring intelligent solutions for aggregating transforming and extracting useful informationIf you ask any Data Engineer at the Big-N companies they may jump to the solution of using Apache Hadoop and its signature MapReduce programming modelThere were a few options in terms of scheduling tools however the two that stood out were Luigi (a data pipelining tool built by Spotify) and Airflow (a data pipelining tool under the Apache workflow)Finally we had all the tools needed to design and execute these ETLs but I soon realized that this task was far from being complete The first issue we came across was the ETL runtimes As a growing organization our data was constantly increasing (in terms of size) What started off as 50MB/day of raw data in November 2018 easily scaled to 1GB/day by the end of January 2019 with no signs of slowing downOur initial idea was to run all 3 ETLs in parallel to one another Since each task was independent of one another this seemed feasible at first What we soon realized was: As the data increased in size so did our querying time Since all the queries for these ETLs were executed on Snowflake (our Data Warehouse) the ETL run times became so large that Data Analysts were not able to run their own queries (due to limited computing resources)Since Hadoop was not a feasible option at the moment we had to find another way to optimize these queriesOut of the three ETLs the one taking the longest time was Supplier Availability with a daily run time averaging 5 hours Our goal was to get this ETL to run in 1 hour or lessThis is where we went back to the drawing board with our BizOps team to really flesh out the use cases for this data As it turned out the main purpose of Supplier Availability was to compare the responsiveness of SnapTravel suppliers relative to one another This meant that we did not need to have exact numbers for each supplier and instead could create a sample population to make these comparisonsWe knew that the type of sampling we wanted was Bernoulli Sampling as each element of the population had an equal chance of being included in the sample And luckily for us the Snowflake Engine supported the Bernoulli sampling methodIntroducing a 10% Bernoulli sample reduced the Supplier Availability ETL runtime by 90% (making the average runtime around 30 min) While this exceeded our initial goal (< 1hr runtime) it was not time to celebrate yet as we had made a huge oversight in our new sampled pipeline causing some of our data to become inaccurate and thus unusable by our BizOps teamAs sampling seemed like a success we wanted to use the same sampled data for the other 2 ETLs (Supply/Demand and Expired Rates) as they would run into the same problems as Supplier Availability quite soon Shortly afterwards it was pointed out by our business team that there were some inconsistencies in the Expired Rates data The root cause of this? Our sampling methodA part of the Expired Rates ETL used a query to find the lowest rate provided by each supplier for a given search request This rate was then classified as either Valid or Expired As you may recall we used Bernoulli sampling on the rates table to select 10% of the raw data This meant that the lowest rate per supplier will not always be included in the 10% sample we extracted Since this minimum cost was used to directly identify if a supplier provided an expired or valid rate we needed to modify our original sampling solutionOne option to fix our sampling issue was to use Downsampling Downsampling is essentially reducing the rate at which data is sampled In our case we would log our search requests (and the corresponding rates provided) in larger time intervals meaning that the sampled data would be reduced in size while maintaining accuracy The second option was to modify our existing solution to take into account the minimum rate caseDownsampling would mean that our primary data for the ETLs (in Snowflake) would be a sampled source of all search requests and rates While this would work effectively for the three ETLs it violates the Single Source of Truth Principle: Every data element should be stored exactly once (and any references to that data element should lead back to a single source)Having an SSOT architecture allows for data accountability/traceability as our Data Warehouse scalesWorking on this project allowed me to gain a deeper understanding of the complexities and nuances associated with big data In addition it gave me the creative freedom to explore alternatives to industry standards and critically analyze the delicate balance between technical ease maintainability and sustaining resourcesAbove all I learned that a Data Engineers work is not finished once data is in the warehouse (otherwise I would have been done in two weeks)! Working with our BizOps team to QA data with our Engineering Manager and Senior Data Engineer to optimize infrastructure and documenting the resultant data (yes column-by-column) was a long (and sometimes painful) but important process to ensure these ETLs were accurate and met business requirements
PZhwNYeWbUFHk3HJNfQph4,Concerns the basics of networking starting with some common terminology of data transmission This can be summarised as: In order for this system to be effective it needs to ensure: This effectiveness can be hampered by: For computers and other electronic devices when messages are sent they take the form of analog or digital signalsAnalog transmissions are broadcast as a wave and the protocol of the signal knows where each part of the wave corresponds to a particular part of data so the peak of the wave may be an A the trough of the wave a BDigital transmission is via a series of voltage pushes where a positive push equals 1 and no push is 0 The sequence can then be translated to bitsIn both scenarios the message can be distorted by the three reasons given above resulting in degradationOnce we know the medium by which the message will be broadcast the next consideration is how to transmit it There are two ways to transmit: Guided The message is sent directly to where it is supposed to go Taking our non-computer analogy this would be a letter Only the receiver receives the sent data In the case of computers This is primarily via cables such as Twisted Pair Coax or Fibre OpticsUnguided The message is broadcast so that anyone can receive it For non-computer analogy this would be talking in a crowded (but quiet room maybe a library!) anyone within range can receive the signal For computers this is primarily WiFi or BluetoothThese transmissions can be sent in three different modes: Simplex only the sender can transmitHalf duplex the sender and receiver can transmit but not at the same timeFull duplex both the sender and receiver can send at the same timeThe next topic is Network Fundamentals
NCXKzBb3jjqbkuuYKtWLgj,We live in a strange time when critical technological decisions are driven by marketing propaganda There is arguably no industry more afflicted with this syndrome than big data In this article I will address one very peculiar manifestation of this phenomenon in the big data industry that has crippled data engineers across the board  a resolute and methodical undermining of the sanctity of strictly-typed schemasLet me give you some context Approximately speaking in the decade leading up to the big data era we saw a meteoric rise in the popularity of dynamically-typed languages like Ruby Python and JavaScript To understand the difference between statically and dynamically typed languages check out this Stack Overflow answer For the engineering community in general this was a somewhat welcome development It became easier to train software developers it became easier to produce code quickly and programming generally became more accessible However our ever-growing reliance on dynamic typing started to introduce a sense of complacency around the subject of data types Want to multiply a boolean with an integer? Want to add an array (of anything really) to a string and then multiply it by a float? Sure no problem JavaScript will let you do anything your heart desires and so will your startup founder boss as long as the colorful NPM stickers on your MacBooks lid continues to infatuate potential investors In this setting was born the NoSQL movement with MongoDB at its helm While SQL developers would carefully craft strict schemas with precise migration scripts rollback functions and versioning history Mongo was hailed as the new schemaless starchild of the OLTP database niche This is when the marketing propaganda really began to drive engineering design As explained in a pertinent article by Lukas Eder a schemaless database only serves to defer the inevitable work of sanitising your schema to some other later time More importantly it also means deferring this work of schema management to the application layer where it needs to be done manually and is prone to runtime errors Calling a service schemaless doesnt change the reality that a data point without a data type is unusable for a computer thus leaving the datas integrity to the type inference capabilities of the languages runtime It is noteworthy that in December 2015 MongoDB version 32 was released with new schema validation capabilities likely in response to the fallout of these issues Mongos shortcomings drew an onslaught of criticism over the years the details of which would stray away from the point of this articleWhile the ideological war over the OLTP empire raged on the analytics and data science ecosystem which had thus far been dominated by strictly-typed and schematic data warehouses started to see an infiltration of the new and disruptive schemaless ideas As marketing propaganda across the world started to hail big data as the new oil corporations of all sizes became obsessed with hoarding data If AI is data-driven then surely more data implies more AI magic and who in their right mind doesnt want more AI magic? In times like this schemas became nothing but a hindrance like a bouncer at a nightclub which isnt quite cramped enough and really wants to be To fix this problem we turned to the idea of a data lake  essentially a file system on steroids which can store anything that can be put in a file If this wasnt disruptive enough our industry went a step further with the notion of schema-on-read The idea of schema-on-read is to hoard all the data you like and infer a schema when the data needs to be used In theory this is a swell idea and companies with skin in the game wasted no time in marketing it to the moon However just like with the former NoSQL movement holes in the logic started to emerge quickly How does one infer a schema? Do you write an algorithm to make an educated guess based on a subset of homogenous data? Does this algorithm depend purely on deterministic heuristics or non-deterministic machine learning models or a patchwork combination of the two? How do you know that your subset isnt full of anomalies? How do you guard against a sampling bias? What happens when your schema inference algorithm inevitably gets it wrong? And possibly the biggest question of them all what happens when the schema changes? There are many schema inferring tools out there but none of them have found comprehensive answers to these problems As a result a lot of data engineers working on data lakes today spend unnecessarily large amounts of time trying to fix bugs caused by schema mismatches type inference failures and data corruption due to poorly assigned types Problems that may never have arisen had we paid attention to these matters before abandoning the notion of strictly-typed schemas Let us now consider a few practical implications of this issueHeres a little fun fact that has caught many a senior dev off-guard 11 + 22 quite obviously equals 33 in math but try running 11 + 22 == 33 in your favorite programming language and you might be surprised to find that it evaluates to false Veteran engineers will probably know why but just in case you didnt by default programming languages will cast the values 11 and 22 to a float type A float uses a binary (base-2) system and typically offers as much precision as 32 bits of memory can afford This creates limitations on how precisely a floating point number can be represented leading to a situation wherein 11 + 22 actually evaluates to 33000000000000003 For most use-cases this very slight imprecision is insignificant but there are important exceptions in particular monetary values If lets say you are working with a foreign exchange dataset even a small imprecision in a currencys conversion rate can lead to massive discrepancies when you use that imprecise rate in a computation applied to millions of data points and then perform aggregations on those data points Of course this is not a new problem and most languages and databases offer a much more precise 128 bit decimal type which is widely accepted as the best way to store monetary dataNow lets consider this in the context of data lakes with schema-on-read inference architecture Lets say that your data lake is receiving a table with decimal fields Some of these fields dont need absolute precision (for example weight of a purchased item) while others do (for example exchange rate at the time of purchase) Todays schema inference engines are not brave enough to try and infer the required precision based on the field name and rightfully so because naming conventions are another messy can of worms So what options does your schema inference engine have? It could either cast every field with a floating point as a decimal type with very generous precision and scale Or it could cast everything as a float or a double (64 bit precision with a binary system) In the former case we have the problem of unnecessary performance penalties because decimal type objects are bulkier and slower to work with In the latter case we run into the problem of imprecise financial data which introduces all the aforementioned liabilities In my experience all the schema inference engines I have worked with so far default to using floats This leaves us with no choice but to hack our own schema-on-read system and enforce a schema for certain fields It actually takes a fair bit of work to ensure that decimal type objects are maintained with their full precision through an entire ETL pipeline It has to be enforced on the file format you are using on disk and on your application layers codeOn January 29 2020 Pandas released v100 a major version release If you have a look at its changelog youll notice that an overwhelming majority of the updates are related to its handling of data types This includes a dedicated StringDtype an NA type for missing values a BooleanDtype that can hold missing values and more The implications of this release are huge It is a sign that Pandas has acknowledged the core problem discussed in this article and has therefore decided to steer its shipping in the direction of well-planned type systems For some context Pandas is considered the gold standard for single-server data processing applications written in Python and for companies with small-to-medium sized data it is a far more practical option than SparkIn the context of this update lets dig a little deeper to see what happens to data as it moves between type systems of varying type resolutions Traditional relational databases such as Postgres and MySQL offer fairly high-resolution type systems with various options for strings integers etc Lets consider the case of integer data Lets say your data starts in a MySQL table where it can be set to one of five different integer types depending on its size range (min-max values) and each of these types can be set to either Signed which can hold both negative and non-negative values or Unsigned which can hold only non-negative values From here lets say you extract some data and dump it into a CSV file You are now left entirely at the mercy of your schema inference engine to get this right Lets say the engine selects 1000 random rows and assigns data types to the fields based on the data in those rows The accuracy of the dataset once its loaded into memory now relies entirely on the notion that these 1000 rows sampled all possible edge cases For example if a field contained mostly 3 digit integers but a few valid 19 digit integers that did not show up in the random sample the schema inference engine might misallocate this data to an integer type that isnt large enough to hold the outliers resulting in an integer overflow which could lead to anything from runtime errors  if you are lucky  to corrupt data in production datasets Similar problems can and often do occur when an inference engine misallocates signed integers as unsigned integers or vice-versa as was once acknowledged by the founder of PandasProblems around type resolution are by no means limited to numbers The exact same problem described above regarding the length of an integer applies to strings as well If your schema-inference tool is casting types for fields in a data store that casts text data differently depending on its size if the random sample of data selected for inference does not account for grossly oversized but valid outliers they may simply get truncated in the final dataset These are all problems we deal with on a regular basis With regards to string fields the recent Pandas update is a very welcome development because Pandas previously used the object type for strings which is the parent type for all its other types The problem here is that until this recent release a Pandas DataFrames schema offered no way to distinguish between a string field and other fields containing complex objects such as decimal fields which use Pythons Decimal type This becomes a big problem if you are trying to use a Pandas schema to automatically enforce a schema on a destination data store like a Parquet fileUnfortunately there is no conclusion as of now We the data engineering community at large have dug ourselves into this hole While I am complaining as loudly as I can I too drank the schema-on-read Kool-Aid with gusto Today as I lead a team of highly skilled data engineers it pains me to watch our sprints get crowded with firefighting tickets and tech debt produced by poorly inferred schemas We are slowly but surely rewiring our system to enforce schemas wherever possible We are also working on a SchemaPorter class which has been designed specifically to manage the safe movement of schemas across various type systems Our solution for now is in its nascency but if and when it reaches a certain point of maturity we will share more details about it My hope is that over time the many in-house solutions built by teams like ours will eventually converge Meanwhile we continue to put our faith in the champions of open source who build and maintain Pandas Spark and other such majestic feats of human engineering There is no doubt in my mind that the hardest most important work in the data engineering ecosystem is done in the pull requests made to these open source repositories and developments like the recent Pandas release could nip most of our schema-related problems in the bud Moreover it would probably serve us well to treat new trends and lofty promises in marketing campaigns with a healthy dose of skepticism Before we celebrate a new feat of disruptive innovation we should carefully consider what is being disrupted There are reasons why we spent decades laboring over database schema designs and migration scripts A lot of those reasons havent disappeared quite yetPS If you havent already do read my colleague Husseins article on schema evolution Managing schema evolution is possibly the most difficult challenge posed by the schema-on-read paradigm I decided not to dive into it because I would just be repeating a lot of what Hussein already wroteEditorial reviews Mikhail Levkovsky Deanna Chow & Liela Touré
W52VVXmZLAJSVEeFb24DS9,The first of a two part series exploring the fundamental themes of architecting and governing a data lake Co-authored by David Lum & Hussein DanishData engineering is not the most well understood profession From a high-level perspective the problems encountered can seem almost trivial Aggregate company data and make it accessible; while this mandate may seem straightforward the devil is in the details as they say It can be challenging to have a system which accepts a wide variety of types and formats of data accounts for their change over time enables accessibility and governance all while remaining robust Furthermore considering the wide variety of stages at which a company may be there is no one size fits all solution This article will start off by covering some basic concepts and terminology to make sure everyone is on the same page and then move onto more specific topics about data infrastructure with an emphasis on the concept of a data lake Finally well end with how the SSENSE data engineering team has applied these conceptsLets assume youve just started working as a Data Engineer at an organization One of the first things youll want to do is get familiar with the organizations data architecture More specifically youll want to know what kind of data exists and where For example in a microservices architecture your different microservices might be generating events data You might also have a primary database that supports your organizations core application Different sources might generate different types of data which might change at different rates and be stored in different waysChances are that your organization will at the very least have a transactional database supporting the key parts of your production architecture Perhaps there will also be an analytical database to enable data analysis These systems are commonly referred to as OLTP (Online Transaction Processing) and OLAP (Online Analytical Processing) The differences between these systems are well documented (see here and here) and are out of this articles scope That being said its important to emphasize that these systems should be separate from one another Analytics databases usually support large resource intensive and infrequent read queries with very few and predictable write queries while transactional databases are designed to support large amounts of frequent but small read and write queries The two have completely different architectural needs More importantly analytics queries can easily bottleneck production database infrastructure due to the size and latency of their expected responses Needless to say this should be avoided at all costsTraditional data systems usually place constraints on the types of data you can store What kind of database would be best to store different events generated from various microservices and how would you best store any logs they generate? When dealing with a variety of formats like this a data lake can be a good solution While traditional database systems offer more structured but rigid ways to store data data lakes are far less opinionated but provide enough control and information to allow users to examine and govern their data and dive deeper to answer more fine grained questionsA data lake cannot be conceptualized in the same way as a database A database contains data in a refined consumable format whereas a data lake ingests data that is raw heterogenous and generally harder to use Much of the challenge involved in building a data lake is managing the datas transformation from its original raw state to a final consumable format Concepts such as storage governance and access that are well understood in traditional databases need to be planned carefully Determining processes for ingestion and cataloging are necessary and often require applying schemas and transformations to unstructured data in order to render it accessible to end-users In the same vein the choices made with respect to partitioning based on either time or type of content will heavily influence the way in which you process your dataAt SSENSE we are currently building a data lake from scratch At this stage our primary mandate is to aggregate refine and make available much of the companys data in a centralized location that can serve as the single source of truth for the organization However blindly collecting data can quickly take you from building a data lake to a data swamp A data strategy is required so that solving business problems is a guiding principle for building the data lake More information about data strategies can be found here and here This sentiment is captured by Rob Casper Chief Data Officer of JPMorgan Chase (as published here): The best advice I have for senior leaders trying to develop and implement a data culture is to stay very true to the business problem: What is it and how can you solve it? If you simply rely on having huge quantities of data in a data lake youre kidding yourself Volume is not a viable data strategy The most important objective is to find those business problems and then dedicate your data-management efforts toward them Solving business problems must be a part of your data strategyOur data lake is split across the following three AWS S3 buckets (a cloud based flat file storage system) each of which plays a very important role in the lifecycle of our data sets: By organizing data this way problems can be compartmentalized all data can be governed and traced to its source and all pipelines can be replayed if necessary Beyond this organizations can also consider separating hot data with short to medium term access needs and cold data which simply needs to be archived Cloud providers will often provide different services for these with different pricing strategies for storage and accessWell begin this section by considering what characteristics the ideal data architecture might exhibit and then accounting for real world encumbrance see how this manifests itself in practice At SSENSE our data pipeline and data lake architecture is closely coupled In breaking down an ETL (Extract Transform Load) pipeline the simplest aspect is the T Although the transformation might be where the majority of your code lies it is the system you have the most control over You do not rely on external APIs to be functional or connections to external systems (such as a remote SFTP server) to be operational as you often have to in the E and L phasesThe transformation can be modeled as a simple function from the raw data to the transformed data In a perfect world that would be the end of it The client has data in one form needs it in another and were done The reality is of course that things break requirements change and pipelines need to be retried This leads to a first desirable characteristic of a data pipeline  its replayability Ideally the input the data we are extracting should be immutable Another desirable characteristic is idempotence Something that is idempotent can be applied several times without changing the result beyond the initial application Formally this is f(f(x)) = f(x) In practice this means that running your pipeline more than once beyond the first time will not duplicate or corrupt data In cases where pipelines are rerun  automatic retries for example  this can be essentialAlthough we do interact with a stream of events data produced by our microservices most of the sources we deal with can only be processed in batch format Batch processing is typically divided into two general workflows; extract transform load (ETL) and extract load transform (ELT)In an ETL pipeline data is extracted from a source transformed to the required shape and inserted into the target The advantage of ETL is that data enters your system in the shape you want it to be in and can easily be modeled for analytics This works exceedingly well when the data is coming from consistent and trusted sources but can quickly become too brittle in cases where there is a possibility of a schema change or the data cannot be re-extracted Imagine a scenario wherein data is extracted from a third-party API transformed and then loaded into a data warehouse Some time later the process needs to be rerun but the source data is no longer available due to the third-party aggregating its data after a certain amount of time (to reduce storage fees) In other words your system does not offer immutable dataELT addresses this by extracting and loading the raw data immediately into storage From there you can rerun the transformation portion of the pipelines to your hearts content The drawback here is that the raw data can potentially be schemaless and unstructured Managing this raw data becomes the main difficulty in this configuration and in an era of ever increasing compliance proper cataloguing is criticalAt SSENSE we have developed a system that can be considered a hybrid of the ETL and ELT paradigms As explained earlier we load raw data into our first bucket  this offers immutability For extracting and managing batch data we use Apache Airflow a robust pipelining framework for Python To ingest our events stream we use Amazon Kinesis Data Firehose which dumps date partitioned raw data From there we perform the minimum transformation required to allow AWS Glue to catalog the data Finally we transform the data into its consumable form and make it available via AWS Athena  a distributed SQL query engine built on Presto Depending on the size of the data we are transforming the transformations are handled either using PySpark scripts hosted as Glue jobs or using Pandas All our transformation scripts are encapsulated in an internal version-controlled pip package Throughout the process we also track data lineage which combined with cataloging allows us to retrace our steps to the original source providing better data governance than simply raw ELT (more on this in Part II of this series)Our Airflow pipelines are responsible for ensuring that each of the aforementioned steps is carried out on time and in order thus ensuring temporal consistency Airflow is also responsible for handling task failures retries and external connections management This setup can be thought of as a three tiered system which moves from raw data to normalized and catalogued data and finally to consumable business data gradually refining the schema and homogenizing the data at each stage Our system offers immutability temporal consistency and idempotence granting us fearless replayability with no risk of data duplicationApart from this we also rely on and maintain certain auxiliary systems such as Terraform modules to manage our infrastructure as code Jenkins pipelines to manage continuous integration and deployment GitHub for source control management Docker for containerization and Kubernetes with Helm charts for orchestrating containerized deploymentsAlthough the benefits of a data lake architecture have been highlighted there are a number of cases for which it is not suitable If you want mutable data for example use a database The advantages of the data lake are constructed around having an immutable source of truth Furthermore data lake management is less well-defined and has less industry standard tooling built around it although this may change over time If you are operating a relational database and need some sort of visibility or reporting on it there are many robust open source solutions available immediately that you can turn to This isnt necessarily the case with a data lake If your data engineering team has limited bandwidth and scarce resources a more traditional data warehousing approach might be wiserIn this article we have covered two of the most important elements of data engineering and shown how they are tightly coupled  storage and pipelining We discussed the apparent simplicity of these systems while also highlighting some of the key challenges that a data engineer might face when implementing them particularly in the context of a data lake However these arent the only important concepts A critical aspect of every data engineers role is data governance The data you store is a source of both tremendous value and liability Ensuring that all such data is properly managed protected catalogued tracked and accounted for is of utmost importance We briefly touched on subjects such as cataloging and lineage Stay tuned for the second installment of the Principled Data Engineering series in which we will delve deeper into such subjects and share our perspective on the art of data governanceEditorial reviews by Deanna Chow Liela Touré & Prateek Sanyal
QdzKmtvLZxyN8YEUsyXZEa,The second of a two part series exploring the fundamental themes of architecting and governing a data lakeThe fundamental mandate of a data engineer is to deliver clean accurate and meticulously supervised data Herein lies the difference between a data and software engineer  for the latter their product is the software whereas for us our product is the data and any software we build is auxiliary to the data In other words it is the responsibility of a data engineer to treat data (rather than software) as a first-class entity In this context it goes without saying that data governance is a central theme in data engineeringOur industry has not yet reached a consensus about what data governance entails Factors such as legal regulations and the nature and size of the data play a major role in determining how to govern the data Having said that there is an underlying theme that is common across the board  the concept of meticulous supervisionArguably the very foundation of data governance is metadata management In this case metadata refers to data about your data  this includes any meaningful information about your data which can help you understand it better At the very least for any data set you store your metadata should be able to answer two questions  what data can be found in the dataset and where this data came from The what question is usually answered by a data catalog (sometimes known as a meta-store metadata catalog etc) A data catalog is usually some representation of the schema for a particular dataset This might include information about labels (column names keys etc) data types partitioning rules memory footprint and other relevant information about certain units of data (such as columns)At SSENSE we use the AWS Glue Catalog to catalog our data Note here that while frameworks such as Glue usually offer schema inference capabilities its always prudent to revise inferred schemas and use a file type that supports explicit schemas (such as Parquet) preferably one that is compatible with your frameworks automatic inferring toolAnswering the second question about where the data comes from requires metadata about data lineage Conversations about tracking data lineage in data ecosystems have not yet reached a point of maturity to identify or prescribe best practices While projects like Apache Atlas are trying to standardize best practices around tracking data lineage the data engineering industry at large has only agreed upon the problem not the solution In essence tracking data lineage involves maintaining a recorded history of the origin transformation and movement of each unit of data The granularity of a unit may vary from an entire data set to single points of data Usually more granularity leads to better governance When done right recording data lineage can make your data a lot easier to understand reproduce and debugAt SSENSE all our data pipelines are managed by Apache Airflow DAGs (Distributed Acyclic Graphs) This has allowed us to develop a custom lineage solution wherein every time we load data into our lake (in any S3 bucket) we tag the object storing the data with certain metadata containing lineage information The actual logic for adding metadata to each object is added to the load tasks of our Airflow pipelines Some of the metadata is made easily available by the DAGs metadata and some of it is generated based on the programming logic At the very least we track the following data: While there is undeniable business value in proper metadata management one of our primary goals in all of this is to protect the personal data of our customers and business partners while respecting every customers right to be forgotten Not only is this critical in order to meet regulatory compliance standards such as the EU General Data Protection Regulation (GDPR) it is also an ethical obligation towards anyone who entrusts our organization with their data Moreover we believe that regardless of the size and nature of an organization it is advisable to strictly regulate personal data and to avoid storing Personally Identifiable Information (PII) in any situation where it is not absolutely business-critical In our experience anonymizing PII almost never disrupts analytics and machine learning systemsWhile metadata management handles governing the data itself there is also the matter of governing your data stores relationship with the outside world  the sources and consumers of your data Once again due to the blistering pace of change in the data ecosystem there are no unanimously accepted standards of how this should be done We can however agree on the importance of governing these relationships carefully All data stores rely on certain expectations of both its data sources and consumers  it expects the interface for its data sources to be accessible under certain constraints and its consumers to access its data under certain constraintsGood governance involves ensuring that all such implicit informal or ambiguous constraints are turned into explicit formal and specific agreements (or contracts) This is where concepts such as Data Contracts and SLAs (Service Level Agreements) become handy An SLA explicitly and precisely defines for the user of a software service the constraints within which the service is expected to perform Usually the underlying infrastructure and software of your data source (such as an S3 bucket) will be able to provide clear SLAs about uptime consistency reliability etc Sometimes your data sources might be able to do the same (such as well built REST APIs) In the ideal case where all your underlying software and all your data sources are able to provide such SLAs your data store may also be able to provide reliable SLAs to its consumers This is particularly useful if a lot of your consumers are other automated services that cannot account for ambiguous constraintsA Data Contract acts like an SLA but for every individual unit of data For example a contract might mandate that a certain column must contain the sum of certain other columns Depending on the maturity of your data ecosystem these may be implemented as modules of code that validate data during runtime (as shown here) or they could be implemented manually as an agreement between the data stores team and the stakeholders to define the constraints for every unit of dataAt SSENSE for every data pipeline we support we first manually draft a contract with our stakeholders and then implement their constraints programmatically as runtime validators In terms of SLAs thanks to Airflows reliable dependency management we are able to provide very specific consistency guarantees such as 100% temporal consistency For other guarantees such as availability and durability the SLAs provided by the AWS services we use (such as S3 and Athena) remain applicableThe subject of testing code has been discussed thoroughly for many decades Software engineers have come a long way from sprinkling unit tests onto code to adopting comprehensive paradigms like Behaviour Driven Development and aiming for 100% test coverage Data engineering however is slightly different Traditional testing strategies often do not cater to the specific concerns of data pipelines Unlike most client-facing services a data pipeline is not subject to large numbers of unpredictable users A lot of the code a data engineer writes will only be used by other data engineers in very predictable and specific ways This negates the need to worry about a lot of the behavioural testing that is so important for APIs Also as mentioned earlier the product for a data engineer is the data and not the software itself This implies that testing in data engineering should prioritize data quality over code quality This can be especially challenging since your data sources are usually out of your control and catering for all incoming data anomalies in unit tests is practically impossibleKeeping all this in mind we have developed a two-fold approach to testing Firstly we use snapshot testing to test the code for our data transformations For every pipeline we maintain some static input data and its resultant output  the transformed data (validated by the relevant stakeholders) as the mock data for our tests While this is supplemented by some traditional unit tests for the rest of our code we only aim for 100% coverage for our transformation logic These tests are then run in our CICD pipelines guaranteeing that any code we push to production does not corrupt our data Secondly to cater for corrupt incoming data we have runtime validations built into our data pipelines which run immediately after incoming data is transformed These validations reflect our data contracts with our stakeholders and ensure that bad data never enters the data lake Unlike client facing services we are often comfortable with our pipeline tasks raising errors and failing in production Airflow allows us to handle this very easily by fixing the issue(s) and re-running the failed tasks What we cannot afford is the unnoticed corruption of our dataIn Part I of the Principled Data Engineering series we introduced our data lake architecture as a reference point for discussing the challenges of storing and pipelining enterprise-scale data In Part II we focused solely on the subject of data governance It may seem unusual that we invested half the series into discussing data governance a subject that is often treated as an afterthought in both industry and academia However in our opinion this neglect of good data governance has led to serious problems in the tech industry As data engineers we must assume an ethical responsibility to ensure the integrity validity confidentiality and accountability of our data We owe this to our clients associates and the company itself Having read both parts of this series you should now have a mental framework for architecting a data system in a practical and principled manner We encourage you to let business needs drive your technical decisions and to ground all such decisions in the philosophy that your data is your product and by extension your responsibilityEditorial reviews by Deanna Chow Hussein Danish & Liela Touré
RqumVpVmvC3VrvhF3pDCF3,There are countless articles to be found online debating the pros and cons of data lakes and comparing them to data warehouses One of the key takeaways from these articles is that data lakes offer a more flexible storage solution Whereas a data warehouse will need rigid data modeling and definitions a data lake can store different types and shapes of data This leads to the often used terms of schema-on-write for data warehouses and schema-on-read for data lakes In other words upon writing data into a data warehouse a schema for that data needs to be defined In a data lake the schema of the data can be inferred when its read providing the aforementioned flexibility However this flexibility is a double-edged sword and there are important tradeoffs worth consideringAt SSENSE our data architecture uses many AWS products The current iteration of our data lake makes use of Athena a distributed SQL engine based off of Presto in order to read data stored in S3 The majority of these files are stored in Parquet format because of its compatibility with both Athena and Glue which we use for some ETL as well as for its data catalog One advantage of Parquet is that its a highly compressed format that also supports limited schema evolution that is to say that you can for example add columns to your schema without having to rebuild a table as you might with a traditional relational database However Parquet is a file format that enforces schemasBefore answering this question lets consider a sample use-case In an event-driven microservice architecture microservices generate JSON type events that will be stored in the data lake inside of an S3 bucket This data may then be partitioned by different columns such as time and topic so that a user wanting to query events for a given topic and date range can simply run a query such as the following: SELECT * FROM datalake_eventsWithout getting into all the details behind how Athena knows that there is a table called topicA in a database called datalake_events it is important to note that Athena reads from a managed data catalog to store table definitions and schemas In our case this data catalog is managed by Glue which uses a set of predefined crawlers to read through samples of the data stored on S3 to infer a schema for the data Athena then attempts to use this schema when reading the data stored on S3In our initial experiments with these technologies much of our data was kept in its raw format which is JSON for event based data but for many sources could also be CSV Here are some issues we encountered with these file types: Consider a comma-separated record with a nullable field called reference_no Let us assume that the following file was received yesterday: Now lets assume that the sample file below is received today and that it is stored in a separate partition on S3 due to it having a different date: With the first file only Athena and the Glue catalog will infer that the reference_no field is a string given that it is null However the second file will have the field inferred as a number Therefore when attempting to query this file users will run into a HIVE_PARTITION_SCHEMA_MISMATCH error Essentially Athena will be unable to infer a schema since it will see the same table with two different partitions and the same field with different types across those partitionsAnother problem typically encountered is related to nested JSON data For example consider the following JSON record: When Athena reads this data it will recognize that we have two top-level fields message and data and that both of these are struct types (similar to dictionaries in Python) Both of these structs have a particular definition with message containing two fields the ID which is a string and the timestamp which is a number Similarly the data field contains ID which is a number and nested1 which is also a struct Now consider the following record received in a different partition: The addition of a key/value pair inside of nested1 will also cause a HIVE_PARTITION_SCHEMA_MISMATCH error because Athena will have no way of knowing that the content of the nested1 struct has changed Even though both of these columns have the same type there are still differences which are not supported for more complex data typesFixing these issues however can be done in a fairly straightforward manner By declaring specific types for these fields the issue with null columns in a CSV can be avoided Furthermore by flattening nested data structures only top-level fields remain for a record and as mentioned previously this is something that parquet supports Flattening the data can be done by appending the names of the columns to each other resulting in a record resembling the following: This brings us back to the concept of schema-on-read While conceptually this convention has some merit its application is not always practical While upstream complexity may have been eliminated for a data pipeline that complexity has merely been pushed downstream to the user who will be attempting to query this data With an expectation that data in the lake is available in a reliable and consistent manner having errors such as this HIVE_PARTITION_SCHEMA_MISMATCH appear to an end-user is less than desirableUltimately this explains some of the reasons why using a file format that enforces schemas is a better compromise than a completely flexible environment that allows any type of data in any format There can be some level of control and structure gained over the data without all the rigidity that would come with a typical data warehouse technologyNevertheless this does not solve all potential problems either Other nested complex data types can still pose problems For example consider an extended version of the previous JSON record: An additional field nested2 which is an array-type field has been added Similar to the examples above an empty array will be inferred as an array of strings But perhaps this is an optional field which itself can contain more complicated data structures For example an array of numbers or even an array of structs The latter case is a troublesome situation that we have run into It has required some creative problem solving but there are at least three different approaches that can be taken to solve it: Perhaps the simplest option and the one we currently make use of is to encode the array as a JSON string Therefore the above field nested2 would no longer be considered an array but a string containing the array representation of the data This approach can work with all complex array types and can be implemented with no fuss The main drawbacks are that users will lose the ability to perform array-like computations via Athena and downstream transformations will need to convert this string back into an array However this can be implemented easily by using a JSON library to read this data back into its proper format (eg jsonloads() in Python) This approach also simplifies the notion of flattening as an array would require additional logic to be flattened compared to a structWhereas structs can easily be flattened by appending child fields to their parents arrays are more complicated to handle In particular they may require substantial changes to your data model Considering the example above an end-user may have the expectation that there is only a single row associated with a given message_id Flattening an array with multiple elements would either involve adding a number of columns with arbitrary names to the end of the record which would diminish the ability to properly query the data based on known field names or it would involve adding multiple rows for each element of the array which could impact logic that aggregates data based on an ID Although the latter is a viable solution it adds more complexity and may require a completely separate table to store the array resultsIn theory this option may be the best in terms of having full control and knowledge of what data is entering the data lake The approaches listed above assume that those building the pipelines dont know the exact contents of the data they are working with They are schema and type agnostic and can handle unknowns However if the exact format and schema of messages is known ahead of time this can be factored into the appropriate data pipeline There has been work done on this topic but it also relies on more stringent change management practices across the entirety of an engineering departmentThe goal of this article was to provide an overview of some issues that can arise when managing evolving schemas in a data lake Although the flexibility provided by such a system can be beneficial it also presents its own challenges Much research is being done in the field of Data Engineering to attempt to answer these questions but as of now there are few best practices or conventions that apply to the entirety of the domain Different technologies can offer different pros and cons that may help with these issues: Avro is a comparable format to Parquet and can also handle some schema evolution It also has specific files that define schemas which can be used as a basis for a schema registry Googles BigQuery is a data warehousing technology that can also store complex and nested data types more readily than many comparable technologies It is important for data engineers to consider their use cases carefully before choosing a technology The tools should ultimately serve the use case and not limit itEditorial reviews by Deanna Chow Liela Touré & Prateek Sanyal
EAsgECzPQHqRW3whzCqJR8,Recently (11/1/2017) I hosted a meetup of Chicagoland Airflow users Despite being only 4 people it was probably the most educational and productive meetup Ive ever attendedStanton Ventures is a data consultancy
V5824TicwHRkn5ks2bc9Rb,There are so many tutorials out in the wild about how to take an application containerize it and run it in your enterprises on-prem/public cloud securely but hey this will be yet another oneDuring my daytime work we help our clients to use the power of Python calculations in a data visualization tool called Tableau which requires a small middleware that actually evaluates the python code  outside the standard platform This middleware is called TabPy and this story is about how to deploy it in Kubernetes with the minimum configurations for scaling and hardeningTo give you an overview this is what you will learn: I have a few assumptions to start like you have access to some Kubernetes based cloud service (Amazon EKS GKS or on-premise cluster) If this is not the case I would suggest to check out AWS EKSs getting started guide first Also you will need basic Unix scripting and docker experience tooGreat lets get startedFirst things first we need a Dockerfile that runs our TabPy server with all the potential python modules we might leverage from our application These python packages need some pre initialization as well as the TabPy service itselfThe outline of the Dockerfile will look like something this: https://githubSee what we do and why We start from Alpine Linux due to various reasons: The sequence to build and configure TabPy is the following: What is missing here is the SSL configuration and user authentication SSL can be configured on TabPy level along with basic authentication However in our case we will configure SSL on the edge In case you required to encrypt all in-cluster communication it is preferable to configure SSL both in the container and on the edge tooNow take a closer look at securityIn the previous step we made a few steps to ensure a secure environment for our application: In addition to these steps it is a best practice to set up SELinux domains for our application similar to httpd_t This could give additional security enforcing rules like no execution of child processes or cannot read files from specific folders  even if the classic Unix permissions allow it SELinux is out of the scope of this post but I highly encourage you to use itIn minimal setup we need a Service and a Deployment resources to start our pods (pod is the combination of container storage IP etc) The Service is responsible to make our service discoverable internally and externally while Deployment just explains what containers we need in what setupTo add the Service just type: For Deployment: Make sure you have the following sections in your Deployment file: Now we should see something like: Now we can test our service: All looks goodIn most of the cases we do not need any outgoing networking connection from our pods other than intra-namespace connections In our specific case we do not need any outgoing connection at allTo deny network connections we need the Calico network policy system to be installed on our kubernetes cluster In case we dont have simply install it with: Our NetworkPolicy is fairly easy we simply disable all Engress connection in the namespace where our pods deployed: Now there are no outgoing connections from pods  not even DNS requests: We are done with basic security/hardening we can proceed with the scalingWhat is autoscaling and why do we need it? The idea here is to provide automatic horizontal scaling based on resource consumption If the average CPU consumption of our containers goes up to 70% we might need to start new containers to handle the load If the load goes down  we should downscale our servicesIn order to scale up our services horizontally we need to get CPU and memory information from our containers While in the past heapster was enough these days (kubernetes>111) we need metrics-server to be installed If you do not have metrics-server the easiest way to get it with curl and jq : If all looks good we should see something like: Now its time to define the horizontal scaling rules like if the CPU usage is more than 70% then scale up to ten containers I kept the minimum as two to have some basic high availability during worker node crash or rolling upgrades: To check the results: This looks good we see the current usage (1%) and the threshold (70%) to scale In case the load will reach 70% autoscaler will increase the number of pods to reduce the load When the load decreases autoscaler reducing the number of pods to the desired stateIn case you see unknown CPU usage and your metrics-server emits a no metrics known for poderror just make sure you have resources/requests defined in your Deployment definition: Now your horizontal scaling rules are in place security is around acceptable it seems you are ready to invite your usersKubernetes could be easy or complex depending on the depth you are using it However to deploy our TabPy service things were fairly easy However if you had any issues just drop a message Ill try to sort it out
V2QKA6wetAcuPXy72dHuuY,Following the implosion of the US housing bubble when the mid-2000s Great Recession began to hit markets worldwide many enterprises found themselves in dire straits In the ensuing crisis enterprises vied for suddenly limited resources amidst a global economic downturn As consultants Starschema provided them with data-driven insights into their economic processes that allowed them to identify efficiencies without jeopardizing every enterprises most valuable resource: the people that make it greatOver the years weve helped companies great and small weather many a storm From economic downturn to supply chain interruptions we are no strangers to the challenges many companies are now facing Weve walked with our clients through successes and trying times What we learned from a decade and a half of committed work is a core pillar in the foundation of Starschema: our belief that data has the power to change the world for the betterThe current crisis will be no different  it will deeply affect peoples lives and livelihoods We know from experience that data-driven approaches can make a difference for public health outcomes and help companies respond to the challenges ahead and navigate though uncharted territoryGlobalization has created a uniquely interdependent economic system with long and often obscure supply chains Many enterprises especially in the manufacturing sector expect to face shortages of raw materials and components Workplace shutdown mandatory quarantines and border closures are putting a strain on service providers worldwideSupply chain analytics can help enterprises understand their supply chains better As providers of sophisticated graph-based analytical solutions we understand your companys exposure to affected areas and how this cascades throughout your organization Through mapping your supply chain you can better understand the effect of various governmental measures and staff outages have on your company To this end we created a database of curated COVID-19 incidence data The intention is to ingest reliable data from multiple sources and make it analytics ready so it can be easily accessed and used A number of leading companies are already using this database in their business continuity analysesThis information can be integrated into a business continuity dashboard and a supply chain status board which allow companies to make data-driven decisions on whether manufacturing needs to be suspended or diverted In addition our expertise in operations research (OR) and optimization algorithms can be used to reschedule jobs to leverage available resources and available production capacity The range of OR tools and mathematical optimization algorithms used in predictive maintenance and supply chain prioritization can also be invoked to help companies achieve peak productivity even amidst staff absences raw material and component part shortages and public health limitationsAt Starschema we believe data belongs to everyone and should be used for the common good For this reason we have joined forces with Snowflake to make available to the public a gold standard data set that collates data on the COVID-19 outbreak  free of charge This data set was developed with the support of our partners at Tableau DataBlick and Mapbox and is now maintained by Starschemas data engineers and data scientists It is of course available worldwide for public and private use alike under a BSD-3 licenseAs a long-standing partner of various global organizations like the UNs World Food Programme the European Commission and PATH Starschema continues to work with international aid organizations and NGOs to achieve better health outcomes throughout the planet Data analytics companies can play a decisive role in the battle against COVID-19 and Starschema is proud to be part of the solutionOpen data can be used to better understand both disease dynamics and the human landscape in which it takes place: As this difficult situation continues to evolve please know that for the first time in history a unique effort is being mounted to defeat a pandemic Alongside the committed experts of public health services worldwide the doctors and nurses on the front lines of this battle the contingency planning staffs in all affected countries civil servants police and the military there is for the first time a new string in humanitys bow in the battle against COVID-19 By leveraging all the discoveries and innovations of data science data engineering and machine learning we now have a new tool to bring to bear on this challenge And just as firmly as we believe in data we believe that the incredible creative genius of humanity and the right data will see us through this crisis
PTh2jJu3TLiSfdJVr7nxsi,Tableau Server and Desktop logs each and every action you perform The log data is a gold mine for people eager to understand what is happening under the hood and why However there is no easy way to query multiple log files and filter for the relevant information only Tools like LogShark require Windows installation and complex infrastructure and they are not even real-time Using command line magic to parse logs (theyre in JSON format) with tools like jq isnt really convenient as the different entries are distributed across multiple log files So how are professionals are doing this? Let me explainExciting things are stored in Tableaus JSON-based logging format called native logs Desktop Server Prep and its processes dealing with data or visualizations use this format A typical log line looks like: With the tool jq we can quickly visualize this structure in a more human-readable way: As we can see all log entries are JSON objects separated by new lines All entries have ts (timestamp) pid (process id) tid (thread id inside the process) sev (severity) req (request identified) sess (Server vizql session or Desktop session) site (site in Server) user (user in Server) and log keys (k) and (v) values In some circumstances we have ctx for contextual information (like who called a specific service) and a for ART Logging which logs CPU and memory counters These last two examples deserve a separate post so Ill focus on the basic JSON formatFor us keys and values hold the information we need: what type of operation (key) produced what log message (value) The log message itself is also a nested JSON object; different keys provide different value objects in various structuresBoth Tableau Server and Desktop store information in multiple files adding complexity Multiple Desktop instances use multiple log files like logtxt and log_1txtHyper logs its operations to hyper logs and yes to multiple files Complexity increases when we want to see the specific hyper log entries for a specific Desktop or Server session We need to switch between multiple logs quickly without ingesting them in real-time into additional databases like Splunk or LogstashDrill is a Schema-free SQL Query Engine for Hadoop NoSQL and Cloud Storage and SQL engine that allows us to SQL query everything loosely defined JSON structures in particular Its installation is fairly easy on every operating system as it is written in pure java Lets see how can we use it on Tableau Desktop and ServerThe installation is straightforward we just need homebrew to install drill: We just need to start the standalone version: First we need to tell Drill where the logfiles are Drill comes with a configuration UI by default We can graphically set the storage configuration from thereWe should also set two system settings at http://localhost:8047/options as well: Enable union type will allow Drill to dynamically change data type for the v column while disabling number conversions speed things up Finally lets try it: We can also go inside the v JSON object and directly query its attributes Also selecting records from multiple files is easy Here we select the ten longest-running queries using tabprotocolNow comes the fun part I want to see all tabproto logs related to that to one of my bigquery session: Without ingesting anything defining a schema or any hard work we were able to drill from one log file to another by using protocol id and Desktop process id Anyone who has spent time with Tableau log forensics will appreciate this methodThings get serious when performing the same exercise on Tableau Server; more processes more directories and obviously more logsFirst download the Drill package from https://drillapacheorg/download/ and make sure you have JDK installed on the serverThis time instead of using the UI to add a storage configuration we will register our own plugin as tableau in conf/storage-plugins-overrideconfDont forget to set up union and all text mode in conf/drill-overrideconf : We are ready with the configuration After starting the Drill shell we can immediately switch to this new workspace: How about reading some vizql logs? Works like a charmFirst we should find the k value for end bootstrap (boostrap is vizql command for opening a visualization): Please note how easy it is to aggregate values inside logs  again without loading it to any kind of database Now select the top CPU usage across sessions: Well it seems my top workbooks require about 1300ms user CPU time But what are these sessions? Yes you can simply select for this vizql_session_id in the logs and see what exactly happened what were the most CPU intensive steps in rendering: From here we can make assumptions like we have tons of data queries and zones to renderWell this deserves a different post but here is the short version Drill has a distributed service called Drillbit that can run queries across multiple nodes slurping logfiles from all the workers It does not require anything other than drill and zookeeper Usually I use Tableaus own zookeeper as a coordination service for DrillUsing SQL to traverse between logs and parse nested JSONs is just simply cool With tools like Apache Drill or Dremio and small overhead administrators and developers can easily dig into the Tableau Logs  on their Desktop and on their ServersIf you have any questions or comments or interested in Tableau Server forensics or performance optimization just drop a line
Srjoo7RT87DFLEaxGA6gWo,While developing Spark applications one of the most time-consuming parts was optimization In this blog post Ill give some performance tips and (at least for me) not known configuration parameters I could have used when Ive startedSo Im going to cover these topics: OpenCostInBytes (from documentation)  The estimated cost to open a file measured by the number of bytes could be scanned at the same time This is used when putting multiple files into a partition It is better to over-estimated then the partitions with small files will be faster than partitions with bigger files (which is scheduled first) The default value is 4MBI did tests on 1GB folder made up of 12k files 78GB folder from 800files and 18GB made out of 16k files My point was to figure out if my input files are small maybe its better to use a lower than the default valueSo when testing 1GB and 78GB folders  winners for sure were lower values but when testing ~11MB files bigger parameter values performed betterUse openCostInBytes size which is closer to your small file sizesStarting to work with Spark I somehow got the idea that the configuration Im setting up when creating a spark session is immutable Oh boy how was I wrongSo in general shuffle partitions are a static number (200 by default) in spark when you do aggregation or joinsThis neat configuration can be changed in the middle of runtime wherever you want and it will affect steps that are triggered after its set You can use this bad boy when creating a spark session as well This number of partitions is used when shuffling data for joins or aggregations Also getting data frame partition counts: you can estimate the most appropriate shuffle partition count for further joins and aggregationsIe you have one huge data frame and you want to left join some information to it So you get the number of partitions of the big data frame Set the shuffle partition parameter to this valueVery simple scenario: we have a huge table containing all our users and we have one which has internal ones QA and other ones which shouldnt be included The goal is just to leave non-internal onesIt looks like a straightforward and performance-wise good solution If your small table is less than 10MB your small dataset will be broadcasted without any hints If you add hints in your code you might get it to work on bigger datasets but it depends on optimizer behaviorBut lets say its 100 200MB and hints dont force it to be broadcasted So if youre confident that it wont affect the performance of your code (or throw some OOM errors) you can use this and override the default value: In this case it will be broadcasted to all executors and join should work fasterIf youre working with Spark you probably know the repartition method For me coming from SQL background method coalesce had way different meaning! Apparently in spark coalesce on partitions behaves way differently  it moves and combines several partitions together Basically were minimizing data shuffling and movementIf we need to just decrease the number of partitions  we should probably use coalesce and not repartition because it minimizes data movement and doesnt trigger exchange If we want to split our data across partitions more evenly  repartitionBut lets say we have a recurring pattern where we do a join/transformation and we get 200 partitions but we dont need 200 ie 100 or even 1Lets try to compare Were going to read the 11MB file folder and do the same aggregation as beforeBy persisting data frame on storage option only disk we can estimate data frame size So small_df is only 10 MB but the partition count is 200 Wait what? It gives us 50KB per partition on average… Thats not efficient So were going to read big data frame and set partition count after aggregations to be 1 and to force Spark to execute well do count as an action at the endHere are execution plans of our three cases: So all in all what we can see that setting the shuffle partition parameter we dont invoke additional step of Coalesce / Exchange (repartition action) So we can save some execution time by skipping it If wed look at the execution time: Shuffle Partition set finished in 71min Coalesce 81 Repartition 83There are a lot of small and simple tips and tricks on how to make your Apache Spark application run faster and more efficiently Unfortunately with Spark most of the time solution is individual In order to make it work most often youll have to understand whats happening under the hood in Spark internals as well read the documentation from top to bottom several timesIn this post Ive mentioned how to read faster multiple small files how to forcefully suggest broadcast join choose when to use shuffle partition parameter coalesce and repartition
o93TtxePbseg9bZ3vWAqJw,Apache Airflow is an open-source tool for orchestrating complex workflows and data processing pipelines It is a platform to programmatically schedule and monitor workflows for scheduled jobsApache airflow makes your workflow simple well organized and more systematic which can be easily authored and schedules based on the requirementLets start with the basicsWhat do we mean by workflow?Workflow can be your simple calculation creating infrastructure perform some query in the database bash command python script MySQL queries Hive queries etc Workflow is divided into one or more than one task which relates to each other and forms a DAG (Directed Acyclic Graph)What is DAG?In simple terms DAG is a collection of all small task which joins together to perform a big taskStill Confused? Let us take a better exampleLets understand this by Phases of CompilerThis is what happens after compiling a piece of code Our code gets converted into a character stream Lexical Analyzer converts it into tokens then syntax analyzer converts it into a syntax tree then semantic analyzer intermediate code generator code optimization target assembly code Here each step is very crucial and very much dependent on the previous steps  Error in any one of the steps will not compile our code successfullyIf your task is dependent on some other task you can set dependencies based on your requirement We will discuss this briefly in getting started with Apache-airflowSuppose your workflow must be run on every Sunday you can schedule it in such a way that it will only be trigger on Sundays How cool is this? In simple terms you can automate your workflow The best part of automation is you can avoid future human errors  Your DAG can be easily monitored controlled and triggeredWell keeping all such things in mind apache-airflow has given such features like If your workflow gets fail you can set it as to send an Email alert slack notification to the required person/team You can also set it to send an email when the DAG ran successfully   What if your task got successfully run but took more than expected time? (In real case scenario this is a problem)  For this you can set your SLA Suppose your SLA time is 600 seconds But your task took 900 secondsApache-airflow has got quite a few advantages which makes it a better tool then compare to other tools in the market  First well discuss its advantages and then a few benefits of using airflow over other similar toolsI will be using a cloud composer (a GCP based managed services) to create an airflow environment We can also create a local environmentAs soon as you create a cloud composer it creates a bucket in your cloud storage automatically which is eventually mounted with your composer environment Similarly you will have the same directory structure when you will install on your local environmentIn the DAG folder you need to upload all your python script or DAG which will get rendered into the airflow server and show in the UI and then you can trigger it manually or if scheduled it will trigger automaticallyLet us understand this code line by lineThese are the import statements for the facility which we are using in our DAG Since we are using BashOperator we need to import BashOperator from the airflow libraryThese are the default argument which we can set for each task by setting the argument to each tasks constructor We can define a dictionary of default parameters that we can use when creating tasksStart_date as yesterday means start as soon as it loaded into the server We can set it at any timeEmail_on_failure as False if it is true it will send email to the specified person/team if any particular task gets failRetries as 1 mean the number of retries after the task get failsRetry_delay is set as 5 minutes means after any specific task gets fail it should wait exactly 5 minutes to start a retryEmail_on_retry as False if it is true then after a task gets fail after every retry it will send email to the specified person/teamIn DAG everything works as an operatorWe are using Bash Operator in this examplet1 is a value that is calling the BashOperator class and sends all the required arguments to itEvery task has a task_id which uniquely defines a task and other required arguments based on what operator you are usingIn our DAG we are running two different tasks such as t1 and t2 one is creating a directory and the other is deleting the directory So its obvious t1 has to be run before t2 so we have set dependency such as t1 must run before task t2There are two ways to set dependency: In order to do vice-versa the syntax would be like this: Finally we need to place our py file into the DAG folder and then it will get loaded into the server automaticallyYou can see the task graph view as: The arrow denotes that make_directory is dependent on delete_directoryWell this was a very simple example of how we create tasks and run the workflowWhile DAGs describes how to run a workflow Operators determine what actually gets doneAn operator describes a single task in a workflow Operators are usually (but not always) atomic meaning they can stand on their own and dont need to share resources with any other operatorsNote: If two operators need to share information like a filename or small amount of data you should consider combining them into a single operator If it absolutely cant be avoided Airflow does have a feature for operator cross-communication called XComAirflow provides many operators some of them includes: Sensors are a special type of operator which runs behind the scene all the time The sensor class is created by extending BaseSensorOperatorIt has a poke method which executes the task over and over after every poke_interval seconds until it returns True and if it returns False it will be called againExample: Sensor to check whether a file is present in a specified directory After every poke_interval the poke method of the sensor class will be executed if the file in not present it will send False once the file is present in the directory it will return TrueAll operators have a trigger_rule argument that defines the rule by which the generated task gets triggered The default value for trigger_rule is all_success and can be defined as trigger this task when all directly upstream tasks have succeededCommunication among two operators If any operator returns some value it gets store in xcom airflow provides a mechanism to pull xcom value using xcom_pull() and use it in some other operation and also to push value using xcom-push()Example: View code in GitHub Xcom_exampleLets end this article by listing other alternative tools in the market
9Hpd8y7M2e3YWAmgBVuuQ5,Off late ACID compliance on Hadoop like system-based Data Lake has gained a lot of traction and Databricks Delta Lake and Ubers Hudi have been the major contributors and competitors As both solve a major problem by providing the different flavors of abstraction on parquet file format; its very hard to pick one as a better choice over the other In this blog we are going to understand using a very basic example of how these tools work under the hood We will leave for the readers to take the functionalities as pros/consWe would follow a reverse approach as in the next article in this series we will discuss the importance of a Hadoop like Data Lake and why the need for systems like Delta/Hudi arose in the first place and how Data Engineers used to do build siloed and error-prone ACID systems for LakesEnvironment Setup Source Database : AWS RDS MySQLCDC Tool : AWS DMSHudi Setup : AWS EMR 5290Delta Setup : Databricks Runtime 6Data Preparation Steps: Now lets load this data to a location in S3 using DMS and lets identify the location with a folder name full_load For the sake of adhering to the title; we are going to skip the DMS setup and configuration Heres the screenshot from S3 after full loadNow lets perform some Insert/Update/Delete operations in the MySQL tableLets again skip the DMS magic and have the CDC data loaded as below to S3NOTE: DMS populates an extra field named Op standing for Operation and has values I/U/D respectively for inserted updated and deleted records The below screenshot shows the content of the CDC Data only The screenshot is from a Databricks notebook just for convenience and not a mandateNow lets begin with the real game; while DMS is continuously doing its job in shipping the CDC events to S3 for both Hudi and Delta Lake this S3 becomes the data source instead of MySQL
Y85epRF7FpR8Lcz2jdXKaJ,Five years back when I started working on enterprise big data platforms the prevalent data lake architecture was to go with a single public cloud provider or on-prem platform Quickly these data lakes grew into several terabytes to petabytes of structured and unstructured data(only 1% of unstructured data is analyzed or used at all) On-prem data lakes hit capacity issues while single cloud implementations risked so-called vendor lockinToday Hybrid Multi-Cloud architectures that use two or more public cloud providers are the preferred strategy 81% of public cloud users reported using two or more cloud providersCloud providers offer various tools and services to move data to the cloud as well as to transform the data Here in this article we will create a cloud-native data pipeline using Apache SparkThe use case we are going to build here is: We will create a Spark ETL Job on Google Cloud Dataproc which will load ECDC Covid19 data from Azure Blob storage transform it and then load it to 3 different cloud stores: Amazon Redshift Azure SQL and Google BigQueryHere we automate cloud infrastructure provisoining using Infrastructure as Code(IaC)with Terraform  IaC allows you to easily spin up and shutdown clusters this way you only run the cluster when you use itThis use case can be built and run on AWS/Azure/GCP resources which qualify under free tier Sign up for Azure/ GCP and AWS public cloud services free credits Follow the links below to create each of themGCP Free Tier  Free Extended Trials and Always Free | Google CloudGet hands-on experience with popular products including Compute Engine and Cloud Storage up to monthly limits These…cloudgoogleOnce you signed up and logged in to the GCP console Activate Cloud Shell by clicking on the icon highlighted in the below screenshot Cloud Shell provides command-line access to a virtual machine instance and this is where we are going to set up our use caseaRun the following command in the cloud shell terminal: Create and enable project: Create a terraform service account: Grant terraform service account permission to view and manage cloud storage: Enable Dataproc API service: bCreate your Azure free account today | Microsoft AzureTest and deploy enterprise apps Use Azure Virtual Machines managed disks and SQL databases while providing high…azuremicrosoftRegister terraform with Azure AD and create a service principal * : a Create Application and Service Principal: Navigate to the Azure Active Directory overview within the Azure Portal  then select the App Registrations blade Click the New registration button at the top to add a new Application within Azure Active Directory On this page set the following values then press Create: bNow that the Azure Active Directory Application exists we can create a Client Secret that can be used for authentication  to do this select Certificates & secrets This screen displays the Certificates and Client Secrets (ie passwords) which are associated with this Azure Active Directory ApplicationClick the New client secret button then enter a short description choose an expiry period and click Add Once the Client Secret has been generated it will be displayed on screen  the secret is only displayed once so be sure to copy it now (otherwise you will need to regenerate a new one) This is the client_secretyou will needAssign a role to the applicationTo access resources in your subscription you must assign a role to the application Here we will assign a role at the subscription scopea)b)c) Select Access control (IAM)d) Select Add role assignmente) Select Save to finish assigning the role You see your application in the list of users with a role for that scopeYour service principal is set upOn the Azure SQL Home page you will see the newly created SQL server and databaseFollow the AWS documentation to create a user with Programmatic access and Administrator permissions by attaching the AdministratorAccess policy directlyCreate a user DataflowTerraformApp and attach the existing policies directly: When you create the user you will receive an Access Key ID and a secret access keyCreate Amazon Redshift cluster and database: Verify the Redshift cluster on your AWS console: Finally we are done with the infrastructure setup We have created the required resources on GCP AWS and AzureThe next step is to set up the Spark JobDownload and build the spark ETL frameworkhttps://githubcom/ksree/dataflowabYou will see the two newly created tables: dbocasesInCanada and dbocasesInUs • AWS Redshift: Login to Amazon console Redshift Query editor to view the 2 new tables generatedNow that we are done its time to terminate all the cloud resources that we createdHere we created a hybrid cloud infrastructure and used Apache Spark to read process and write real-time Covid19 dataset into three different cloud storage locationsData Sink/Destination: We wrote to 3 different cloud storage
VVY843YzZseokieG7ouYt4,In a black hole no one can see you disappearIn the past month I have written about basic SQL queries and used examples from astronomy Here are the posts: For a final tutorial on basic SQL queries I will cover CASEstatements This is like an if/then statement in other languages in that it determines a result from conditionals Keep this in the back of your mind while I re-introduce you to the SQL table that I will use in this post (if youre curious this table of information was used in Black holes planets and SQLThe table black_holes has characteristics of black holes specifically the name (which for most entries hints at the location of the object) and massTo read the mass in this table use scientific notation: base_mass x 10^power The unit of mass for these black holes is in solar masses or the mass of the Sun For example the Sombrero galaxy is 1x10⁹ times the mass of the SunWe can see that from the power column the mass of the last black hole is very much less than the other three Using a WHEREclause  something covered in Black holes planets and SQL  we can verify thisResult: Indeed the result is the name of three massive black holes in the table specifically those with power larger than 0 The 4th black hole Cygnus X-1 should appear by itself if we update the querys WHERE clauseResult: And we see that there is only one black hole where the power is equal to zeroLets take this one step further Suppose we need to create a SQL table view that classifies each entry For this task CASE statements are usefulRemember I said thatCASE acts on conditionals? Here I am creating a conditional that tests the value ofpower(This is in line with the definitions of a supermassive black hole  found at the center of galaxies  and a stellar-mass black hole which is often found orbiting another star) By the way these classifications are listed in a new column bh_descriptionHere is the result of the previous query: This CASE example wraps up my introduction to SQL
B8uvbkJH5f7GNTuJD4qi8A,"When your system starts to grow and split into multiple services you will presumably need a way to send messages between those services for processing In a clean and nice architecture each service should have a bounded context that defines its space of responsibility and other requirements outside of that context should be handled by other components including messages delivery itself so the service will only emit a message (fire-and-forget) and other services that are interested in processing those messages will catch themA message queue (publish-subscribe-based) will be a good fit here And in my opinion Kafka should not be the default go-to choice when you think of a pub-sub because of its complexity and there are other solutions already that are designed to simplify the development and infrastructure work with reasonable trade-offs such as GCP PubSub AWS Kinesis or NATS however if none of them met your requirements and you want to deploy Kafka then there are a few things you need to take care ofIn this blog post We will build a simple and powerful Scala client in a functional style that solves some of hidden Kafka client challenges and potentially simplifies testing error handling and other aspects as you will see later in the companion projectKafka  is a publish-subscribe based durable messaging system exchanging data between processes applications and serversKafka consists of two sides: A producer that produces messages to a topic and a consumer that subscribes to a topic and consumes messages from that topic Multiple consumers can subscribe to the same topic and poll all messages from it as long as they belong to different consumer groups Each consumer group has an offset that points at the position in the topic where it consumed untilThis is not meant to be a full comparison so its enough to list some available clients with a couple of words about each of themLets first start with the vanilla Java-based official client and see how we will enhance it with Cats and Cats Effect to make it more compatible with Scala and functional programmingCats  is a library that provides type classes and data structures for functional programming in Scala like Monad Semigroup Monoid ApplicativeCats Effect  is another library based on cats that mainly provides the IO monad which describes an operation that performs a side effect and the result of the effect can be returned synchronously or asynchronouslyWith Cats and Cats Effect you can define all the logic of your program without running anything (including operations that have side effects) and separately run the program (or part of it) synchronously or asynchronously This will help us to improve testability and maintainability and help understand potential sources of errors""As you saw above the consumer is wrapped in while(true) block which runs in a single thread You need to handle messages in multiple threads for better utilization but if you try to call kafkaConsumerpoll in a different thread other than the one where it was defined you will get an exception! To solve this we will keep calling kafkaConsumerpoll in the same thread and we will use another thread pool for handling messages Looks complicated isnt it? Let's see how this is easy with cats""Cats provides a class called ContextShift as the pure wrapper of Scala's ExecutionContextf1 will run on the global execution context and before running cs will switch to anotherExecutionContext and run f2 on it and then switch back to global execution context and run f3Now we will define a thread pool with a fixed number of 1 thread to initialize kafkaConsumer and call poll and close and we will use ContextShift to switch between this singular thread pool and the other thread pool used for handling messages which can be of any kindKafkaContextNow we can create the Kafka consumer in the Kafka thread that we just defined in KafkaContext You can do this simply by creating a new instance of KafkaConsumer and wrap it in KafkaContext""Since we need to close the Kafka consumer after we dont need it anymore its more suitable to define it as a closable resource and Cats provide a resource abstraction for that called Resource used for acquiring and releasing resources using Resource"
QqPn6nTLkabbqghpyhFkFj,"Last December I got an interesting bug report from the VWO support team It seemed like the loading time of a particular analytics report was ridiculously slow for a big Enterprise customer Since I am part of the Data platform a flag was immediately raised and I got involved in debugging the issueTo provide a bit of context Ill explain the basics of VWO Its a platform through which people can run targeted campaigns of various types on their websites: perform A/B experiments track visitors and conversions do funnel analysis render heatmaps and play visitor recordingsThe real power however lies in the reporting of the platform All of the above mentioned features are connected together And for Enterprise customers ingesting all this vast data would be useless without providing a powerful platform that can deliver those insightsUsing the platform you can make arbitrarily complex queries on large data sets Here is a simplistic example: Notice the boolean operators Those are provided in the query interface for customers so they can make arbitrarily complex queries to slice and dice the dataThe customer in question was trying to do something which should be reasonably fast if you think about it intuitively: This was a site that got huge amount of traffic and we had more than a million unique URLs stored for them alone And they wanted to search for a fairly common URL pattern that relates to their business modelLets dive into what was happening in the database The following was the original slow SQL query: Here are the timings: It ran on around 150k rows The query planner showed some interesting details but no immediate bottlenecks jumped outLets examine the query a bit further As it can be seen the query does a JOIN on three tables: Also notice that we already have all of our tables partitioned by account_id So there is no possible scenario of one large account overwhelming other accounts and causing slowdownUpon close inspection we see that there is something different in this particular query The following line needs to be examined: Initially I thought that perhaps the ILIKE operation on all those long URLs  we have more than 14 million unique URLs captured for this account  might be causing the slowdownThe pattern matching query itself is taking only 5 seconds independently Matching millions of unique URLs is clearly not a problem""The next suspect on the list is the multiple JOIN statements Perhaps the excessive JOIN s have contributed to the slowdown? They are usually the most obvious contender when it comes to query slowdown but I've not really believed that to be the typical caseAnd it wasnt the case The JOINs are quite fastI was ready to start tweaking the query to achieve any possible perf gains possible My team and I brainstormed 2 tactics to try out: Yep The subquery when wrapped with EXISTS makes the entire thing super-fastBut this was still extremely slowThere was this one little thing that Id been looking at and dismissing all along Since nothing else was working I decided to take a look at it anyway It was the && operator While EXISTS was the performance booster that made things faster it seemed like the && was the only remaining common factor in all versions of the slow query"
KkQKYcVG2inUKQ5Yykmoxv,In my last post I went over how we went about implementing a fitness program at Pandera the need for a custom solution to handle a little friendly competition and an architecture that would support thatIn this post I want to focus on what a Data Vault model is and what it looks like in BigQuery I am covering this first instead of the data pipeline as it really shapes the way I handle the messages and events that come inFirst I want to start off by saying Data Vault is extremely overkill for this scenario It is really good for when you have high data velocity auditing and traceability requirements We come across a lot of these scenarios especially in financial services healthcare and other regulated industries which is why I wanted to get a better understanding of some of the gotchas during implementationThe main concepts in Data Vault are Hub Satellite and Link tables Hubs primarily store business keys and a hash of the business keys Links store a relationship between hubs by capturing the associated business keys their hashes and a new hash of the business keys from both hubs And Satellites store additional attribution about hubs and satellites This is a basic explanation of Data Vault But what this setup allows us to do is load data without much in the way of dependencies so tables can be loaded in parallel and not have to worry about much other than getting the data loaded So for the purposes of the Strava data we had two entities the athlete and the activity This is a fairly simple relationship but it will demonstrate some of the things I learned implementing a Data VaultLesson 1  The number of tables involved For those two entities alone I needed five tables: In most scenarios this could be as simple as one table or three Since the scope of this implementation is very small the additional tables are not very noticeable but on a large scale implementation it could be a hassle This is typically why automation takes a key part in data vault implementations and being that most of the patterns of these base tables repeat (hashing of business keys hashing of attributes inclusion of meta columns) this can be programmed with pretty low effort or by using a COTS product But for the time being I will be doing this manuallyFirst though I need to model those tablesI had to rework this model a few times in developing it mostly because I worked on this project between 4:45 am and 6:30 am Which brings me to: Lesson 2  ironing out the data model pays dividends in the long run To do this you really need to analyze the data ahead of time and make sure you have a good understanding of relationships uniqueness and the attribution Once this is all done though you are in a good place because one of the principles of data vault is to get the whole dataset into the data vault What this does is allows me not to have to reengineer a pipeline because I now need an extra field Get it all because the addition of a field or ten during this phase is very low but if you have to go back and add it becomes a painWith the model set up youll notice that each table has a few common columns primarily the _seq and _load_date columns These assist in a few different ways: When I look at the _seq columns these are my hashes of the business key(s) that identify a unique instance of something I used an MD5 hash because it is readily available in python and BigQueryBecause of the above data vault features my pipeline into BigQuery can be very simple I can focus on only inserting records and I have a clear method of distinguishing duplicates and determining my latest version This is where the benefits of Data Vaults speed in loading comes from No need to look up to see if a record exists already or search for an existing surrogate key just insert and goThis does leave me with a bit of a gap and: Lesson 3  Data Vaults are not suitable for a reporting layer Which means…more tables or in my case views I need to build a data mart on top of the Data Vault to service my visualization toolYou can see that I have two datasets The first being my data vault and the second being my information vault to serve as a reporting layer This is a very important aspect as it pertains to BigQuery and data security Today you are unable to secure individual tables or data elements security is all at the dataset level This is not a huge issue because you can simply create datasets to serve various security requirementsSo my view layer consists of one singular row level table fact_activity where I have applied some logic: This will serve as a denormalized springboard for my other aggregate views each of which serves a specific purpose of calculating activities and time spent for each week or at an overall level additionally theres a stats table where I collect some of the ancillary metrics like miles travelled calories burned etcAs a couple of quick notes on the BigQuery implementation I am not doing any partition or clustering in this instance The data is small enough to where I will not get any real performance or cost savings by implementing itWith all the structures put into place I can start building out the actual pipeline which I will cover in my next postAs a whole I really like the Data Vault method Just like other methods there are clear guidelines that can make it successful And it is important to really understand the overhead involved in that success However given the resources a platform like Google Cloud provides having the ability to have massive parallel processing out of our data pipelines is becoming simpler with a method like Data Vault
XumDP3JyCjndpYCMHXepUH,This is a memo to share what I have learnt in Apache Airflow capturing the learning objectives as well as my personal notes The course is taught by Mike Metzger from DataCampA data engineers job includes writing scripts adding complex CRON tasks and trying various ways to meet an ever-changing set of requirements to deliver data on schedule Airflow can do all these while adding scheduling error handling and reportingI have learnt the following topics: My next steps would be: More notes and codes can be found on my GitHub
nFogNkFLq2Br3LrmLQxpFr,There are a lot of ways to move data from database to database using Amazon Redshift but one of the most efficient ones is the use of COPY and UNLOAD commands these commands allow you to move data between databases almost seamlessly This tutorial will show you the steps to move tables from one Amazon Redshift schema to anotherTo be able to use the UNLOAD and COPY commands effectively we need to make use of the Amazon S3 service create a S3 folder and have an IAM role with permissions to access Amazon S3 The S3 folder is going to be used as a bridge between the two Amazon Redshift databasesIf you dont have permissions to create an IAM role to access Amazon S3 try to talk with the infrastructure or DevOps team of your organization so they can create it for youFor unloading the tables you need to migrate it is convenient to run a couple of queries before hand in your source database to make sure you are unloading the right data also keep in mind what fields your query returns so you can use them on the COPY commandAn interesting advantage of the the UNLOAD command is that you can use a query instead of selecting a specific table to be loaded on S3 this has several benefits like the use of UNION statements and JOINS to different tables this is why the UNLOAD command can be used pretty much like an ETL tool and can be very powerful on automated environmentsOne of the best ways to load tables from Amazon Redshift to Amazon S3 is the use of the UNLOAD command The UNLOAD command uses a SQL query a S3 path and an IAM role with permissions to access Amazon S3 to load the result of a query into a S3 folder Here is an example of how the command looks like: The allowoverwrite parameter help us to overwrite the files that we create every time we use the command on the same S3 folder this is useful for certain ETL processes where you need to clean and re-create your data The format as csv part forces the unload command to generate files with comma separated values instead of the default format that is separated with pipes (|)  Also make sure that the S3 path in the command finishes with a slash (/) this is to avoid unloading the files on the parent folderOnce the UNLOAD command is executed in your source database you can check the unloaded files on the folder you specified before usually the UNLOAD command creates several partitions (files) of your data and doesnt provide the csv suffix to themTo be able to copy data from Amazon S3 to Amazon Redshift we need to have a schema and a table created on our destination database we have to make sure that the structure of this new table (data types and column names) is the same as the table we unloaded the data fromTo create the new table on the destination database we can make use of a simple CREATE TABLE statement like this: If you are using a database administration tool like DBeaver you can generate the CREATE TABLE statement from the source table by right clicking the table select Generate SQL and then select DDL this would show you a dialog with the CREATE TABLE statement on it you can copy it and execute it on the destination database to create the tableThe COPY command allows you to move from many Big Data File Formats to Amazon Redshift in a short period of time this is a useful tool for any ETL process We are going to use this COPY command to copy the data we loaded previously with the UNLOAD command moving the data we have on our Amazon S3 folder to our destination databaseOnce your destination table is already created you can execute the COPY command this command uses the schema following the name of your table the fields you want to copy the path to your S3 folder the IAM role with access to Amazon S3 and the format of the files you are copying from (CSV on our case) The COPY command should look like this: Once the COPY command is executed the data that you are migrating from the source database should appear on the new table try to verify the data using a simple query (select * from your_schemayour_table) just to make sure that all the data is there If any of the commands is failing or generating permission errors it is very likely that the IAM role that you are using doesnt have permissions to access Amazon S3 filesIt is not always evident what tools should we use to migrate data from database to database when we are working with Amazon Web Services we can get entangled on the variety of different tools and services that Amazon provides making migrations more complicated than they need to be that is why we always need to strive for simplicity when we are looking for a good solution (Occams razor the KISS principle)
HosWxyw7JvDHw7WWJhKNaY,Every data engineer especially in the big data environment needs to deal at some point with a changing schema Sometimes your data will start arriving with new fields or even worse with different data types The challenges that one will face with schema changes also heavily depend on the file format of the data In this note we will take a look at some concepts that may not be obvious in Spark SQL and may lead to several pitfalls especially in the case of the json file formatAll the code and results in this note are produced using the Python API in the current stable version of Spark which is 245 (written in April 2020) and it is checked against 300-preview2 and 310-SNAPSHOT to see whether the situation changes in Spark 30In Spark SQL when you create a DataFrame it always has a schema and there are three basic options how the schema is made depending on how you read the data It is either provided by you or it is inferred from the data by Spark or it is picked up from the metastore (in case of a table) In this note we will talk mostly about the first two optionsFor the sake of simplicity we will consider a basic example in which we have two json files and the second one will arrive with a changed schema Imagine a situation in which some system is exporting the data to a folder and your task is to read the data process it and any possible change of the schema of the raw data is not under your control The first file contains the following two records: The second file has also two records but notice that the second column has a different data type: The change in the data type that happened in the second file is that the score column is now exported as floating point number but it is unexpected to you because you were told that this field would always be long Assume now that both of these files are in the same folder and we want to read it into a DataFrame Because you were told that both fields user_id and score are long types you will provide this schema for reading the data: Spark SQL provides an option mode to deal with these situations of inconsistent schemas The option can take three different values: PERMISSIVE DROPMALFORMED and FAILFAST where the first one is the default Let us first take a look at what happens in the default mode: The DataFrame now represents data with inconsistent schema Calling count shows the correct number of records however when looking at the data we will see that two records contain null values and these null values are actually in all fields not just in the score column that has the incorrect data type This is something unexpected and it is actually improved in Spark 30 where only the score column will have null values and the other columns will be unaffected Also as you can see from the code snippet when you select only user_id column and ask how many rows are not null it returns 4 which is in contradiction to what you see after calling show and this is because if you select only the user_id column Spark will not notice the problem which is in the score column and will act as if there was no problem at all This confusing situation will be avoided in Spark 30 because here the show will return the following output: Even more confusing situation happens with the second mode option DROPMALFORMED: According to the official documentation in this case Spark should ignore the whole corrupted records This is what really happens if we call show and as you can see in our example only two records will be collected but if we call count on the DataFrame we get again 4 So here the count may lead to big confusions since it does not give you the same number as if you actually collect the data first and count the collected records Also when you select only user_id column and collect the data you will get different number of rows as if you select only score and collect itThe last mode option is FAILFAST: According to the documentation it should throw an exception when it meets corrupted records and this is indeed happening After calling show the query is going to fail and notifies you that there are records that do not match the provided schema The count is however still going to return the total number of rows in the data because to count the rows Spark simply doesnt have to parse the actual records so it doesnt find out that the schema is inconsistentWhen summarizing these three mode options you can see that the default mode leads to situations in which you will silently load null values and pass them downstream not even knowing that there is something wrong Before Spark 30 you will even loose data in all columns even though the problem might be in just a single field With the mode DROPMALFORMED you will not pass the nulls downstream but you may be confused by counts that do not correspond to the number of rows that you are actually passing The last option FAILFAST seems to be the most protective it doesnt let you pass nulls and at the same time it actually notifies you that there was a change in data types by failing the querySo far we talked about situations in which a data type is changed but what if all data types are correct but instead some files will contain records with a completely new field? In that case if this field is not present in the schema that you are providing Spark will simply omit this new field and the query will continue without any problems however you will not find out that there is a new structure in the dataAnother option how to deal with evolving schemas is to avoid providing the schema for the DataFrame creation but instead let Spark do the inference After Spark infers the schema you may apply some custom logic and compare the inferred schema with a schema that you expect If the schemas are the same the job can continue but if there are some inconsistencies the job can take some other code path depending on your predefined rulesWhen implementing the logic for the schema comparison one also has to face some challenges so lets briefly list couple of them: As we can see dealing with schema evolution of json file format in Spark SQL brings several challenges Using the mode option provided by the native api may not be sufficient in many applications and can lead to bugs and confusions if not understood properly Custom schema evolution might be often necessary to handle the changes in the schema on a more advanced level
XsyQu8WtkU5MVSQ5dEcAq3,Wait what? Isnt OWASP something to do with web applications? That was my first reaction too when some years ago I was asked to do an OWASP top 10 analysis on a project that had nothing at all to do with web applicationsBefore delving into that question lets take the first of several detours and think a bit about what a web application is If we take a sufficiently distant view we can hand wave and say a web application takes data from end users through a web browser transforms it somehow and updates a data store Or it goes the other way pulling data from a store transforming it and passing it back to the user For example the user uploads a 9Mb picture of a cat the web app creates a tiny thumbnail and pushes both into some storageHang on That sounds an awful lot like the daily activity of a lot of data engineering code Take some data from here do this to it and shove the result over thereThe role of data engineers is often a lot more broad than other application developers Our cat-munging application developer doesnt have to worry too much about the CSS on the web site or exactly where the data store is They just get the bytes do the thing and call the APIData engineers wear a lot more hats Our responsibilities and activities extend out in all sorts of directions  building platforms on premise or in the cloud; setting up and managing databases; working on architecture; and coordinating the exchange of data across organisational or enterprise boundaries And write code in the time thats left over Thats a lot of hatsBecause as IT professionals we should always put security in first placeIts particularly important for data engineers to be concerned about security as we are especially likely to be be working with sensitive and critical information We can goof up rendering a picture of a cat but we cannot allow the accidental disclosure of a few million credit cardsSo lets start off with a quick introduction to OWASP In their own words: The Open Web Application Security Project (OWASP) is a nonprofit foundation that works to improve the security of software Through community-led open source software projects hundreds of local chapters worldwide tens of thousands of members and leading educational and training conferences the OWASP Foundation is the source for developers and technologists to secure the webThe OWASP project was founded in 2001 and in 2003 they published their first top 10 report Based on considerable research of actual attacks breaches and vulnerabilities this listed the top 10 root causes of most security flaws in web applicationsThis research has been repeated every few years  I believe the next report is due out in late 2020  and rather depressingly the list hasnt changed much between reports Most flaws they found in 2003 are still around and are now old enough to voteTheres more to OWASP than the Top 10 report though As well as their very mature and well respected vulnerability research they curate a collection of open source tools for security mainly focussed on testing and validating code security In addition they have built an interesting suite of security guidelines and standards informed by the direct experience of security practitioners based on real world security breaches and remediation These are somewhat more digestible for coders than materials from (eg) NIST or ISO and as such can be a really helpful basis for your own local security standards and guidelines Finally they perform quite a bit of community led training and educationTo start delving into the project more deeply Id suggest you start here: The current Top 10 was published in 2017 and research is being done at the moment with a hope to releasing the next report in late 2020 As I mentioned the depressing thing about this list is that it has remained virtually unchanged since 2003 If you take away only one thing let it be dont keep making the same security mistakes for 17 yearsA caveat about this list: dont think you can plug holes based on the OWASP framework and walk away It still remains the case that most security breaches are due to phishing attacks insider leaks bad access control to corporate environments and simple ham-fisted human errorHow could we use OWASP in data engineeringLets look at each of the top 10 and see how we might apply it or what we might learn from itInjection attacks remain the classic flaw By this we mean where malicious content is passed into a system through its interface where it can cause damageThis is the classic drop tables SQL attack but its also things like data that causes a crash in rendering software  there are current bugs in both iOS mail and the iOS Twitter app where particular sequences of unicode characters can crash the app or the phoneThis is directly applicable for data engineers Dont assume incoming data is safe and dont assume that data is safe because it comes from some internal sourceOne of the biggest reasons injection attacks work is that writing interpreters is really hard By interpreters I mean something like a web browser where the behaviour of the program is driven by the data it receives In the data engineering space we really really dont want the stream of data going through our code to define the behaviour of our code in unpredictable ways Dont build interpreters unless you have no choice and then dont build interpretersThe second most common failure is broken authentication  where some sort of principal authentication is in place but its flawedThe main suggestion I have is that if you have in place any sort of authentication test it test it and test it again  or better yet get someone else to test it for you Penetration testers are our friendsDont rely on the security of your network or corporate infrastructure and leave databases or services wide open because they are inside the perimeter Attacks by bad guys on your databases will probably come from a web server being compromised and then a pivot inside your networkFinally dont assume that your data sources or sinks are not being spoofed  use service-to-service authentication ideally something like mutual TLS if you are building both ends of the pipelineThink of this as like a government minister leaving their brief case on a trainIf the bad guys are inside your perimeter you are the first and last line of defence Data engineers are in the particular position of being able to control or at least influence whether data is exposedLimiting data visibility is a non-trivial problem for us though If we are working to put data in front of data scientists and analysts we are daily faced with the quandary of giving them enough data to do their work while limiting the amount of data that can potentially be leaked There are no hard-and-fast rules to be had here every situation is going to be different and is going to be influenced by things like GDPRThe best bet is to always apply the principles of least-privileges and zero trust Dont let anyone or anything access the data unless they can prove their identity to you and provide the minimum possible access to the dataDo use encryption wherever you can The compute cost of encryption is so trivial that your watch can do it now so theres not much excuse for not encrypting data at restFinally have a good understanding of your data flows  where does the data come from and more importantly where does it go? Put measures in place to make sure that data only goes where you expect it to goIf you want a good scare google for the billion laughs and xml bomb attacks (see https://cwemitreorg/data/definitions/1030html) If you are accepting XML into your data processing and dont know about these you shouldThis is a special case of an injection attack Its trivially easy to make XML interpreters explode in horrible ways either by making them recursively expand XML entities or by causing them to try to pull gigabytes of data in from the open internetFirst rule of thumb is: dont build or use an XML interpreter if you can possibly avoid it If you must dont pull in external references without being able to validate what they are and only use local static DTD documentsOr just deprecate the use of XMLA fence with a hole in it is not much useFor data engineers this is more or less what I said earlier Do use access controls on your data stores do test the access controls and do get somebody else to test them as wellMany many cases of data breaches in the wild came about because once the bad guys were inside the security perimeter there was no protection of the data storesThis is the main place we see stupid ham-fisted dumb human errors where security constraints are put in place but are misconfigured It takes particularly special effort now to create an AWS S3 bucket and make it open to the internet with critical data in it but somehow people still manage itAgain test and test and test your security configuration particularly in the cloudInfrastructure-as-code and automated configuration can be a big help here particularly in the cloud  you get the opportunity for all configuration changes to be reviewed before they are applied and it becomes harder to do something dumbIf youre going to let some giant complex thing inside your gates you probably should find out whats inside itXSS attacks are where a piece of data is stored on a service and then when used later redirects the user to a Bad Place or causes the service to blow up This is a particular favourite of crypto currency miners who want to use your CPU after you browse somewhere thats not choosy about where it gets its advertisements from Historically it also featured prominently in attempts to steal user credentialsStoring URLs provided from external sources is dangerous if there is any chance that they can be used downstream in a web browser or other interpreterBe particularly wary of storing any kind of interpreted script (eg chunks of Javascript or pickled Python) from some external source Who knows what it might beThe best known recent case of this is the 2017 Equifax hack which famously exploited an unpatched Apache Struts vulnerability as route into their serversThe risk comes where we have data  like a pickled Python object or a serialised Java object that we receive and execute Who knows what it will doWe can do all sorts of clever things involving controlling data flows via the data that is flowing but probably shouldntAlways assume that data you need to unpack in some fashion  unzipping unpickling deserialising  could be malicious content Always validate what you have before you attempt to execute itDont do thisAgain the Equifax Struts vulnerability is the canonical example of why you patch your systems The Apache Struts project discovered a serious security vulnerability fixed it and told everyone they needed to update So of course the bad guys knew that unpatched systems were vulnerable Equifax knew they needed to patch and just… didntThe single best way to immediately improve the security of your platforms and tools is to update them with the most recent security fixesPatch YourYou cant know whats going on if you dont monitor whats going onIn this case OWASP is particularly talking about run time system failures  you cannot know something is broken or if Bad Guys are doing something bad if you dont have the right monitoring and logging availableAs far as data engineering goes we can take a different approach to thinking about logging and monitoringFor a start its really useful to keep track of data provenance in our data flows By this I mean being able to identify where some piece of data came from when and how it was processed and where it wentFinally if we can detect bad data as it flows past dont let it go past into downstream consumersSo thats the top 10 I think we can be agreed that with a little bit of stretching all of the current top 10 vulnerabilities are things that affect data engineering practicesTheres more to OWASP than the top 10 though and I encourage you to explore their resourcesTheir frameworks guidelines and recommendations are a very solid foundation for your own guidelines and practices and they have the advantage of being open sourced and easily re-usableDo have a look at their open source security tool kits as wellYou can think about the first two groups as looking at your code or application from the outside and static code analysis as looking at it from the insideI particularly recommend using static code analysis in your CI/CD pipelines Not only should your build fail if theres unit and integration test errors it should fail if security flaws have been introduced Using these tools regularly and diligently can really improve your code quality and reliability because a lot of security flaws arise from poor-quality codeYou must be diligent though The tools are only useful if you use them and fix the problems they findI cant emphasise this enough If you do put in place automated security scans and code quality checks you have to take them seriously Dont treat warnings about potential security flaws as something that can be ignored  treat them as bugs that need to be fixed The OWASP security frameworks and recommendations can help you here by letting you put in place code quality standards that promote security and educating yourself and your engineers on ways to code more securelyWe can agree that the OWASP 10 is a useful framework for thinking about the most frequent types of vulnerabilities even if we had to stretch them a little to make them apply to data engineeringData engineers have a crucial and critical role to play in security The OWASP project is a source of top quality tools informations and guidelines to help us
JJEbYf3sQ67CuW9UXE2vfL,"Apache Flink is an open-source distributed system platform that performs data processing in stream and batch modes Being a distributed system Flink provides fault tolerance for the data streamsTo make our platform fault tolerance we want to preserve the systems state from time to time Achieving fault tolerance means being capable of not losing any data when a node or more of the network goes downFlink has three options for a stateful backend and one of them includes RocksDB However only RocksDB can provide incremental checkpoints in Flink In this blog post we will go through a standard example of implementing Flink with RocksDB backend to have incremental checkpointsClick here to head over to the official documentation to install FlinkIn Mac open the configuration file with the following path: /usr/local/Cellar/apache-flink/1101/libexec/conf/flink-confIn case you extracted the binary then seek the relative path in the extracted folder: The version 1101 might be different in your system so take care of thatIn this file make two changes: Search for statebackend and set it to: stateSearch for statecheckpointsdir and set it to your desired path""hdfs won't work unless you install the Hadoop dependency for Flink So keeping it minimal I decided to move forward with the normal filesystemWe will be running the Fraud-detection exampleRun the following command to create the above projectWe need to be able to run the above example in our IDE with breakpoints and set RocksDB as our backend Following is my pomxml: After setting the above dependencies we need to build the jar file so that we can submit our job to Flinkmvn package This command should be executed in the frauddetection directoryOnce done we will have the jar file in the target folder/usr/local/Cellar/apache-flink/1101/libexec/bin/start-clusterThis is how the dashboard looks: We can also take a look at the path mentioned in the above image to check the persistence that our application is having with RocksDB as the backendIn this blog post we used RocksDB for stateful streaming in Flink We began with installing Flink configured it for using RocksDB as the state backend and for having incremental checkpoints We then modified our code to create checkpoints every five seconds built the jar file and observed the checkpoints created by Flink In the next blog of this series I will publish the results of profiling this code"
B5eHYp4bKPpSXTXxsS8tM4,8 things learnt from a university graduateRecent graduates have a new dilemma of what style of work they want to do early in their careerLooking back a few decades before the dot-com boom and the generation of your parents a large majority of individuals focused on getting good grades at school with the intent of joining an established company that offered a fair salary and a stable lifestyleNow career paths such as working for a start-up freelancer social media influencer and starting your own business have become increasingly popular All these career options share common principles and go through similar processesI hope to provide you with some insight into what its like working at an early-stage start-up and share some of these principlesThis summer I had the opportunity to intern in Tokyo at an exciting new start-up called Zenu The companys mission is to transform the customer experience in restaurants using technologyZenu provides a service where customers can view the menu order and pay for dishes all on their mobile phone by scanning a unique QR code associated with the restaurant they are dining in The menu can be viewed in multiple languages has a selection of photos and descriptions and ingredient tags to cater to individuals with allergies and specific dietary requirementsRestaurant owners have the capability of creating a flexible menu making it easy to add edit and update dishes as well as set up dynamic pricing They will gain access to a previously untapped source of data (the static paper menu) that will help them reduce operational costs manage inventory and gain insights into customer behaviour giving them a competitive edge in a crowded marketHere are 8 things I learntWith only three people in the founding team theres no time for anyone to babysit you That isnt to say that I didnt receive any help My tasks were often divided into several short 1-week sprints and a weekly Skype call where I would report my progressI was mainly working on the data engineering side of the product which involved researching designing and deciding on what type of databases would be used for storing data My two technical colleagues (front-end and back-end engineers) were less familiar with the architecture and technologies involved in the data lifecycle so I was solely responsible for this side of the projectHaving said that I received a lot of advice and guidance on the high-level aspects of the topics which helped me narrow down my area of researchWorking in a small team makes it easy to communicate and express your own opinions for many business decisions For an early-stage start-up there are hundreds of decisions that need to be made which will influence the future course of the companyIn the past I worked in a large corporation where I was just a small cog of a very large machine I was assigned straight-forward tasks based on my limited experience of the industry most of which I wasnt told how my work would contribute to the overall project All decisions were made by senior management so I didnt have a say in most thingsAt Zenu even though I was an intern and the only non-founder I was able to express my own opinions and make an impact on important business decisionsFor instance I convinced my team to store analytical data in a fully-managed low cost and scalable cloud environment such as the Google Cloud Platform as opposed to storing it in the existing operational database Choosing the correct database to store your data is very important because the data must be stored in a secure and reliable environment whilst making it very easy for developers to manage it You dont want to be redesigning and migrating databases once operational data starts streaming inWith just a laptop and a Wi-Fi connection it has become easier to work in any space at any timeAt Zenu the team is spread across three different continents which meant I never met two of my colleagues in person during the internshipHow can a company where everyone is not in the same place operate? Crazy right? Fortunately the tools available now make it much easier to work in such an environment On top of our weekly Skype meetings we used Slack for messaging Google Drive for sharing files and Gitlab for organising tasks and sharing codeAt an early-stage start-up you want to build a team with complementary skill sets Each member has an individual skill or an area of expertise which when combined through team effort accomplish goals efficiently and effectively Since our team was designed like this there werent many tasks which depended on another member finishing theirsThe focus of the work is to constantly make progress and complete tasks within a set deadline Such a work style means that flexible working hours can be enforced As long you finish by X date you can take Tuesday off or work between 11:00 19:00 etcThere are still managers that like to enforce the 9 5 work lifestyle where your commitment is measured by the number of hours sat at the desk However remote work and flexible hours are starting to gain popularity (and has become more feasible) especially among the younger generationAt Zenu we didnt have an office or a co-working space I was free to work at any time anywhereI worked in various cafes around Tokyo so I got to explore the city at the same time I am not a big fan of this style of work because it can often get quite boring working on your own Occasionally I worked together with friends who were working on their stuff Afterwards we would go grab lunch or dinner and this made each day a little bit more enjoyableDuring the early stages of a start-up energy levels are high and your brain is exploding with new ideas Understandably the team gets excited as they take small steps in converting a boardroom idea into reality Here at Zenu I experienced this positive vibe and passionHowever the constant flow of new ideas occasionally hindered our progress to focus on our high priority tasks A couple of hours of meeting time were wasted discussing minute features that could be added to the product or talking about things that are further down the timeline Oftentimes I questioned whether we were making our internal deadlines because progress seemed to slow down and we were talking about plans far into the futureOne example of prioritising things in the wrong order was when one of the founders wanted to start marketing the product through paid advertisements on social media to onboard some customers and see how the market reactsHowever the website was incomplete so there was no information on what the product was or how it worked Even if the website was complete and customers who believed in the idea did sign-up the product was incomplete so we would run into the risk of underdelivering (and we know what happened with Elon Musk…)This emphasises the importance of a minimum viable product (MVP); the minimum features required to serve the customers needs You dont want to be spending hours working on an idea or feature to find out that the customers dont care about itAn appropriate approach would be to adopt the Lean method by deploying the product with the minimum functions needed to serve the customer and pivot (make changes) based on market reaction and customer feedbackThe take-home is prioritise your tasks and focus on the urgent ones If you have buzzing ideas that relate to the product jot them down somewhere and bring them up when a technical or business decision related to that arises in the futureAt a pre-seed start-up time is limited and money is scarce For that reason all my colleagues have full-time jobs to pay their rent and support their families Whatever spare time and energy they had was dedicated to lifting the company off the ground into a fully serving productThe same applied to me I knew in advance that I wasnt going to earn a salary so I had to find a part-time job alongside my internship to make a living Typically my days were divided into three parts 2 hours tutoring in the morning 3 4 hours on the internship 2 hours on self-studying and the rest for travelling eating socialising and hitting the gymAfter several weeks it became a routine which helped with time management and accomplish as much as I could in the limited time I had I was more productive knowing that I only had a couple of hours to work for my internship as opposed to a typical 9 5 working daySurrounding yourself with people from different backgrounds and cultures will help you have a global mindset and become more creative On a personal basis it is important to realise that different cultures have different perspectives of the same things and by keeping an open mind you will build strong relationships with people and easily resolve conflicting opinionsIn the work environment a diverse team will contribute a lot of creative ideas based on their background and experience and ensure there are no missed business opportunities based on an incorrect preconception of the target audienceThe team at Zenu is based all over the world We have an American based in Tokyo a Hungarian in Switzerland a Malaysian in New Zealand and me a British/Japanese in Tokyo or LondonPooling our knowledge from our respective backgrounds was key to making several business decisions For instance what currencies would we display on the digital menu Restaurants in the UK and Japan tend to only accept a single currency the Pound and Yen respectively but restaurants in Switzerland operate with both the Euro and Swiss FrancAnother realisation was the importance of a diverse age range in the team Just like how people from different backgrounds and genders have different perspectives various age groups also have different views and lifestyles mostly due to the advancement of technologyAt Zenu we have two generation groups: me a member of Gen Z (loosely people born between 1995 2010) and my millennial colleagues (1980 1994) For the long-time survival of the company we need to serve the present and future The combination of my colleagues years of experience in the business and my knowledge and experience of a typical Gen Z behaviour (think of work-life balance lifestyle and lots of Instagram) ensured that the product would suit both the current and future generationsWhat do I mean when I say Work when youre not working? A lot of my ideas came from when I wasnt working in front of my laptop Since Zenu is creating a service which aims to transform the way we order food and beverages in the restaurant using technology I was always looking out for which type of restaurants this service would work in whenever I went out to eatOne time I discovered a food market hall which used a similar service to Zenu (using a phone to scan QR code) Another time I ate at a restaurant where they used an electronic stylus and a laminated menu to click and order dishes On both occasions I reported back to my team on the competitors that were currently on the market along with their pros and consTalking with my friends and family gave me a lot of new ideas and I was able to view things from a different perspective Conversations at the dinner table in the share house led to one of the tenants giving me a lot of advice on the technical side of my work as well as another tenant loving the start-up idea so much that he liaised with his boss for the potential idea of working together in the future (since we had solved a problem his company had yet to do)As much as you would like to know about everything whether its related to your career path or you just want to impress your friends there isnt enough time to learn everything However what you can do is find a mentor (or several mentors) that can guide you on your desired career pathThese people will guide you in the right directions and give you advice and point out pitfalls to avoid based on their area of expertise and personal experience Having someone point you in the right direction can save a lot of time and energy whilst accelerating your progress to meet your career goalsBefore interning at Zenu I had little to no experience in data engineering but my colleagues would often suggest certain areas to look into by posting links to articles or videos which may be relevant to the assigned taskOne of my tasks involved building a pipeline between our operational database and our analytical database which I had been pondering for several days since there are many ways to do thisI asked one of my share housemates whos an experienced ML engineer on how I could tackle this problem without overcomplicating things He was like Thats simple just use these two services I wasnt even aware that these services existed and thanks to him I saved a lot of time researching because I was looking in the completely wrong directionWorking at an early-stage start-up has its fair share of thrills and struggles The key to surviving in such a turbulent and fast-paced environment is to believe in the founders vision The everyday tasks may not have all been interesting and problems may have dragged on for days but the idea that your work is contributing to creating the vision into a reality is most definitely an exciting one
Vh2c8J4pSLBehgMrZMFw9G,In this fourth article of our Apache Spark series (see Part I Part II and Part III) we present another real-life use case that we faced at Teads and cover methods to consider when optimizing a Spark job For those who are not familiar with Spark User Defined Aggregation Functions (UDAF) we take a relatively simple but useful example when dealing with sparse arrays (an array of data in which many elements have a value of zero)At Teads we deliver ads to 15 billion unique users (called viewerId or vid) every month We use an unsupervised machine learning algorithm to create clusters of those users based on a set of features This clustering is soft meaning that for each user we compute the probability to belong to each of our clusters (we have 100 clusters in this example): This clustering has various applications ranging from improving our prediction models to creating custom user segments (predefined lists of people sharing some known properties) For some of them we need to normalize the resulting probabilities by their average at different levels In our example we calculate the average probabilities by countryThe input is the dataset described above But we store sparse and short-encoded arrays to S3 to reduce storage costsSince our arrays are very sparse we only want to store significant values (above some thresholds) into two arrays: For example considering a threshold of 01 and supposing that the missing values in the above table are zeros the sparse version would be: Short encoding: We store this data in snappy Parquet files and encode probabilities as Short to further reduce the size of the datasetAfter this processing our input dataset looks like this: We would like to decode these Short values to probabilities have the sum of all probabilities per array be equal to 1 and then compute an average by array index (aka cluster) Here is the step by step illustration of what we want to achieve: Step 1  Probabilities are decoded back to DoubleStep 2  Probabilities are normalized the sum of probas should equal 1 for each userStep 3  We calculate by country the average membership of people to each clusterThe first intuition was to create a UDF that decode and normalize data and a second one to densify the arrays We could have a single one to do the 3 operations Having elementary UDFs is a best practice check the Avoid UDFs or UDAFs that perform more than one thing paragraph in Part I for more detailsHowever grouping some functions in the same UDF sometimes helps to avoid intermediate data structures and conversion operations between Sparks internal data representation and the JVM There is a trade-off to make between performance and code complexityThen we use Sparks avg aggregation function to calculate the average by country for each cluster • min on an EMR cluster of 50 r3xlarge nodes (AWS instances)Dataset size: 700GB in compressed parquet/snappy (a lot more when uncompressed at least by a factor of 10) and 15 billion rowsLets have a look at it step by step: WholeStageCodegen: Since version 200 Spark can optimize some physical operations by collapsing them into one java function that it generates on the fly In our case this includes the parquet file read operation and the execution of our two UDFs This is explicit in the execution plan in the SQL tab of the Spark UI: Exchange: this is the shuffle triggered by the aggregations ( avg )HashAggregate: we can see a first aggregation operation before the shuffle and another one after it This is how some aggregations are implemented in Spark The Data is pre-aggregated within each input partition before shuffling (also called partial aggregation or map side aggregation); it is then further aggregated post shuffle to obtain the global resultIn this first implementation the densifying operation instantiates 100 length array for each row this is time and memory consuming Also converting back all these arrays from Java objects to an InternalRow data-structure (Spark-SQL optimized representation) at the exit from densifyUdf is also a heavy operationFollowing our first results a way to identify optimization leads could have been to perform a JVM profiling In our case we decided to directly try a UDAF to calculate the average on the arrays: Then we use it in our job so we dont need the densifyUdf anymore: 11 min on an EMR cluster of 50 r3xlarge nodesDataset size: 700GB in compressed parquet/snappy and 15 billion rowsThe UDAF implementation is 25X faster than the first implementationAs discussed above using a UDF besides the UDAF may create unnecessary intermediate data structures and we were not sure if Spark is smart enough to not do a further serialization/deserialization between the UDF and the UDAF We think it will do because UDFs are black boxes for Catalyst it wont even try to optimize them according to The Internals of Spark SQL by Jacek LaskowskiSo we tried to eliminate the decodeAndNormalizeUdf and apply its logic in the UDAF itself to see if it changes anythingThe inputSchema and the update methods of the UDAF become the following (the other methods remain the same): The execution of the above took 10 min on the same cluster so we can conclude its almost 10% fasterAn important difference that we noticed on the execution plan is the peak memory by task Its more important with the UDF applied separately from the UDAFHere we need to make a choice keeping the UDF is more readable and granular but a little bit less efficient We finally chose to put the decode function in the UDAFEven if the UDAF solution is much faster it uses the sort-based aggregation (cf the execution plan) which is most of the time less performing than the hash-based aggregation since it involves some extra sorting that is useless in our caseWhy Spark uses it? Lets dig a bit furtherHonestly this is not well documented and we had (like several other times) to search in Sparks code to find the answerReading the code we understand that all fields of our bufferSchema should have mutable data-types But wait! I thought we were actually using mutable types! Here is a reminder of our buffer schema we are using an array of double and a long: Looking at the code we can see how Spark defines mutable data-types here and hereIt shows that the Hash aggregation was not selected because we are using an ArrayType in the aggregation bufferTo make sure Spark is able to perform a Hash aggregation we slightly edited the UDAF to use 100 (nbClusters) Double columns instead of an arrayAt the end of the class we added some methods to the MutableAggregationBuffer via an implicit conversion to make the code a little bit less verbose and more readableHurray! We are now using a hash-based aggregation again: Avoiding the Array also avoids allocating a new array and copy all the data to the aggregating buffer on each row (on each update call) triggered by: A thread dump of the previous implementation (with array) via the Spark UI showed this: The execution of this new version only takes 92 minYes! We gained another 8%Thanks to all those who have helped me working on this use case and writing this post especially Benjamin Davy Cyrille Dubarry Yann Moisan Robert Dupuy Han Ju Louis Fruleux and Lucie Bontour If you like this kind of challenge diving into the internal working of technologies have a look at our other articles and our job opportunities
RvUbngw53kAFAghbK4Yj3R,In this third article of our Apache Spark series (see Part I Part II and Part IV) we focus on a real-life use case where we tried several implementations of an aggregation jobAt Teads we distribute ads to over 15bn people every month within professionally-produced contentOne of the main components of our platform is responsible for handling bid requests (an opportunity to display an ad) and for sending back a bid response (the ad to display and the associated price)An advertising campaign can be set up with delivery constraints : As a result ads are filtered according to these requirementsIn order to analyse the ad delivery we generate a log for each bid request containing the reasons why an ad was filtered (we also use this log to train our prediction models) For example a filtering reason can be geolocation if an ad targets users from a specific country For the sake of simplicity we will use four filtering reasons: from A to DThis article will focus on how to implement a reporting job that counts these filtering reasons for each ad Such a job can be used to build troubleshooting toolsHere is the schema of the logThe field filtering_reasons is a map with ad identifiers as keys and arrays of filtering reasons as valuesLets take an example with 2 bid requestsSo ad1 is filtered two times with reason A one time for reason B (in request 1) never for reason C and one time for reason D (in request 2)If we do the same exercise for ad2 and ad3 we see that the expected result is: A variable named allFilteringReasons contains all possible filtering reasonsThe idea is to explode the map to have an ad column and then group filtering reasons by ad with an aggregate function The desired aggregate function doesnt exist in Spark so we have to write a custom oneLets have a closer look step-by-stepExplode  An explode method returns two columns when applied on a MapType column : one for the key and one for the value There is also an as function made for this specific case that takes a sequence of aliasesHere is the resulting DataFrame : nafill  An ad may never have a given filtering reason resulting in null values The nafill replaces all null values by 0Custom UDAF  For the record here is the UDAF code (thats not the aim of the article) The basic idea is to accumulate each filtering reason in a Map whose key is the reason and value is the counterA note on Catalyst: When using the DataFrame/Dataset API a query optimizer called Catalyst is used This optimizer rewrites the query based on predefined rulesFor performance reasons Spark tries to group together multiple operators inside a Whole-Stage CodeGen In order to do that Spark generates Java code on the fly and compiles it with Janino (see here for further details)On the Web UI in the SQL tab we can click on a query to see a graphical representation of the physical planIn this second iteration the idea is to avoid using a UDAF by transforming each row with a UDF and then use built-in aggregate functions (that could be better optimized by Spark)If we look at this step-by-step we first have the same explode as in implementation 1User defined function  For each row of the dataset filtering reasons are transformed into a Map Hence built-in aggregate functions sum can be usedAggregation  All values for a given key must be aggregated togetherHence we can do: sum($mapThe only difference with implementation 1 is that the SortAggregate is replaced by a HashAggregateThis time we want to avoid the use of the explode method by mapping directly on existing partitionsHere the aggregation by ad is made manually inside the loopA given ad may appear on multiple partitions so we need an additional step to aggregate each result on the ad_idWe can also embrace a more functional programming styleSimilar to implementation 3 with some additional operatorsInterestingly enough even if we use groupByKey and reduce (reduceByKey does not exist) a partial aggregation is done on the reduce side CfIn all previous implementations counters are stored in a Map But Map is not a very efficient data structure It uses a lot of memory it is not cache friendly and it generates boxing/unboxing in Scala to store Long valuesAnd given that all keys are known in advance we can use an array insteadThis doesnt change our previous implementations a lot For instance here is the array-based version of our 3rd implementation: We have precomputed a cache (reasonToIndex) between filtering reason and indexThe application used for this benchmark processes one hour of our real production log with each implementation As we can see in the SQL plan the log contains 50M of input rows and the job produces a 3600 rows outputThis benchmark runs on an EMR cluster of 10 nodes (r3xlarge) with 10 executors 4 cores per executor and 20GB of RAMAll possible implementations need at least one shuffle phase to put all data for a given ad on the same node In this case we have only one shuffle so our implementations are optimized regarding network exchangeHashAggregate is more efficient than SortAggregate because we do not need to sort the data Nevertheless implementation 1 (with a SortAggregate) is more performant than implementation 2 (with a HashAggregate) Its related to the complexity of both algorithms: In this use case using the explode method costs a lot because the explode factor is around 20 (number of output rows in generate / number of output rows in scan as seen in the SQL plan) This is due to the generation of multiple InternalRowThanks to Wassim Almaaoui Han Ju Amine Er-Raqabi Benjamin Davy Gaël Rico and Robert Dupuy for their help in writing this article
TiES7fjZTtuJLGhWCqHCwP,POPxo has always been a data-driven company We have kept an eye on data to check whats working for us and whats not Since the inception of POPxo in 2013 the Data Engineering and Analytics team have worked together to answer complex questions regarding user behaviourThroughout these years our main source of tracking data has been Google Analytics Two years ago we also started using Clevertap mostly for higher customer engagement As our user base grew in these years we realised that we wanted to do more with our data which wasnt possible with these tools To begin with we wanted to track our users across multiple domains we wanted all our event data to be available in real time and most importantly we wanted COMPLETE OWNERSHIP of our data Thats where Snowplow came into the pictureSnowplow is an open source event analytics platform for mobile and webSnowplow provides two pipelines for event tracking real-time and batch At POPxo we have implemented both the pipelines The real time pipeline has Elasticsearch as the data sink It helps us to see what our users are more interested in and showing more relevant content to them It also helps us in fraud detectionThe batch pipeline runs on AWS EMR service every few hours For the batch pipeline we have kept Redshift as the data sink Access to redshift has been given to our data analysts so that they can keep a track of data using SQL queriesSnowplow takes advantage of various AWS services such as EMR Kinesis S3 Redshift and in our case Elasticsearch as well It is better to use AWS Elasticsearch Service because it can be easily scaled up as and when necessaryTrackers are used to fetch and send the user event data to the collector Trackers are client or server side libraries We currently use the Javascript iOS and Android trackersThe collector is an HTTP web service which receives the data from trackers and forwards it to the raw data kinesis stream for further processing We use the Scala Stream Collector as it helps us in tracking our users across multiple domainsEnrichers are used to widen the data dimensions and enriching the data One example of data enrichment is IP lookup This enricher fetches extra information about the users IP address and sends the information to the enriched kinesis streamThis component involves storing data By default the batch pipeline saves data in S3 From there the data is copied into Redshift Snowplow provides an option to save data in PostgreSQL as wellThe EmrEtlRunner is service that is used in the batch pipeline It fetches the raw logs from an S3 bucket enriches it and finally stores the data in S3 and eventually in Redshift All the steps involved in the EmrEtlRunner can be viewed hereSnowplow does not provide an out-of-the-box data visualisation interface like Google Analytics and Clevertap To overcome this we started using 3rd party tools namely Kibana Redash and SupersetKibana is used for the data that is stored in Elasticsearch Kibana helps us in visualising our users in real time It also has a dashboard feature with which we can create our own dashboard and track many metrics in a single pageRedash and Superset are tools that we use with the data that is stored in Redshift Data can be fetched using simple sql queries Superset goes one step ahead by having great interface for exploring the data without writing SQL queries Both provide great interfaces when it comes to data visualisation The tools can be used with other data sources like PostgreSQL Elasticsearch BigQuery MongoDB MySQL etcThis was an introduction to event tracking pipeline at POPxo So far we have been able to track around 300 million events averaging to 2 million events everyday But the more important questions is what do we do with this data? We have created a Bot Detection System and Recommendation Engine using the data that is tracked using SnowplowWant to be a part of POPxo? Check out our job openings page and let us know if you are interested
fMdNrLGsxbLZGLAjmcUdxT,Lean agile process is a very effective way to handle any project The methodology helps eliminate unnecessary processes reducing ambiguity and allowing the team to utilize their time optimally in developing an effective software with better functionality With lean agile process team is better equipped to focus on objectives improve productivity and deliver superior value to the customersFew of the building blocks for implementing lean agile principles are grooming the user stories and the functionalities or epics by understanding acceptance criteria prioritizing and conducting retrospectives While grooming is an important part of the process retrospective is equally imperative that enables the team to understand what went well and what did not The key objective of retrospective meeting is to understand the drawbacks of the process and make necessary changes However theres no right or wrong way for retrospectionIn this post I am sharing our approach to retrospective meeting that helped my team during the development process My team is a standard scrum team comprising software developers test engineers data engineers and analystsWe followed a process where the first two sprints were development sprints and we started testing from the third sprint While we conducted retrospective meeting after the end of each sprint most of it looked good when the code was not testedRetrospective meeting led to greater discussions from the third sprint onwards We also had to dig deeper into the first sprint as the test engineers had more insights about their findings from the sprint-1 This resulted in code re-work in order to fix the bugs altering the velocity of the following sprintsDepending on the severity of the bugs the velocity varied from sprint to sprint because the priority of the client differed In some sprints we were expected to fix key functionality bugs first whereas in certain sprints we had to complete coding on priorityThe issue with this process was that every time the client engineers produced better results than we did with the same set of data While customer expected better results in each demo test engineers were out of pace and didnt have the clarity on what test cases to writeTo address this challenge we decided to make some changes to the process in order to produce better outcome The changes made to the process include: Here is an example of the scenario where we implemented the preceding changesThe change in the process helped us in several ways As we made assumptions we also provided test cases to the test engineers right at the beginning This made it easy for them to understand the expected output and write better test cases while clarifying doubts as and when required helped optimize time and effort We conducted the demo every time we reached a significant stage improving our clients experience On the other hand as we kept our key stakeholders informed throughout the process making it easy to justify our assumptions in retrospective meetingsApart from this we accelerated the process of fetching data by optimizing code The changes made were as follows: 1We also deployed coding into the test environment While the test engineers tested the code developers started to optimize the code further We re-designed the architecture  fetched first 1 million records using optimized SQL and then fetched next chunk in parallel with different set of code We also used Byte channels to read the data and during the process we continually cleared the buffer in the code which helped boost the speedWe incorporated these changes at the optimization stage while testing was done parallelly that enabled us to save time and improve efficiency It helped reduce coding time by 70% (it took total 69 seconds to fetch the entire datasets) and man hours by 35% In addition to this we built a cross functional team that allowed us to think from various perspectives facilitating knowledge sharing and enhancing productivityCombining lean and agile principles helps take advantage of key elements of both the methodologies and customize them to meet project-specific requirements It allows to integrate continuous improvement and follow best development practices enabling the team to optimize performance and deliver valueLet us know how you liked the post and share your experience of using lean agile methodologies we love to hear from our readers
6uRh48a4ikzizB7GgLPEaE,A very common initiative these days is data warehouse centralization across an organization DWaaS (data warehouse as a service) has become commoditized to the point that organizations of every size can begin setting up a reporting infrastructure starting at only a couple hundred dollars a month This is really exciting and when copying data from other structured databases a relatively simple processLike anything though there are a number of ways to centralize data  and every solution has tradeoffs Even between SQL databases (relational structures) subtle differences exist and some data types supported in one will require data to be converted from another But generally the conceptual relationship of rows and columns carries over to any structured database It gets trickier however when we want to translate document-oriented (eg MongoDB) or object-oriented structures (eg REST APIs) into the 2-dimensional SQL spaceThe good news about working in non-SQL space is that storing data in JSON (JavaScript Object Notation) allows for a flexible schema to be built over time with a comparable (or higher) performance to that of a structured DB Yet the ubiquity of SQL knowledge in data teams means that NoSQL databases (and by extension JSON) have yet to really supplant SQL as the default format for basic querying and reporting There are use cases where NoSQL makes sense for more difficult problems at a much larger scale but thats a blog post for another timeAll that to say knowing the options for storing data will help you make the right decisions for your company when youre ready to take this step There are the three aspects to take into consideration when designing a data warehouse  an OLAP (online analytical processing) database  and three forms your data can take to appeal to these aspectsNOTE: When using a hosted database or warehouse that stores your data across a cluster there are additional setup considerations (dense compute vs storage; indexing; sort/dist keys) that will affect query performance This guide does not go into them and instead focuses on the factors directly related to importing and normalizingStorage speed and ease-of-use each factor heavily into your database choice The first two are pretty straightforward Broadly you want to make sure that your database is avoiding record duplication and that whatever records relate to each other are organized together to make your queries as efficient as possible These considerations are somewhat at odds (as well soon see) and efforts to reduce either storage size or query latency usually result in sacrifices in the otherThe third consideration (ease-of-use) however is more subjective Good analysts are going to care a lot about how their queries are performing but on agile analytics teams they should also care about how quickly they can navigate the data structures and create various views If you dont normalize your data at all any nested data will just be put into the table nested (and become essentially unusable) but if you normalize too much you might end up forcing your analyst to work between a taxing quantity of tables Its important to remember that analysts rarely generate a dashboard and never touch it again They (should!) iterate on their analysis quickly always taking their questions a level deeper and this requires a delicate balance when choosing how to store the data in the first placeIn balancing the above considerations a method of how to think about structuring data efficiently emerged in the early 1970s by Edgar Codd then a researcher at IBM Although IBM initially wasnt terribly interested in data storage efficiency (they made their money off of charging for that same storage) IBMs customers were But knowing exactly how to put it into practice (and when to stop) turns out to be harder than you might expectTake the JSON example below: This can easily be converted to a structured format as follows: The values that were nested within the value of hockey have been moved up to the top level with the prefix hockey_ added to them to ensure they are unique Had we not included the hockey_ to each value we would have had a conflict between the two favorite_team values as one already exists at the top level This process is called flattening  whereas there used to be a complex structure now they are all on the same row This creates a greater uniformity between records and easier traversing for analystsThis is a much harder example to flatten because weve introduced Sports that has an array of corresponding values each with the same sub-fields If we were to flatten this JSON object the result would look something like this: This isnt great For every person we add to this list a corresponding number of columns for each field get created Instead of a names column we would get people_0_name people_1_name people_3_name This makes even basic analysis impossible because you need to complete rebuild the table through convoluted queries to build the table you actually want where each column is the comprehensive source of truth for a given metric or traitAn alternative would be to associate the top level information to each person like the followingThis organization is called First Normal Form where all attributes are represented by individual columns and each value within that column represents an atomic value (eg
b7sq8CMVnsqEhSjV6GyWwB,Born in 2018 Astronomer Cloud remains the easiest way to run Apache AirflowAfter 12+ months of strong growth in both usage and support from customers prospects and Airflow community members alike it became clear that our single multi-tenant GKE Cluster that hosted Astronomer Cloud needed an infrastructural makeoverOptimizing for both maturity and scale our team has spent the past few months upgrading our backend to create a more reliable and robust managed Airflow offeringToday we are excited to announce that the next generation Astronomer Cloud platform is readyDesigned to set the foundation for multi-cloud and multi-region support New Astronomer Cloud boasts our latest features and a strong backbone built to handle a regular and consistent stream of soon-to-come product releasesNew Astronomer Cloud brings our existing customer base to our latest product version Astronomer v0103 Our latest and greatest includes a slew of features that we are excited to expose to both our current users and those exploring our offeringRun Airflow v110New Astronomer Cloud is built to run and fully support the latest version of Airflow v1105 By upgrading users gain access to the features and bug fixes most recently released by the core Airflow projectFor more info reference Airflows v1105 ChangelogWith Astronomer Cloud weve re-architected our Airflow + Astronomer logging stack for added observability and usability For the first time as an Astronomer Cloud customer you can search and filter your Scheduler Webserver and Worker logs from the Astronomer UI Leverage these logs for informed real-time troubleshooting and gain deeper exposure into Airflows internal processesCheck out our logging doc for a closer look at our new viewYour deployment on Astronomer Cloud now comes with a dedicated Metrics tab for a birds-eye view of your deployments resource usage the overall health of its components and a real-time stream of task failures and successesIn a significant step towards making serverless Airflow a reality weve added support for the KubernetesExecutor the most resource-efficient way to run Airflow yet The Executor eliminates the possibility of interruption to running tasks during a deploy allows for task-level resource configuration and is built for ultimate scaleWhen you use the KubernetesExecutor on Astronomer youll be billed dynamically according to true CPU and Memory usageAstronomer Cloud is now fully integrated with Airflows native RBAC functionality You can now assign users one of three granularly mapped roles and control the level of access they have to your deployments and Workspace-level settings You can also apply these RBAC policies to Service Accounts at the Workspace levelFor a detailed breakdown of roles refer to our docs hereYou can now increase the resources allocated to your Deployments Airflow components on Astronomer to a maximum of 100AU each a significant bump from our legacy 30AU capOur newly built infrastructure is optimized to deliver faster and smoother deployments to Astronomer Cloud across the board Applicable to Local Celery and Kubernetes ExecutorsTo enhance the Astro CLIs functionality weve refactored commands for more intuitive troubleshooting added a feature for real-time log streaming and added support for authentication via Service AccountsIn an effort to improve the administrative experience for our users Workspace Admins can now manage their billing information directly in the Astronomer UIOriginally published at https://wwwastronomerio
QqGzyuiuridvEuTsEndNa6,Stockport UK  July 7 2020  The Data Value Factory a University of Manchester spinout with expertise on data integration and data cleaning announced today the availability of its paradigm-shifting Data Preparer platform onto AWS Marketplace and Azure Marketplace to further reduce costs associated with unlocking data valueData Preparer is a self-service data preparation platform that uses intelligent automation to quickly make sense of and prepare data sources for analysis The software provides users with the flexibility to onboard new datasets and enables transformation and merging of data in an automated agile manner Data preparation normally involves laborious hand-crafting of data pipelines In contrast Data Preparer automatically searches for ways that data sets can be combined and made more consistentThe Data Value Factory enabled its Data Preparer offering to run both on-premise and in the cloud recognising the business needs to: Consequently any organisation can now launch a virtual machine instance containing Data Preparer in their preferred cloud provider Data Preparer can then be accessed from any web browserMoreover The Data Value Factory realizes that budgets are more constrained than ever in todays business environments As such with a free month trial Data Preparer aims to encourage all types of data users to try out and see for themselves how far full automation can get them when crunching dataThis cloud offering comes as a natural next step in our line of work towards embracing data democratisation said Dr Nikolaos Konstantinou Managing Director at The Data Value Factory Our goal is to provide a self-service data prep solution that can be deployed virtually anywhere and connect to a wide range of databases and file formats at a competitive price he commentedOverall the ability to run Data Preparer in the cloud: Data Preparer is now available to purchase in the AWS Marketplace in the Azure Marketplace and on-premise using the Mac Windows or Command Line version A one-month free trial is offered at all cases To learn more about Data Preparer you can check out some demonstrations on youtube or schedule a demo
ZfRjsAqVG8bk76SWBtzbqR,To give some background the government announced their Open Data plan in 2018 with the target of launching 700 dataset from more than 80 government divisions Critics on the data has complaining about the data quality usability and update frequencies Before we comment on this we decided to test it out and share with you our findings The first Open Data API we tested is the Journey Time Indicator API Before we deep dive into our findings lets take a quick overview on what is Journey Time Indicator API: There are currently 10 sets of Journey Time Indicators (JTI) installed on major routes on Hong Kong Island and in Kowloon to provide the latest cross harbour journey time and 5 sets of JTI installed on major routes at the New Territories to provide the latest journey time to Kowloon for motorists to make an informed route choice The displayed journey time refers to the average journey time of vehicles from the JTI to the respective destinations The journey time XML file is updated every 2 minutesWe built a data processing pipeline on Google Cloud (will be explained in another article) for this API The pipeline grabs the latest data every minutes transforms it in a way we think would be best practices and store it for later usage Throughout the process there are 1 good thing and 3 areas of potential improvements we would like to shareThe API is very stable and we encounter 0 downtime during our test (and our data pipeline is still running) Not quite sure if this is due to great scalability of the services or low usage rateAccording to the data specification there were multiple data fields encoded by numbers (eg COLOR_ID = 1 2)or meaningless code (eg LOCATION_ID = H01 H02) It is totally understandable that it might be trivial internally to use these IDs However this might not be best practice to share data in this format for a simple reason  maintainability for data users Imagine at some point in the future the publisher want to update their schema for some reasons by changing the definition of 1 2 3 or adding an extra code the data users who have no idea about this might wrongly interpret the data Congested traffic might become smooth traffic and vice versa This is especially important for Open APIs that require no registration process as the publisher have no means to update the users for API changesDuring the data exploration phase the first thing to do is of course trying to understand the schema There was one description caught our attention: When JOURNEY_TYPE = 2  1 means traffic congested Traffic congestion bitmap will be displayedWe are then very interested in how a bitmap will get included in the API so we try looking for peak hours protest days etc just to take a look on the bitmap However we were not able to find a day with JOURNEY_TYPE=2! Here we are not sure why we were not able to identify an hour with congested traffic but the API specification did not mentioned when JOURNEY_TYPE would be 2 neither If that is something in the future roadmap it would be great to include a potential launch date so data engineers could decide how and when to support thatIn the specification a coordinate of the traffic tracker is also included but in HK1980 format (eg 835776133E 815604834N) This is not a universally supported format If we are looking at Google Maps and other mainstream location and geo-related application WGS84 is a more popular and well-known format at the moment Data Engineer who are planning to further leverage this HK1980 coordinates will need to go through an extra step in converting the coordinates (we tried and details in another article)These are all good learnings on designing Open Data APIs or even internal APIs within an organisations Going forward we are going to test more of the HK Government APIs with similar approach
ECnj2SmVJ4LrXJh6yaod2a,Google Cloud Storage supports versioning but it may not serve our use case The versioning is on object-level and there is no relationship between file objects Here is the direct quote from the official documentation: There is no relationship between the generation numbers of unrelated objects even if the objects are in the same bucketAs a result if there is a data dependency between files we need to keep track of it on our own Or we can just backup the entire group of files regularly into another bucketThere isnt off the shelf solution to backing up the files stored on Cloud Storage Here we will discuss how to do it with Cloud FunctionsBelow is a basic data flow diagram of the whole system Cloud Functions will be used for copying files and Cloud Scheduler will be used for triggering periodically eventFirst of all creating a secure bucket for those backups is essential The access right of the bucket can be updated on the dashboardCreating an event emitter is the starting point of the whole system We can use Cloud Scheduler to implement it conveniently Below is the image showing the job creation page there are 3 parameters that are importantFrequency:To define the period of the job common unix-cron instruction can be found here https://crontabguruTarget:Select Pub/Sub here Pub/Sub is a useful feature of Google Cloud it is used for publishing and subscription events Here we want the scheduler send events to the Cloud Function we definedTopic:This is the identifier of the eventAfter setting up the Pub/Sub we need to set up a Cloud Function to handle the file copying There are also two important parameters on the Cloud Function creation pageTrigger:This defines the triggering event of the function we select Pub/Sub hereTopic:Here we type in the same topic name as aboveWe replace the original code with a simple consolelog for debuggingTo test our function we can trigger the Pub/Sub event manually as below: We can browse the logs of the Cloud Function we can find the View Logs button on the Cloud Function management page: The code below demonstrates a simple way to copy files from one bucket to another Please also amend the packagejson to include all required libraries
VhUvmVcjTTpuhmebAMaVX5,Apache Cassandra is an open-source distributed row-partitioned database management system (distributed DBMS) to handle large amounts of structured data across many commodity servers providing high scalability (linearly scalable) and high availability (fault-tolerant) with no single point of failure It supports for only structured or semi-structured data and the database consistency levels can be tuned to considerably high degreesListed below are some of the notable points of Apache Cassandra: In Apache Cassandra you want to model your data to your queries and if your business need calls for quickly changing requirements you need to create a new table to process the dataHandy References: For cloud solution for Cassandra refer Amazon Keyspaces or DataStaxs Cassandra as a Service called Astra on Google Cloud MarketplaceKeyspaces: Keyspaces are containers of data similar to the schema or database in a RDBMS Typically a keyspace contain many tables A table belongs to only one keyspaceReplication is specified at the keyspace levelTables: Tables are also referred to as column families in the earlier version of Cassandra Tables contain a set of columns and a primary key and they store data in a set of rowsEvery table must have a primary keyColumns: Column represents a single piece of data in Cassandra and has a type definedThere are some special columns: Primary Key Partitioning Key Clustering Columns and Data Columns: Every table must have a primary key with unique constraintPartition key is the first component of Primary key Its hashed value is used to determine the node to store the data The partition key can be a compound key consisting of multiple columns We want almost equal spreads of data and we keep this in mind while choosing primary keyAny fields listed after the Partition Key in Primary Key are called Clustering Columns These store data in ascending order within the partition The clustering column component also helps in making sure the primary key of each row is uniqueYou can use as many clustering columns as you would like You cannot use the clustering columns out of order in the SELECT statement You may choose to omit using a clustering column in you SELECT statement Thats OK Just remember to sue them in order when you are using the SELECT statement But note that in your CQL query you can not try to access a column or a clustering column if you have not used the other defined clustering columns For example if primary key is (year artist_name album_name) and you want to use city column in your querys WHERE clause then you can use it only if your WHERE clause makes use of all of the columns which are part of primary keyWhen inserting records Cassandra hashes the value of the inserted datas partition key; Cassandra uses this hash value to determine which node is responsible for storing the dataTokens: Cassandra uses tokens to determine which node holds what data A token is a 64-bit integer and Cassandra assigns ranges of these tokens to nodes so that each possible token is owned by a node Adding more nodes to the cluster or removing old ones leads to redistributing these token among nodesA rows partition key is used to calculate a token using a given partitioner (a hash function for computing the token of a partition key) to determine which node owns that rowCassandra is Row-partition store: Row is the smallest unit that stores related data in CassandraDont think of Cassandras column family (that is table) as a RDBMS table but think of it as a dict of a dict (here dict is data structure similar to Pythons OrderedDict): This model allows you to omit columns or add arbitrary columns at any time as it allows you to have different data columns for different rowsStarting a Cassandra cluster is as simple as: To start a new container we just tell each new node where the first isFor separate machines you need to tell Cassandra what IP address to advertise to the other nodes (since the address of the container is behind the docker bridge)Refer Official docs on Cassandra ArchitectureCassandra organizes data into partitions Each partition is stored on node and consists of multiple columns Nodes are generally part of a cluster where each node is responsible for a fraction of the partitionsCassandra has masterless and peer-to-peer (that is each node is connected to every other node) distributed system across its nodes and data is distributed among all the nodes in a cluster: Cassandra has peer-to-peer distributed system across homogeneous nodes where data is distributed among all nodes in the cluster Each node frequently exchanges state information about itself and other nodes across the cluster using peer-to-peer gossip communication protocol A sequential written commit log on each node captures write activity to ensure data durability Data is then indexed and written to an in-memory structure called a memtable which resembles a write-back cache Each time the memory structure is full the data is written to disk in an SSTable (sorted string table) data file All writes are automatically partitioned and replicated throughout the clusterAll nodes in a Cassandra cluster can accept reads and writes no matter where the data being written or requested actually belongs in the cluster All nodes are essentially identical and as a result Cassandra has no single point of failure and therefore no need for complex sharding or leader election Cassandra is able to achieve both availability and scalability using a data structure that allows any node in the system to easily determine the location of a particular key in the cluster This is accomplished by using a distributed hash table (DHT) design based on the Amazon Dynamo architectureReplication: The replication strategy of a keyspace determines which nodes are replicas for a given token rangeSimpleStrategy allows a single integer replication_factor to be defined This determines the number of nodes that should contain a copy of each row SimpleStrategy treats all nodes identically ignoring any configured datacenters and racksNetworkTopologyStrategy allows a replication factor to be specified for each datacenter in the cluster Even if your cluster only uses a single datacenter NetworkTopologyStrategy should be preferred over SimpleStrategy to make it easier to add new physical or virtual datacenters to the cluster laterTunable Consistency: Write operations are always sent to all replicas regardless of consistency level The consistency level simply controls how many responses (ONE TWO THREE QUORUM ALL LOCAL_QUORUM EACH_QUORUM LOCAL_ONE ANY) the coordinator waits for before responding to the client Here QUORUM means majorityIt is common to pick read and write consistency levels that are high enough to overlap resulting in strong consistency This is typically expressed as W + R > RF where W is the write consistency level R is the read consistency level and RF is the replication factor For example if RF = 3 a QUORUM request will require responses from at least two of the three replicas If QUORUM is used for both writes and reads at least one of the replicas is guaranteed to participate in both the write and the read request which in turn guarantees that the latest write will be read In a multi-datacenter environment LOCAL_QUORUM can be used to provide a weaker but still useful guarantee: reads are guaranteed to see the latest write from within the same datacenterIf this type of strong consistency isnt required lower consistency levels like ONE may be used to improve throughput latency and availabilityCommitLog: CommitsLogs are an append-only log for all mutations local to Cassandra node Any data written to Cassandra will first be written to commit log before being written to a memtable This provides durability in the case of the expected shutdown On startup any mutations in the commit log will be applied to memtablesMemtables: Memtables are in-memory structures where Cassandra buffer writes Eventually memtables are flushed onto disk and become immutable SSTablesSSTables: SSTables are the immutable data files that Cassandra uses for persisting data on diskData modeling is a process that involves identifying the entities (items to be stored) and the relationships between entitiesDenormalization of tables (based on the queries) in Cassandra is absolutely critical The biggest take away when doing data modeling in Apache Cassandra is to think about your queries first (in fact for all NoSQL databases) In fact we create one table for each similar set of queries (these similar sets of queries can answered through same definition of the partition key and clustering columns that is via same data model)There are no JOINs in Apache Cassandra So we need different queries for different tablesAlso there is no support for GROUP BY and sub-queries in CQLSince Apache Cassandra requires data modeling based on the query you want you cant do ad-hoc queries So you cant do analysis such as GROUP BY statements on Apache Cassandra However you can add clustering columns into your data model and create new tablesData modeling in Apache Cassandra is query focused and that focus needs to be on the WHERE clause The PARTITION KEY must be included in your query and any CLUSTERING COLUMNS can be used in the order (omitting one of some clustering columns is OK though) they appear in your PRIMARY KEY But note that in your CQL query you can not try to access a column or a clustering column if you have not used the other defined clustering columnsThe WHERE the clause must be included to execute queries It is recommended that one partition be queried at a time for performance implications It is possible to do a SELECT * from <table> if you add configuration ALLOW FILTERING to your query This is risky but available if absolutely necessaryAn index (formally named secondary index) provides means to access data in Cassandra using non-primary key fields If there is no index on such a columns it is not even allowed to be conditionally queried (that is such columns are not normally queryable)An index indexes columns value in a separate hidden column family (table) from the one that contains the values being indexed The data of an index is local only (that is within a node; of course because the column used for index in a non-clustering key) This also means that for data query by indexed column the requests have to be forwarded to all the nodes waiting for all the responses and then the results are merged and returned So if you have many nodes the query response slows down as more machines are added to the clusterEvery node first writes the mutation to the commit log and then writes the mutation to the memtable Writing to the commit log ensures the durability of the writer as the memtable is an in-memory structure and is only written to disk when the memtable is flushed to diskSince Cassandra is a masterless a client can connect with any node in a cluster The chosen node is called coordinator and it is responsible for satisfying the client requests The consistency level determines the number of nodes that the coordinator needs to hear from in order to notify the client of a successful mutation All inter-node requests are sent through a messaging service and in an asynchronous manner Based on the partition key and replication strategy used the coordinator forwards the mutation to all applicable nodesAt the cluster level a read operation and write operation are similarQUORUM n/2 + 1 where n is a replication factor is a commonly used consistency level which refers to a majority of nodesA memtable is flushed to an immutable structure called SSTable (Sorted String Table) The commit log is used for playback purposes in case data from the memtable is lost due to node failure Every SSTable creates three files on disk which includes a bloom filter a key index and a data file Over a period of time a number of SSTables are created This results in the need to read multiple SSTables and scan the memtable for applicable data fragments to satisfy a read requestA row key must be supplied for every read operation The coordinator uses the row key to determine the first replica As with the write path the consistency level determines the number of replicas that must respond before successfully returning data If the contacted replicas have a different version of data the coordinator returns the latest version to the client and issues a read repair command to the node/nodes with the older version of the dataEvery SSTable has an associated bloom filter which enables it to quickly ascertain if data for the requested row key exists on the corresponding SSTable This reduces IO when performing a row key lookup A bloom filter is always held in memory since the whole purpose is to save disk IO Cassandra also keeps a copy of the bloom filter on disk which enables it to recreate the bloom filter in memory quicklyIf the bloom filter returns a negative response no data is returned from that particular SSTable This is a common cause as the compaction operation tries to group all row key-related data into a few SSTable as possibleIf the bloom filter provides a positive response the partition key cache is scanned to ascertain the compression offset for the requested row key It then proceeds to fetch the compressed data on disk and returns the result set The partition summary is a subset to the partition index and helps determine the approximate location of the index entry in the partition index The partition key is then scanned to locate the compression offset which is then used find the appropriate data on diskThe API to Cassandra is CQL the Cassandra Query Language To use CQL you will need to connect to the cluster which can be done: The cqlsh is a command-line shell for interacting with Cassandra through CQL It is shipped with every Cassandra package and can be found in bin/ the directory alongside the Cassandra executable It connects to the single node specified on the command lineThe datastax/python-driver is Github repo for Python client library for Cassandra which can be installed as simple as pip install cassandra-driverThe API to Cassandra is CQL the Cassandra Query Language which is a dialect similar to SQL While similar to SQL there is a notable omission: Apache Cassandra does not support JOIN operations or subqueries or GROUP BY clause rows are identified by primary key and no duplicates are allowed INSERT does not check the prior existence of the row by default so a row is created if none existed before and updated otherwiseHeres how you can get info about existing keyspaces and tables: Apart from native data types Cassandra supports: Cassandra 22 introduced JSON support for SELECT and INSERT statements This support does not fundamentally alter the CQL API (for example the schema is still enforced) it simply provides a convenient way to work with JSON documentsA table can be created using create table command Creating a table amounts to defining which columns the rows will be composed and optional options for the table Every row in a CQL table has a set of predefined columns defined at the time of the table creation (or added later using alter statement)A column definition is primarily comprised of the name of the column defined and its a data type Additionally a column definition can have the following modifiers: Within a table a row is uniquely identified by its PRIMARY KEY and hence all table must define a PRIMARY KEY (and only one) A PRIMARY KEY definition is composed of one or more of the columns defined in the tableSyntactically the primary key is defined the keywords PRIMARY KEY followed by a comma-separated list of the column names composing it within parenthesis but if the primary key has only one column one can alternatively follow that column definition by the PRIMARY KEY keywords The order of the columns in the primary key definition matterA CQL primary key is composed of 2 parts: A table always has at least a partition key and if the table has no clustering columns then every partition of that table in only comprised of a single row (since the primary key uniquely identifies rows and the primary key is equal to the partition key if there are no clustering columns)Here row 1 and row 2 are in the same partition row 3 and row 4 are also in the same partition (but a different one) and row 5 is in yet another partitionThe most important property of partition is that all the rows belonging to the same partition are guaranteed to be stored on the same set of replica nodes In other words the partition key of a table defines which of the rows will be localized together in the Cluster and it is thus important to choose your partition key wisely so that rows that need to fetch together are in the same partition (so that querying those rows together require contacting a minimum of nodes) A partition key that groups too much data can create a hotspot (as all of that data has to be stored on the same set of replica nodes)The clustering columns of a table define the clustering order for the partition of that table For a given partition all the rows are physically ordered inside Cassandra by that clustering orderA CQL table has a number of options that can be set at creation (and for most of them altered later) These options are specified after the with keywordThe clustering order of a table is defined by the clustering columns of that table By default that ordering is based on the natural order of those clustering order but the CLUSTERING ORDER allows us to change that clustering order to use the reverse natural order for some (potentially all) of the columnsCQL supports creating secondary indexes on tables allowing queries on the table to use those indexesIn a CQL table new rows can be inserted and existing rows can be updated or deletedMaterialized views can be created for a CQL table Each such view is a set of rows that corresponds to rows that are present in the underlying or base table specified in the SELECT statement A materialized view cannot be directly updated but updates to the base table will cause corresponding updates in the view
HN2DbGxezhP9N5CdQ9qvmX,Earlier this year BiLD launched an innovative data engineering academy based in Lisbon This idea started to gain credibility when we realised there was a lack of experts in the market compared to the demand and recent graduates were also unable to enter the Big Data world without previous experienceThe concept for the first Academy was to train and develop the skills of five recent engineering graduates with different academic and personal backgrounds The program is taught by experienced professionals with great theoretical and practical knowledge in all the tools within the program and this differentiates our Academy from other options in the market These five trainees spent three months in an intensive training course designed entirely by BiLDs senior consultants getting them prepared to enter the market as Data EngineersThis first academy was held in BiLDs offices in Lisbon and the program was based fully on Microsoft tools and ecosystems in partnership with Avanade Portugal a leading Portuguese company in the field of data analytics that resulted from a joint venture between Microsoft and AccentureThe academy program was originally divided into seven major categories starting with a Big Data and Data Engineering basics introduction going to Azure Data Warehouse Analysis Service Data Visualisation and Power BI Python programming essentials Databricks Azure Data Factory and finally a final capstone project before the client onboarding to finalise the training part of our academy Through our partnership with Avanade after the months of training our recent data engineers were able to enter the market in innovative projects gaining more experience and being able to work on what they had always dreamed ofAll five junior consultants present in our pilot academy are now successfully integrated into BiLDs clients working as data engineers and they together with our senior consultants are helping us build a new academy with perfected course content even more closely adapted to what the market is demandingGiven the accomplishments achieved with our first academy and with the corrections in the training program using the feedback from the first batch of junior consultants BiLD launched a new academy starting in October 2020 Our goal is to keep perfecting this academy changing careers and training new quality data engineers so that recent graduates can have the jobs they have always dreamed of and the market demand can be fulfilled
MSu8t2kGwKpsiXxSGTZrVh,Ada Lovelace the first person to recognize the opportunity offered by programmable machines was not at first blush an obvious pioneer of computing Born on December 10 1815 in London she was to her father the great romantic Lord Byron a disappointment He had wished for a glorious boy and unusually for the time given that custody rights were exclusive to the father did not press his case when Lady Byron left him with baby Ada after events led her to suspect that Lord Byron ever flamboyant and notorious had gone completely madLady Byron was determined that Ada would not suffer her husbands insanity and pushed her to learn all she could about mathematics and technology She believed these were the antidotes to romanticism and could save her daughter from developing any of the same behaviors as her father Ada kindled a strong interest in the scientific developments of her day and in particular the work of Charles Babbage a polymath and builder of machines Babbage had a new mechanical invention called the Analytical Engine that was the first programmable general-purpose computer Though not a computer of transistors the machine could nevertheless be described as being broadly similar in principle to computers today and if desired could very slowly replicate any modern computer programAda fell in love with the Analytical Machine and despite her mothers efforts she did not escape her fathers romantic influence She mixed with the great minds of her day from Charles Dickens to Sir Walter Scott and came to consider herself a poetical scientist Someone who could conceive through empathy of the potential benefits that this new technology could bestow on the world
WmyBpoWP2T3QcHtAXdpyAb,As a business relying on ad serving platforms like Google Ads and Facebook the majority of our data at Pixability comes from 3rd party APIs By ingesting data from these APIs we are able to add value on top of the native platforms that cant be found elsewhereOur first software product was built around the YouTube Data API which has been used for years to help advertisers succeed with their video strategy As the marketplace grew and platforms like AdWords for Video (now Google Ads) and Facebook Ads Manager took off we built a system to ingest this campaign data for use in our reporting dashboard It served us well for several years but as our business grew and we added more accounts and thus exponentially more data we started to see the limitations with how this system could continue to scale To set us up to more efficiently ingest data in the future we set out to redesign and rebuild these data harvestersIn the earliest conversations about what our new ingestion pipeline should look like we first took a good look at the current architecture The components below make up the core of this legacy systemOur primary goal for the new harvesting system was to simplify the entire data harvesting process In achieving this goal we also looked to ease debugging decrease processing time increase scalability and reduce infrastructure costs Our first step was to get raw data from 3rd party APIs to Snowflake as efficiently and transparently as possible This is in contrast to our legacy system which was very much a black box where the raw data is transformed in many ways along the path from API to MySQL all occurring within async Celery tasks spread across many workers This makes debugging much more difficult since at first glance we can never be sure if an issue with data in MySQL or Snowflake originates within our system or the 3rd party API First wed need to check Snowflake then MySQL and then finally run imports locally with a debugger to see what raw values were actually returned from the platform API Our new design would have to provide easy access to the raw data exactly as its returned from the platformIn addition to removing complexity to ease debugging we wanted to eliminate MySQL as the middleman data store and Debezium as the vehicle for transporting data from MySQL to Snowflake We now use Snowflake as the source of truth for our advertising data and other microservice applications run their own Extract Transform Load (ETL) processes to fetch and aggregate data as needed Going directly to Snowflake reduces cost and complexity eliminates several points of failure and decreases the amount of time it takes before our applications can present the most recent data to usersAnother bonus by putting all raw data directly in Snowflake is that we could have stateless harvesters Our legacy harvesters used auto-increment primary keys in MySQL to keep track of external objects each already having their own IDs coming from the advertising platform This required the harvesting process to follow a strict sequential pattern to ensure correct foreign key relationships first harvesting the highest level parent objects then their children grandchildren and so forth If an error occurred in processing one campaign (parent) this means that the system could not import any ads or insights (children) for that campaign This stateful harvesting also meant that the system had to load all objects for an account into memory to determine the correct parent child relationships and set inherited fields As you can imagine this frequently led to Out Of Memory errors with the only quick recourse being to beef up the worker hardware to machines with more RAMWhere Do We Go From HereAfter spending time analyzing our legacy system and mapping out key requirements for the new harvesting pipeline we started putting the pieces of this puzzle together Since we already rely on AWS and Apache Kafka elsewhere in our system we figured it made sense to start there with the tools we already know The first piece was an easy one: AWS Cloudwatch Events are a natural replacement for cron scheduling and we already use Cloudwatch event triggers for several other applications The next pieces to replace were the Python functions that queue up Celery tasks for processing These functions were generally the entrypoint to our entire import and told the subsequent Celery tasks key information such as which accounts and dates to harvest We decided to use another familiar tool for this: AWS Lambda This Lambda function queries Snowflake for a list of accounts that meet the criteria to be imported and then publishes records to Kafka that contain information including account ID and which dates to harvestWhile most of our legacy harvesting work was processed in Celery tasks we could use Kafka consumers to perform similar functions  now just calling the 3rd party APIs instead of applying extra transformations along the way As discussed in a previous blog post we have used Robinhoods Faust framework for stream processing in other projects and we decided to use Faust here as well since each worker can have one responsibility and remain largely stateless (other than the data coming from source Kafka records) In its simplest form this allows us to have many independent consumer groups that read from our Lambdas output topic call an API endpoint for one type of report (eg campaign) and publish the response data directly to Kafka without any transformation beyond serializing the data to the Avro formatFor some cases we still do need a bit of serial processing for instance if a certain API endpoint requires the ID of a parent object to get access to all children We can quickly chain together multiple Faust consumers  the second one in line can consume the parent output topic instead of the high level account topic produced by the Lambda It can then take the ID from the parent and make the next API request to get the children If there is another level further down this chain can continue all while we are simultaneously inserting the parent data to Snowflake in its raw form with the Kafka Connect frameworkTo make our harvesting processes more stateless we abandoned the use of auto-increment IDs instead using the already existing IDs that come directly from the ad platforms This allows us to harvest different types of data concurrently without worrying about linking it together manually For example our campaign performance data will automatically be linked to the campaign object metadata by its campaign_id even though they could be harvested at different times and ultimately end up on different tables in Snowflake The result is that we can do much more processing in parallel and in combination with Fausts horizontal scaling we can drastically cut down on the time a full import takesSo far weve discussed getting raw data from the ad platforms into Snowflake and how we were able to cut out the many transformations that happened in our legacy Celery tasks Getting the raw data was half the battle but our other objective for this new harvester was to replace the legacy system in a way that no consumers of the data could tell a difference That meant we needed to populate the same tables in Snowflake maintain the same schemas and ensure the same transformations were appliedWe ended up solving this all within Snowflake using a combination of views streams & tasks and stored procedures The first step involves using streams & tasks to track changes of a raw history table that contains inserts for every record on a Kafka topic We use Snowflake streams to listen for changes then use an intermediate view to track the changes deduplicate records and determine if each row should be an insert update or delete statement in relation to our primary table Finally we have a scheduled task that merges the data from the view into our primary table Once our raw primary tables are updated our main views for object metadata will automatically updateTo update our performance data tables which drive all of our insights we use Snowflake tasks to call stored procedures that aggregate and transform the raw data into the correct format for our legacy tables The procedures then insert this transformed data directly into the tables as the last step of a very simple ETL process This replaced an incredible amount of Python code from our legacy system and is much easier to debug Instead of relying on external infrastructure and lots of custom code using Snowflakes views streams and tasks for the scheduling and execution of various ETL processes allowed us to simplify years of custom business logic code into a few SQL statementsOverall our new data ingestion pipeline has been running smoothly since we first rolled it out We were able to replace more than 20000 lines of Python code in our legacy system with 5000 in our new harvesters We also eliminated numerous tasks that could fail saved thousands of dollars per month in infrastructure costs and built a system that will scale with our growing business Through this work we are in a much better place to import even more data from 3rd party APIs heading into the new decade
SsasV25sjqQYFv2QdZieKP,One year ago we open-sourced the Prefect engine and the response has been incredible Our lightweight Python framework for combining tasks into workflows has been embraced by users otherwise forced to contend with cumbersome and disappointing tools Today were taking another huge step forward and open-sourcing an orchestration layer for Prefect  including the Prefect Cloud UIYou can get the code and fire it up right now: just prefect server startThe response to Prefect has exceeded all of our expectations The vibrancy of our community is wonderful and the breadth of use cases weve seen has been incredible Weve gotten to work with everyone from climate scientists to astronauts; online educators to World Series champions; single-person data teams to the largest banks in the world One of the benefits of working directly with such great partners is that weve had the opportunity to hone our product with a gauntlet of real-world use cases Despite the variety of applications we see the very same negative engineering frustrations repeated across every environment rediscovering our original thesis over and over: Negative engineering problems are not always complex or sophisticated or difficult On the contrary: they are often minor annoying and repetitiveThis has translated into Prefects insurance-like design goal to be minimally invasive when things run smoothly and maximally helpful when things go wrongCOVID-19 is certainly an extreme example of things going wrong At Prefect weve tried to figure out how we as a company can help The clear solution is to take features that were formerly part of our proprietary platform and release them as open-source Our major motivation is to ensure that individuals and businesses can start taking advantage of our software with the least friction possible As an open-source community this furthers our objective of delivering a best-in-class workflow management experience without compromise; as a business allying ourselves with our users at the earliest possible moment  for free  is the ultimate expression of our commercial directive: deliver value dont extract itPrefect was not started to be a business; Prefect was started to solve a problem The successful company that followed is merely evidence of the size and scope of that problem as well as the value and efficacy of our solution By open-sourcing more of the Prefect Cloud stack we hope to deliver a valuable workflow management system to our users at a moment when saving time and money is especially importantIn true Prefect fashion weve worked to make this as simple as possible and our team has worked hard to ensure that the open-source user experience is on par with Prefect CloudIf you have the latest version of Prefect installed (we released 010 last night) and Docker running simply enter the following in your terminal: Now you can visit http://localhost:8080 to launch the UIIts really that easyWeve expanded the Cloud deployment tutorial to include the local server and will be releasing more tutorials soonAs a business were keenly aware of the potential for tension between our open-source and proprietary products Historically weve tried to maintain a strict rationale for what we could open-source and what we couldnt and today well be updating that mandate in favor of our open-source communityThe Prefect engine has always been licensed under a permissive Apache 20 license and will remain that way forever Following the model of open-core companies we admire we are open-sourcing the new server backend and UI under a new Prefect Community License that allows their use for any purpose including modification or distribution other than directly competing with Prefects own products This model allows us to continue to invest in our commercial relationships while offering elements of our powerful Prefect Cloud service to our open-source communityWe will continue to offer Prefect Cloud our managed orchestration service to organizations that require more than basic workflow management Cloud includes features like our patented Hybrid Model that allows customer code to remain completely secure and on-premise as well as authorization and security settings secrets third-party integrations HA infrastructure and services that ensure workflow semantics are respected even in cases of total failureLast week we informed each of our current customers that the UI a feature previously exclusive to their Cloud licenses would be made open-source Every single reply was in favor of this decision  in fact the most popular response was that if we had done this sooner the decision to evaluate and purchase a Cloud license would have been much simpler! We are grateful to have formed such amazing relationships and we are confident in our ability to deliver a best-in-class experience for both our open-source and commercial productsWeve collected enough datapoints to know that Prefect can immediately drive major cost savings and efficiencies for businesses that adopt it; were also aware that the lack of an open-source UI historically introduced some friction to Prefects adoption In fact a UI is the most-requested feature weve ever had on the open-source repo By releasing these key pieces of the Prefect Cloud stack we hope to make it as easy as possible for you your team or your company to get started with a complete workflow management solution The new open-source Prefect platform encompasses every stage of the workflow pipeline: building testing deploying orchestration monitoring and interventionWere incredibly excited to hit this milestone for our product and grateful to the enormous and enthusiastic community thats made it possible
gPGA9NoAP49MMHADevr5Zo,Data engineers are often responsible for the planning building and maintaining of data pipelines During that process they may face the challenging decision to either create a custom system or use an existing framework In this post Ill address some of the common pitfalls one may encounter when evaluating such an important decision as well as the ongoing consequences of each choiceThe decision to adopt a workflow management system or build one from scratch is daunting The data engineering ecosystem has many frameworks and services available for building pipelines You can expect these systems to handle a wide range of features including being robust to occasional outages handling large amounts of data in real time having meaningful metric handling and much more Some popular choices include Apache Airflow Luigi and my companys new platform PrefectThe advantage of using an existing framework is that the effort required to create new workflows becomes dramatically lower Most of the code is already written! You can incur enormous technical debt trying to replicate that in a custom system especially because youll need to educate your colleagues (and well talk about documentation in a minute!) However if the framework doesnt do exactly what you need then youll probably end up spending a lot of time integrating it with your infrastructure This tradeoff  would you rather code now or code later?  is a critical part of the framework versus custom decisionSometimes a relatively minor detail can be the difference between a framework working for you or not For example you might be really excited about a products headline features like a UI or user permissions but even the most beautiful dashboard wont help you if the framework simply doesnt support the type of distributed computations you need to run or doesnt know how to talk to your data warehouseAn immediate question therefore is what infrastructure and resources do you require  and how hard is it for a framework to address them? Are you using Kubernetes Mesos Swarm or another container orchestration system? Does your pipeline run on AWS Google Cloud Azure or another cloud provider? Many of the popular data engineering frameworks were originally created by a single company to solve that companys own infrastructure challenges and may not be immediately applicable to the data challenges you need to solveFlexibility in a data pipeline is crucial It needs to be developed in a way where both major and minor changes can be made efficientlyA common requirement is adding a step to the pipeline that performs a new transformation: a seemingly simple request but one that many systems cant easily accommodate Some tightly couple tasks in a way that makes it impossible to add new tasks Others have no way to migrate to a new pipeline version in a way that keeps history  and API endpoints  intact Yet others are too flexible allowing changes at any time and providing no guarantee that the same pipeline is running on all workers All of this means that users must often resort to deleting the old pipeline and uploading a completely new one just to add one new taskAnother more drastic example would be changing the data warehouse In some cases this could require a total pipeline redesign However a flexible pipeline would be designed to abstract the database logic into a separate set of tasks from the core data transformations This would minimize the impact of changing those tasks Typically this is only possible in frameworks that have first-class support for dataflowIt helps to think of flexibility as a system of building blocks that are connected with common methodologies This allows each chunk of the pipeline to easily change as development progresses without requiring much (or any) change in any other moduleWhen designing a pipeline with flexibility in mind it helps to refer to the SOLID principles of software design: https://enwikipediaImagine a scenario where you plan out a massive pipeline choose the latest trendy tech to build it with and though you encounter a few small hiccups along the way youre finally there: your awesome pipeline is ingesting real-time data and running smoothly in productionEven if your job description states that you build workflows youll be spending a large chunk of your time maintaining them It sure would be nice if you could just provide the infrastructure define your pipeline and let someone else take over the maintenance… but if you build a custom system you need to be prepared for this realityI dealt with this problem over and over in my previous jobs No software engineer should be surprised that theyre responsible for their work but debugging distributed data pipelines can be a full-time job in and of itself  and thats assuming you bothered to add debug hooks to your custom system! (You did right?) Pipelines are never fire-and-forget projects so you need to remember this when youre planning your approachInfrastructure is increasingly available from large cloud providers but I see many people focusing on code and ignoring maintenance Unfortunately ignoring it doesnt make it go awayIn a perfect world pipelines and workflows would be built with perfect knowledge about how much data they will consume That would be great because you could plan all your resources ahead of time However its often not the caseMost pipelines you build will probably have a fluctuating amount of data being consumed and outputted so you better be prepared to handle it Lets say one of your tasks is slower than the others Along comes a large amount of data Now all steps before the slow one execute quickly and the data gets clogged up at the slow step; this leads to a bottleneck in the pipelineWhat do you do now? Obviously it would be irresponsible and poor design to just wait for that step to complete its backlog of computation This is where a system of scale needs to be planned out and (preferably as to avoid another aspect of maintenance) automatically enabled One way of doing this consists of placing checks which are aware of the amount of data queued up to be processed for that step and scaling the amount of resources accordinglyCluster auto-scaling and web server load balancing are life savers: use them A delay in data delivery is a critical error in pipeline development especially when that pipeline is used in a production environmentIf you build a custom pipeline then youre going to need to support it There are no online documentation examples or previously answered Stack Overflow questions for other developers on your team to take advantage of This becomes an even bigger roadblock when new engineers join your teamIm joking but Im seriousYoure promising your team that your framework will support their needsThe best time to write documentation was yesterday Failing that do it contemporaneously Keeping a running repository of up-to-date docstrings best practices and examples will pay off significantly in the long runDefinitely not me: I dont know I wrote that eight months agoNew engineer: Oh heres some documentation… it says TODO: Comment about why this is hereAgain definitely not me: Ah I guess I just never got around to itDont be definitely not meThis point is a bit of a branch off of the flexibility section When building a custom data pipeline you should hope for the best  but be prepared for the worst Of course this is true of all production software but I believe it is especially important in data engineering where data delivery is crucialLets say you have a step in a pipeline that hits an outside service to get some data and that service happens to be down or maybe the format of the data changes Your pipeline needs to know how to handle cases like this to prevent errors from taking down the system You can accomplish this with mechanisms such as retries error handling queueing and whatever else you can imagine that could prevent the pipeline from stopping due to semi-random eventsWhile attempting to alleviate all possible mishaps it is also important to realize that things just happen to break or go down Sometimes there are forces simply outside of your control and the amount of things you have less control over substantially increase when building a pipeline that is distributed In this case it is important to implement the storage of relevant metrics metadata and loggingKeeping a record of what happens in your pipeline through combinations of recording metadata and logging is crucial This will be a huge help both in allowing you to keep track of pipeline metrics and quickly diagnosing issues that you were previously unaware of You want to be prepared for when the unexpected happensThere are a lot of tools available that are easy to plug into your pipeline which can provide this functionality In the past I have used Grafana for metrics and Logstash for logging You can also take it a step further and use something like Sentry to monitor for errorsThree words: directed acyclic graphsLoops in pipelines are bad There is a reason most (if not all) frameworks available enforce DAGs for pipeline building: cycles introduce unnecessary complexity and make it very difficult to reason about your systemConsider: if you had a real pipeline that processes oil it should never refine the oil and then throw it back into a pipe with unrefined product(…unless thats actually part of the process; I must admit that I am not well versed in petroleum engineering)With that said there actually are ways to build interesting control-flow mechanisms that can mimic loop-like behavior but they require incredible care to get rightWhen building a pipeline there is no one true way It is important to make an informed decision about the total work involved with a custom solution versus the ability of that solution to better support your internal practicesThe point of this post isnt that you shouldnt write a custom pipeline; its that there are many second-order effects to be aware of if you do All pipelines require some custom effort; the real question is how much you actually need to do and what framework will support you bestFor the record we think ours is PrefectThe images in this post are from undraw
HkSZ97H7kksXTvz337bTDY,Prefect has a culture of transparency which involves sharing news  both good and bad  with our employees and investors In keeping with those values and the spirit of open-source wed like to also include the broader community when possible This post is from our March 2020 investor updateExactly one year ago we launched Prefect Core as one of GitHubs trending projects and weve maintained that level of success ever since Just last month we released Prefect Cloud to the public and crossed a major milestone in ARR More importantly our users and customers are excited and engaged to a degree even we did not anticipateI am incredibly proud of our teams work over the last twelve months and we have taken steps to ensure our continued success in this uncertain time COVID-19 represents a unique challenge for every business Fortunately Prefect is well-positioned; in fact we stand to benefit from businesses renewed focus on expense reduction and efficiency Nonetheless COVID-19 provides a good opportunity to reexamine our business plan and improve our standingIn this letter I will review the four key drivers of our strategys success and provide an update on the steps weve already taken to secure our positionThe most important question for our company is the strength of our belief that negative engineering is a huge unsolved problem The answer is emphatically yes; we are showered in evidence that it transcends industry and size We continuously discover new markets and new customers and our hybrid model has allowed us to engage them all Every day we grow more certain that we have uncovered an incredibly attractive if nascent market opportunity COVID-19 will only increase our customers focus on this topic given its implications for wasted engineering spend and we plan to position Prefect as the perfect solutionThe second question is whether we have a winning product and again we have no doubt that we do Our open source community is thriving and Cloud received a perfect NPS score in its first user survey Our competitors have even started referencing us in their designs In light of COVID-19 we will be re-emphasizing our products ability to reduce cost and drive efficiencies We have collected benchmarks that demonstrate teams can benefit from six figures of tangible savings by deploying our software In the last week our open-source user engagement has jumped and we completed our fastest-ever commercial close: just two business days from warm introduction to signed contractNext we need to think about funding and ensuring that our finances are aligned with our corporate strategy I spent my entire career managing financial risk and am always thinking about downside protection It looks different in a company than a portfolio but the core principles are the same: make conservative assumptions prepare for many outcomes and do not be surprised For these reasons Prefect has always maintained a cash reserve and we have never included any revenue in our 24-month budget forecasts Beginning two weeks ago we took aggressive steps to further preserve our capital and extended even our zero-revenue runwayFinally we must ensure that we select a go-to-market and commercial strategy that will best accelerate the growth of our user base and company This is an important topic that frankly weve grappled with in the past especially as market expansion enterprise customers and the hybrid model have all allowed us to continuously experiment with different delivery mechanisms and pricing models Our evergreen debate is how much of our platform should be open-sourced as opposed to retained for our proprietary platform The COVID-19 crisis helps clarify this point because businesses will increasingly choose the system that delivers savings as quickly as possible Consequently our commercial strategy is coming into focus We have decided to release an open-source version of our platform in order to deliver value to businesses at a time that they desperately need itWe call this effort Project EarthIn The Hitchhikers Guide to the Galaxy a supercomputer was commissioned to find the ultimate question of life the universe and everything The computer was called Earth and it was so large and powerful that it was often confused for a planet Unfortunately Earth was destroyed in a cosmic accident and a new Earth must be built to complete the original missionAt Prefect we also have a large and powerful system Prefect Cloud and COVID-19 is our cosmic accident We must reposition our product to maximize its efficacy in a business environment newly driven by cost savings and efficiencies Through Project Earth we will add a version of the Prefect Cloud platforms API and UI to our open-source offering This will address the most common friction we hear and allow a far greater number of businesses to immediately adopt Prefect softwareWe have channel-checked this strategy with current customers and prospective ones and found the response to be overwhelmingly positive An open-source version of Cloud would smooth Prefects adoption in large enterprises while reducing time-to-value for customers with limited time or resources for exploring alternative solutions It provides a more natural on-ramp to enterprise service contracts and critically it will permanently eliminate the sole advantage of our fully-open-source competition Our strategic goal is to use this approach to disproportionately substitute user growth for customer growth in an adverse economic climate For avoidance of doubt we will continue to offer our paid platform and features as well as a new support product for businesses with needs beyond basic workflow managementOpen-sourcing Cloud is a massive undertaking that would normally require months to plan coordinate and execute However we will be ready to release it next week By operating at this pace (typical at Prefect!) our software can become available to any business that needs it at exactly the moment businesses are demanding efficiency the most Allying ourselves with our partners at the earliest possible moment  for free  is the ultimate expression of our commercial directive: deliver value dont extract it Relatedly we are advancing plans to formalize a non-profit/open-source program working with partners like the Special OlympicsThank you for your continued support and please contact me with any questions about our updated strategy or anything else We remain highly enthusiastic about Prefects mission and market opportunity and more conscious than ever about the need for strategic and financial discipline and focus in these uncertain times We hope you and your families remain safe and healthy
WUQoGt7iv7Mkt336pZtYXj,Today is the largest release weve ever done across the Prefect platform including Prefect Core 013 Prefect Cloud and two completely-new open-source projects: Prefect Server and the Prefect UIEarly versions of Server and the UI were previously available inside Prefect Core Graduating them allows us to finally deliver tons of new highly-requested features; four months of bugfixes and performance improvements; new dedicated open-source development hubs; enhanced deployment options; better maintenance and support  and much much moreTrue to our commitment to making software thats easy to use our open-source users still only need to remember one command to launch the entire platform: prefect server startIn this post I will describe the highlights of this coordinated release and how it exemplifies our true open-core business model For a more technical look please see our CTO Chris Whites accompanying post which will be posted shortlyA few months ago we released an initial open-source version of our workflow orchestration platform called Prefect Server As we wrote in March: Open-sourcing Cloud is a massive undertaking that would normally require months to plan coordinate and execute However we will be ready to release it next week By operating at this pace (typical at Prefect!) our software can become available to any business that needs it at exactly the moment businesses are demanding efficiency the most Allying ourselves with our partners at the earliest possible moment  for free  is the ultimate expression of our commercial directive: deliver value dont extract itIn order to release Server on that schedule we elected to fork our proprietary Cloud codebase What this gained us in velocity it sacrificed in maintainability: the amount of code our small team was responsible for instantly doubledNarrator: It was not a good problem to haveThe public response to Servers release turned out to be extraordinary: it instantly became the most popular thing weve ever built Our open-source community and customers both doubled last quarter and show no signs of slowing downWe have always made a major commitment to supporting our users in near-real time as any of our 1200 Slack members can attest However the extreme popularity of Prefect Servers forked codebase has made it challenging for our lean team to deliver that standard Server and its UI have not received the attention they deserve: community PRs lack visibility and we havent found enough bandwidth to port the enhancements we make in Prefect CloudTherefore weve spent the last quarter doing what we said we didnt have time for in March: planning coordinating and executing a proper open-source product launch Our entire team was involved in this effort  from gathering feedback to studying user requirements to writing code to reviewing docs polishing our presentation and more Todays release of a brand-new Prefect Server and Prefect UI is the culmination of that effort and a rededication of our commitment to producing a new standard in workflow automation softwareAnd its just the beginningThe clear highlight of todays release are the two new repos for Prefect Server and the Prefect UI which were previously only available deep inside the Prefect Core source codeFor most users these repos may only exist as a curiosity  there is no need to install or even look at them to run our software For our growing development community however they will provide an excellent experience that we were not able to offer before in a single-repo formatThe new version of Prefect Server is no longer a fork; its a shared codebase with Prefect Cloud This means that improvements our team makes to our flagship product will automatically and immediately benefit our open-source community Conversely our community can help shape the roadmap for Prefect Cloud through a dedicated development hub Cloud will continue to deliver additional features performance and infrastructure to our paying customers but its core will remain fully available Earlier this week we addressed a Cloud customer comment and improved their user experience by making a change to the open-source Server repo It was an exciting moment (were nerds sue us) and perfectly captured our objective of building an aligned open-core businessThe new Server gains a few popular Cloud features that the old one did not include: …and countless other improvements many of them invisible that have been added to Cloud over the past four monthsThe Prefect UI also has its own repo but unlike Prefect Cloud its source code is fully available The UI in this repo IS the Prefect Cloud UI This means that starting today users of Prefect Server can immediately benefit from months of enhancements that have been added to Prefect CloudOne of the new UIs most innovative features is that it can instantly be pointed at any Prefect API whether Cloud or Server by changing a setting in the UI itself This means that Prefect Cloud users can self-host the UI  they can even fork and completely customize it  while still running against the managed Cloud service In addition Prefect Server users who do not want to host a UI can log in to Prefect Cloud and point the Cloud UI at their local Server instance! (Note that no Server data is sent to Cloud when you do thisWe work with many companies that build internal dashboards on top of the Prefect Cloud API; now they can bring their custom analytics directly into the UI We are not aware of any product that has taken this approach before which puts it hand-in-hand with our Hybrid Execution Model as an example of how Prefect has even innovated the open-core business modelWith the release of Prefect Server and the Prefect UI we are dramatically expanding our product lineup to meet the needs of our users Prefect Cloud remains our flagship offering leveraging our Hybrid Model to offer on-prem guarantees through a SaaS model We have invested heavily in its infrastructure performance and security Prefect Server and the Prefect UI allow users to run parts of Prefect Cloud locally This is appropriate for smaller single-node deployments or complete customization And Prefect Core is still the very best tool for building testing and executing data workflowsWith this lineup we are able to significantly advance our mission to eliminate negative engineering We are beginning to publish case studies that show the incredible gains companies can make by adopting Prefect software  such as the 99% decrease in model development time experienced by SymphonyRMIn addition having aligned these codebases our team is now able to offer true commercial support (in addition to our wonderful community channels) We are grateful to all of the partners across our open-source and Cloud communities who helped us reach this milestoneOver the next couple quarters well be growing a lot Open positions are always posted on our website  there are three as I write this  and we will do our best to circulate the job descriptions as widely as possibleIts hard to believe after all the incredible work that went into making todays release possible but this isnt even the biggest news we have to share
V3g4K4dQjZtuBdwyNsAkbX,Airflow is a historically important tool in the data engineering ecosystem and we have spent a great deal of time working on it It introduced the ability to combine a strict Directed Acyclic Graph (DAG) model with Pythonic flexibility in a way that made it appropriate for a wide variety of use cases However Airflows applicability is limited by its legacy as a monolithic batch scheduler aimed at data engineers principally concerned with orchestrating third-party systems employed by others in their organizationsToday many data engineers are working more directly with their analytical counterparts Compute and storage are cheap so friction is low and experimentation prevails Processes are fast dynamic and unpredictable Airflow got many things right but its core assumptions never anticipated the rich variety of data applications that has emerged It simply does not have the requisite vocabulary to describe many of those activitiesThe seed that would grow into Prefect was first planted all the way back in 2016 in a series of discussions about how Airflow would need to change to support what were rapidly becoming standard data practices Disappointingly those observations remain valid todayWe know that questions about how Prefect compares to Airflow are paramount to our users especially given Prefects lineage We prepared this document to highlight common Airflow issues that the Prefect engine takes specific steps to address This post is not intended to be an exhaustive tour of Prefects features but rather a guide for users familiar with Airflow that explains Prefects analogous approach We have tried to be balanced and limit discussion of anything not currently available in our open-source repo and we hope this serves as a helpful overview for the communityAirflow was designed to run static slow-moving workflows on a fixed schedule and it is a great tool for that purpose Airflow was also the first successful implementation of workflows-as-code a useful and flexible paradigm It proved that workflows could be built without resorting to config files or obtuse DAG definitionsHowever because of the types of workflows it was designed to handle Airflow exposes a limited vocabulary for defining workflow behavior especially by modern standards Users often get into trouble by forcing their use cases to fit into Airflows model A sampling of examples that Airflow can not satisfy in a first-class way includes: If your use case resembles any of these you will need to work around Airflows abstractions rather than with them For this reason almost every medium-to-large company using Airflow ends up writing a custom DSL or maintaining significant proprietary plugins to support its internal needs This makes upgrading difficult and dramatically increases the maintenance burden when anything breaksPrefect is the result of years of experience working on Airflow and related projects Our research spanning hundreds of users and companies has allowed us to discover the hidden pain points that current tools fail to address It has culminated in an incredibly user-friendly lightweight API backed by a powerful set of abstractions that fit most data-related use casesWhen workflows are defined as code they become more maintainable versionable testable and collaborativeProduction workflows are a special creature  they typically involve multiple stakeholders across the technical spectrum and are usually business critical For this reason it is important that your workflow system be as simple and expressive as it can possibly be Given its popularity and omnipresence in the data stack Python is a natural choice for the language of workflows Airflow was the first tool to take this to heart and actually implement its API in PythonHowever Airflows API is fully imperative and class-based Additionally because of the constraints that Airflow places on what workflows can and cannot do (expanded upon in later sections) writing Airflow DAGs feels like writing Airflow codeOne of Prefects fundamental insights is that if you could guarantee your code would run as intended you wouldnt need a workflow system at all Its only when things go wrong that workflow management is critical In this light workflow systems are risk management tools (weve written about this before) and when well designed should stay out of users way until theyre neededTherefore Prefects design goal is to be minimally invasive when things go right and maximally helpful when they go wrong Either way the system can provide the same level of transparency and detail for your workflowsOne way we achieve this is through our functional API In this mode Prefect tasks behave just like functions You can call them with inputs and work with their outputs you can even convert any Python function to a task with one line of Prefect code Calling tasks on each other like functions builds the DAG in a natural Pythonic way This makes converting existing code or scripts into full-fledged Prefect workflows a trivial exerciseNot to worry Prefect also exposes a full imperative API that will be familiar to Airflow users The imperative API is useful for specifying more complex task dependencies or for more explicit control Users can switch between the two styles at any time depending on their needs and preferencesTime is an illusion Lunchtime doubly soPerhaps the most common confusion amongst newcomers to Airflow is its use of time For example were you to run the Airflow tutorial you might find yourself running: and wondering what all these different times meanAirflow has a strict dependency on a specific time: the execution_date No DAG can run without an execution date and no DAG can run twice for the same execution date Do you have a specific DAG that needs to run twice with both instantiations starting at the same time? Airflow doesnt support that; there are no exceptions Airflow simply decrees that such workflows do not exist Youll need to create two nearly-identical DAGs or start them a millisecond apart or employ other creative hacks to get this to workMore confusingly the execution_date is not interpreted by Airflow as the start time of the DAG but rather the end of an interval capped by the DAGs start time This was originally due to ETL orchestration requirements where the job for May 2nds data would be run on May 3rd Today it is a source of major confusion and one of the most common misunderstandings new users haveThis interval notion arises from Airflows strict requirement that DAGs have well-defined schedules Until recently it was not even possible to run a DAG off-schedule  the scheduler would get confused by the off-schedule run and schedule future runs at the wrong time! Ad-hoc runs are now possible as long as they dont share an execution_date with any other runThis means that if you want to: then Airflow is the wrong toolIn contrast Prefect treats workflows as standalone objects that can be run any time with any concurrency for any reason A schedule is nothing more than a predefined set of start times and you can make your schedules as simple or as complex as you want And if you do want your workflow to depend on time simply add it as a flow parameterThe Airflow Scheduler is the backbone of Airflow This service is critical to the performance of Airflow and is responsible for: Conversely Prefect decouples most of this logic into separate (optional) processes: Scheduling a flow in Prefect is a lightweight operation We simply create a new flow run and place it in a Scheduled state In fact when we talk about Prefect Clouds scheduler that is its sole responsibility Our scheduler never gets involved in any workflow logic or executionPrefect Flows themselves are standalone units of workflow logic There is no reason for a scheduler to ever parse them or interact with the resulting statesAs proof you can run an entire flow in your local process with no additional overhead: When a Prefect flow runs it handles scheduling for its own tasks This is important for a few reasons: This last point is important While Airflow has support for a variety of execution environments including local processes Celery Dask and Kubernetes it remains bottlenecked by its own scheduler which (with default settings) takes 10 seconds to run any task (5 seconds to mark it as queued and 5 seconds to submit it for execution) No matter how big your Dask cluster Airflow will still only ask it to run a task every 10 secondsPrefect in contrast embraces modern technology When you run Prefect on Dask we take advantage of Dasks millisecond-latency task scheduler to run all tasks as quickly as possible with as much parallelism as the cluster offers (weve written about that too!) Indeed the default deployment specification for Prefect Cloud deploys Dask clusters in Kubernetes (this is also customizable)Besides performance this has a major implication for how flows are designed: Airflow encourages large tasks; Prefect encourages smaller modular tasks (and can still handle large ones)Furthermore when running a flow on Prefect Cloud or with a custom database Task and Flow Runners are responsible for updating database state not the schedulerOne of the most common uses of Airflow is to build some sort of data pipeline which is ironic because Airflow does not support dataflow in a first class wayWhat Airflow does offer is an XCom a utility that was introduced to allow tasks to exchange small pieces of metadata This is a useful feature if you want task A to tell task B that a large dataframe was written to a known location in cloud storage However it has become a major source of Airflow errors as users attempt to use it as a proper data pipeline mechanismXComs use admin access to write executable pickles into the Airflow metadata database which has security implications Even in JSON form it has immense data privacy issues This data has no TTL or expiration which creates performance and cost issues Most critically the use of XComs creates strict upstream/downstream dependencies between tasks that Airflow (and its scheduler) know nothing about! If users dont take additional care Airflow may actually run these tasks in the wrong order Consider the following pattern: This task explicitly depends on an action taken by a push task but Airflow has no way of knowing this If the user doesnt explicitly (and redundantly) make that clear to Airflow then the scheduler may run these tasks out of order Even if the user does tell Airflow about the relationship Airflow has no way of understanding that its a data-based relationship and will not know what to do if the XCom push fails This is one of the most common but subtle and difficult-to-debug classes of Airflow bugsAn unfortunately frequent outcome for Airflow novices is that they kill their metadata database through XCom overuse Weve seen cases where someone created a modest (10GB) dataframe and used XComs to pass it through a variety of tasks If there are 10 tasks then every single run of this DAG writes 100GB of permanent data to Airflows metadata databasePrefect elevates dataflow to a first class operation Tasks can receive inputs and return outputs and Prefect manages this dependency in a transparent way Additionally Prefect almost never writes this data into its database; instead the storage of results (only when required) is managed by secure result handlers that users can easily configure This provides many benefits: Im sorry Dave Im afraid I cant do thatIts often convenient to have a workflow that is capable of handling or responding to different inputs For example a workflow might represent a series of steps that could be repeated for information coming from different APIs databases or IDs  all of which reuse the same processing logic Alternatively you might want to use an input parameter to affect the workflow processing itselfBecause Airflow DAGs are supposed to run on fixed schedules and not receive inputs this is not a first class pattern in Airflow Of course it is possible to work around this restriction but the solutions typically involve hijacking the fact that the Airflow scheduler reparses DAG files continually and using an Airflow Variable that the DAG file dynamically responds to If you must resort to taking advantage of the schedulers internal implementation details youre probably doing something wrongPrefect offers a convenient abstraction for such situations: that of a Parameter Parameters in Prefect are a special type of Task whose value can be (optionally) overridden at runtime For example locally we could have: When running in deployment with Prefect Cloud parameter values can be provided via simple GraphQL calls or using Prefects Python ClientThis provides many benefits: Earlier we noted that Airflow didnt even have a concept of running a workflow simultaneously which is partially related to the fact that it doesnt have a notion of parameters When workflows cant respond to inputs it doesnt make as much sense to run multiple instances simultaneouslyHowever with first-class parametrization its quite easy to understand why I might want to run multiple instances of a workflow at the same time  to send multiple emails or update multiple models or any set of activities where the workflow logic is the same but an input value might differYoure gonna need a bigger boatIn addition to parametrized workflows it is often the case that within a workflow there is some Task that needs to be repeated an unknown number of times For example imagine a setup wherein Task A queries a database for a list of all new customers From here each customer ID needs to be fed into a Task which processes this ID somehow Within Airflow there is only one option: implement a downstream Task B which accepts a list of IDs and loops over them to perform some action There are major drawbacks to this implementation: Because this is such a common pattern Prefect elevates it to a feature which we call Task mapping Task mapping refers to the ability to dynamically spawn new copies of a Task at runtime based on the output of an upstream task Mapping is especially powerful because you can map over mapped tasks easily creating dynamic parallel pipelines Reducing or gathering the results of these pipelines is as simple as feeding the mapped task as the input to a non-mapped task Consider this simple example in which we generate a list map over each item twice to add one to its value then reduce by taking the sum of the result: This workflow execution contains 10 true Prefect Tasks: 1 for the list creation 4 for each of the two add_one maps and 1 for the get_sum reduction Task mapping provides many benefits: It is not enough for code to workRobert CAn important feature of any code-based system is the ability to version your codeRecall that in Airflow DAGs are discovered by the central scheduler by inspecting a designating DAG folder and executing the Python files contained within in order to hunt for DAG definitions This means that if you update the code for a given DAG Airflow will load the new DAG and proceed blindly not realizing a change was made If your DAG definitions change or are updated regularly this leads to a few headaches: In practice this means that teams tend to resort to a combination of Github + the old-fashioned method for versioning: appending version information to filenames Once again this is not a burden if your workflows truly are slowly changing over time However as data engineering has become a fast-paced science embracing experimentation and frequent updates if only to deploy new models and parameters this approach fails quicklyIn Prefect Cloud we have elevated versioned workflows to a first-class concept Any workflow can become part of a version group for easily tracking and maintaining your history As always we have sensible defaults: Both of these settings can be customized if you have more complicated versioning requirements For example you could specify that any flow is a version of any other flow regardless of name or project You could override the automatic version promotion to unarchive and enable old versions (for example for A/B testing) Or you could use versioning simply to maintain a history of your workflow without polluting your UIThe major difference between a thing that might go wrong and a thing that cannot possibly go wrong is that when a thing that cannot possibly go wrong goes wrong it usually turns out to be impossible to get at or repairBecause both Airflow and Prefect are written in Python it is possible to unit test your individual task / operator logic using standard Python patterns For example in Airflow you can import the DagBag extract your individual DAG and make various assertions about its structure or the tasks contained within Similarly in Prefect you can easily import and inspect your Flow Additionally in both Airflow and Prefect you can unit test each individual Task in much the same way you would unit test any other Python classHowever to test your workflow logic can be significantly trickier in Airflow than Prefect This is for a number of reasons: In Prefect on the other hand recall that flows can run themselves locally using flowrun (with retries) or with a FlowRunner for single-pass execution Additionally each of these interfaces provides a large number of keyword arguments designed specifically to help you test your flow critically including a way to manually specify the states of any upstream tasksFor example to make sure your trigger logic works for an individual task you can pass in all upstream task states through the task_states keyword argument; because Prefect returns fully hydrated State objects (which include such information as data exceptions and retry times) you can easily make assertions on the nature of the returned State for the task of interestI want to believeOne of the most popular aspects of Airflow is its web interface From the UI you can turn schedules on / off visualize your DAGs progress even make SQL queries against the Airflow database It is an extremely functional way to access Airflows metadataFrom day one we designed Prefect to support a beautiful real-time UIThe Prefect UI supports: The UI is part of Prefect Cloud because it is backed by the same infrastructure that allows us to deliver production-grade workflow management to our customers However we are committed to making it increasingly available to users of our open-source products beginning with its inclusion in Clouds free tier We are working on other ways of delivering elements from the UI to our open-source usersIf I have seen further than others it is by standing upon the shoulders of giantsAirflow popularized many of the workflow semantics that data engineers take for granted today Unfortunately it fails to meet companies more dynamic needs as the data engineering discipline maturesPrefect is a tool informed by real use cases collected from hundreds of users that addresses the changing needs of the industry It is an engine for enabling arbitrary proprietary workflows exposing a vocabulary well-suited to a wide variety of data applicationsIf youre curious to learn more please check out our docs or visit our repo
2Y7H4GkBDrrh422YWvCdoD,The Telegraph is a 164-old-company where data has always had a central role With the advent of the cloud and the need to build a platform able to process a huge quantity of data in 2015 we started to build our big data platform We decided to use Google Cloud and since delivering our first PoC over the years we have kept improving our platform to better support the businessDuring the last 4 years I had multiple discussions on how to handle data transformation or more extensively (ETL) Extract Transform and Load processes The number of tools that you can choose on the market is overwhelming and committing to the wrong technology could have a negative impact on your capabilities to effectively support different business units and drive decisions that are based on reliable figuresAt The Telegraph the datalake has been built on the top of Cloud Storage and BigQuery and according to Google the natural choice to perform ETL or ELT should be Dataflow (Apache Beam) For most companies this might be true But when you go outside the general use-cases exposed in the getting started guides and you start to relate with real-world challenges what is supposed to be an easy choice might be not so easyIn our case adopting Apache Beam has proven not to be the easiest solution for the following reasons: As a side note we adopted Apache Beam in a second phase but only for real-time data pipelines In fact in this specific case having a windowing strategy and being able to perform operations on a stream of records was paramount for the success of certain projectsThe second product you might be tempted to use if you are using Google Cloud Platform (GCP) might be Dataproc If you already have a Spark cluster or Hadoop in place and you want to migrate on GCP it would make perfect sense to consider this option But in our case we only had a small Hadoop cluster and to rewrite the logic of the pipelines that were running there was not a problemThe third product that we considered and even used for a while is Talend (free version) If your company wants to fully commit to Talend and buy its enterprise version then it is a great choice but if you dont have a strong enough case and you decide to adopt the free version you might face some of the following challenges: For the reasons above we considered building our own Python ETL library as a wrapper of functionalities provided by Google and AWS in order to make our life easier when interacting with the main services of both clouds Even this approach has proved to be far from perfect Due to the effort required to design and develop our own library and all the maintenance required to keep it updated and include new features we started to look for something that could integrate well with this approach and reduce the scope of the libraryIn June 2019 we started to test DBT for the transformation part with the idea of continuing to perform Extraction and Load using the Python library and relying on Apache Beam for real-time data processingDBT (Data Building Tool) is a command-line tool that enables data analysts and engineers to transform data in their warehouses simply by writing select statementsDBT performs the T (Transform) of ETL but it doesnt offer support for Extraction and Load operations It allows companies to write transformations as queries and orchestrate them in a more efficient way There are currently more than 280 companies running DBT in production and The Telegraph is among themDBTs only function is to take code compile it to SQL and then run against your databaseMultiple databases are supported including: DBT can be easily installed using pip (the Python package installers) and it comes with both CLI and a UI DBT application is written in Python and is opensource which can potentially allow any customization that you might needThe CLI offers a set of functionalities to execute your data pipelines: run tests compile generate documentation etcThe UI doesnt offer the possibility to change your data pipeline and it is used mostly for documentation purposesIn the image below you can see how data lineage of a certain table is highlighted in DBT UI This helps to quickly understand which data sources are involved in a certain transformation and the flow of the data from source to target This type of visualisation can facilitate discussion with less technical people who are not interested in the detailed implementation of the process but want an overall viewInitialising a project in DBT is very simple; running dbt init in the CLI automatically creates the project structure for you This will ensure that all engineers will work with the same template and thereby enforces a common standardDBT also offers the maximum flexibility and if for some reason the project structure produced doesnt fit your needs it can be customised by editing the project configuration file (dbt_projectyml) to rearrange folders as you preferOne of the most important concepts in DBT is the concept of model Every model is a select statement that has to be orchestrated with the other models to transform the data in the desired way Every model is written using the query language of your favourite data warehouse (DW) It can also be enriched using Jinja2 allowing you to: Below an example of a model using BigQuery Standard SQL syntaxIn this specific case Jinja is used to inject in the output a set of technical rows that are not present in the source Jinja allows iteration on the matrix of values and reshapes each row in a way that can be included in the main table enabling you to keep your query more conciseThe output of each model can be stored in different ways depending on the desired behaviour: Every DBT model can be complemented with a schema definition This means that the documentation lives in the same repository as your codebase making it easier to understand every step of what has been developed Also since the documentation is always in front of the engineers who are working on the pipelines and since it is directly generated from the codebase minimal maintenance is required to keep it up to dateIn data-driven companies ensuring the quality of the data delivered to the business is always of paramount importance DBT helps to serve high-quality data allowing you to write different typologies of tests to check your dataSimple tests can be defined using YAML syntax placing the test file in the same folder as your modelsFor this specific example the defined tests are: More advanced testing can be implemented using SQL syntaxThe query below ensures that the bk_source_driver field from model fact_interaction doesnt have more than 5% of the values set as NULLModels on DBT rely on the output of other models or on data sources Data sources can be also defined using YAML syntax and are reusable and documentable entities that are accessible in DBT modelsIn the example below you can see how it is possible to define a source on top of a daily sharded BigQuery tables It is also possible to use variables to dynamically select the desired shard In this specific case the execution_date variable is passed in input to DBT and defines which shards are used during the transformation processDBT offers also the possibility to write your own functions (Macros) these can be used to simplify models but also create more powerful queries adding more expressive power to your SQL without sacrificing readabilityThe macro in the example below is used to unite multiple daily shards of the same table depending on the execution_date variable passed and the number of past shards we want to take into considerationThe Telegraphs data engineering team has tested DBT (Core version) for the past two months and its proved to be a great tool for all of the projects that required data transformation As a summary of our experience here is a list of the tools pros and consPros: Cons: If you are interested in practical tips to get the best out of DBT have a look at this series of articles: Part 1: https://mediumcom/photobox-technology-product-and-design/practical-tips-to-get-the-best-out-of-data-building-tool-dbt-part-1-8cfa21ef97c5Part 2: https://mediumcom/photobox-technology-product-and-design/practical-tips-to-get-the-best-out-of-data-build-tool-dbt-part-2-a3581c76723cPart 3: https://mediumStefano Solimito is a Principal Data Engineer at The Telegraph You can follow him on LinkedIn
2VTwdW8yfD9R943eJtis7P,Technology enables publishers to measure the impact that a piece of content is having as soon as it becomes public These days reacting to this data is a vital part of promoting quality journalism in the sea of online articles competing for our attention The real-time understanding of how a story is performing can significantly help to improve the customer experience on both our website and mobile apps Its important to know what our registrants and subscribers want to read and how we can deliver articles that are relevant to our audienceUnder this premise in 2017 the data team was challenged to build a real-time dashboard to display in the newsroom (pictured below) to show which articles were driving registrations and subscriptionsThe first step was to identify a reliable data source on which we could build our analytics The Telegraphs entire website ran on Adobe Experience Manager and for this reason we decided to consume the Adobe Livestream API in order to ingest behavioural information as soon as it was collected Unfortunately since no post-processing was applied to those data sets filtering out the noise and retrieving only relevant records posed a challengeWe took an Agile approach and built a simple proof of concept (PoC) to establish that from this specific data source it was possible to extract meaningful analytics We came up with the following designA poller consumes the live stream of data and without any transformation writes record by record in Pub/Sub Then a Dataflow real-time pipeline consumes the queue and filters out irrelevant records The rest of the data are cleaned and enriched before being uploaded into Elasticsearch One of the beauties of Dataflow is how clean the data transformation process looks once the code is deployed on Google Cloud A flow diagram is automatically generated that shows the different logical steps implemented in the pipelineIn this way it becomes easy to identify bottlenecks and errors in your process Also all the generated logs are automatically available in Stackdriver which takes care to monitor the application and alertIn Elasticsearch only a rolling window of 8 days of data is kept while all the history is available in real-time in BigQuery with the possibility to plug this into the top DataStudio dashboards There are multiple reasons why we decided to adopt Elasticsearch for this specific use case: Less than a couple of months since we started the proof of concept a basic Kibana dashboard was readyFigures in the dashboard above are purely illustrativeThe solution went live in September 2017 and despite some limitations it was really well received by the newsroomBy the beginning of 2018 the product had already been tested for a few months and most of the stability issues intrinsically linked to real-time data processing were solved Due to the high scalability of Pub/Sub and Dataflow spikes the handling of requests on our website to show how content is performing had become trivialWe decided at that point to move further and build our bespoke dashboard on the top of the same backend system A few months later a second version of the dashboard with richer information was releasedFigures in the dashboard above are purely illustrativeDuring this second iteration we decided to remove Kibana and decouple the visualisation from the storage through an API developed in NodeJS and using GraphQL This was actually one of the first times that we played with GraphQL at The Telegraph and it was a pleasant surprise since it allowed much more flexibility We moved away from a rigid contract with multiple endpoints in favour of a simpler approach with fewer endpoints and a clear schema allowing us to extract and filter data from Elasticsearch in a cleaner way Below is the updated designAfter this second release we decided to undertake a new challengeIts one thing to have a dashboard displayed on a big wall that doesnt allow much interaction but quite another to have a product that allows users to conduct real-time exploration of how our content is performing The idea of Pulse was bornThe PoC phase was officially terminated and we started to consider Pulse as a product with a well-defined roadmapA new team led by our Head of Data was created with the right mix of UX designers data engineers and frontend developers We ran a few workshops with different business users to understand the needs and priorities of the newsroom After a couple of weeks the first designs were readyOnce these sessions were concluded it was clear which metrics and dimensions were relevant to measure the performance of our articlesLuckily from a backend point of view the changes to the design were minimal since we were starting from already a strong base but most of the features requested would have required us to massively extend the solutionOnce we finished collecting the requirements and we had a clear understanding of what we were trying to achieve we updated the architecture as shown belowIn this third phase it was not possible anymore to rely on a single data source to serve the data Next to Adobe live stream we added Chartbeat while Adobe post-processed Hitlog and the Unified Content Model (UCM an article storage platform developed in house by The Telegraph Engineering team)The new integration with Chartbeat has been developed in order to offer metrics that was not possible to track through Adobe analytics An example of this might be the average engaged time on a page for a specific audienceThe post-processed Adobe Hitlog was added in order to offer a historical view of how our content was performingAside from the new data sources further development work was necessary The API used to serve the dashboard was rebuilt from scratch using Python and GraphQL to conform with the stack of technologies that we normally use A new Redis cache was introduced to improve the response time and offer a smooth experience to the end user The real-time data pipeline that consumes Adobe live stream data was updated to include the new metrics and offer better data cleansingThe need to also classify our articles through a set of tags in near real-time led to a hybrid design where both real-time near real-time and batch data pipelines coexist For this purpose a tags data pipeline was developed This specific pipeline runs every N minutes and for each article published on the day it checks if a set of conditions is satisfied in order to classify our content accordinglyThe frontend has been built from scratch as well Since we didnt have anything in place yet our frontend team started from a blank canvas and in record time developed a responsive dashboard that offers the possibility to our users to explore the statistics of each article or section under a set of predefined filtersFigures in the dashboard above are purely illustrativePulse went live at the beginning of 2019 and it is now part of the tools that are constantly used by our journalistsIn next weeks (from writing this blog) we will release Pulse XL to replace the old editorial dashboard This will introduce a historical data view geographic information in our main dashboard and also will uniform all our real-time dashboards under the same productRegardless of whether you are on mobile on desktop or in The Telegraph newsroom Pulse will provide support with reliable figures on our strategyPulse has changed our newspapers attitude to data; we are placing more confidence and trust in the information captured about our content Put simply we have one of the best available pieces of technology for capturing and analysing the stories that we publish in real-time Pulse flags segments such as engaged registered visitors then prompts journalists on how to convert them to subscribers in real-time This will be customisable for every team across editorial to ensure all content is achieving its purpose and contributing to the Telegraphs broader strategy
dGJvJaQAUos4fQPpKhMfRD,Every day in THRON we collect and process millions of events regarding user-content interaction The reason we do so is because we enrich user and content datasets analise the timeseries extract behaviour patterns and ultimately infer user interest and content characteristics from those; this is done to fuel lots of different cool benefits such as recommendations Digital content ROI calculation predictions and many moreIf we only had to use such data to present shiny BI reports it would have been a simple matter but since this data has to fuel real-time recommentations and other user-facing services we need to update both model and data in real-time and be able to query results with sub-second query times (well… hundredths of a second time actually) We have also to grant a 2 year retention period and for every update on data model we need to reprocess all past events to ensure an accurate customer profile Such requirements led us to focus on systems that can scale horizontally in a cost effective wayBeing a multitenant system we cannot even optimize the data processing for a specific workload (based on specific velocity variety and/or volume) because it has to accomodate any workload of all our customers at the same timeIn THRON our primary language is Scala and we have been using it from the beginning to develop most of our services When we started building our own recommender evaluating Spark was natural and we found it very flexible to develop and thanks to AWS EMR which provides out-of-the-box Spark support we had the chance to scale it without the need to know how to configure an Hadoop cluster and at the same time we could easily integrate Spark applications in our existing AWS Data Pipeline jobsAs a big benefit we could use lots of big machines for the enrichment process for a limited amount of time and keep costs under control by leveraging EC2 spot instances The output of spark processing is then loaded into ElasticSearch clusters to achieve outstanding query performances Unfortunately at the time we tackled this problem only RDD data structures were available on Spark (nowadays you could use DataSets and DataFrames which are more powerful and easier)From this experience we learned some lessons that we think you might appreciateOur data is stored in S3 with different data formats: events in parquet users in plain JSON etc In the enrichment phase we need to join the events with the other data to get the results Events have a different cardinality compared to the other data sources and at first we were not reading all the events together but we were using batches of few daysA result of those 2 issues was that we couldnt scale the application horizontally: adding machines was not improving the performance because we were just increasing the idle timeBefore digging into complex optimizations ensure you can minimise the cluster idle timeTo avoid the idle time between batches we naively tried to read all the data in a single step instead of reading it in multiple batches We failed because we realised our data had a quite serious skew problem Unskewing data allowed us to distribute it in a more even way across nodes and basically remove the idle times increasing cluster efficiency and allowing horizontal scale to bring benefitsDont be afraid of launching a lot of nodes: its better to have many nodes for a short time than having few nodes for a long timeWhen you work with a lot of data its very important to know how your data is distributed and your application must be designed accordingly Join is a very common transformation with Spark and if you are using hash join your data will be distributed in the nodes based on the hash of the join key: every record with the same hash key will be stored in the same nodeIf your data is skewed you will probably end up having two different scenarios: I am not sure whats the worst one but since we are unlucky we first encountered the former and after the latterWhen you need to join (or distribute your data across the nodes based on the hash key) you need to know if that key is well distributed and the maximum cardinality fits into a single nodeIn case the keys arent safe to be distributed across the nodes you can always cheat by adding a step to analyse the dataset and add a salt in the unskewed keys: this way you can safely join the data and the application parallelism will gain in efficiency because all tasks will have most likely a similar size and durationIf you have searched how to optimize RDD join in Spark in Google you probably already know what I mean: when you join a big RDD with a small one broadcasting the smaller RDD in all nodes and then performing the join is very fastWhats the drawback? If your small RDD at a given time is not small anymore you will be presented with a memory problem in all your nodes and you need to update your code to handle the RDD properly Thats what happened to usWe now pay a lot of attention to small RDDs and we must be 100% sure their size will not grow in limit cases before leveraging the broadcasting jointhe executor and driver default memory allocation is just 1GB and when you reach this limit the log message will suggest you to increase the memoryOverhead (a memory safe margin used by Spark) instead of increasing the memory allocation Remember to always review Spark settings some parameters depend on the machine characteristics like memory and cpu others depend on your application like the parallelismWorking on parallelism may be used to lower memory usage ensure you set the parallelism considering the size of your datasetA rule of thumb regarding memory settings configuration: its a good starting point to use for the Spark driver the same configuration as the executorsOne of the common task in Spark is to read or write data from a data lake and if you use AWS you will read from and write to S3 Its VERY simple to write the output to S3 the only thing to do is to specify the output location in the form s3://bucket/prefix What we realised is that if you do so even after the application has finished to compute the result the cluster will still keep running doing nothingThis is because each Spark task writes the output to a temporary path and then will move the files into the destination folder at the end of the write process S3 is not a file system and there is no native move operation so what happens under the hood is that it creates a copy of the file in the destination folder and then (after the copy has finished) it deletes the source file This step could take a long time and it looks like it isnt executed in parallel even if you have to move just a few files To avoid this we found that its quicker to write the application result in HDFS and to add a second step to copy the files from HDFS to S3 using S3DistCPWe went from over 1 hour copy time to about 2 minutesIn the public cloud world this translates to both time and money savings because you pay for your cluster uptime even when it does nothingIn Spark you can use many different file formats for input or output (text CSV JSON Orc Parquet etc…) compressed with a wide range of encoders (bzip gzip etc…) and its not wise to choose randomly among the different file formats or encoders the format should depend on how data is usedYou should consider that not all compression formats create splittable files: GZip as an example is very fast but its not splittable so when you have a single big file the read process cannot be parallelised We found that we had better performance by keeping the file uncompressed and let Spark parallelise the readAnother consideration was how we read the output One of our data consumers is AWS Athena and we chose Parquet because it stores compressed data in a columnar format Athena has a price model based on GB read Using columnar format saving costs Athena query performance improves too because each query will address just the data it needs without needing to scan the whole datasetWe never successfully wrote a Spark application at the first try Spark application design can be very complex and unusual so we often have to update the application logic to improve its scalability with the application workload: Writing an application that brings the correct result on a sample dataset is easy but delivering the same result with lots of data and distributed computing is just a different thingWe found very useful writing a lot of unit tests: every time we have to optimize the computation logic and refactor code we make sure we dont change our job result by simply running the tests Validating this without automatic testing would be really expensive and error prone this is why we spend extra time and effort to create many unit tests for Spark applicationsIn these years we have learned a lot of lessons about writing and managing Spark applications and we are still learning a lot One of the evolutions we plan to undertake in order to further improve the performance and scalability of our code is to move the application that uses the old RDD with the newer DatasetsWe are also developing an even more complex application with many different inputs and we want to use Spark accumulators to track how many records we are discarding to increase the input data validationWe also plan to integrate in our applications some tools to analyse spark runs (such as Sparklens) and create better tests to catch issues without relying on huge test datasets
axSjn4SYL2SjiLCCKYTm4A,At Tile data is at its heart of everything we do The Tile community has grown from 4 million daily active Tiles (Sept 2018) to 56 million daily active tiles todayThese growing numbers and successful product launches also imply an exponential and abrupt increase in data volume This calls for scalable robust and effective data pipelining to do near effortless data analysisData Science Hierarchy of Needs (shown below) most appropriately demonstrates how important a solid data engineering foundation is for building out effective metrics analytics experimentation and machine learning systemsAt the bottom of the pyramid is data collection Since Tile works in a unique space where we control the hardware design the variety of data is richer than a typical software company and so are the challengesThe frequency at which data points are collected varies from sub-seconds to once a week Airflow is our tool for managing jobs scheduling Multiple incoming data sources implicitly mean more points of potential failuresWhich brings us to the 2nd layer from the bottom of the data needs pyramid (Refer Figure 1) This also is one of the most important components of Reliable data flow infrastructure Airflow has been the tool to help us achieve thisNext comes processing (aka ETL) We rely on Apache Spark to do most of the ET (Extract Transform) We use spot instances on AWS heavily to achieve this at a minimal possible expense A major part of this phase also involves data cleaning This is considered one of the most infamous jobs yet everyone is doing it under different titles (Data Engineers/ Data Scientists/ Machine Learning Engineers/ Analysts)For all the storage requirements we rely on Amazon S3 Besides using standard storage we also use glacier storage to help reduce overall costOnce we have cleaned the data we move on to phase-3 ie Anomaly Detection We use a own homegrown system for data monitoring (metadata and data) The system was contributed by Ben as part of his 2018 Tile internshipFor Analytics we rely on mighty Presto and Hive metastore combination deployed on EMR If you have not used this combination earlier you can think of it as shown in Figure 4 We use Tableau Mode and Grafana for all of the reportingZooming out a bit the current infrastructure which serves Tile data analytics is as shown belowTying it back to the Hierarchy of needs (Figure-1) we are successfully accomplishing (mostly) the lower 5 strata of the pyramid We have achieved this with continuous improvement and proactive issue resolutionWe at Tile have taken a conscious approach towards building the pyramid correctly We are building it with a bottoms-up approach We are not trying to plug in data thats dirty & full of gaps that spans years but not understood yet We are looking forward to 2019 to unleash the power of quality dataAnd more … We will publish more as we get closer to some of the challenging problems we are solvingWed love to have some help  see our current open Engineering positionsIf you would like to learn more about any of these components in details free to reach out to me
2vXcYgM8TgRE6mdo2r5yyZ,Tobias Macey of the Data Engineering Podcast interviewed Timescales Co-founders Ajay Kulkarni & Mike Freedman about how Timescale was started the problems TimescaleDB solves and how it works under the coversHeres a quick summary: As communications between machines become more commonplace the need to store the generated data in a time-oriented manner increases The market for time-series data stores has many contenders but they are not all built to solve the same problems or to scale in the same manner In this episode the founders of TimescaleDB Ajay Kulkarni and Mike Freedman discuss how Timescale was started the problems that it solves and how it works under the covers They also explain how you can start using it in your infrastructure and their plans for the futureIf youd like to learn more about the history of TimescaleDB or if you just enjoy podcasts on data engineering and related technology then please give it a listen (And if you like it wed appreciate a shareQuestions or comments? Please feel free to reach out via email on Twitter our join our growing Slack Community
6grFRybm99nCwCHvHmEx9x,As one of the biggest Indonesian technology companies Tokopedia has hundred of million users who fulfill their daily needs through various services it offers One of Tokopedias DNAs that embodies in our daily work as Nakama* is to Focus on Consumer as each of them is special and cherished However with the magnitude of our user base the challenges we face to engage and embrace them are enormousConsider a hypothetical case of John from Tokopedia Internet Marketing team who is in charge of building millennial user retention and at the same time promoting trending products from our merchantsTo solve the problem and in turn reinforcing data-driven culture we need a tool that can assist us in understanding our customers more Specifically customers are not only limited to buyers; they can also either be merchants or any partners within Tokopedia ecosystem The tool must be capable of organizing the data of million Tokopedia customers efficiently and at the same time supports multiple use cases of data retrievalCustomer Data Platform (CDP) collects and processes data from multiple sources and unify them in a single data platformThis collection of customer profiles is made accessible to other systems supporting multiple use cases from company-wide stakeholdersAn example of its utilization would be as a data source for the Data Scientist team to build predictive models based on the users individual preferences and their spending patternsThe internet marketing Johns use case mentioned above can take advantage of the segmentation service that comes as the features of CDP The purpose of this service is to support marketing decisions based on certain customer criteria divided into three types of the following data: To elaborate on how or what the service does we will get back to John who plans to create a campaign to target urban millennials who are inactive for the last three months He needs to get the list of users belong into the aforementioned segments and send them some special deals with products that may draw their interest John will input age (User Data) and activity (Behavioral Data) criteria in the segmentation dashboard that connects into the Segmentation Services The service will handle the request and John will be notified once the process is complete and results availableThe rest of this post will explain the technicalities of process taking place in the background while John is waiting for the segmentation resultIn general the CDP Segmentation Service is divided into the following parts: Data from multiple origins are collected cleaned and transformed into a single customer profile database We utilize existing Tokopedia ingestion data platform doing the data processing job on the data lake and manage separate storage using Google BigQuery for segmentation objective in normalized data form Cleansed and normalized data is appended on a daily basis using Apache AirflowThe service is intended to accommodate broad use cases by different teams across the company Consequently the user may have different roles and therefore disparate access to the data To ensure that the user is authorized an audit in the finest granularity must be enforced Access to our customers data is regulated to the column levelThe mechanics of the main segmentation system is abstracted in this part including the normalization and translation of user input to the database-specific language We use the typical Golang Postgre and Redis stack Additionally the service also logs and monitors all of the segmentation activity using Prometheus and Grafana in this moduleActs as the last layer of the service executor abstracts the data storage from the logical layer and will perform data retrieval job defined by the user-specified criteria Big Data capable tools are presented in this layer At the end of the job a notification will be sent to update and notify the status of the taskAs each of the components mentioned is decoupled from one another this setup provides scalability to the system Changes in one layer are isolated and each of them comes with an extensible set of tools it supports The executor layer for example supports multiple ways and tools to retrieve data from our customer-profile database You can think of this as strategy pattern in system levelJohn has the list of users he plans to engage in a blink of an eye and sending them the promotional emails that they might be interested in He empowers Tokopedia merchants and at the same time creating engagement with the potential buyers Segmentation Service as part of Customer Data Platform is an essential tool in the customer-obsessed culture and data-driven organization Provide the team with the right set of technology solutions and they will help to retain both buyers and sellersTokopedia Data Team does democratize access to knowledge We build cool stuff that empowers our customers
ibu2QFe7cfmrre594JYeXJ,Talking about hub of course it cant be separated from one of network technologies called hub network Hub network is one of technologies for connecting multiple computers The function of hub is also as a center line for distributing a package that sends items into computer network Basically this paradigm can be found in our daily lifeIt is clear that the items are not sent directly to buyers Instead the items will initially be collected in central office to be checked and recorded The central office then decides when each item is scheduled to ship who the courier is and where the destinations areWe have many partners for marketing purposes They usually have their own services and we have to feed data to them At first every event-data is sent directly by each platform to one or more partners such as tracking partners You can imagine if event-data in each platform needs to be published to some partners we have to update the data in every platform And if we add a new partner it will be more complicated because we should update the platform by using SDK or library that are required by partners As a result it makes the application size bigger That is why we need Data-hub to distribute data using one centralized systemData-hub is a centralized system in which the data from platforms will only be published to one destination Those data will be managed completely by data-hub With data-hub a platform can publish and stop the data from its partners easily even if we add new partners And the most important thing is that we can copy and publish the data to many partners directly through data-hubData-hub is not a portable machine but some servers that work together as a centralized system for user to publish data Data-hub is designed to handle variety of data So we build this platform as flexible as possible by using generic method for parsing and formatting the original dataData-hub server works based on metadata in which it always checks metadata out for each dataMeta-hub is an information from data that is managed by data-hub At first we have to register the data in meta-hub in order to be completely managed by data-hub The functions of meta-hub are : Data is important asset we have to make sure that data is managed well Data-hub is prepared for handling Volume Velocity and Variety of data With data-hub we can reduce publishing from platform side so platform will not be burdened to publish many data into partners
bW8MeCJhgoVxV4mo7R6MZg,Less is more L MA s some tech aficionados eloquently put it we are now entering the Age of Context an age in which the joint effect of several technological trends (big data social networks mobile phone sensors) promises the biggest revolution in consumers life since the InternetWhen used correctly the unprecedented wealth frequency and quality of these data streams would power a more personalized and anticipatory experience for all usersAt Tooso we are building a next-generation search and discovery platform in our mission to make online shopping a seamless and customized experience To achieve this goal our whole infrastructure needs to support real-time and almost real-time processes at scaleMost readers will be already familiar with the most famous web analytics platform on Earth Google Analytics (GA): by simply dropping some Javascript into your website Google gives you a nice app with all sorts of insights about how users interact with your pages GA use cases are not unique though: any service  like ours  that needs to collect browsing data efficiently and tell users apart (is X the same user that was on page Y yesterday? What is the ratio of new/returning visitors? etc) will need its own web analytics pipelineWhen designing the third major update of our APIs we decided to completely revamp our protocol to collect and ingest data: can we build our own Tooso Analytics by relying entirely on AWS PaaS (Platform-as-a-Service) services? In this post we are sharing our code infrastructure and tooling choices hoping that our experience will save you some time when making your own decisionsWe start with a small overview of pixels and use cases for web analytics: if you already are a web ninja feel free to skip to the next sectionAdvertising is the greatest art form of the 20th century MWeb analytics start with pixelsWhat is a pixel? While real-time personalization for online shopping is still a cutting-edge frontier everybody is familiar with pixels through at least one product: online advertising (which explains why they have such a bad rep these days)Lets start with a classic (and somewhat simplified) example You go to awesomeBookscom and take a look at book XYZ then you go to your favorite social network mySocialcom and you get an ad showing you precisely XYZ How does that happen? It happens through a pixel a 1x1 transparent gif (invisible to the final user) that gets loaded by both awesomeBookscom and mySocialcom through (and this is the trick) a third domain say gugolAnalyticscomWhen loading the gif from gugolAnalyticscom the webpage sends a normal request to the remote server complete with parameters containing user information In response to that gugolAnalyticscom sends the 1x1 image and a cookie to your browser: it is by using these data that you can get on mySocialcom an ad based on what happened on awesomeBookscomObviously pixels can be used without any advertising purposes as it is the most unobtrusive way for online services to collect information needed to improve their productsFor example at Tooso we use data from pixels to improve our AI models and enhance our understanding of how users browse our partners websites and personalize their experience accordinglyMore generally as mentioned any online service that relies on the ability to send real-time information from client to server can use a pixel-like architecture to satisfy its use cases Now that we know what happens in the browser lets detail a bit more the requirements for an end-to-end solutionWhile pixels are very important they are just the front-end piece of the puzzle: a data platform needs back-end components to reliably store data for further processing  in our case do our data science magic Adding a bit more details to our previous example a general pixel architecture will look like the following: Users visit a website that loads a Javascript library from a CDN (say trackinggugolAnalyticscom); the library will make a pixel request to gugolAnalyticscom/pixelgif as discussed before (so it will pass browsing information in the request and it will get back a 1x1 gif and a cookie) Once data are exchanged  thanks to the magical pixelfrom client to server the backend can process everything and power all the analytics use cases as neededPutting together front-end and back-end any analytics platform needs to satisfy three basic requirements: While you may think setting up and maintain all this is a devOps nightmare you will be delighted to know that you can add an analytics library to your services without deploying a single serverWe will sketch our solution in the next sectionSimplex Sigillum Veri TLP (5As promised our solution is completely in the spirit of todays AWS PaaS offeringOur architecture overview should be enough to give you a clear idea on how to start your own PaaS solution There is a bunch of notes mistakes we made and random thoughts (in no particular order) we would like to share before wrapping up hoping some of these considerations will save you some time or highlight some important point for your use casesIf you have question feedback or comments please share your serverless story with jacopotagliabue@toosoaiDont forget to get the latest from Tooso on Linkedin Twitter and InstagramFabio Melens exceptional ingenuity was behind every single good idea in the development of the new platform: if we moved pretty fast and broke not that many things its mostly thanks to his talent and visionBen from Queens has been invaluable in understanding some AWS subnet intricacies and saving us countless sleepless nights: he was indeed the devOps hero we needed (but Queens still deserves him more apparently)Katies help as usual scaled up automatically to give us her timely invaluable feedback on earlier versions of this articleFinally many thanks to Ryan Vilim and Ang Li for very helpful comments on previous drafts of this post
BRbWmz3hzB36T2vAa866Kn,As an Analytics Architect and a Technology Advisor working with big data cloud platforms and analytics technologies the aim is to write succinctly about some noteworthy technologies/tools The posts will introduce you to new technologies talk conceptually about them and will have occasional quickstart guides  straight to the point
o93wxz3r239kbgQshyebCt,"Data quality issues may ruin the success of many Big Data data lake ETL projects Whether the data is big or small the need for data quality doesnt change High-quality data is the absolute driver to get insights from it The quality of data is measured based on whether it satisfies the business by deriving the necessary insightsIn this blog we are going to see the steps to ensure the quality of data is correct when you migrate the data from source to destinationWe have migrated the data from MySQL to Data Lake The quality of data needs to be verified before it is consumed by downstream applicationsFor demo purposes I have read sample customer data (1000 records) in Spark Dataframe Though the demo is with a small volume of data this solution can be scaled to the humongous volume of dataThe same data in two Dataframe so our validation framework will be a green signalI have purposely modified the data in the last 2 records in the 2nd data frame so that we can see how this hash validation framework helpsLets see the steps in reverse order as it is the core part of this validationIn this example I have chosen SHA256 there are some other hashing algorithms as well like MD5Hashing algorithms are functions that generate a fixed-length result (the hash or hash value) from a given input The hash value is a summary of the original data Check the SHA-256 hash values in the online hash generatorWidely used in digital signatures Authentication indexing data in hash tables detecting duplicates or as checksums (to detect if a sent file didnt suffer accidental or intentional data corruption)But here we are going to see how we are going to leverage the power of hashing in Big Data ValidationCollision or clash is a situation that occurs when two distinct pieces of data have the same hash value checksum fingerprint or cryptographic digest It is possible to generate the same hash code for different strings However the probability of two strings having the same hash is very low unless there is a brute force attackThe impact of collisions depends on the application in our scenario we are handling those collisions in another validation step (4  Stats Comparison)When we have millions of records and 3000+ columns it is tough to compare the source and target system for data mismatch For doing this we need a lot of memory and computation power engines To address this we are using Hashing to concatenate all the 3000+ columns into one single hash value column which is just 64 characters in length This volume is negligible when you compare 3000+ column length and sizeBy leveraging the power of hashing without exploiting huge resources we came to know there are 2 corrupt records(primary key  999 1000) Now by filtering only those 2 ids we can compare these 2 data frames to get the corrupt valuesFurther drilled to get only the mismatched columns and their corresponding values in both data frameIn rare case there can be a collision in hash validation which may lead to corrupt data This can be avoided by calculating stats on each column in the dataCompare the summary stats on two data frames to find a mismatch Numeric columns will count mean std dev min max and percentiles String columns will give count min and max based on Ascii valuesThis check is apple to apple to comparison which means that this will validate the actual data without applying hash But this will be limited to a few records as this may consume more resources if we do it a huge volume of dataBut a pitfall in this check is we cant 100 percent sure that our data is valid Because we have invalid data in row number 999 and 1000 But when we compare the check with only 100 records it returned that our data is validThis check will happen in a typical data migration pipeline""Poor data quality will drain your team's quality time in fixing them I hope this blog helps to address the data quality issues after migration from source to destination using Spark This ready solution will not only fit for a distributed system but the same can also be replicated to run in plain python if the volume of data is lessIf you like this blog please share it with your teammates friends and clap 👏 in the range of 1 to 50 in Medium You can clap up to 50 times per post"
YgPWQcrYptXLtBEoZKy8Rq,We are attempting to build a better solution to for web analytics data To learn more about why and what we are building checkout our previous blog post here A large portion of this project began with researching the technology that would power our project After several discussions we came up with the frameworks that would power the platformThrough several hours of research and testing we began to put together an a basic setup about how our code would be laid out We also defined how each component would communicate with the other One of the main goals in the early stage was to stick to open sourced technologies This makes it easier for components to swapped out with leisure and also makes the code more accessibleThe technologies and reasonings: Divolte Collector: A server designed to collect & send click-stream data into the Hadoop Distributed File System (HDFS) and Kafka topics It utilizes client-side JavaScript to collect on user behavior on web pagesHadoop & Hive: The HDFS & Hive components will help us to leverage batch processing for report generation & data analysis on web data gathered from Google and from our Divolte ConnectorKafka: This is real time pipeline store that allows to take real-time data from Divolte and store the theme using its Consumer APIDruid: A high-performance data store designed for interactive analytical queries Its designed to work with real-time streams and/or batch data setsSuperset & Plotly Dash: Both are highly configurable plotting libraries designed to communicate with Druid We plan on using this to create a dashboard to view and filter the web dataPostgreSQL & Google Data Studio: A short-term testing area for BI analytics
j6QRmPrZcdMBxbJ9Y7bguT,Feedback is everything at TrustYou similarly to how others claim that data is the new gold we understood the potential of receiving feedback from the market and most importantly to listen to this feedback in order to make better decisions and ultimately succeedThe TrustYou platform is consisting of a plethora of modules however in the context of the present technical article we will be focusing on a use case covering the Meta-Review Badges and the Reviews widgets The first is a precomputed aggregation of reviews a review containing all reviews if you like the second is awarded to hotels which are excellent in one or more aspects of their service while the last presents the reviews themselves along with a set of statistics about themThese components serve data that is precalculated by data pipelines that essentially crunch the underlying data and store them to MongoDB so that apps can take and server the information from there via APIs in HTML or JSON formatThis all sounds fine and well but whats the link with the title? Why the good ? Why the bad? Why MongoDB? Well as with every story all is fine while things work within reasonable constraints however looking back just a several months ago for these components the situation was quite far from acceptableHistorical tickets prove that in the beginning when the data pipelines were implemented actually the execution times were reported as acceptable everything was all sunshine and rainbowsHowever as time passed some features were added while some others were removed some technologies got outdated while some new business requirements were added The result of all this is that the system collecting the data for Meta-Reviews ended up taking more that 5 days The bottleneck of the process being the import of data into the MongoDB store which took more than half from the effective runtimeWith an execution time spanning over most of the working week releases were difficult to do mostly in the weekends while having the kids running around and throwing toys towards the laptop screen  Due to the long runtime the pipeline frequently overlapped with other daily jobs competing for the same MongoDB resources causing a cascading effect that spread the impact also to other modules from the ecosystem The effect of this were random errors that the on call engineer needed to investigateThe monitoring of the MongoDB cluster merely consisted of a basic set of metrics exposed by Ganglia that are mostly describing the status of the server executing the engine (network RAM CPU etc) While useful these metrics are ignoring what is going on inside the MongoDB engine especially within the key features that have the most impact features that will be more obvious as we move on with our investigationIn order to illustrate the problem the following charts are proposed for a more in depth study These present the runtime patterns that overlap between the pipelines at various points in time during the weekThe cycle repeats weekly since we have a weekly pipeline that computes meta reviews and badges and a daily pipeline that processes reviews The former graph is the load averaged to a period of one minute while the later is the bytes coming in on the networkPattern 1 is the Reviews running daily in isolation pattern 2 is Meta-Review together with Reviews and pattern 3 is Badges with Reviews Production incidents are reported in cases 2 and 3 that manifest as excessive runtime and workers timing out after many hours losing the progress mostly in the Meta-Review pipeline Interestingly there are no CPU RAM DISK issues reported during the incidents Network shows spikes which so far cannot be linked to a root cause The jobs that are running during the crashes are the MongoDB import jobs from different pipelines These jobs simply take formatted JSON documents and bulk upsert them to the database During the upsert the pipelines seem to be competing for the limited MongoDB resources which can be a first clue towards the root cause of the problemThe main focus point is the Meta-Review pipeline identified with Pattern 2 a system that is required to process data at the scale of terabytes with an average document size around 60 KB while waiting for write confirmation from all members of a geographically distributed replica set Unfortunately the constraints cannot be relaxed since the data is required to be available globally and needs to be consistent at all times Moreover the data is constantly refreshed for the entire data set since the Meta-Review is an aggregation of reviews where even the slightest change in the data set can impact the overall statisticsAccording to what has been presented so far crystal clear are only the effects of the problem while the root cause remains hidden In consequence even before trying to solve the problem first there is a required step to buy time A contingency plan is developed that reduces the number of production incidents and allows the team to focus on the fix: increase visibility by adding a MongoDB monitoring solution with dashboards that present clearly the metrics stabilization of the pipeline by refactoring jobs to take at most one hour before writing the checkpoint to disk implement fault tolerance in our internal library developed in PyMongo to resist connection drops and retry on timeouts for idempotent operations such as upsertFortunately in the case of the Reviews widget the pipeline is not handling aggregations but rather the reviews themselves In this case a first low hanging fruit performance improvement is possible by changing the processing methodology from full to differential that is instead of processing the entire database every day process only a differential set consisting of new modified or deleted reviewsWith the stabilization efforts the team has won critical time that instead of being spent on production incident firefighting now can be spent in starting the quest to find the root causeEssentially a runtime that spans in most of the cases over the largest part of the working week leaves very little room for failure in attempts to fix the problem directly in production The corrective pull requests are required to have surgical precision in order not to lose a significant amount of time with the long delays from the production feedback loop Learning through coding by failing fast in combination with swift corrective actions is clearly not applicable in this scenarioA new plan is required where every step needs to have proved efficiency backed up by numbers from studies that ensure the change is bringing a tangible performance improvement to the systemSince production did not allow the require levels of experimentation the focus is turned towards the existing staging system that replicates to some degree the live configurationThe starting point of the investigation was the hypothesis that the client side setup is affecting the performance In order to verify this claim a set of tests are created with different MongoDB clients with different parameters The contenders are: mongoimport (standard from MongoDB) tymongoimport (same interface as mongoimport implemented in PyMongo) and Spark MongoDB connectorAfter running a first batch of tests the following results were achieved in comparison with production runtime statistics: Legend: cluster means geographical distribution importer is the client program write concern is how many members of the replica set needs to confirm the write executor and all executors is the average import rate per executor and for all executorsThe actual absolute values might vary depending on the underlying conditions like hardware network and other factors However doing a relative comparison on the staging system between different configurations that sit on top of the same infrastructure revealed some interesting conclusions to be considered: the client side technology does not seem to have a big impact since the results show comparable speeds in contrast the same cannot be said for the replication Once the US servers are added to the cluster the transfer rates seem to drop significantly which can be expected for such loads and constraintsThe PyMongo based tymongoimport implementation was faster that the standard mongoimport the Spark MongoDB connector being the fastest Unfortunately the Spark MongoDB connector has a hidden problem that revealed itself only when working with bigger datasets: it removes null fields What this means is that for instance if there is a JSON field that is set to null that field will be removed from the upsert This caused issues since dependencies relying on the presence of the fields even if null There is a fix for this in Spark 3 however the pipelines use Spark 2 where this behavior cannot be controlled at the time of writing this articleAfter the first round of experimentation comparing client side technologies the decision based on the collected metrics is to stick with tymongoimport The other contenders seem to fall short either by being slow or having unwanted behaviors that cannot be suppressedA short recap what has been achieved so far: identified three patterns from three different components influencing each other causing performance issues the situation is stabilized in such a way that the performance issues are still there however they are not creating production incidents there is now an improved monitoring that can tell us more about what is going on inside the database engine and certainly the problem is not a client side issueNow is the time to turn the investigation server side Having an on-premise installation right from the beginning everybody was aware that this is going to be expensive since without blocking time from the Infrastructure team this is not going to work While most probably in the foreseeable future cloud platforms will become less expensive and will replace on premise setups therefore empowering developers to have full responsibility over entire solutions reality in present times is that there are legacy systems that were created many years ago that still service a large client base In our particular case we are talking about an on premise MongoDB 32 installation in a geographically distributed cluster modeThe production release windows are mostly in the weekends Every release requires most of the working week to check the effect of a given change In similar conditions there is little room left for trial and error we need surgical precision but the question is how can somebody achieve this level of precision with such a complex system? From experience the most reliable way to predict the behavior of a complex system with high precision without being necessarily an expert of the system is to replicate the configuration to a test environment that allows fast experimentation Therefore the staging system is instrumented with replica set members from the US in addition to the existin servers from DEGreat! Now there is a way to do fast prototyping the only aspect remaining unsolved is what direction to take to converge as fast as possible to quick results that can justify the investment made so far The team was aware that there is a need to move to newer MongoDB versions the first step being a mandatory upgrade to 34 since upgrades are incremental However the piece of the puzzle that was still missing is the proof that this will work and will already bring a significant performance improvementLuckily our company uses a well organized ticketing system from where some interesting information surfaced during the investigation about when exactly did things turn really bad for the runtime performance The records revealed that at some point the average document size was 8KB but at the time of the investigation the average size was 60KB Why do we have this difference? Apparently in the beginning there was an application level compression in the form of encoding the JSON document body to BASE64 With new storage engines the BASE64 encoding is no longer necessary in order to save storage space therefore the decision was made to remove the encoding in favor of the built-in compression from WiredTiger Shortly after that the execution time exploded since the data on the wire was no longer compressedAt the time when the MongoDB upgrade from 32 to 34 was started there was no clear proof that this will work simple boy scout rule was applied to attempt to migrate to newer technology leave the place in a better condition than it was beforeIt was during the study of the migration changelog for MongoDB 34 where the Eureka moment kicked in when reading the following line: Added message compression support for internal communication between members of a replica set or a sharded cluster as well as communication between mongo shell and mongod or mongosThe link with the BASE64 story was immediate this is it the snappy intra-cluster compression is what we need to regain the performance that existed before the extraction of the application level compressionHell yeah it was about time! Finally progress… finally a ray of confidence that the solution is really close The application code base is now upgraded to support both 32 and 3The staging measurements with the new configuration reveal the following statistics: Thats it! The proof that we can expect a considerable improvement in production obviously the magnitude of the actual performance increase in the live system will be different because of the other external factors not present in staging (for ex load) the metric brings confidence that the upgrade and the configuration change will have a positive impact on the pipelineThe upgrade to MongoDB 34 progresses like a breeze no downtime no complications no unexpected surprises no issues whatsoever Kudos to MongoDB their upgrade process is really well suited for our purposesFortunately after the upgrade similar performance improvements are achieved in the production system as the ones outlined in the staging systemGreek mythology offers many lessons of life such as the tale of Daedalus that builds wings for him and his son to escape from the labyrinth He tells only one rule to his son before flying out that is not to fly too close to the sun because the glue holding the wings together will melt Shortly after starting their journey their flight ends with a fatal accident because the rule was not respectedThe temptation is of course great to continue in the same way in our case to force our luck There is MongoDB 36 that advertises a significant amount of new features one of them being zlib compression that claims to have higher compression rates at the expense of more server side resourcesSame process followed that produced the following statistics: Collected metrics show that in staging for our systems and for our workloads the average transfer rate is lower with zlib in comparison with snappy While for different systems and payloads this might very well be a completely different story we currently lack the proof that the setup involving zlib is going to perform better in production At the same time MongoDB 36 with snappy seems to outperform production however this might not be relevant since essentially they are different systems with different loadsWith the indicator that the new version is able to perform better the production upgrade is proposed Once at the new version however results show that the suspicions were true the system is performing not as fast but the difference is insignificant for our current needs therefore the last major version from the 3 series is left as the production engineLooking into the future there are many aspects that can be still further improved The production system is significantly more powerful which makes a zlib test run a low risk operation that can actually yield better performance The fact that staging metrics proved the contrary should not be discouraging since the difference is small and allows a margin of risk without visibly impacting the runtime of the pipelinesWhile working on adjusting the source code to be compatible with MongoDB 34 then with 36 it quickly became obvious that having a common component that is responsible for serializing the data to MongoDB can be really beneficial Imagine for instance a solution that is decoupled from the batch ETL processing pipelines that is streaming records to be upserted to the database Not just one code base to maintain when moving to a new version but also less runtime for the pipelines that allows more frequent releasesNew versions of MongoDB starting with 40 42 and what will follow will bring better features faster performance Continuously upgrading the system seems to make a lot of sense in order to avoid performance problems similar to the ones tackled in this round of improvementsAll in all since thanks to the improvements brought to stability processing logic and infrastructure the system now is both running stable and finishes within the specified time constraints In consequence the decision is made to put aside for now further improvement efforts and focus on capitalizing on the gains achieved in the context of the improvement efforts described in this articleIn the quest of finding answers to complex performance problems it is imperative to have deep knowledge of the underlying technology to have a testing setup that allows verifying theoretical claims without affecting production and last but not least to follow and understand all details carefully because these are leading to the solution
5xSHpKUUxVbm9gMYC6UBca,By the end of this article youll know how the Data Engineering team at Unruly setup alerting for our Apache Airflow instance in a really simple but powerful way Ill start by giving a lightweight intro to some Airflow concepts but if youre already familiar with Airflow you can skip this Together well go through a worked example of a failing task in an Airflow DAG that triggers a concise super informative and actionable alert in our teams alerting system Were using Opsgenie but dont worry if youre using something else the same can be applied to any alerting system that supports creating alerts via an API Finally as a bonus Ill show how these alerts look when theyre automatically propagated to our notification channels like SlackApache Airflow is an open source technology for orchestrating workflows and is typically used for data processing pipelines Its a great tool for batch ETL (extract transform and load) style jobs moving data from A to B changing the format or schema of your data into something more convenient for some downstream work etc These workflows are setup as Directed Acyclic Graphs or DAGs for short DAGs are typically composed of independently executable Tasks that are sequenced together to form some logical pipeline of work you want to conductAs you can see from the above graph the ability to branch and merge these tasks provides a lot of flexibility to create most workflows you could imagine When building a DAG tasks are instantiated as Operators an operator is just a class that takes some configuration Airflow operators are ultimately responsible for doing the work whether thats triggering an action in another system transferring data between systems or sensing when something should be triggered and making it happen Finally theres Hooks these are operator building blocks that can be used to interface with external systems like a database or an API Hooks utilise Connections that are configured with the credentials needed to access these external systemsSo now you should have a basic idea of whats meant by a DAG a Task an Operator a Hook and a Connection which is enough to grasp the rest of this article There are of course much better and more in-depth introductions to Airflow and Id definitely recommend exploring those to find out more if youre interestedAs our starting point lets define a DAG with a single task that runs a simple BashOperatorWeve made sure this operator always fails by simply calling exit 1 so we can test what we want to do off the back of the failureAll operators in Airflow inherit from the BaseOperator class which means for any operator were using including our BashOperator well have access to the parameters available in this class The BaseOperator parameter well be taking advantage of specifically is: on_failure_callback (callable)  a function to be called when a task instance of this task fails a context dictionary is passed as a single parameter to this function Context contains references to related objects to the task instance and is documented under the macros section of the APIThis can be configured to call a custom function in the event of our operator failing which is exactly what were going to do next with the BashOperator we defined earlierThe do_something function thats triggered when our task fails currently isnt defined but well get there The context dictionary thats mentioned in the documentation is also key for us this will allows us to get metadata about the instance of the task that has just failed Later on well use this context dictionary to customise our alerts making them informative and user friendlyAirflow comes with a bunch of built in hooks some of these are published by the Airflow team and others are community contributed Well be using the community contributed OpsgenieAlertHook in our example but you can find pretty much anything you need here If youre using a different alerting platform with an API you can achieve pretty much anything with HttpHook which is also the parent class of OpsgenieAlertHookFor this hook to work we need to configure a connection to Opsgenie looking at the code for this hook its already setup to use the existing opsgenie_default connection that comes with Airflow For simplicity I can edit this existing connection in the Airflow UI this will save and encrypt it in our Airflow database However if youre not keen on having this state lying around because youre using the default in-memory Airflow database or havent encrypted your secrets then you can also configure connections using environment variables or at run-time in your Airflow code This is great if you want to grab the credentials from your secrets manager and the way Id recommend doing this in a production environment For now were just going to drop our Opsgenie API key into the password field and save thisLets make our do_something function actually… do something this will be our hook into our alerting platform Lets rename do_something to opsgenie_hook and import this functionality from a different Python file This will allows us to build something generic and then easily re-use this code in all of our DAG definition filesSo hookexecute() will as configured in OpsgenieAlertHook by default send a POST request to the default Opsgenie alerts API endpoint at https://apiopsgeniecom/v2/alerts Its able to authenticate against the Opsgenie API utilising the opsgenie_default connection that we just configured Were supplying the name of this connection as the argument to OpsgenieAlertHook constructor As youve probably noticed the execute function currently takes an empty object but not for much longer this will be our JSON payloadRemember we talked about the context dictionary earlier? You may have noticed that our opsgenie_hook is taking context as an argument the contents of which we can use to build and customise our JSON payloadTheres a little bit going on here so let us unpack it piece by piece Firstly were extracting some useful stuff from the context dictionary: These seemed the most useful but you can see all thats available in the context dictionary in the TaskInstance class Were using dag task and a slightly nicer formatted ts to construct a short informative message from which you can immediately discern whats wrong An example would be: Airflow DAG eternal_failure failed to run task_that_always_fails scheduled at 2019 06 21 16:00We can also embed the log_url as metadata within the JSON payload itself this makes our alert actionable anybody responding to this alert can immediately head to the first port of call when troubleshooting an issue Were replacing localhost with the actual DNS of our airflow instance to avoid a manual step here too The rest of this payload is pretty Opsgenie specific but of course were routing the alert to the appropriate team setting the appropriate priority for the alert adding useful tags and so on These are all things youll obviously want to tweak depending on your own implementationOpsgenie like any alerting platform also takes care of notifying us through various channels like Email SMS and Slack Having a concise informative and actionable alert is critical when youre on call Its also great to be able to interact with these alerts via Slack directlyWe can all agree its important to know immediately when and why things arent working in production especially in an era where were responsible for building running and owning our complete stack end to end The tasks you run as part of your Airflow DAGs are no exception to this especially as its often likely youre holding up some downstream workloads that can be business critical Theres also no reason why setting up this kind of alerting should be a hassle either Airflow delivers that ease with some really simple functionality thats built in and trivial to leverageAll of the Gists embedded here are available publicly and a concise final version of this example is available here
nucVdVWqvvv5qTaPxC5d4i,Whether youre a photographer on Unsplash or you just use it for its resources and community youve probably noticed that we report the number of downloads each photo gets as well as the evolution of that number recently If youre a photographer youve also seen the aggregated count of downloads across all your photos These numbers usually refresh every 10 minutesFirst you need to know what a download really represents To get there Im presenting you our API The Unsplash API is open to every developer and can serve any application that requires it with different services like getting photos (random search recent …) or downloading them Whats important is that the API not only serves 3rd party applications but it also serves our website unsplashcom our different apps (iOS Android) our browser extension (Unsplash Instant) and our TV appsThis means that when a photo gets downloaded anywhere in the Unsplash ecosystem in any app or website that uses our API the API is alerted We work with API applications to fire a request to our API whenever a user performs a download action The Unsplash API handles about 8 photo downloads per second across all of the ecosystem Thats 60000 different photos being downloaded 700000 times every dayWhenever the API delivers a photo for a download it fires 2 requests to our data pipelineThe first request goes to our classic data pipeline: the request hits a server which produces a log Once a day all these server logs are parsed to extract the data which is then stored in our data warehouse We call this data setup the batch pipelineThe second request goes to our stats server: the stats server receives the request and lists it in a Redis index Every 10 minutes an ETL job extracts the data from Redis and stores it in our warehouseThat second setup is what makes us able to update the downloads count for each photo and the aggregation per photographer every 10 minutes We wrongfully call it the realtime pipeline The problem is that this pipeline is not as reliable and is much more expensive than the batch pipelineTo solve the lower reliability concern at the end of the day the batch pipeline overwrites the data from the realtime pipeline repairing some rare but possible missing downloads From then a new day starts on a fresh base: the counters continue to follow the 10 minute update rule until the next batch processing runs and replaces the values for the dayWe call it a pseudo lambda architecture because in a classic lambda architecture only one request is sent and the server is the one splitting that request and sending it into the 2 pipelines (realtime and batch)We could manage to make our realtime pipeline be … realtime We could do this by using some technologies like ElasticSearch but we made a choice to use a smaller number of services to simplify our architecture This is not definitive and we might move to an actual realtime pipeline at some point when well have a true need for it and the time thats necessary to implement itWhat follows is for the techies Ive mentioned a smaller number of services let me take you inside the tech stack that allows us to track photo downloadsLets not talk about our API because I wouldnt be able to honor it as it should be Ill just mention that its mainly resting on Ruby Props to Bruno Aguirre Aaron Klaassen Roberta Doyle and Luke Chesser for #TeamAPIInstead lets discuss a bit about our data architectureOur batch pipeline is powered by Snowplow Its an open-source data pipeline that lets you collect process and store data What you collect how you process it and where you store it is entirely up to you Its a very flexible tool that makes you the only owner of your dataWe deployed the Snowplow pipeline in Amazon Web Services (AWS)An Elastic Beanstalk instance is the event collector Its a web server that logs any incoming request and stores these logs in Amazon S3 The API sends it a request anytime a download happens in the Unsplash ecosystem The collector collects all sort of events currently logging 350 requests per second at peak timesFrom there an EC2 instance (that we also use for proxy purposes) runs a daily cron job that fires up a brand new Elastic MapReduce cluster responsible for processing the logs in multiple steps: This EMR job from Snowplow also saves backups of the state of our data after each step If something fails we can always reprocess itThe (almost) final destination for our data is our warehouse: an Amazon Redshift cluster The cluster contains pretty much all our website apps and API activity For the AWS geeks its a Redshift cluster currently made of 21 dc2large nodes for a capacity of 33TB of compressed dataThe realtime pipeline is more of a homemade solutionA web server living on Heroku receives the requests sent by the API when theres a download transforms them into readable events and logs them in RedisEvery 10 minutes a scheduler process (living on Heroku as well) enqueues a handle realtime downloads task in a Redis queue When our worker process finds time its going to take the task out of the queue and process it That whole scheduler/queue/worker setup is built in NodeJS with node-resqueIn that case what the worker does is pulling the downloads out of Redis and inserting them straight into our warehouse the same Amazon Redshift cluster we mentioned previouslyIf you ask me why not send the downloads directly there instead of buffering in Redis? Ill answer that Redshift doesnt couple well with really frequent inserts Bulking the inserts also helps lowering the load on the cluster that needs to perform well on a lot of other tasksThe rest is pretty straight forward All our data whether its realtime or batch processed lives in our warehouse in different tables By querying our warehouse we can tell exactly how many times each photo has been downloaded and we can aggregate these counts by userThe only issue remaining is that our warehouse does not perform well enough to answer the thousands of requests for download stats we get each hour fast enough for a good user experience So we need to move the data … once againFor a faster delivery of stats we use a PostgreSQL database living in Heroku We gave it the really fancy name of stats database Its total size is about 40GB but it holds more than just downloads counters An ETL job that runs every 10 minutes pulls the downloads from our warehouse transforms them in a performant format and pushes them into our stats database This database is the leader of a read-only follower that our API queries to get up-to-date stats Our website asks our API for the most recent stats and displays it for you to seeAnd boom! This is how we track and update photo downloads at Unsplash
BSexgN35jjGdvH3oNkvidA,Here at Unsplash none of our data processes is built within any of the main products (website API etc …) Theyre all gathered in a specific project that we call Unsplash-data Like any other product Unsplash-data uses different third party services to manage enrich store visualize and distribute dataLet me take you on a tour of our data stackFirst of all we need to collect data from all our different applicationsSnowplow is an open-source data pipeline that offers different solutions to collect data from different platformsIts ideal for Unsplash because we have a lot of very different products It includes all the tools to setup a pipeline (either batching events or real-time) to collect enrich and store your data all that inside your own infrastructureOur Snowplow pipeline is running on Amazon AWS It contains: Thanks to Snowplow we completely own our data Were free to modify archive derive or replay it exactly the way we wantThe data processed by Snowplow ends up in our main data warehouse a Redshift clusterRedshift is a warehouse that you can query with SQL (a limited version of PostgreSQL with less/different functions) Read and write operations are free and you pay only for the volume and computing power you want available Its an easily scalable solution and like most warehouses it uses a columnar storage In short this means it can access millions/billions of rows very quickly if you limit the number of columns read in your queryThe ability to query it via SQL makes it an awesome solution when it comes to developing around it and integrating it in our other systems or third party servicesWe also have other data stores for other usagesBigQuery is another warehouse we use We use it more as a intermediate storage than a final data storage Its a destination for logs but also a data source for the rest of the pipeline We read from it compute calculations and store the result elsewhere Its where our own API logs and all the photo views end up before being processedWhats interesting about it is that the computing power is dynamic and handled by the system itself You dont have to worry if the BigQuery cluster is going to be powerful enough it will be The price you pay is based on the volume of data that you read and write Storage itself is cheaper (001 002$\xa0for\xa01Gb) The pricing model is completely opposed to RedshiftsSo depending on our use case we might use Redshift over BigQuery or the other way around to lower costsAnother data source for the Unsplash-data system is simply the product database hosted on Heroku where all the photos user accounts collections API applications etc … are stored The data system reads from it to keep an up-to-date copy of certain data models within itself but also to compute calculations or transform the data in a more readable formatThe data system has its own PostgreSQL database that is also hosted on Heroku Everything data-related is stored in there Whenever the API needs to access data like views or downloads it will query this databaseOnce all the data sources are plugged-in and the data available in our PostgreSQL database we use different services to enrich the dataEnriching the data means collecting more data purely based on the data you already possess Its like deducing new dataFor example you can generate tags for a photo by using an AI service and then store the generated tags with your photo You enriched your photo with tagsGoogle Vision allows all kind of detections on your photos We use it to extract colours detect landmarks and evaluate the content to understand if its somehow NSFW or violent contentA nice example is that we used the enriched data from Google Vision to build Unsplash LandmarksCloudsight offers a nice feature called Whole scene description Its an API that analyses the content of your photo and formulates a sentence describing it We grab and store that formulation mainly for accessibility and to provide an alternative text if the image has a loading problemRekognition works a bit like Google Vision except that we only use it to generate tags and keywords Rekognition analyses the content of the photo and provides a set of keywords with a confidence level that describe your photo This is one the first stones of our search engineWe picked Amazon Rekognition over its competitors because we found that its confidence level is well calibrated and the list of keywords offers a solid and conservative base on which we can safely build an accurate search engineThe Geocoding API from Google helps us in enforcing the format of locations that user enter for their account or their photos When sending an unformatted location like NYC to the Geocoding API it will provide a formatted version splitting city name county state country etc …We store that formatted version to be able to query our data based on these different parametersOur main visualization tool is LookerMore than a visualization tool Looker allows anyone in the company to explore the data available in our different data stores and create dashboardsThis exploration is automated and managed by a set of data models that you can generate or write yourself These models can be persisted in your warehouse for faster retrieval of the dataYou can control the limits of the exploration that your teammates can do Controlling how people access the data and what they do with it is key Not because you want to censor things but because you want to be absolutely sure that they cant make mistakesMistakes create uncertainty which leads people to stop looking at the data out of fear of taking decisions based on wrong dataThe models you build can also clarify the data for your teammates For example your model can interpret a set of integer statuses (1 2 3 4) as readable statuses (submitted review accepted deleted)Looker is plugged to our different data stores but mainly to our Redshift warehouse where all our product events end up and where our calculations and enrichments are backed up dailyWere doing most of our analysis with Looker but sometimes its not enough When we need to dig more or lead true research projects we create data notebooksIf youre a data analyst you probably know what a data notebook is For the normal people its a document that can regroup code data visualizations and textual explanations to show and explain how a specific research is led step by stepGoogle Colaboratory is an engine that allows you to collaborate on a data notebook So if you work in a team on a single research project you can edit the same notebook without having to synchronize your code Collaborative code editing is tricky but Colaboratory is also a nice and easy way to share your notebooks with people without having them to run a notebook engineData engineering and analysis is not only for business intelligence or for statsThis type of research is also very present at Unsplash To make sure each project ends up in being something concrete that the team can try and play with we built a small UI called Unsplash Labs Unsplash Labs showcases all the different prototypes that we build when researching new product featuresThe lab not only shows the prototype but also links it to code in Github cards in Trello and interesting related documents that we found on the internet while researching That way anyone in the team can test these prototypes learn more about them and give feedbackAnd just like that… the data stack tour is over! If you have any question feel free to ask in the comments! Well be happy to answer as soon as possible
S3RHSAWHZfLbP9BiGoRjH9,At Velotio were big fans of learning! In tune with our Silicon Valley ethos we believe knowledge is meant to be shared freely and learning is a continuous process We encourage our engineers to read about new technologies domains and startups by sending out a weekly reading list We thought it would be a good idea to share these publicly Typically we post about software engineering cloud DevOps machine learning and UI/UXYou can follow Velotio Technologies to get these weekly learning updates
7EeGk6WGmJ2kDf4nsmdzSz,Todays businesses need to look beyond what the forefathers of silicon valley saw and delivered on They said we will have information at your fingertips Were hereThis publication will provide business platform leaders technical architects data scientists engineers and analysts the why the what and the how of creating modern realtime business platforms best practices design patterns tools and testimony of our own experiments in realtimeEven though we have been using realtime technology for the last decade we recognize that the amount of knowledge available has expanded exponentially Were always learning so Im happy to share our journey with you Join us by following us
ZSCRAZrFtXPwV8VpLPkvDU,We built a system to forecast demand in Walmart Stores which comprises of data engineering and ML pipelines Ensuring batch jobs in our pipeline run according to schedule is critical to business Delays in job runs or missing a run has impact in the order of millions of dollars This blog post explains what we did beyond the capabilities of our scheduler to detect job run misses and delaysWe use Apache Airflow(Open Source) for composing & scheduling data pipelines This blog post requires a basic idea of Apache Airflow and data pipelines in general The focus is not on Airflows features but on how we built an audit system on top of itWe have taken care of the following aspects of Airflow to make sure the scheduler is available 24/7 and jobs are triggered : We had situations where AF did not trigger our ETL Jobs(spark jobs in our case) but its Web UI wrongly shows the corresponding task in running status There is no way to know about this unless the on-call engineer verifies it manually  Airflows SLA feature did not help here as AirFlow itself is behaving erroneously This could be due to a bad configuration in Airflow Infra (less likely) or unstable moving parts of Airflow or a bug in AirFlow itself Irrespective of the root cause we must know when a job is delayed so that the on-call engineer takes a manual corrective action immediately We implemented an auditor which runs independently from AF and alerts when any of our Spark jobs in a pipeline doesnt start on timeEstimate ETL task start times using run history from Airflow backend Every Spark batch job(our ETL execution unit aka Airflow Task) registers itself to a database table with a unique id This denotes to the auditor system that the job needs to be audited The job also sends its heartbeat whenever it runs Alert if task heartbeat is not received on time as per the estimate We assume Airflow history is sane ie majority of the previous runs happened per usual schedule One may wonder why estimate from history when you can just use the scheduling information of DAG directly but it is not enough since schedule gives expected start time of entire DAG as a single unit but not that of individual tasks insideAuditor runs in a fail-fast mode ie the java process of auditor will fail as soon as it sees something wrong eg it cannot complete thread execution(steps 2 & 3) within X hours it is unable to reach Airflow backend alerting service is not reachable This triggers an alert to Ops by our process monitoringThis saved us multiple times from missing SLAs and is critical to our ETL pipelines today This in turn allowed us to safely depend on relatively new but feature-rich Apache Airflow as it continues to mature This can also be used with any other Scheduler by adapting to its backend data model
UbSU4e3rzYt89Zye7RsSUd,There are different techniques to learn a new craft or skill Some people prefer reading others prefer to watch videos and there are those like me that just need to get their hands dirty and learn on the way or by example For me personally that works best when Ive got an actual problem or challenge that needs to be solved in order to learn a technique In this article Id like to share some Akka stream techniques and gotchas that Ive learned on the way while building a real-life use-caseThis article will require a basic understanding of Reactive Streams and Akka Streams in particularWhat I essentially had to build was a file aggregation stream which would read multiple (optional and/or required) files from a directory and combine the result into a case class for further processing In my case that further processing meant streaming the data to ElasticSearch For brevity sake Ill leave that part outImagine that you have a directory called documents This directory contains N directories that each represent an identifier of a document Within each of these directories you could have up to 2 different files present Each one of those 2 files is the output result of a completely separate process Each of those processes may not have run yet for a specific directory so we need to consider that the files that Im looking for may not always be thereThe following directory structure and files could be a possibility that our stream needs to be able to handle: As for the requirements for the stream: To list directories and read files contained in each directory in a streaming fashion theres an Alpakka dependency available If youre not familiar with Alpakka its an integration library built on top of Akka Streams to provide connectors to a lot of different systems For interacting with the filesystem add the following dependency to your build
SfFWR7wDnPRnkSTPvvXyij,All stateful applications need a state storage mechanism In traditional applications this often is a relational or a non-relational database When using Apache Flink state storage and management is offered out of the box This on one hand makes it easier to get started with Flink but on the other hand it forces you to learn how you can leverage the built-in state management Flink offersOne of the major aspects that are vital for any production-grade application is state migration You dont just build software in one go you start small and make continuous iterations to improve your software This is no different when writing a Flink job With software relying on databases you will most likely resort to migration tools that update your database schema and data as your software and data model evolves With Apache Flink this has long been one of the harder issues to get rightThis article will first go into the history of how state schema evolution in Flink evolved over time It also explains our trial-and-error approach to state schema migration offered out-of-the-box in version 18 To conclude it shows how we eventually used custom serializers to support Apache Avro-backed schema evolution for our Scala-based jobsThroughout this article Ill use snippets which all come from the sample end-solution that Ive put together on GitHub Going through all the code in this article would distract from the main topic Thus if you find yourself wondering where a reference or variable in one of the snippets come from please take a look at the sample codeBefore version 17 Flink relied on the Java Kryo serialization framework to serialize and deserialize your data objects When taking a savepoint of your jobs state Flink would use Kryo serializers to serialize your data models Both the serializer itself and the serialized data would be persisted in the savepoint The problem lies in trying to restore from a savepoint with a job that contains a change to your data object (eg adding an additional field) Any change to your object would result in a different hash code of the class than the one calculated at the time of creating the check- or savepoint Subsequently Flink wouldnt be able to match any of the persisted serializers to the updated data object on the classpathTo support data schema evolution in Flink pre 17 the only solution was to create custom serializers In a previous article my colleague Niels Denissen explained how that was achieved using Apache AvroThe release notes for Flink 17 however showed to be very promising in offering an out-of-the-box solution to state migration: With Flink 170 the community added state evolution which allows you to flexibly adapt a long-running applications user states schema while maintaining compatibility with previous savepointsState schema evolution now works out-of-the-box when using Avros generated classes as user state meaning that the schema of the state can be evolved according to Avros specificationsHowever in version 17 the POJO serialization support did not yet support composite types like Scala case classes With the release of Flink 180 all built-in serializers have been upgraded to use the new abstraction All composite types supported by Flink like Either or Scala case classes now are generally evolve-able as well when they have a nested evolvable type such as a POJOThe Flink documentation was very specific about the state schema evolution working out-of-the-box when using Avros generated classes We decided to give this a try firstIn our pre 18 solution we didnt generate case classes from Avro schemas yet so this is the first step to take In Scala its incredibly easy to generate case classes based on Avro schemas using sbt-avrohugger Avrohugger can generate 3 different formats: Standard (for use with Avros GenericRecord) SpecificRecord and ScavroOur previous solution used to serialize and deserialize GenericRecord so we thought wed give the Standard format a go However we quickly ran into problems with this approach: Flink expects each data object to have a no-argument constructor Scala case classes are inherently immutable offering only 1 constructor which takes all fields the case class contains as argumentsBy switching to the SpecificRecord standard we would achieve what we needed to adhere to the POJO rules as stated in the documentation: As an example this is what a generated case class would look like for a simple object: You can see that it uses var for all the fields to make it mutable It also adds the no-arg constructor required for initialization It also adds the entire Avro schema as a val SCHEMA$ into the companion objectAt the core of Flinks typing system is the TypeInformation class It contains information about the data types that you use as state You can either declare a specific implementation of TypeInformation yourself or let Flink infer it automatically for you The documentation clearly states that state schema evolution is only supported if you let Flink infer the types By using the following import we were able to let Flink infer the TypeInformation for our generated Avro case classesUnfortunately even with the use of case classes adhering to the Flink POJO rules we werent able to get it working and received the following error: If we look at the source code of SpecificData we can see that it tries to resolve the Avro schema by looking for a static property called SCHEMA$ in the Java class using Java Reflection Our Scala case classes have a companion object which contains the SCHEMA$ property but Java reflection techniques are unable to read Scala properties (#111)The only solution we came up with for this issue is to switch to Java generated Avro classes We decided to not go down this road for now Instead we decided to see how far wed get with plain old case classesGiven a basic Scala case class like ProductDescription: Well start with the most basic example that contains state that you can think of Namely consuming a DataStream with only a keyByIn this example the keyBy is the function that creates state For this type of state you need the native POJO serialization and deserialization with state schema migration support In Flink 17 state schema evolution would only work for this example if the ProductDescription case class did not contain any composite typesWith POJO schema state migration there are a few rules to what you can and cant do: So far so good We can use the out-of-the-box state schema evolution to manage DataStream state Lets now take a look at state kept within a processor like ValueState
7vs6sk7bCT8JBuUxVjebSD,Airflow Airflow Airflow… how I love and hate thee The siren calls of scale and flexibility tempt me even as I have been scalded by my trust in you As Airflow projects of the future loom I am reminded constantly of the past I hear the bellows of my projects booming in the wind They call to me Bound in irons Doomed for eternityI dismiss them as the nags of the past I cannot give them the attention that they demand I must focus on the futureEndeavours with Airflow require patience research wisdom I read blog posts warning of anti-patterns and thought I knew better So I dismissed themI was a man of technology! I understood the risks of wielding my tools! The lessons of the blogs mattered naught They fell on deaf earsAnd so I began my workFrom the beginning it was clear that Airflow was a strange beast I created DAGS operators plugins and began to consider myself an intermediate user However I was just digging myself deeper and deeper into my Airflow holeMy only way forward was to learn from my mistakes Rethink Airflow Share my findingsIn writing this blog post my mind is returned to the Airflow projects that I have contributed to sometimes even lead I stare into a maelstrom of mistakes that I have made  all so obvious in retrospect I am one of the unfortunate many that has battled and lost with AirflowI have committed many sins against data engineering It is through an introspection of these misgivings that I may share my knowledge with you I do this with the hopes of preventing future crimes against Airflow So! Cast off your false assumptions of Airflow for it is here and now that we shall discover how to avoid falling victim to its many pitfallsComplexity is where most of my Airflow transgressions began By increasing the complexity of the Airflow instance more could go wrong When things did go wrong they were harder to fix as I wasnt fixing just my code I was fixing my code in Airflow There comes no greater relief than that which is achieved through moving from a complex Airflow environment to a lean Airflow environmentEmbedding ETL logic in Airflow is something which Ive seen too many times Airflow supports custom python operators allowing you to embed any kind of logic you choose into your Airflow DAGs So why not empower Airflow with ETL logic taking it from simple orchestrator of tasks to fully fledged data ingestion pipeline? Well most of the time this is asking for troubleThe additional complexity of ELT logic inside of Airflow will lead you down the path of frustration when it comes to ensuring that the logic behaves as expected is performant is terms of execution time is testable and so onIt is almost a natural behaviour of those standing up Airflow and managing DAGs to try to make complex DAGs Chief among these over-complexities is the dynamic DAG This is often one DAG to rule all DAGs One DAG that is influenced and directed by a metadata store outside of AirflowThe dynamic mega-DAGThis may seem like a great idea on its surface you only have to write one DAG which will do different things based on what the data dictates in the external metadata storeUnfortunately the battle has already been lost When something goes wrong with one of the jobs that is run through this pipeline it is a living nightmare to debug it A great amount of digging through logs is required in addition to endless navigation of the Airflow UI The Airflow UI is not designed for this kind of behaviourAirflows UI provides pagination of DAGs It provides the ability to search for DAGs by name The UI is guiding you towards more DAGs not less  definitely not one dynamic DAG to run all jobsKubernetes has had an influential effect on the global technology scene and Airflow has been very much swept up in it Indeed Google Clouds Airflow-as-a-Service offering is in fact Airflow running on a GKE cluster (together with a smattering of other services) The main problem with running Airflow on Kubernetes is a result of using Airflow in a non-Kubernetes wayAirflow has excellent support for Kubernetes by using their Kubernetes Executor (something that I recommend you look at if you are running Airflow on Kubernetes) However using Airflow on Kubernetes with a non-Kubernetes Executor is a bit of a waste unfortunately You gain all of the overhead of a Kubernetes cluster (together with the pain of managing administering governing and securing it) with none of the benefitsThe process of refreshing plugin code is also a pain Through a lot of trial and error I have found that it takes restarting the scheduler and the webserver(s) in a certain order to properly replace the plugin code for a given Airflow instance The errors given from Airflow often paint a very unclear picture of what is happening as a result of missing or erroneous codeThese reasons provide yet more ammunition for the idea of removing as much custom logic from Airflow as possibleKeep Airflow lean its complexity low and its DAGs and operators deterministicHaving reformed my usage of Airflow in recent days let me offer some sage advice The suggestions here will again lean heavily into the idea of lowering the complexity of the Airflow instance wherever possible The more understandable and deterministic your systems are the better you will sleepThere are plenty of warnings of Airflows anti-patterns floating around in blog posts and documentation Heed these warnings do not ignore them do not think of yourself as above such worries I have been victim to slowly slipping into overly dynamic DAGs in the past as I did not stop to consider is this an anti-pattern? when making changes to my Airflow codeTalk to someone who has used Airflow heavily and if possible share your instances details They will most of the time have some pearls of wisdom for you  and maybe a few war stories tooAirflow can be a particular beast As I see it there may in fact be more anti-patterns in Airflow than patterns When you can understand how it was intended to be used you can use Airflows strengths to make a great orchestration systemAbuse Airflow and you will be in a world of hurtIve recounted above why ETL logic can really bog down Airflow make it hard to test unstable  a plain old bad place to be The solution to this is to place your ETL logic somewhere outside of Airflow Airflow itself offers solutions to this believe it or not For Airflow running on a VM theres BashOperator and DockerOperator Both of these allow you to write and execute code in such a way that you can properly test the ETL code in isolation away from Airflow This is a huge benefit to you as an Airflow developer Most of the time I would prefer Docker over Bash as you have more control over the execution environment rather than running Bash locallyIf you are running Airflow on Kubernetes then you also have an Airflow-native solution to removing custom code from Airflow  the KubernetesPodOperatorThe KubernetesPodOperator is an absolute shining gem When it comes to removing custom logic from Airflow Dockerising that logic and triggering it from Airflow is a great ideaGood lord you can achieve a lot if you know what the Airflow configuration can do Its an absolute mammoth of a file and can be quite daunting to those just learning about Airflow I urge you to experiment with the configuration  at least in a safe place where you dont mind blowing away the Airflow instance I have a startling example of how important the Airflow configuration is to the behaviour of Airflows jobs An Airflow-on-Kubernetes cluster that I was managing was executing a given DAG to completion in 30 minutes I noticed that each task was taking a tiny amount of time to complete but there was a large amount of time that the DAG waited for the next task to be scheduled Given enough fiddling with the configuration the DAG was performing much better  it went from a 30 minute execution to just under 5Such a performance gain was achieved purely through tweaks to the Airflow configuration file Consider that you may be able to speed up your jobs improve stability and even increase your resource utilisation just through experimenting with your Airflow configuration for a day or two Its always worth a tryId like to leave you having confessed my sins and made reparations with some further reading material that has helped me in my time with Airflow: An excellent article broaching the topic of using container-based logic instead of embedded logic: https://mediumA post from Kubernetes itself about using Airflow on Kubernetes: https://kubernetesA guide to scaling out Airflow (though it is an Astronomer-specific guide it still has some great tips for all Airflow developers): https://wwwastronomerAn open (at the time of writing) issue with Airflow regarding their poorly named configuration variables  this is why I urge experimentation to properly understand the configuration: https://issuesapacheNow its up to youMatthew Grey is a senior technology engineering consultant at Servian specialising in Google Cloud Servian is a technology consulting company specialising in big data analytics AI cybersecurity cloud infrastructure and application developmentYou can reach me on LinkedIn or check out my other posts here on Medium
93qsgVj27TVmJynLUHPUPj,<tl;dr>: REA Group engaged Servian to help plan and successfully deliver the repatriation of its core Google Cloud data assets This required 500TB of BigQuery data to be shifted from the EU multi-region to the Sydney region whilst also minimising disruption for any downstream consuming systems and teams across the business Augmented by REA Groups SMEs a team of two Servian consultants specialising in Google Cloud helped complete the project in just five weeksREA Group has come a long way from its beginnings in a garage in Melbournes eastern suburbs The business is now a multinational operation that includes Australias leading residential commercial and share property websites a mortgage broking franchise and property data services provider as well as businesses and investments throughout Asia and North America REA Group provides a platform  accessible via the web or mobile app  that connects property buyers and renters to agents or property vendors such as developers or individual ownersREA Group began its Google Cloud journey several years ago primarily focusing on leveraging the data analytics tools and services on the Google Cloud Platform (GCP) technology stack Back at the beginning of the journey the region in Sydney did not exist Like many other GCP customers at that time REA Group chose the EU multi-region for analysing their data in BigQuery Fast forward to today and because of newly established contractual obligations and data sovereignty requirements REA Group wanted to repatriate its BigQuery EU datasets to the relatively new Sydney regionREA Groups data that resides in BigQuery serves a pivotal role by serving large-scale data-driven analytical workloads and critical reporting functions across the business It comprises sales operational marketing/audience and analytical data Although highly skilful and talented technologists themselves REA Group enlisted Servian to help plan and implement this projectWorking closely with REA Groups engineering teams we came up with a robust cost-effective and scalable solution Google Cloud Storage (GCS) and its transfer service was used as the main technology in the solution We used GCS to extract the data into and then reload it back into BigQuery on the Sydney side GCS is the glue that holds everything together on GCP We chose not to use Cloud Composer as the orchestration tool for a few reasons but mainly to keep the solution simple and costs down Instead we orchestrated the entire process by using Cloud BuildWe decided against using the native BigQuery cross-regional dataset copy service due to some limitations associated with it but also because the engineering team wanted more control over how the data was handled during the process Not only did the data itself need to be migrated but also all the dataset and table IAMs views UDFs and partitioned tables which is a limitation of the existing serviceAnother determining factor was that the hot data (data frequently accessed/used by the business) needed to be moved within an aggressive 48 hour window This coupled with the need to validate that the data had been migrated successfully and without corruption made it all the more challenging from an engineering perspective Finally having a well planned and robust roll-back strategy is an important piece of the puzzle Although we didnt need to use it you have to plan for things going wrong  it is just software after allUsing bespoke Python scripts and leveraging Python Pools for parallelisation the process to migrate the data from EU to AU was as follows: Although the high-level architecture and outlined process above paints a somewhat trivial workflow the movement of that much data did in fact throw up a few considerations that werent in play for smaller repatriation projects that wed done in the pastFor example BigQuery has limits and quotas for extracting and loading with GCS that we needed to consider and engineer solutions for The main 2 quotas that needed to be considered were: Additionally we had a few tables over 100TB and they were hot data tables being updated in real-time by streaming jobs Migrating these tables were by far the most technically challenging hurdles that we needed to overcome The first thing that we wanted to do was migrate any long tail data in the partitions that hadnt been modified or updated in the last 5 days This would allow us to migrate those partitions in the large tables ahead of time and before the 48 hour migration window To help us identify those partitions that we could migrate early we had to revert to some good old Legacy SQL in BigQuery to work it out: Based on that information we could then break the 100TB+ tables into smaller chunks so we could more easily migrate them On the other side when reloading them back in we of course needed to reassemble/recombine then back into one table with the right partitions This also involved some more heavy duty engineering effort The overall process of migrating the big 100TB+ tables looked like this: By using this approach we were able to gradually day by day migrate 500TB of data without any data loss and with minimal disruption to the business Pro tip: request your quota increase for data extraction with enough lead time because it can take some time to be approved/implementedThe team also needed to dynamically provision dedicated BigQuery throughput via flex-slots in order to meet the scalability demands of the migration but at the same time using techniques to keep GCP costs as low as possible for REA Group during the migration We also tried to use Avro file format for the extraction and reloading but we ran into a lot of problems with data type conversions and trying to reload the data in Sydney which just became too hard In the end we gave up on Avro and went back to using compressed JSONFlex-slots were also needed for the extract and load jobs due to the amount of data we were working with Using the default slot pool for extract and load jobs (which is separate to the slot pool for queries) would not cut the mustard Pro tip: when using flex-slots for extract and load jobs you need to use the  pipeline parameter when setting up the slot reservation via the APIKeeping costs down for REA Group was an important consideration and part of the project success criteria Handling data at such scale does incur higher costs and the team needed to be very mindful of this Examples include the often overlooked cost of network egress charges for the transfer of data from the EU to AU region when using GCS and the fact that any long-term storage in BigQuery would be reset once reloaded into its new home in Sydney However with the help of Google SMEs and by leveraging our GCP expertise the team managed to keep costs down and within the budget allocated for the projectOperating at that scale does not come without its challenges Moving almost 500TB of data without causing disruption to the business needed to be carefully planned and executed Servian had done the same work for other customers in the past but with just a fraction of the data However by all accounts the repatriation of REA Groups BigQuery data assets was a great success And on that note Ill leave you with a nice quote from our happy customer/stakeholder: We turned to Servian to help us with this important piece of work Their team had significant GCP experience and in particular their in-depth knowledge and experience with BigQuery gave us the level of confidence we needed for such a critical project to be delivered successfully By using Servian and leveraging their experience in having already done this type of work before with other customers it saved us a huge amount of time Servian was able to assemble a team with considerable expertise and no doubt saved REA Group substantial development time They always carried themselves professionally and engaged early with issue identification and remedies Servian was instrumental in delivering the solution for repatriating our BigQuery data and supporting us to manage costs I look forward to working together again in the future  Leigh Tolliday General Manager Data Services REA Group
B6Hz8NCB7C7T8X3s3yJ8ML,Kevin is currently a Data Engineer but initially joined Wealthfront as a Front End Engineer back when there were less than 40 employees in the company He has been part of a variety of projects related to feature development growth and infrastructure Most recently he worked on an event ingestion pipeline to analyze user activity and helped deliver analytics related to Wealthfronts new retirement planning feature PathIn 2014 I had attended a programming accelerator called Dev Bootcamp Upon graduating Dev Bootcamp connected me with Wealthfront I had done some research about the company and was drawn in by the opportunity to be a part of a fast-growing startup with an experienced executive teamYou started your career at Wealthfront as a Front End Engineer and you are currently with the Data Engineering teamI was hired as a Front End Engineer and worked on projects focused on our desktop browser app for about a year It was then when my manager asked if Id like to try working on a couple of projects that would help ETL data from third party vendors into our data platform I hadnt pushed any Java code into production before but the rest of my skills were a good fit because I could understand the structure of web-based API calls I took on the project as I was excited to try out a new skill and learn more about data processing With the mentorship of the engineers on the Data team I was able to successfully deliver on the projectAfter this there were a sequence of projects that we wanted done in our data platform and I was asked if Id be interested in working on them I took on the challenge and over time I built up my skills and knowledge in the data platform code base I enjoyed the work that I did and the team I was on and after several months I made a more official transition to Data EngineeringHaving been at different companies before this I will say that our engineering culture and core values go a long way towards improving the quality of life as an engineer Notably maintaining a list of proportional investment projects allows us to appropriately balance projects that help automate and improve developer productivity with client-facing features Another value we have is maintaining monitoring around production systems which not only allow us to improve visibility for on-call rotations but also help us appropriately assess the urgency for scaling existing systemsIn the three years of working here the company has participated in the JP Morgan Corporate Challenge which is a charity race where we get to race as a company in downtown San Francisco Ive gotten more involved in running since thenI drive a Honda Element and I think its the best car ever: Wealthfront Inc is an SEC-registered investment adviserNothing in this communication should be construed as an offer recommendation or solicitation to buy or sell any security Wealthfronts financial advisory and planning services are only provided to investors who become clients pursuant to a written agreement that is available at wwwwealthfrontcom are designed to aid our clients in preparing for their financial futures and allow them to personalize their assumptions for their portfoliosAll investing involves risk including the possible loss of money you invest and past performance does not guarantee future performance Wealthfront and its affiliates rely on information from various sources believed to be reliable including clients and third parties but cannot guarantee the accuracy and completeness of that information For more information please visit wwwwealthfrontcom or see our Full Disclosure
JLVWZyCTTVveodz3ipgDM9,Rob grew up in a small town in New Hampshire before venturing out to Stanford to earn his BS in mechanical engineering He worked for a few years at a company that makes communication satellites for other companies like Sirius radio and DirecTV He is currently the technical lead for the Data Engineering team here at WealthfrontI first heard about Wealthfront from my good friend Emilio He recommended Wealthfront both as a useful service and a great place to workYou previously worked in the Defense and Space industry where you built software for satellitesOur extensive onboarding curriculum made the transition very easy for me The biggest high level difference for me is how we interact with clients At Wealthfront were building a product for people all over the country with many different needs In the satellite industry however youre writing applications that are going to be used by just a handful of people This drastically changes how specs and requirements for features are generatedOne bit of advice that has stuck with me is something Andy Rachleff our CEO has emphasized: A good decision today is better than a great decision tomorrow While you may get value from extra time spent deliberating it pales in comparison to the value you get from having made a similar decision earlier It is so much more important to learn about the results faster or free people up to work on a new feature soonerI love that we get together as a whole company for happy hour every Friday We go over weekly wins get questions answered by Andy and have presentations for projects people are working on I think this does a great job to connect the team together over how were working towards a common goalThe project I worked on that Ive found most interesting is called Sirius It is a key-value store that we use to make all the data analysis we do offline available to our production systems Things like the time- and money-weighted returns you see on your dashboard go through this systemMy guilty pleasure is Magic: the Gathering I got into it about two years ago and I love that I get to play the game with a large contingent of people at Wealthfront from across all our different functionsInterested in working with Rob? Check out our open roles: Wealthfront Inc is an SEC-registered investment adviserNothing in this communication should be construed as an offer recommendation or solicitation to buy or sell any security Wealthfronts financial advisory and planning services are only provided to investors who become clients pursuant to a written agreement that is available at wwwwealthfrontcom are designed to aid our clients in preparing for their financial futures and allow them to personalize their assumptions for their portfoliosAll investing involves risk including the possible loss of money you invest and past performance does not guarantee future performance Wealthfront and its affiliates rely on information from various sources believed to be reliable including clients and third parties but cannot guarantee the accuracy and completeness of that information For more information please visit wwwwealthfrontcom or see our Full DisclosureOriginally published at https://wwwlinkedincom on December 20 2016
UcYdMxyyHg8k9vcusGDLkj,On the 30th of May XTech Community promoted a hands-on Spark Intro & Beyond meetup The presentation kicked in with the introduction of Apache Spark a parallel distributed processing framework that has been one of the most active Big Data projects over the last years considering both its usage and all the contributions made by its open-source communityIn comparison to Hadoop MapReduce the main advantages of Spark revolve around its facility to write jobs with multiple steps through its functional programming API as well as the ability to store intermediate data in-memoryThe functionalities offered by Spark were introduced with the help of its built-in libraries implemented over the Spark Core Engine and by the possibility to use external libraries available in the Spark Packages repositoryAfter the introduction the focus of the presentation shifted towards the important Spark Core concepts We learnt: A simple word count in Spark was used to demonstrate the ease of the writing process for a first job as well as all the components involvedBy presenting the execution result of the word count in Spark UI a web interface available during the Spark application execution that allows its monitoring we explained the meaning behind the physical execution units in which a Spark application is divided: Job Stage and TaskThe next topic of the Spark Intro & Beyond Meetup was Sparks partitioning in which the number of RDD partitions directly depends on sources data partitioning and other possible parametersUnderstanding how the partitioning process works is crucial since the number of partitions of an RDD determines the execution parallelism between distributed operations and its control allows the developer to write more efficient applications and better use the clusters physical resourcesFinally we showed 3 techniques that minimize the quantity of data sent between two Stages of a single Job called shuffle data and help improving the applications performanceShuffles in Spark are extremely expensive because data is sent over the network between Spark Executors that reside in different cluster nodes or written and read by Executors that reside on the same node
9iv7ozBqizGwXGojtEQJAK,At YipitData our Analysts often need to view S3 buckets to check 3rd party data deliveries as well as our own data exports This can be pretty painful! Our Analysts work inside a Databricks Notebook which gives a decently high bar for visualization expectationsYour options are not great: The AWS Console has a decent interface but presents a few problems Primarily that you now need to give console logins to everyone who needs to use it This falls outside of our desired security model and created a lot of friction with Analysts who needed to check data deliveriesThe aws-cli is a decent interface for small buckets However it can be a bit painful to constantly navigate deeper into nested directories and interpret the command line output when working with large numbers of files Your eyes will glaze over pretty quickboto3 is the wildcard it is extremely flexible since you can write Python However it puts the burden of the interface on the user which isnt a great outcome for Analysts who are focused on data not writing Python wrappersWhat is the better option? We wrote browse_s3 to give the dataframe interface to browsing S3 This eliminates the need for AWS Console logins while providing a clean and familiar interfaceThe goal is to provide a consistent interface for browsing We dont have full interactivity but we can get close enough While S3 doesnt actually have folders it is helpful to present information this way when browsing Helpfully if you stick to some basic conventions boto3 easily satisfy this experience between Contents and CommonPrefixes when using list_objects_v2While not on display here this interface seamlessly displays folders and files together keeping folders at the top per convention In addition to the normal S3 metadata (etag LastModified and size) we also include extra columns with the full prefix path and S3 URL Finally with this information stuffed in a dataframe you can easily use the sorting UI elements and join against it in SQL and PySpark
S8YPqCJWNPQepZaGUHxzJV,This talk was inspired by some extensive (historical) usage of gevent at YipitData general confusion about the two terms and the state of our current architectureIn the past YipitData used gevent to manage concurrency in our applications We would have top-level jobs devoted to solving a problem (generally gathering web data) and handled pieces of each job through different co-routines One job might be finding all items in a marketplace given a category URL Within the job one co-routine might be pulling URLs from a queue the next might be requesting the URL the next parsing the data and the last saving that information to a database This approach had the advantage of a single Python process running many co-routines and being very CPU efficient  most of the time the CPU could be parsing data while it waited for various web requests or DB inserts to come backToday we write simpler applications; theyre composed of small and independent functions that we scale across multiple machines This was enabled by standardizing our queueing framework (check it out at readypipeio) and a lot of investment automating our server provisioning We moved the complexity out of our applications and into our infrastructure; we still get the benefits of a concurrent application model but without necessarily concurrent programming practicesCaveats: I use the term process very frequently in this talk and mean a few different things at different times (a Python process a CPU an OS process etc
YgRNwMnHTFAegDKKAwAG6T,Welcome! 😁 Im JooYoung Lim Data Engineer in YPLabs CoTodays post will be Automatically Save AppsFlyer Marketing Data To BigQuery By Using Pull APIPull API  Push API and Datalocker are slightly differentWe want to save Performance Reports so we should use Pull APIWe can download several reportsI want to save Performance reports > Partners daily reportsWe need Account Owner API Key start date and end dateYou should know iOS/Android reports file is differentWe will use Apache Airflow to schedule saving reportsCreate BigQuery table by using csv file that we downloaded beforeI referenced blog (http://sanghunDAG file is so simple You can understand without explanationFirst of all save iOS / Android reports fileBy load_to_BigQuery function we can load csv file to BigQuery tableIn short stages will be save reports through URL -> load reports to BigQueryFor addition I used Google Cloud Composer to deploy Apache AirflowUpload DAG file to your Airflow DAG folder
nYhaAmudr8jCh6eFNR3UgR,It was the spring of 2019 when the Data Engineering team was officially created at Zendrive The charter was simple: manage the petabytes of time-series data available to usThere was a lot to do then and theres even more to do nowWhen we sat down to figure out the current challenges in our processes one in particular stood out A sheer number of processes would be run manually whenever the need arose So too often we were deploying resources  personnel and our Spark cluster  to handle the task It was sapping too many of our resources and too much of our attentionTake one of our core technologies the Hard Braking Detector- which does exactly what it says on the tin Lets say we are running a version v5 of the detector on the field while we have also simultaneously developed a v6Now we want to test out how they compare This process would require you to load up the data for say 30 days Then you extract the hard braking numbers for each version then you compare their performance numbers in different segments: at the trip-level then at the phone-level even at the OS-levelEach of these aggregate operations is expensive to compute when you are dealing with millions of trips getting recorded every daySo we decided Lets make a dashboard out of this! We wanted to automate it so that the dashboard would get populated with aggregate stats dailyFor automating a recurring job our first instinct was to run a Cron job But Cron as you know comes with its limitations Even ignoring the lack of an interface to monitor jobs or the difficulty in debugging a failed Cron job the main drawback we wanted to avoid is the lack of dependency management We wanted to adopt a solution that would scale  and thats where Airflow came inTake it straight from the docs: Airflow is a platform to programmatically author schedule and monitor workflows The Airflow scheduler executes your tasks on an array of workers while following the specified dependencies The rich user interface makes it easy to visualize pipelines running in production monitor progress and troubleshoot issues when neededBasically it schedules work as it should beAirflow uses directed acyclic graphs (DAGs) to manage workflow orchestration Tasks and dependencies are defined in Python and then Airflow manages the scheduling and executionIn our case our program has two main steps: compute the numbers and push the aggregate stats to our BigQuery data warehouse (chosen for ease of visualization via DataStudio) By now you may realize where the dependency management fits inWe need to first finish computing the numbers before injecting them into BigQuery So we designed a metrics generator and a metrics injector classSince Cron does not care about dependencies it would be a pain to maintain such a job If the preceding job finished on time all would be fine However if it exceeded its time things were bound to breakThats where Airflow stepped upWe wrote the corresponding DAG*: *(Note that the DAG is in PythonWhat if one task fails? Airflow allows you to set the number of times youll retryWhat if there are old tasks to rerun? It lets you backfillWe in the data engineering team love Airflow for the powerful functionalities it offers Currently we maintain 13 Airflow jobs in production  ranging from DAGs that are used by one of our core algorithms for model building and subsequently model prediction to DAGs that are used for integration-testing production codeI am quite fond of the dashboard It allows us to monitor all tasks at a glance identify issues quickly firefight in real-time and fix them before an angry emailThe best part? More UI less code
ay2mPRBwymbqfdb95bENK7,iForm Enhanced Security (ES) is a version of the iForm mobile client that is compliant with the federal governments FISMA and HIPAA requirements However since it was first released eight years ago it has only been available on iOS mobile devices iOSs reign of the iForm ES app has come to an end The long awaited Android app is now available for download from the Google Play StoreAs the Android developer at Zerion Software the task became mine to undertake Because iForm ES already exists for iOS (and has for some time) this became my biggest blessing as well a my biggest curse On one hand the task had well defined parameters Basically make the Android client mimic the iOS client On the other hand the iOS client has gone through many cycles and iterations in the development process to bring it to where it is today and now we had to play catch up in a months timeOne of the most notable differences between iForm and iForm ES is the ability to set an idle timeout A user-customizable time can be set for how long this timeout is When this specified time has elapsed and if no user activity has taken place the current signed in user is required to re-enter their password in order to continue using the app via a challenge password screen This is where I began developing the ES client The idle timeout timer is a simple Android Count Down Timer customized to fit the needs of the app Each activity in the application needs to be able to track user interaction and start or stop the timer as needed For simplicitys sake I created a base activity from which all of my other activities could then inherit This base activity will do all the heavy lifting for the idle timeout (ie creating the timer objects starting and stopping the timer when necessary handling what happens when the timer runs out and detecting user interaction)One behavior of the Android system that I became aware of while testing is that using the keyboard did not trigger the callback to on User Interaction This became a problem as even if a user was inputting data into a field via the keyboard the timeout timer would continue to countdown and present to the user the challenge password dialog if they didnt finish entering the text in the specified time Fortunately for text fields Android has a convenient way of dealing with text as the user inputs keystrokesThe iForm app is well known for its offline capabilities We have users that spend months offline collecting data One of the nice aspects of iForm ES is that it inherently makes implementing offline login to the app much simpler This is because of a user specific password encrypted database When online and attempting to login the entered credentials are authenticated with our server If the credentials authenticate correctly the server responds back to the client that the user is allowed to login In the absence of this server call when no network is available the database is used to verify the users identity The database will only open if its supplied the correct password If an incorrect password is used to try to open the database a SQLiteException is thrownWith the user now signed in the the users password needs to be accessed routinely when database reads or writes occur However the password cant be written to the device Android apps have an Application classbase class for maintaining global application state…The Application class or your subclass of the Application class is instantiated before any other class when the process for your application/package is createdThe iForm app already makes use of a custom Application class so this seemed like a logical place for dealing with the password problem since the password needs to be tracked in all areas of the applicationWhenever the user is presented with the challenge password screen or logs into the app this password is updated if the entered password successfully opened the databaseMy goal of this article was to give the users some developer insights/behind the scene functionality to the app that they use daily and these were just some examples from the iForm ES for Android development process If you have any new and useful features that you would like to see incorporated into our app you can make feature requests here
DqpGng7dejkqLpX3Khrtzo,The Cloud Network Infrastructure that Netflix utilizes today is a large distributed ecosystem that consists of specialized functional tiers and services such as DirectConnect VPC Peering Transit Gateways NAT Gateways etc While we strive to keep the ecosystem simple the inherent nature of leveraging a variety of technologies will lead us to complications and challenges such as: Cloud Network Insight is a suite of solutions that provides both operational and analytical insight into the Cloud Network Infrastructure to address the identified problems By collecting accessing and analyzing network data from a variety of sources like VPC Flow Logs ELB Access Logs Custom Exporter Agents etc we can provide Network Insight to users through multiple data visualization techniques like Lumen Atlas etcVPC Flow Logs is an AWS feature that captures information about the IP traffic going to and from network interfaces in a VPC At Netflix we publish the Flow Log data to Amazon S3 Flow Logs are enabled tactically on either a VPC or subnet or network interface A flow log record represents a network flow in the VPC By default each record captures a network internet protocol (IP) traffic flow (characterized by a 5-tuple on a per network interface basis) that occurs within an aggregation intervalThe IP addresses within the Cloud can move from one EC2 instance or Titus container to another over time To understand the attributes of each IP back to an application metadata Netflix uses Sonar Sonar is an IPv4 and IPv6 address identity tracking service VPC Flow Logs are enriched using IP Metadata from Sonar as it is ingestedWith a large ecosystem at Netflix we receive hundreds of thousands of VPC Flow Log files in S3 each hour And in order to gain visibility into these logs we need to somehow ingest and enrich this dataAt Netflix we have the option to use Spark as our distributed computing platform It is easier to tune a large Spark job for a consistent volume of data As you may know S3 can emit messages when events (such as a file creation events) occur which can be directed into an AWS SQS queue In addition to the s3 object path these events also conveniently include file size which allows us to intelligently decide how many messages to grab from the SQS queue and when to stop What we get is a group of messages representing a set of s3 files which we humorously call Mouthfuls In other words we are able to ensure that our Spark app does not eat more data than it was tuned to handleWe named this library Sqooby It works well for other pipelines that have thousands of files landing in s3 per day But how does it hold up to the likes of Netflix VPC Flow Logs that has volumes which are orders of magnitude greater? It didnt The primary limitation was that AWS SQS queues have a limit of 120 thousand in-flight messages We found ourselves needing to hold more than 120 thousand messages in flight at a time in order to keep up with the volumes of filesThere are multiple ways you can solve this problem and many technologies to choose from As with any sustainable engineering design focusing on simplicity is very important This means using existing infrastructure and established patterns within the Netflix ecosystem as much as possible and minimizing the introduction of new technologiesEqually important is the resilience recoverability and supportability of the solution A malformed file should not hold up or back up the pipeline (resilience) If unexpected environmental factors cause the pipeline to get backed up it should be able to recover by itself And excellent logging is needed for debugging purposes and supportability These characteristics allow for an on-call response time that is relaxed and more in line with traditional big data analytical pipelinesAt Netflix our culture gives us the freedom to decide how we solve problems as well as the responsibility of maintaining our solutions so that we may choose wisely So how did we solve this scale problem that meets all of the above requirements? By applying existing established patterns in our ecosystem on top of Sqooby In this case its a pattern which generates events (directed into another AWS SQS queue) whenever data lands in a table in a datastore These events represent a specific cut of data from the tableWe applied this pattern to the Sqooby log tables which contained information about s3 files for each Mouthful What we got were events that represented Mouthfuls Spark could look up and retrieve the data in the s3 files that the Mouthful represented This intermediate step of persisting Mouthfuls allowed us to easily eat through S3 event SQS messages at great speed converting them to far fewer Mouthful SQS Messages which would each be consumed by a single Spark app instance Because we ensured that our ingestion pipeline could concurrently write/append to the final VPC Flow Log table this meant that we could scale out the number of Spark app instances we spin upOn this journey of ingesting VPC flow logs we found ourselves tweaking configurations in order to tune throughput of the pipeline We modified the size of each Mouthful and tuned the number of Spark executors per Spark app while being mindful of cluster capacity We also adjusted the frequency in which Spark app instances are spun up such that any backlog would burn off during a trough in trafficProviding Network Insight into the Cloud Network Infrastructure using VPC Flow Logs at hyper scale is made possible with the Sqooby architecture After several iterations of this architecture and some tuning Sqooby has proven to be able to scaleWe are currently ingesting and enriching hundreds of thousands of VPC Flow Logs S3 files per hour and providing visibility into our cloud ecosystem The enriched data allows us to analyze networks across a variety of dimensions (eg availability performance and security) to ensure applications can effectively deliver their data payload across a globally dispersed cloud-based ecosystem
Qb37RGMcniu2yHXm8ebXQC,At Robinhood we ingest and process terabytes of data every day This data ranges from real-time streams of market data and in-app events to large files that are essential to our clearing operations To deliver the best possible experience to customers as we grow weve invested in scaling our data architecture After evaluating multiple approaches we designed a data lake system that can support petabytes of data and offer a unified query layer to employees while ensuring safe and secure data access In this post we detail the journey from identifying the challenges that led to the inception of the data lake at Robinhood to how it was deployed and is currently helping Robinhood scale its data platform efficientlyWorking with various data types means there are often different solutions for storing data We use PostgreSQL and AWS Aurora as our relational databases Elasticsearch as a document storage and indexing solution AWS S3 for object storage and InfluxDB as a time series database Depending on product needs we maintain several clusters of each of these systems As a result internal users often had to join data across multiple data stores some of which might have even used different query languagesWe initially used Elasticsearch and Redshift as our analytics and data warehousing solutions After a couple of years of maintaining these systems we determined that scaling these systems to support our increasing data workload was neither efficient nor economical With storage and compute coupled together on these systems we would need to scale up our cluster to allow for ingesting more data or adding more jobs on the existing dataTo improve the process of joining data across multiple data stores we had set up a Jupyter server which had access to multiple data stores However these environments lacked a strong identity management system which made it harder to maintain different levels of access and achieve query-level isolation across different user groups of the research environmentWe wanted to build a solution that would solve all of these problems and provide us a future-proof and extensible framework as we undergo rapid growthWe researched several potential solutions that would address the unique challenges we face at Robinhood We read about how Netflix and Uber leveraged Presto and Parquet to scale their data platforms and how other financial institutions like FICO and National Bank of Canada leveraged big data tools on the cloud We also looked into the criticism and failure cases for data lakesWith this insight we developed an initial architecture for a data lake on the cloud leveraging managed solutions It was important that the solution we opted for could be set up with minimal effort and operational overheadData enters the systems at the ingestion layer in two forms: stream or batch We use Kafka as our databus to stream data which usually comes from external feeds or from internal Faust apps We use Secor to archive our Kafka streams to S3 and for batch data we have internal systems directly post the data to S3Our goal for the storage layer was to provide cost-efficient performant and reliable storage We primarily use S3 for this layer since it provided all of these functionalities and abstracted away all the complexities of managing a distributed file storage system We use Redshift in a more focused role for storing derived datasets that power some frequently used dashboards in our business intelligence tool We also backup the source data stores in AWS Glacier for longer durations as a cold storageSome of our datasets like in-app events or load balancer logs are too big to query efficiently in its raw form and most of the data is likely irrelevant to any particular query Therefore we process these datasets further to split them up into mini-sets and calculate aggregate metrics using distributed data processing tools like Spark To avoid the overhead of managing our own cluster we use AWS Glue which offers a serverless Spark framework After processing these datasets we post the results back to S3 in Parquet columnar format using Snappy compression To automatically generate the schemas for these datasets and make these datasets discoverable using a metastore we use Glue crawlers and the Glue metastore respectively We use Airflow to orchestrate the process which usually involves these steps: After we developed a solution to process our data we needed to setup some tooling to allow employees to query this data We primarily use Presto and in order to get up and running without having to set up our own Presto cluster we enlisted AWS Athena a managed Presto solution Athena works seamlessly with the Glue metastore to discover the processed data stored in S3 In addition to Presto a subset of our queries from the visualization layer also go to the Redshift cluster that stores some aggregated datasetsMost data lake failure cases discussed a disconnect between producers and consumers of the data If producers post arbitrary data with no checks and balances into the data lake consumers tend to lose faith in the quality of data turning the system into a data graveyard We tried to proactively tackle this issue by creating a data validation framework and enforcing a producer-driven validation model The framework very simply allows the owner of the dataset to specify some common-sense thresholds about their data like the expected number of rows uniqueness constraints or categorical values Once the data is processed we run a verification task that runs queries using Athena or Redshift and verifies that all the preset tests pass before making the data available to usersEmployees that need to interact with the data in the lake can do so in a couple of different ways We use Looker as our business intelligence and data visualization tool at Robinhood For more complex data use cases we provide a Jupyter notebook research environment based on top of Jupyterhub This system is usually used by data scientists and engineers for exploratory data analysis and training and testing machine learning modelsAfter building the initial tooling we spent quite a bit of time training users writing documentation and customizing our internal tools to work with the data lake Robinhoods data lake was quickly adopted within the company since it was built to meet the needs of internal users and quickly delivered valueHere is a snapshot of where we are today with our Data Lake: Were continuing to make the onboarding process as easy as possible for technical and non-technical users alikeData lake has had a positive impact on our internal teams so were investing in strengthening different pieces of this systemTo gain more fine grained tuning controls and visibility into our Spark jobs we set up a couple of self-managed Spark clusters with around 50 nodes each Similarly we brought up a 50-node Presto cluster to work past some of the limitations we noticed with AthenaA few improvements in the works are: Weve created a simple yet strong foundation to build out most of these complex functionalities If youre as excited as we are about solving these types of challenges we encourage you to come join us on the Data Platform team at RobinhoodIf youre interested in learning more we talked about our journey of building the data lake and our use of AWS Glue at AWS re:Invent 2018
JYCAqC57wBQo4Ce99r62GS,My wife Kim and I are refurbishing a home we recently purchased in her hometown of Southside Alabama The house was built in 1978 and perhaps had a hammer last swung in maintenance that same year Always the planner I accumulated a list of over 250 tasks that would need to be done before we could move in The closing day left us both thrilled to have acquired our Fixer Upper and with trepidation of the months aheadSome tasks in such an undertaking require a great deal of skill and experience I marvel at how the craftsmen fit together the flawless miter in the trim around our new entry doors with speed and efficiency But there are also tasks that require only strength and determination to finish such as removing the popcorn ceiling so popular in the 1970s Or having successfully tested negative for asbestos removing the lime green and yellow linoleum lurking under the cracking tile in the laundry room In a home rehabilitation journey you have to know when to swing a hammer and when to write a checkSoftware development has been compared to construction in many ways for many years But in Shipts journey to maintain business visibility while moving from a monolith to microservices we too have had to determine when to swing a hammer (or our keyboards) and when to write a checkAt Shipt a principle driver of our migration from monolithic architecture to one based on microservices is to increase development agility The prime directive of our Data Warehouse team is to create a unified data platform for continued business visibility through our microservices transformation As our journey is a continual migration we need to consume data from both our monoliths Postgres database which we call Platform DB and the backing stores for the individual microservices The Data Warehouse needs to use both data sources to create a unified conformed view of the businessThese twin data pipelines of our project are being developed in parallel One half of the project is composed of the data pipelines for consuming Create Replace Update and Delete (CRUD) and business events from the various microservices into our Data Lake for eventual surfacing in our Data Warehouse The remaining half of the project is the data pipeline for replicating our Platform DB into our selected Data Warehouse platform SnowflakeJeopardizing the Platform DB by attempting to service analytic queries is a problem that has spread with contagion at Shipt Overloading the Platform DB necessitates the maintenance of five read replicas in order to serve the analytic query needs In a pattern certainly repeated by other data-driven organizations query workloads submitted to the read replicas create a replica lag which in turn both elongates query execution times and creates latency problems with the results of the queries The analytic queries asking for real-time data on business activities instead return data that is anything but real-time with replication lags that routinely exceed an hourA logical replication solution allows our client Data Warehouse pipeline to consume changes to the Platform DB without creating additional workload on it Specifically we desired a replication strategy utilizing the Postgres Write Ahead Log (WAL) that would supply our data pipeline with real-time change data capture using what is essentially a by-product of the Platform DB The WAL is a Postgres structure used to log changes committed to the database before those changes are written to Postgres table and index data files As the master ledger of Platform DB activity it is ideal as a source of truth for logical replicationAdditionally our Platform DB contains information not necessary in the context of the Data Warehouse Filtering columns from our Platform DB rules out yet another read-replica or byte-level replication Instead our final solution involves logical replication to move selected data from the platform to the Data Warehouse databaseWhen we first put it to the test our analysis yielded a handful of constraints for our solution: Another consideration was the Postgres maintenance window that our Platform DB would have to undertake in order to make the parameter changes to enable WAL-based replication As a 24/7 service maintenance windows involving a service outage which a restart of the Platform DB would doubtless cause are avoided except when absolutely necessaryWhen a Postgres Replication Slot is created the ability for the master database to flush changes from the WAL is dependent upon the replication slot clients timely processing of the changes in the WAL If the client replication process fails to notify the master database that changes have been processed the files for the WAL will continue to grow consuming space that can put the health of the Postgres master at riskAs illustrated below the Postres WAL contains the data necessary to apply changes in order as they are committed to the source database The vertical lines separate transactions each beginning with an address in the data stream that is used as the Log Sequence Number or LSN The LSNs can be used to determine the amount of replication lag in bytes by calculating the difference in the location of the first transaction and the last transaction in our exampleAfter successfully applying changes from the WAL to the target database our Fivetran connector will notify the source database the WAL may be flushed and the recovery LSN may be advanced to the next unprocessed transaction in the stream As depicted below the transactions in green have been consumed by the target database and the source database no longer needs to maintain the data for replication purposes It is important to note additional changes will likely be continuing to grow the WALWe put in place two monitoring strategies to detect and alert of this condition The first is a tool developed by our DBA team who are responsible for the health and monitoring of all of our Postgres instances Their tool which we fondly dubbed Snoopy reports on the WAL consumption via a Slack channel Snoopy was integrated with our existing on-call alert processing to notify the DBA and Data Warehouse teams in the event that our WAL consumption becomes excessive or threatens the health of the Platform DB The second is configuration within our Fivetran connector to trigger an email alert to the Data Warehouse team in the event of detected lag in processing the changes from the source Platform DB Fivetran looks for changes every five minutes and if a set of changes takes longer than thirty minutes to process the Data Warehouse team is alertedFivetran is one of many products in the market that provide logical replication from Postgres to a target database including Snowflake The Postgres WAL contains all of the changes to the source database as a set of JSON messages For logical replication to succeed the WAL messages must be monitored mapped to the target schema applied to the target and indicated as processed to the replication slot in order to flush WAL storage and avoid the WAL storage runaway conditionPrior to landing on these products we considered writing our own software to consume the replication slot messages However as we explored this option the maintenance load for the in-house option seemed to outpace the benefits As we migrate from the monolith to microservices the forecast service life seemed to be small  on the order of a couple of years before no changes would be happening in our Postgres sourceIn logical replication mode the data stream reflects data manipulation language (DML) consisting of inserts updates and deletes Data definition language is not explicitly accounted for in the replication data stream Each change does contain the table name column name column data type and column contents so create table and alter table statements can be inferred from the data stream Another benefit of Fivetran is the effect of schema-on-read flexibility with a relational and schema-on-write Snowflake databaseThe initial load of our Platform DB was planned for the weekend following our maintenance window for the required Postgres parameter changes After 36 hours of processing our Platform DB initial synchronization was completed The Fivetran connector was showing change data capture occurring on a regular basis within our targeted processing windowThe implementation of logical replication for our Platform DB was not a simple matter of creating a replication slot and configuring Fivetran to utilize it A maintenance window was scheduled and executed Specifically the configuration parameter wal_level was set to logical and after a restart of the database server our source database produced a logical decoding of the data changes data manipulation language queries (DML) and was ready for logical replicationA fragment of the logical decoding of the WAL describing changes to the publicorders table is below Immediately after the table is the operation (in the case of these two rows an insert and update) Finally the value of each column in the table including the data type completes each recordAs a grocery delivery platform the week of the Thanksgiving Holiday in the United States translates into our peak volume The telemetry that both the DBA team and the Data Warehouse team put in place to monitor both the disk storage consumption of the Write Ahead Log and the latency of the Fivetran connector was under a microscope of scrutiny Our teams were on-call with standing orders to drop the replication slot which would render the Data Warehouse out-of-sync in the event that any process associated with logical replication put the Platform DB and our Thanksgiving Week at riskI am thrilled to report that not only did we make it through our initial peak week but spikes in our business volume over the closing weeks of the year never tripped our alarms We have been synchronizing our Postgres Platform DB successfully ever since
7r4rvc5RQSaWdKnv9Gh6U4,Over the last year at busuu I have been trying to move the frontier of our usage and processing of our data from simply business reporting towards using it to provide a better experience for users Knowing how our users are utilising one of our features or AB testing is very useful to improve the product increase monetisation or highlight key areas to work on next however in the age of personalised Spotify playlists Amazon purchase recommendations and tailored Netflix binge sessions we can do betterAt a basic level busuu is similar to each of these players because we all deliver valuable content to our users for consumption or for purchase and users have developed a taste moreover a thirst for personalisation of their online experienceFrom the start the key has been to get the basics right as no matter what the old paradigm of throw rubbish in and you will get rubbish out stands true with any data process or infrastructureI spoke at Snowplow analytics London about my plans for this stack and the state of affairs at the MVP stage (link here  https://wwwyoutubecom/watch?v=wSmTC1JDSyc&t=852s ) In this blog would like to talk here about the results both the wins of this project and the lessons that I faced along the wayDue to the experimental nature of this project I used a lot more managed services from AWS than our company would usually like This reduced the dev time needed from DevOps team and allowed me to move quicker Below is a base diagram of what the stack ended up looking likeOne of the positive lessons from this project is how trying to be as server-less as possible resulted in more flexibility scalability and reduced cost Since Ken Fromm brought this idea to the fore in 2012 systems engineers have been trying to build in the flexibility of server-less apps & stacks whilst reducing vendor lock and retaining control What we have found is the deeper of an abstraction you go to the more you can worry about the code and less about the hardwareThat being said I have found on this project there have been some significant findings when it comes to adopting serverless methodologiesThe core concept of this infrastructure came from snowplow analytics open source event pipeline which we really liked due to the built in schema validation data enrichment and data collection SDKs The instructions from snowplow made it easy to select which of their products you need to achieve what you need and their code builds first time without a problem Assuming that you are careful with your schema and configuration file syntax there have been very few errors in production Try using the snowplow-mini repo It is a useful tool for building quick integration environments  https://githubcom/snowplow/snowplow-miniThis setup was not without issue however The enricher and validator are highly parallelised meaning that there is a lot of strain on your schema file to validate each event In the snowplow setup guide it suggests hosting your validation schemas on s3 however if you host them locally on an apache webserver the increase in performance is phenomenal (see below)To ensure realtime processing high parallelism is the way to chomp through all that data quick enough We achieve this in two different ways: This takes care of the processing power but the good news continues; AWS kinesis has a lease handler build into it that not only ensures each record is only processed by one worker that is attached to the system safeguarding against duplication but it also load balances the data load against the workers automatically This means that as long as the workers are up and running and have the resources they need kinesis will sort everything else outA centralised system for collecting validating processing distributing and storing data will result in better data qualityA soon as the production system was fully online and implemented by each of the data creating platforms this was the first thing we noticed as a company The extra data brought in by this stack has provided us new insights to our users and how they use our platform It has been enhanced with extra metadata such as screen sizes user agents and a dynamic number of event specific parameters for each event type just to name a few Once proliferation of users sending events to the stack was nearing its maximum (we had to wait for apps to update for example) the number of events being output to our data warehouse matched what we were expecting satisfactorily and we were quickly ready to use the dataReal time data will provide better machine learning predictions for use within a users sessionHaving good realtime data has allowed us to cache and update user features that are applied to our prediction models Ensuring that more features are available when needed has meant that the predictions use the most up to date data and the precision and accuracy of our models has exceeded 90%Tracking everything once and distributing the data to third parties after processing will reduce SDKs & reduce frontend team dev time spent on trackingThis was not necessarily the case as most of the third party tools integrated inside the apps had other purposes such as push messaging chat or device level marketing attribution therefore most of the SDKs have had to stay However these are becoming out of the box functionalities instead of internal systems that have to be maintained by our front end developers The tracking that gives these third party tools insight on what to do is delivered to them serverside after validation and enrichment ensuring better data quality and higher effectiveness
GHWJUyht4eRCRk9CcnLwLH,ETL as a concept has existed in one form or another as long as databases have existed You Extract structured/unstructured data from a source Transform the data based on your needs and Load it into its destination for analysis or used by other pipelines/tools However over the years as volume of data warehouses grew and massively parallel processing database frameworks came into existence a few other data flow paradigms came into vogue In this post I discuss about how we started leveraging ELT (actually ETLT) using Redshift SpectrumWe currently have over 130 data flows managed by Airflow that leverage Hive and/or Spark on EMR (Amazons Elastic Map Reduce Cluster) These pipelines extract data from a plethora of data sources like traditional RDBMS S3 FTP and APIs etc We transform the data using Hive/Spark and eventually load it into their final destinations like S3 Redshift and sometimes to RDBMS external API endpoints As you might have noticed this is the traditional ETL data flow paradigm and most data flows fit really well in this paradigmHowever we support data flows for many different stakeholders inside the company and every flow has unique challenges For example over time we noticed most of the data flows that support the marketing team could be better served by switching them to the ELT paradigm These data flows are mostly aggregations of data that are already transformed and enriched and are sitting in Hive staged to send them to an external service Aggregations such as these benefit from a MPP database like Redshift (that we already have in our ecosystem) which is optimized for analytical queries These kind of aggregations work better in Redshift because it is a MPP database that is tailored towards analytical queries as opposed to Hive This was our first foray into thinking about the ELT model (well see later in this post that this is actually more of an ELTL approach)Even though using Redshift makes sense we didnt want to increase our cluster size which would have increased its cost Instead we wanted to leverage the data that was already on S3 (via Hive external tables) There are multiple ways to query this data: Using a more analytics and read optimized query engine like Presto or Athena (AWS Presto wrapper) or Redshift Spectrum Redshift Spectrum fits perfectly into our ecosystem because we are already using RedshiftBefore we get into how we used Spectrum lets take a moment to define ELT ELTL and introduce Redshift Spectrum more formallyELT is a relatively new paradigm that leverages data stored in data lakes and MPP databases Instead of having the data processing layer in between the source and target data stores you extract the data from your sources load it directly into your analytical store and then perform all the required data transformations Redshift is very popular for this type of data flow and many startups are opting for this route during their initial growth phase It reduces the cost of developing new data flows and scaling Redshift is pretty straight forward In addition removing a layer of infrastructure from the data flows makes them easier to maintain However this isnt a panacea and it doesnt suit every type of workload As our company and its data needs grow the data flows will mature and take a hybrid formETLT is essentially a hybrid version of ELT and ELT where it leverages multiple data processing frameworks to their respective advantages In our case we use Hive for data munging and processing and Redshift for aggregations on top of already processed dataRedshift Spectrum is new add-on service for Redshift that Amazon introduced mid-2017 It allows you to leverage Redshift to query data directly on S3 Redshift Spectrum is a good option for those who already have/work with Redshift For those who do not take a look at Athena Athena is much like Redshift Spectrum with the exception of the chosen execution engine (Athena uses Presto) whereas Spectrum uses Redshift It should be noted that Spectrum also follows pay-per-query pricing model like AthenaLets look at how Redshift and Spectrum communicate with each other how tables are created on top of stores such as S3 and just how much interoperability is providedSpectrum needs an external meta store for the data catalog to maintain table definitions; we used a Hive meta store for this purpose Our Hive/Spectrum meta store is simply a RDS instance running MariaDB Once we setup Spectrum to talk with our Redshift cluster and use the newly created schema space in the Hive meta store any external table created in this schema using Hive is visible and usable immediately from Redshift You can query these tables directly from Redshift and Redshift/Spectrum will automatically move the required portion of data (based on the query) on to Redshift cluster and execute it thereWe can create external tables in Spectrum directly from Redshift as well But because our data flows typically involve Hive we can just create large external tables on top of data from S3 in the newly created schema space and use those tables in Redshift for aggregation/analytic queriesOur first attempt was with an ELTL model; however we determined we could simplify and use ELT by skipping over our Hive data processing layer where we didnt need that data in our data warehouse For instance we can skip over a data processing layer in Hive entirely when we dont need that data in our traditional data warehouse Two of the immediate benefits we saw are: Our data flows faster than before as we saw 10x-30x processing time improvements for specific data flowsOur data flows also became simpler  we started skipping a data processing layer when dont need that data internally (for our team) but only used by teams on Redshift via analytical toolsOur approach to new data flows changed and we now evaluate which is the best way to implement a specific pipeline: whether it is traditional ETL or ELT or hybrid ELTL
jMgAy6EX2c64mWTNg6jwon,Picture this: Its Monday around noon and our ETL has just completed the previous days load after dealing with several issues overnight We are at 85% disk full and Redshift VACUUM just kicked in to clean up all of our soft deletes from last week  nothing significant can run during this time We pause all of our processes to let VACUUM complete its jobMeanwhile internal and external customers are raising flags about data freshness and overall slowness The Data Engineering team is heads-down closing out the backlog and dealing with operational issues instead of developing new functionality Migrating to a different cluster configuration can take a full 48 hours but we arent sure that it would resolve our speed issuesThis used to be a typical day for Instacarts Data Engineering team We build and maintain an analytics platform that teams across Instacart (Machine Learning Catalog Data Science Marketing Finance and more) depend on to learn more about our operations and build a better product Redshift was our cloud data storage warehouse for several years and it served its purpose at the time…however we started running into scaling and stability issues as we grew so we made the decision to migrate our platform to Snowflake We pulled together some of our key migration observations and learnings to get you started on your ownWe decided to clone the schemas and tables created in Redshift without major changes in data structure Focusing on migration only (without changing structure) was key for the successful migration of our big complicated system filled with dependencies This approach also allowed us to easily compare the data between the old system and the new system and run them in parallel to gain confidence before the official switchOne of the big advantages Snowflake architecture provides is the separation between storage and compute We built a dedicated warehouse for our major applications and made sure to name the warehouse so it was easy to recognize who within the organization is using it Once we provided the permissions for teams to use the warehouse it was easy for us to identify the cost associated with each application and business unit This is super helpful and something we could not do in RedshiftWe forked the git repository used by Redshift and modified the new branch to work with Snowflake Every few days we merged the master branch to the new branch so we wouldnt have a lot of conflicts to resolve during the final migrationAs our first step we took all of the schemas in Redshift and created the same in Snowflake We used an automated script that scanned Redshift information Schema for a given schema or table We then extracted DDL (data definition language) from Redshift using vw_admin_generate_table_ddl (Redshift view) enhanced the DDL to Snowflake unloaded data from Redshift to S3 and loaded it back to Snowflake in a new table We ran that process several times during migration to make sure Snowflake data aligned with RedshiftHere are some of the code samples we used to pull DDL objects from Redshift: Then we generated the Redshift DDL: Afterward we took the DDL and made it compatible with Snowflake Snowflakes support team provided us this script to migrate the DDL to Snowflake Then we unloaded Redshift data to S3 and loaded it from S3 into SnowflakeRedshift and Snowflake use slightly different variants of SQL syntax Here are the main differences that you might need to consider while migrating the code: DML changes: Redshift has more lenient syntax for deleting tables Snowflake is stricter with correct syntax while Redshift can accept just delete table_name without the from key word (Personally I prefer the Snowflake approach but I was surprised to find how many cases used the Redshift syntaxSemi-Structured Data: Both Snowflake and Redshift provide parsing capabilities for semi-structured data You might need to change SQL to the format below in case you are parsing JSON in Redshift as Snowflake syntax is differentSnowflake provides variant datatype which we found valuable for (JSON Avro ORC Parquet or XML) and performed well as Snowflake stores these types internally in an efficient compressed columnar binary representation of the documents for better performance and efficiencySnowflake Information Schema is kept as UPPER case If you are not using their Information Schema (less likely) you will be fine but if you are referencing Information Schema you might need to change it to lower or upper case letters in your queryA note: This becomes a major problem if you reference it from Tableau Tableau metadata is case sensitive meaning same field with upper vs lowercase is treated as two different fields and as result the report breaks We ended up migrating all Tableau data sources manually with the help of an Interworks consulting teamThe upside to the manual Tableau migration: we performed some major cleanup We discovered most of our data sources and dashboards where unused and ended up migrating just 180 out of 3000+ workbooks overWe also experienced issues while running SELECT with UPPER function: Snowflake will return the following error in case your Information Schema query is not selective enoughSnowflakes default account time zone is set to America/Los_Angeles which means the database clock time will be PST We ended up changing it to UTC after using it for a long time in PST but this change was a bit scary as we did not know what might break (My advice: check it before you start the implementation and set the time based on your needs I strongly recommend UTCWe found Redshift is way more forgiving with dates During our data migration we spotted many cases where Redshift was able to store a future date in a timestamp column but Snowflake rejected that date While I agree with rejecting non-realistic future dates like 11457 11 09 it was hard for us to find the dates and clean them up as Snowflakes error messaging wasnt as detailed as we would have likedIn our case we decided to use NULL_IF in the copy command similar to the example below Snowflake does not provide conditionals like if date > 2030 01 01 replace with null You must specify the exact date in order to mark it nullOur ETL consumes thousands of tables from RDS dbs and produces tens of fact tables The total storage at the time of migration was 12PB compressed We run hundreds ETLs every day and at peak time we can have 50+ ETL runs simultaneously We also migrated around 800 views that are used by our reporting toolsIt was impossible to manually compare the results between Redshift and Snowflake We decided to build a comparison tool that provided us the following results: Select count of each table and compare results with RedshiftSelect count distinct of each string column and compare with RedshiftStructure comparison of each tableSimple check if table existsSelect sum of each numeric column and compare with RedshiftWe ran both systems in parallel for a few weeks to compare data between the two Then once we were confident the migration was successful we slowly switched the applications to use Snowflake as their main data source We left our Redshift clusters live for one month after the migration and shut them down after confirming zero activity in Redshift stl_queryStarting this massive migration project without support from upper management is a recipe for failure Before starting the project make sure you get a commitment from management about resource allocation as different groups might be involved in migrating their code to be compatible with SnowflakeWhile you wont find all of the major hitches and differences between Snowflake and Redshift in this post we hope the items above save you lots of time if youre considering a migrationWhile the migration is complete the work is never over Want to work on projects like this on Instacarts Data Engineering team and share your findings? Check out our current openings
Fr4pGA2QPQ8z8Bn3UDFsbB,"Note: This article was written in 2017 when there was no dedicated task scheduling service on GCP It now offers a hosted version of apache airflow named Cloud Composer We recommend to use it for use cases similar to the one explained belowAt travel audience we use Google Cloud Platform in our data engineering stack Every day we process terabytes of data using Apache Beam streaming workflows (on Cloud Dataflow) Our analytic workload is powered by batch jobs using Beam/Dataflow or Spark/Dataproc All this works well but there are some workloads which are not continuous streaming applications but needs to be executed on a regular basis This demands reliable tasks scheduling But at the moment there is no task scheduler service on Google Cloud Platform that fits our workloads Here I describe a use case that demands scheduling with some of the options which we considered and how (and more importantly why) we decided on one optionThe input data to one of the large BigQuery datasets is available on Kafka topic(s) and the same data is also made available by an upstream service on a Google Cloud Storage bucket We use Cloud Dataflow for crunching this data do necessary transformations and finally load to BigQuery table using the desired schemaBefore I answer that lets have a look at this chart that explains the cost of data loading to BigQueryFor streaming inserts you have to pay 005 per GB of data whereas the batch loading is totally free! So that explains why we prefer scheduled batch loading from Google Cloud Storage in this caseWe can have a cron on a compute engine VM that will check for new files (compared to the previous run) on Cloud Storage and call a dataflow batch job for the new files to load to BigQuery The last processed file is logged somewhere so that the next cron job will know where to start from This seems simple at first glance however we ran into the below issues""You can read about this issue here So make sure that your input file list can be read in this 'prefix' formatCloud storage allows you to set Pub/Sub notifcations for each new file produced on a bucket Cloud Functions can subscribe to Pub/Sub and trigger a dataflow job for each new file""This option is inspired from the above two options Here we run a continous dataflow streaming job which accepts the Pub/Sub notifications for the new files arriving on Cloud Storage as input We then define suitable windowing and trigger (for example a global window with a trigger that fires when a specified amount of 'new file notifications' arrive) For each window firing we call a dataflow batch job using a predefined template This batch job will take care of loading the files in the current window firing to BigQueryWe decided to go with the 3rd option as it addresses the main problems with the other two options described above"
SxiUHfrcWBQB4SsC4rAHvG,In early 2016 the technology department here at Dotdash along with parent company InterActive Corp (IAC) experienced a lightning bolt of wisdom and inspiration leading to a decision to elevate our tech infrastructure and embrace the lofty world of cloud computing
et2uDZfFBLanjxT4vAnY4w,This is part two of our cloud migration blogpost series You can also jump to part one three or fourLifting and shifting our infrastructure into AWS required identifying the AWS services that would enable us to transfer our systems with the least modification Ideally from some point in the stack and upward the transfer of higher level components would be transparent For our Data API stack we migrated everything above the OS layer essentially as-is The AWS Elastic Compute (EC2) service provided a means for us to configure build and run standing servers from a wide range of hardware and software configuration options When configuring an EC2 instance one first chooses an Amazon Machine Image (AMI) which determines the set of software (OS and libs) the new machine will be provisioned with For our purposes Amazons own Amazon Linux image made sense as it provided the same sets of utilities we depended on in our CentOS servers and it was well-maintained by Amazon for interoperability with other Amazon servicesFor our data processing stack we also needed to have access to a hadoop cluster The cluster we integrated with pre-cloud had been maintained by another team but since Amazon provided a managedservice for running Hadoop called Elastic MapReduce (EMR) we chose to run the cluster ourselves for our move to the cloud Since an EMR cluster was composed of EC2 instances running Amazon Linux we decided to use the cluster not just for Hadoop tasks but also for hosting our entire Data Processing stack We found that an EMR configured with a small (compute-wise and memory-wise) Master node and a single Core (worker) node met our needs for Hive and Spark performance On the Master node we deployed our data processing pipeline components in much the same way we had in our pre-cloud environmentFor our MongoDB data migration we chose MongoDBs own managed service MongoDB Atlas as our cloud-hosting solution Atlas itself is hosted on Amazons EC2 service (behind the scenes) so integrating it with our systems just required configuring AWSs Virtual Private Cloud (VPC) peering between our EC2 instances and our Atlas clusters and adding an IP whitelist to our Atlas config The final architecture and stack diagram for our lift-and-shift deployment is belowDespite the relative simplicity of the lift-and-shift approach to cloud migration we described in the previous post there were many details relating to security and network configuration that required cross-team coordination throughout the exercise as various centralized resources were migrated at different times On top of that the full lift-and-shift migration required our team acquiring working knowledge of several AWS services including but not limited to Identity and Access Management (IAM) Route53 (DNS routing) CloudWatch (Amazons logging service) S3 (Amazons data lake) RDS (amazons managed rdbms service) EC2 and EMR In all from the moment our lift-and-shift plan was solidified we spent roughly four months completing it For a team of three engineers we felt that timeframe represented efficient planning and execution Ultimately we completed our initial migration several weeks before our deadline to be out of our physical data centers With our initial goal achieved and our primary requirement for migration satisfied we were then able to shift our focus to re-architecting our systems to best utilize AWSs servicesIn the next post we describe the next stage of our migration: our cloud optimization
28N59ExRkcfs7pqjiU7VH9,This is part three of our cloud migration blogpost series You can also jump to part one two or fourWith our lift-and-shift completed (details in our previous blogpost) we transitioned to the re-architecture phase of our migration The goal for this phase was to optimize our systems for cost and performance in AWS AWS provides many services it characterizes as serverless For those unfamiliar the term serverless refers to the lack of persistent server infrastructure visible or accessible to application developers and administrators The lack of (visible) infrastructure translates to a pricing model that charges per event where an event can be a request or a finite unit of computation time This pricing model is usually cheaper in general and is much cheaper in cases where load is temporally sparse as is the case for our systemsThe flagship of Amazons serverless effort is undoubtedly AWS Lambda This was the first service Amazon promoted as serverless and the one we first encountered in blog posts at Amazon events and in presentations (as its a subject of recent enthusiasm in the tech industry) It was often advertised in combination with their API Gateway service as an ideal platform for web APIs Because of that a lot of early efforts in the open source community focused on tools oriented to building APIs on API Gateway and Lambda (eg zappa and chalice) And it was also the direction we chose for re-architecting our APIsAs alluded to above deploying a web-based API to Lambda requires integration with API Gateway The Lambda service by itself doesnt provide an HTTP interface In order to configure a web API to use these services there are many options and choices to make While youd ultimately benefit from using one of the various automation tools this can also be done by hand either via the AWS Console UI or via the AWS CLI In fact if doing so within the the AWS Console UI it can be done entirely by hand coding included Our team operates primarily in Python so the automation tools we considered were limited to that language and for the remainder of this blogpost Python will be the implied language unless otherwise notedWe first experimented with using shell scripts and Boto the Amazon-maintained Python wrapper for AWS developer APIs to build Lamba packages However we quickly realized that a) there are many tedious steps for building a Lambda package and automating the Lambda and API Gateway deployments and b) it probably wouldnt make sense to build our own library to do this without trying one of the open-source projects devoted to the same purposeWe then discovered Chalice an open-source project from AWS Labs that provides a deployment wrapper around Lambda and API Gateway It provides a library interface and decorators that look almost identical to the popular Flask WSGI library Theres also some apparent value in the fact that its a project maintained by in-house Amazon developers Chalice is a great tool and maybe the simplest tool if you have a simple Python web API with pure-python library dependencies Unfortunately we ran into problems when we tried to package certain libraries with C extensions using Chalice and that necessitated considering other alternativesThat brought us to Zappa another open-source Python library that took the Flask inspiration one step further by creating a wrapper around Flask enabling developers to build Flask apps that can be deployed to Lambda and API Gateway We liked the flexibility of decoupling WSGI and AWS Services and we liked the fact that Zappa provided more built-in configurability than Chalice (with sensible defaults) The projects primary maintainer has also been responsive to questions and suggestions posted in slack (https://zappateamslackcom) which is a huge plusWhile Zappas extensive configurability is a strength we found that we still had to write some Boto logic for configuring our route-mapping within our Custom Domain (custom domains enable us to have multiple API gateways served from a single domain) and for part of the setup of API keys (namely updating Usage Plans) Heres a depiction of our deployment process for our serverless APIs: The JSON excerpt below is an example Zappa config that sets up API keys and lambda networking Some things to note: And heres a Python Boto snippet illustrating how we automate the Custom Domain routing (aka Base Path Mapping) for a new endpoint: By converting our traditional server-based APIs weve been able to eliminate the EC2 instances they originally ran on The cost for those instances was: 2 * T2Large@$66 + 2*T2Conversely with Lambda the first million requests per month are free and the first 400000 GB/seconds per month are free (where GB is GB of memory used by the Lambda function) Thereafter the pricing equation looks like: cost = $020 * requests_after_one_million + $0We recently made our serverless data APIs live so we dont yet have a full month of data but we serve about 750k requests per month on all our APIs in total (which only have internal consumers) so we estimate that our lambda costs will be less than $10/month This reduces our infrastructure costs for our data APIs by 97%In terms of performance Lambda can scale out to 1000 simultaneous request handlers so our worst case performance should be much better We expect the average case performance to be similar to the original architecture as we havent changed the core logic We also expect performance to be more consistent as the server processes dont impact each otherIn our last post we share more steps we took to optimize our systems for the cloud
EqujpnGuJaGYLUXj4XQovS,This is part four of our cloud migration blogpost series You can also jump to part one two or threeThe biggest chunk of our AWS bill after the lift-and-shift migration by a wide margin was the EMR cluster Before we retired it our QA and production EMR deployments consisted of two m4xlarge instances and two m42xlarge instances Altogether the monthly rate amounted to $1123 Since we only needed the EMR for running scheduled Hive queries against S3 data Amazons release of Athena a serverless Big Data Query service based on Presto provided a potentially significant savings opportunity over EMR Athenas pricing is simple Amazon charges $5 per terabyte scanned by all queries run in account (when testing a query the Athena interface will show you the size of data scanned) This also means that queries which only hit the metastore for metadata (and thus dont scan S3) dont cost anything For example count(*) in a partition or show partitions are free to run Since all of our hive queries ran on a schedule it was easy to predict our pricing which projected to be about $30/monthIn order to use Athena however another AWS Service needed to be integrated with it called Glue Athena requires a Glue Catalog which is the AWS equivalent of hadoops metastore In order to create and maintain a Glue Catalog a crawler must be configured which is a process that scans a given S3 bucket for schema changes and new partitions to add That service added an additional $30/month to the cost of our Athena-based Big Data refactor In addition to the above we still needed a place to run the Airflow jobs scheduler and our other data processing jobs For that we still needed two EC2 instances (t2xlarge in QA and t22xlarge in production) with a total monthly cost of $432 In all we saw an over 50% cost reduction ($600 / $1123) by switching from EMR to Athena + EC2We also observed other benefits (some of which were a benefit of using Presto) We found that Athena and Presto were orders of magnitude faster than Hive for our queries While this is more about Presto than about Athena we did find that Athena was about 50% speedier when compared to the same Presto query run on a relatively powerful EMR cluster maintained elsewhere in the organization Using Athena also made our infrastructure simpler as we needed fewer servers and far fewer libraries to run Big Data queries Additionally with Athena there is never any risk of running a query that exceeds a fixed hardware configuration (eg in terms of memory) When using hive on EMR for many of our queries special memory configuration directives were necessary to ensure stable query processing To query Athena we created boto libraries in PythonBelow is a summary of comparative benefits we observed in moving to AthenaAnd this is how we query Athena in Python: (In the example above the query result can then be found at <s3_dir>/<QueryExecutionId>With our data APIs and data pipeline redesigned to better utilize the strengths of AWS we feel weve accomplished a truly complete cloud migration We also feel that starting with a lift-and-shift approach allowed us to gain familiarity with AWS without being overwhelmed with extensive code changes and with that familiarity we were better able to plan our re-architecture However were looking forward to exploring other AWS technologies and we regularly have team discussions about new service announcements from Amazon
DnPp3b2P6QRVm8ZZigcVe6,Data is the new oil Its valuable but if unrefined it cannot really be used It has to be changed into gas plastic chemicals etc to create a valuable entity that drives profitable activity; so must data be broken down analysed for it to have valueA frequent criticism of Kaggle and MOOC (Massive Open Online Courses) is that they are reducing data science to predictive modelling and they are using pre-cleaned or non-real-world datasets Data Science is more than that Data retrieval and cleansing are essential parts of the field too You and I know that data can make a difference If you can get better data you perform better Do you think data retrieval is a task for a data engineer? In a small startup or as an entrepreneur following your passions you dont have access to one You might have any data in the beginningScraping the data from websites is a solution Web scraping has a bad reputation as people think that you are stealing the data It is unethical in my eyes in case you are building up a competitive product with the data But what is about taking the labelled photos to teach a neural network? I think this is OK You might give the company even something valuable back or push their business with your data productI got my hands dirty over the last years to learn data science and to build up my own data products In this time I scraped data from many websites and learned a lot I want to share the most important lessons with you: Before you scrape a website ALWAYS check its Terms of Service and the robotstxt to answer the question of what you are allowed to fetch and what not This saves you a lot of trouble and the risk of a court case Please remember that scraping is a grey zoneIts worth to invest time for technical research before you start scraping Sometimes you dont need to scrape/parse the raw HTML files as there exists an internal REST API in the backend Modern webpages use JavaScript Frameworks which use REST APIs You are even not able to parse HTML as the logic is executed in the browser In this scenario you might need a framework like Selenium that uses a modern browser in headless mode that is executing the JavaScript code That is bad news but you have the chance that there is a RESTful API in this case These APIs just send JSON or XML back which is even better than extracting the data from HTMLYou also need to check what headers your browser sends to your website The developer tools of the browsers eg Mozilla Browser Developer Tools help you with thisBuild a Docker container for your scraping application Scraping configurations can be a bit complicated especially if you fetch the data from the resource with Selenium which uses a browser like Chromium or Firefox You need operating system dependent drivers for this situation You can use my docker image with Firefox/Geckodriver and Chromium/Chromedriver on Dockerhub as a base image for your scraping application The source can be downloaded from Github Use Docker Compose: It is a tool for defining and running multi-container Docker applications This way you can set up a database with no effort along with your scraping container If you are not familiar with Docker then try it out Its a secret weapon for reliable runtime environments for all kind of software projectsUse proxies already during development Rotate the proxies randomly This saves you a lot of trouble My rule of a thumb is to use 10 proxies per scraping process/thread Try to avoid free proxies lists The proxies on these lists are often slow or already blocked It is a waste of time to try them out Google for proxy providers You need to invest some money here You can even buy non-shared proxies but they have their costsWrite first a scraper that just fetches a resource and saves it Ensure that this data is the same as the data downloaded from your browser If this is the case you can increase the number of processes/threads and load more data Once you have a bunch of raw data files (not too many!) you start to develop the parser Extract the data you need and check if the data is OK Do an exploratory data analysis check for data anomalies like missing values or outliers to ensure a high-quality parsing logic Log all errors while you parse This way you can find out if the logic works for all resources the sameKeep the scraping logic separate from the parsing logicI try to scrape each URL only onceYou dont want to download the same resource over and over again especially during development We all have a kind of trial and error programming style We write a piece of code and test if it works or not do some bug fixing This can be fatal during scraper development as you risk to get blocked already in this phase The block is sometimes only for a few hours but the block blocks your programming Separating the parsing logic from the scraper logic helps you to reduce the lines of code in which you can make errors Keep this in mind: Its essential to keep the raw data once the resource is scraped This has several advantages: Set proper headers when fetching a resource from a website Check what your browser is sending and mimic a real browser in your code In my experience one of the most important ones is the user agent header I am sharing a very simple python module for user agent rotation with you It takes randomly one user agent out of thousandsSave the raw data in a database I love MongoDBs GridFS for this task The API is very simple You have different advantages of this: Use a database for the parsed data You are getting improved data access through the use of query languagesMongoDB is an excellent choice here as you dont need a scheme you can just save the data as it is Especially if you able to scrape from an internal API From APIs you get often JSON and you can toss the JSON as it is into the MongoDBOnce you have a bunch of raw files and your parser is written do an Exploratory Data Analysis (EDA) to check the data quality Check for missing values I wrote a blog post about Missing values visualization with tidyverse in R which might help you Do a univariate data analysis to check for anomalies for each feature Do a multivariate analysis to check if values in linked features are consistent Visualizations are your best friends hereThis step is important and missed out by many people You find the weak points in your parsing logic You might go through several iterations to improve the parsing logic until you are happyFeel free to send me a message in case you need support in your scraping projectOriginally published at jenslaufercom on February 19 2019
A7oPJPKx2GhDXHTuwZaiEH,My humble opinion on Apache Airflow: basically if you have more than a couple of automated tasks to schedule and you are fiddling around with cron tasks that run even when some dependency of them fails you should give it a try But if you are not willing to just accept my words feel free to check these posts¹ ² ³ Delve into Airflow concepts and how it works is beyond the scope of this article For that matter please check these other postsLong story short its task definitions are code based what means that they could be as dynamic as you wish You can create tasks and define your task dependencies based on variables or conditionals It has plenty of native operators (definitions of task types)⁶ that integrate your workflow with lots of other tools and allow you to run from the most basic shell scripts to parallel data processing with Apache Spark and a plethora of other options Contributor operators⁷ are also available for a great set of commercial tools and the list keeps growing every day These operators are python classes so they are extensible and could be modified to fit your needs You can even create your own operators from scratch inheriting from the BaseOperator class Also it makes your workflow scalable to hundreds or even thousands of tasks with little effort using its distributed executors such as Celery or KubernetesWe are using Airflow in iFood since 2018 Our first implementation was really nice based on docker containers to run each task in an isolated environment It underwent a lot of changes since then from a simple tool to serve our teams workload to a task scheduling platform to serve the more than 200 people with a lot of abstractions on the top of it At the end it does not matter if you are a software engineer with years of experience or a business analyst with minimal SQL knowledge you can schedule your task using our platform writing a yaml file with three simple fields: the ID of your task the path of the file containing your queries and the name of its table dependencies (ie to run my task I depend on the tables orders and users) and voilà you have your task scheduled to run daily But this unfortunately is a topic for a future articleIt was obvious that we would need to scale our application from an AWS t2medium EC2 instance to something more powerful Our first approaches were to scale vertically to an r4large instance and then to an r4xlarge but the memory usage was constantly increasingOur company grows fast There are dozens of tasks being created every day and suddenly we would be running on an r416xlarge instance We needed a way to scale the application horizontally and more than that to upscale it considering the peak hours and to downscale it at dawn to minimize needless costsI searched on the internet from the official Apache Airflow documentation to Medium articles digging for information on how to run a simple implementation of Airflow on Kubernetes with the KubernetesExecutor (I was aware of the CeleryExecutor existence but it would not fit our needs considering that you need to spin your workers upfront with no native auto-scaling) I found a lot of people talking about the benefits of running Airflow on Kubernetes the architecture behind it and a bunch of Helm charts but little information on how to deploy it piece by piece in a logical way for a Kubernetes beginner And that is the main point of this article Assuming that you know Apache Airflow and how its components work together the idea is to show you how you can deploy it to run on Kubernetes leveraging the benefits of the KubernetesExecutor with some extra information on the Kubernetes resources involved (yaml files) The examples will be AWS-based but I am sure that with little research you can port the information to any cloud service you want or even run the code on-premTo fully understand the sections below and get things running I am assuming that you have: If you do not I recommend that you play a little with Kubernetes and Airflow locally You can find awesome tutorials in the internet even at the official websites For Kubernetes you can start with the Katacoda tutorials⁸ Regarding Apache Airflow this was the first tutorial I ever triedAs a starting point I found a way to get the Kubernetes resource yaml files from the official Helm chart available at the Airflow git repository¹⁰ That brought me a lot of resources some of them came empty (probably because I used the base valuesyaml to fill the templates used by Helm) and some of them were useless to the KubernetesExecutor approach (ie I do not need a Redis cluster or a Flower resource or a result back-end as these are specific to Celery) Removing those useless resources I ended up with something around 15 resource\xa0files and some of them I did not even know at that time Kinda overwhelming! I also removed all resources that were related to the PostgreSQL instance (ie pgbouncer) because I knew that I would use an AWS RDS instance external to the Kubernetes clusterObs: I had these charts locally so when I executed the helm template command helm whined about not finding the PostgreSQL charts (it will not happen if you are using the Helm repositories) If that is your case just create the path charts/ inside the folder containing your helm chart and put the postgresql/ helm chart folder inside of it (available at the official Helm charts github repository) It is also important to notice that the Apache Airflow helm chart available at https://githubcom/helm/charts will bring you a different set of resources when compared to the chart I usedAfter all the cleaning I ended up with these 12 resources: Most of the articles I found describe two ways to store DAG information: storing the DAGs on a persistent volume accessible from multiple AWS availability zones such as the AWS Elastic File System (EFS) or syncing them from a git repository to an ephemeral volume mounted inside the cluster If that pod dies when another one is created it will sync with the repository again to get the last modificationsDue to our present workflow we need to build our DAGs dynamically from lots of tasks written in yaml files meaning that our DAGs are not ready when the files are versioned on a git repository A simple git-sync to bring information would not work for us but it could be a starting point Considering that we also needed some kind of persistence for our logs we decided to go for the EFS approach too using some kind of hybrid of what we found online: git-sync our yaml files to a PersistentVolume mounted on the top of an EFS and to have another pod processing it and throwing the freshly-built DAGs into the folder that the scheduler and the webserver are constantly watching to fill the DagBagAs shown above to mount the EFS inside the EKS cluster I used the official AWS CSI driver¹¹ that must be installed in the cluster And beyond the driver this approach accounts for five Kubernetes resources: These resources were not present in the initial list as the original deployments used emptyDir volumes insteadAnyone that worked with Apache Airflow for some time knows that the airflowcfg file (and maybe webserver_configpy file) is pretty important to set the things up But throwing it inside the EFS volume did not seem wise because of the sensitive information it contains (database passwords fernet key) Then I found out that the Kubernetes way to store configuration files is to use ConfigMap a kind of volume that you mount inside the pods to expose a configuration file for them And there is the Kubernetes Secret too to store sensitive data They work together so I can reference a Secret inside a ConfigMap or even pass a Secret to an environment variableAs you learn a little bit more about Kubernetes you will notice that the plain secrets are somewhat unsafe to version in a repository They contain base64 strings that can be readily decrypted in your terminal using the base64 -d command Take a look on this ExternalSecrets API¹² to store your secrets on AWS Parameter Store and retrieve them from thereIf you check the list of files above you will notice that the ConfigMap is already there you just have to customize itMy little experience with Kubernetes was enough at that point to make me think that I would need at least two deployments: one for the scheduler and one for the webserver And they were there lying inside the scheduler and webserver folders generated by the Helm Chart explosion There was a third deployment of a statsd application that I found later to be related to metrics collection inside the application Cool one thing less to worry! Prometheus will be happy to scrape itI opened the files and noticed that they have some familiar environment variables related to the fernet key and the database connection string I filled them with the data retrieved by the Kubernetes secrets I needed to tweak the volume part a little bit to match my EFS PersistentVolume and PersistentVolumeClaimIt is easy to notice these shell scripts being executed as init containers They are related to the database migrations that happen when Airflow starts The scheduler pod runs the migrations as soon as it starts and the webserver pod keeps waiting for it to finish before starting the webserver container The webserver deployment has a very similar structure so I took the liberty of omitting itThere was A Kubernetes service resource exposing the port 8080 of the container Later I included an Ingress resource to give it an AWS Route53 friendly DNSThe statsd application also listens at an endpoint and has a service associated to it Both the services were included in the files exported by the helm template commandI tried to apply those configurations to the cluster Scheduler and webserver were up both connected to my external RDS PostgreSQL database I thought: If I throw some DAGs into the dags folder then it should work right? And it kinda did! I created a simple DAG with one task based on the KubernetesPodOperator using a container image stored at the AWS Elastic Container Registry I double checked if my pod would be allowed to access the ECR repositoryThen I triggered the DAG but it failed\xa0(you\xa0really\xa0didnt\xa0think\xa0it\xa0would\xa0be\xa0that\xa0easy\xa0right?) Checking the logs I noticed that it happened due some kind of permission issue My scheduler did not have the permission to spawn new pods And then I understood the need for that ServiceAccount resources scattered among the folders and the ClusterRole and ClusterRoleBinding stuff These guys are there to allow your resources to spawn new resources After all the configuration I could make my task run successfully The KubernetesPodOperator also has the service_account_name parameter that should be filled with a ServiceAccount resource name able to spawn pods because that is what it will do: spawn another pod with the image you passed as an argument to the image parameter That pod will be in charge of running your taskIf you want to run tasks directly from your webserver clicking on that Run button inside the task menu you must give your webserver ServiceAccount the permissions to watch and spawn pods too If you forget that your tasks will be triggered but they will never runIf you are running your stuff on AWS you need to make sure your pods will be able to access all AWS resources such as S3 DynamoDB tables EMR and so on To do so you need to bind your ServiceAccount resource to an AWS IAM role with IAM policies attached to grant you all the access you need Just give your IAM role an assume role policy: The ServiceAccount for your worker and tasks should be linked to the IAM Role attached to the policy above You can do it using annotations: If you are following this journey as a tutorial after all the tweaking you can just create all the above resources in your cluster: But wait! Do not apply them yet! If you are a watchful reader you noticed that most of the resources above makes reference to the airflow-on-k8s namespace A Namespace is a way to tell Kubernetes that all resources in the same namespace are somewhat related (ie they are part of the same project) and is a nice way to organize things inside the cluster You should declare your Namespace resource inside the resources/ folder and apply it before applying everything else otherwise you will get an errorFYI not every resource on Kubernetes is namespaced (ie PersistentVolume StorageClass and other low-level resources) and that is why some of them do not have any reference to a namespaceAnd that was a fast-forward take on my journey to deploy Airflow on Kubernetes I tried to cover all kinds of resources generated by the helm chart export but feel free to ask your questions in the comments section if you think I left something behindThese yaml resources above were taken from a functional deployment I made Some of them I built from scratch and others I adapted from the ones I exported I suggest that you take your time understanding them and making changes for better organization and performanceThere is a lot more you can do to\xa0get\xa0the\xa0most\xa0of\xa0this\xa0implementation You can set the limits and requests fields for your containers\xa0inside\xa0the\xa0deployments to make sure they will have the necessary resources available for them to work properly Going further on the benefits of Kubernetes you will see that the KubernetesPodOperator allows you to label your pods and pass a lot of Kubernetes configurations to it such as affinities tolerations and a bunch of other stuff If you have tainted nodes you can assure that just some specific pods will run on them reserving the most powerful nodes to the most critical tasksIf you tried this setup and have something to add something that worked like a charm or turned out to be a bad choice please tell us in the comments
amvTpLyjESzcytLR9pBqSJ,Microsoft SQL Server provides SQL Server Replication features that we can utilise to set a database as Publisher and another one is Subscriber so we can replicate the database with customisations in the scripts However sometimes the requirements may not be that complex so that SQL Server Replication could be a little overkillFor example recently one of my clients want to start a POC project to build a data warehouse They have concerns regarding the production databases that they are relying on to keep the business running So before we can extract the data from the databases to build the data warehouse we need to replicate the databases so that all the development works will be conducted on these replicasIn this case it is not necessary to include any customisations in the replicating processes which is just simply mirroring the database In this article Ill introduce a very easy way of automatically replicating SQL Server Databases between different serversLets firstly create two SQL Servers for this experiment It is highly recommended to use Azure or otherwise youll need to prepare two different machines get proper SQL Server licenses and install the SQL Server software which could take an entire dayIf you are new to Azure youll get $200 credits (in one month) when you register Azure as a new userGo to your Azure subscription create a resource group first which will be the container of the SQL Servers Then go to the marketplace and search sql server choose the SQL Server 2017 on Windows Server 2016 as shown in the screenshotIn the dropdown manual choose the Standard Edition to have all the standard features Then click Start with a pre-set configuration to save timeHere we can choose Dev/Test environment and D-Series VM (both smallest) to save costs especially if you have already spent all the free creditsMake sure you selected the resource group that you created for this Then click nextFill in administrator credentialsThe disk is not important for this experiment so we can skip However the virtual network is very important Make sure you create a new virtual network and subnet and more importantly the other SQL Server that will be created later on must be in the same virtual network and subnet if you dont want to create some accessibility issues for this experiment Also open the RDP port 3389 for later on convenienceDo not change the default port number and create an SQL Authentication After that we have done the configurations Click create button to create the resources in AzureWhile waiting for the resource being deployed we can create the other SQL Server Ill call it SQL-Server-TestAfter the first SQL Server has been deployed go to the resource group and find the Windows VM for SQL-Server-Prod Because we have opened the RDP port for the machine so we can remote control this VM using its public IP addressLets install a sample database from Microsoft On the production machine download the database backup: https://githubcom/Microsoft/sql-server-samples/releases/download/adventureworks/AdventureWorks2017In SSMS login to the DB instance using the default Windows admin accountRight-click Databases and select Restore DatabaseIn the popup window click the browse button on the right of Device radio button -> Add -> Browse the bak file that youve just downloaded -> OK -> OK -> OK It may take about 1 minute to restore the whole databaseNext we need to create a folder that the backup files will be put into Also we need to share this folder in the network so that the Test machine can access the backup file and restore itI created the directory C:\\backup\\ Then share the fold to the same admin account at the other machineNote that it is important to add MSSQLSERVER user to the sharing Otherwise SQL Server might not be able to write backup files into this folder after the sharing Just simply type the MSSQLSERVER username and click Add button then make sure to grant Read/Write permission to itOn the Test machine we will be able to access the backup directory nowSince the SQL Server Agent is disabled by default we need to remote control the Prod machine again to enable it Right-click on SQL-Server-Agent -> Start Then in the pop-up confirmation window click YesWait until SQL Server Agent is started go back to the testing machine You will need to reconnect the production SQL Server to see the SQL Agent enabledRight-click Jobs -> New JobIn the New Job window input a name and go to Steps tabIn Steps tab click New buttonIn the New Job Step window input step name and the script as follows: After that go to theAdvanced tab configure the success and failure behaviours Here you need to consider your situations such as how large your database is and etcClick OK to confirm apply the Job Step then go to Schedule tab and click New button You may configure the schedule based on your requirements In this example Ill let the backup happens every day at 1:00 AM Then click the OK button to confirm the scheduleYou may also want to create alerts or notifications such as sending emails when the job fails In this experiment well skip this step because it is not always neededNow rather than wait until tomorrow at 1 AM I would like to test the job We can right-click the job we created and select Start Job at Step… to test the job Since we only have 1 step in this job it will directly start and run the backup stepAfter a while the job is success and you can also find the backup file at the backup directoryFirst of all lets check the accessibility of the backup file from the test machine by access \\\\SQL-Server-Prod\\backup\\<backup-file>Another crucial step is to mount the remote backup folder to a local drive This is because usually the SQL Server Windows Service is run as a service account which does not have permission to use network/remote resources so the remote resource is not visible to itSimply right-click on the backup folder and select Map network drive…Lets mount it to Z: drive on the test machineThen we need to let SQL Server identify the network drive using the xp_cmdshell command By default this command is not enabled so we need to enable it Open a new query sheet and run the following script to enable itThen define the network drive using xp_cmdshell You can use any Windows user that can access this network drive Note that you only need to run this command once so the password will not be retained anywhere in plain formatFrom the result panel you should see the output saying it was successfulWe can also test whether it works or not by the following scriptIf it works you should see the file list including the backup fileAfter that we just need to create another SQL Server Agent Job with a step for restoring the database from the identified network drive The script of the step is as follows: Then repeat what we have done before in the production machine right-click the agent job and select Start Job at Step to test the jobMake Network Path Visible For SQL Server Backup and Restore in SSMS:https://wwwmssqltipsBACKUP Statements (Transact-SQL Microsoft official Docs):https://docsmicrosoftRESTORE Statements (Transact-SQL Microsoft official Docs):https://docsmicrosoft
5t8Hs3ojNZUrUeErod3cic,A few days ago Google Cloud announced the beta version of Cloud Composer In brief Cloud Composer is a hosted solution for Airflow which is an open-source platform to programatically author schedule and monitor workflows For data folks who are not familiar with Airflow: you use it primarily to orchestrate your data pipelinesAs I had been looking at hosted solutions for Airflow I decided to take Cloud Composer for a spin this week The nice thing about hosted solutions is that you as a Data Engineer or Data Scientist dont have to spend that much time on DevOps  something you might not be very good at (at least Im not!)So below is a very brief write-up of the experience testing out Cloud Composer This is by no means an exhaustive evaluation  its simply my first impression of Cloud ComposerIts extremely easy to set up If you have a Google Cloud account its really just a few clicks away (plus ~20 minutes of waiting for your environment to boot) You can also easily list your required Python libraries from the Python Package Index (PyPI) set environment variables and so onDeployment is simple Your DAGs folder sits in a dedicated bucket in Google Cloud Storage This means you could literally drag-and-drop the contents of your DAG folder to deploy new DAGs Within seconds the DAG appears in the Airflow UI Of course drag-and-drop is not the only option You could also do deployment programmatically by using Google Clouds gcloudWhen your environment is up and running the Google Cloud UI is clean and hassle-free: it just links to the DAG folder and to your Airflow webserver which is where youll be spending most of your timeOverall the theme on the positive side is simplicity and ease-of-use which is probably what youre looking for in a hosted solutionCloud Composer runs Python 27 There should at least be an option to run Python 36  I would honestly have expected it to be the defaultThere are still a few bugs to iron out  after all this is a beta For instance when trying to clear a task I get this screen: The pricing could be more transparent After 3 days of running Cloud Composer I have a bill of €25 so assuming thats somewhat linear Id be paying approximately €250/month This is pretty much in line with the pricing example provided by Google Cloud (note they assume 25% utilisation in their example)Honestly this is great value if youre startup in need of Airflow and you dont have a lot of DevOps folks in-house But its likely too expensive if youre primarily looking at Airflow for your hobby projectsIf youre looking for another hosted solution for Airflow where you can get more hands-on support and training Astronomer might be a good optionWhat does your Airflow setup look like? Let me know in the comment field below
Wpm3psG2WgxuajK9RMGJfM,Apache Spark provides an important feature to cache intermediate data and provide significant performance improvement while running multiple queries on the same data In this article we will compare different caching techniques benefits of caching and when to cache our dataRefer DataSetThe cache method calls persist method with default storage level MEMORY_AND_DISK Other storage levels are discussed laterThe rule of thumb for caching is to identify the Dataframe that you will be reusing in your Spark Application and cache it Even if you dont have enough memory to cache all of your data you should go-ahead and cache it Spark will cache whatever it can in memory and spill the rest to diskWe can use different storage levels for caching the data Refer: StorageLevelWe can explicitly specify whether to use replication while caching data by using methods such as DISK_ONLY_2 MEMORY_AND_DISK_2 etcWe can also specify whether to serialize data while storing Methods like MEMORY_ONLY_SER etc Using serialized format will increase processing time but decrease the memory footprintSerialization with replication is also available egBelow tests are run on the local machine with 12GB driver memory and input data of size 14GB and time was recorded using `sparktime` for each iteration It outputs run time of a query which was executed 10 times in a loopNote: Remember that cache() in spark is lazily evaluated So when first action was called at that time data will be cached Caching will take additional time which can be seen in the graph above For re-runs we observe significant performance benefit with cacheIn this post we learned that caching is an important technique to optimize spark jobs We should cache our data if we are going to use it multiple times in our code It can give us a significant performance benefit
6hKqt87mEHLew8BCA7n7sQ,Querying data in Spark has become a luxury since Spark 2x because of SQL and declarative DataFrame API Using just few lines of high level code allows to express quite complex logic and carry out complicated transformations The big benefit of the API is that users dont need to think about the execution and can let the optimizer figure out the most efficient way to execute the query And efficient query execution is often a requirement not only because the resources may become costly but also it makes the work of the end user more comfortable by reducing the time he/she has to wait for the result of the computationThe Spark SQL optimizer is indeed quite mature especially now with the upcoming version 30 which will introduce some new internal optimizations such as dynamic partition pruning and adaptive query execution The optimizer is internally working with a query plan and is usually able to simplify it and optimize by various rules For example it can change the order of some transformations or drop them completely if they are not necessary for the final output Despite all the clever optimizations there are however still situations in which a human brain can do better In this article we will take a look at one of these cases and see how using a simple trick we can lead Spark towards a more efficient executionThe code is tested in the current version of Spark which is 245 (written in June 2020) and it is checked against Spark 300-preview2 to see possible changes in the upcoming Spark 30Let me now first introduce a simple example for which we will try to achieve efficient execution Imagine that we have data in json format with the following structure: Each record is like a transaction so the user_id column may contain lots of duplicated values (possibly including nulls) and besides these three columns there can be many other fields describing the transaction Now our query will be based on a union of two similar aggregations where each of these aggregations differs by some conditions In the first aggregation we want to take users for which the sum of the price is less than 50 and in the second aggregation we take users for which the sum of the price is more than 100 Moreover in the second aggregation we want to consider only records where user_id is not null This model example is just a simplified version of a more complex situation that can occur in practice and for the sake of simplicity we will use it throughout the article Here is a basic way how to express such a query using DataFrame API of PySpark (very similarly we could write it also using the Scala API): The key to achieve a good performance for your query is the ability to understand and interpret the query plan The plan itself can be displayed by calling explain function on the Spark DataFrame or if the query is already running (or has finished) we can also go to the Spark UI and find the plan in the SQL tabThe SQL tab has lists of completed and running queries on the cluster so by selecting our query we will see the graphical representation of the physical plan (here i removed the metrics information to make the plot smaller): The plan has a tree structure where each node represents some operator that caries some information about the execution We can see that in our example there are two branches with the root at the bottom and the leafs at the top where the execution starts The leafs Scan json represent reading the data from the source then there is a pair of HashAggregate operators which are responsible for the aggregation and in between them there is Exchange which represents the shuffle The Filter operators carry the information about the filtering conditionsThe plan has a typical shape for union operations there is a new branch for each DataFrame in the union and since in our example both DataFrames are based on the same datasource it means that the datasource will be scanned twice Now we can see that there is a space for improvement Having the datasource scanned only once can lead to a nice optimization especially in situations where I/O is expensiveConceptually what we want to achieve here is reusing some computation  scanning the data and computing the aggregation because these are the operations that are the same in both DataFrames and in principle it should be sufficient to compute them only onceOne typical approach how to reuse a computation in Spark is using caching There is a function cache that can be called on a DataFrame: It is a lazy transformation which means that the data will be put to the caching layer after we call some action Caching is a very common technique used in Spark however it has its limitations especially if the cached data is large and the resources on the cluster are limited Also one needs to be aware that storing the data in the caching layer (memory or disk) will bring some additional overhead and the operation itself is not for free Calling cache on the whole DataFrame df is not optimal also from the reason that it will try to put all the columns to memory which may be unnecessary More careful way is to select a superset of all columns that will be used in the following queries and then call the cache function after this selectApart from caching there is another technique which is not that well described in the literature and this technique is based on reusing the Exchange The Exchange operator represents shuffle which is a physical data movement on the cluster This happens when the data must be reorganized (repartitioned) which is usually required for aggregations joins and some other transformations The important thing about shuffle is that when the data is repartitioned Spark will always save it on the disk as the shuffle write (this is an internal behavior and it is not under control of the end user) And because it is saved on the disk it can be reused later on if it is required Spark will indeed reuse the data if it finds an opportunity for that This happens each time Spark detects that the same branch from the leaf node up to an Exchange is repeating somewhere in the plan If there is such a situation it means that these repeated branches represent identical computation and thus it is sufficient to compute it only once and then reuse it We can recognize from the plan whether Spark found such a case because those branches would be merged together like this: In our example Spark didnt reuse the Exchange but with a simple trick we can push him to do so The reason why the Exchange is not reused in our query is the Filter in the right branch that corresponds to the filtering condition user_id is not null The filter is indeed the only difference in our two DataFrames that are in the union so if we can eliminate this difference and make the two branches the same Spark will take care of the rest and will reuse the ExchangeHow can we make the branches the same? Well if the only difference is the filter we can certainly switch the order of transformations and call the filter after the aggregation because that will make no impact on the correctness of the result that will be produced However there is a catch! If we move the filter like this: and check the final query plan we will see that the plan has not changed at all! The explanation is simple  the filter was moved back by the optimizerConceptually it is good to understand that there are two major types of the query plan  logical plan and physical plan And the logical plan undergoes an optimization phase before it is turned into the physical plan which is the final plan that will be executed When we change some transformations it is reflected in the logical plan but then we loose control over the next steps The optimizer will apply a set of optimization rules which are mostly based on some heuristics The rule related to our example is called PushDownPredicate and this rule makes sure that the filters are applied as soon as possible and are pushed closer to the source It is based on the idea that it is more efficient to first filter the data and then do the computation on the reduced dataset This rule is indeed very useful in most of the situations however in this very case it is fighting against usTo achieve custom position of the Filter in the plan we have to limit the optimizer This is possible since Spark 24 because there is a configuration setting which allows us to list all the optimization rules that we want to exclude from the optimizer: After setting this configuration and running the query again we will see that now the filter stays positioned as we need The two branches become really the same and Spark will now reuse the Exchange! The dataset will now be scanned only once and the same goes for computing the aggregationIn Spark 30 the situation is changed a little bit the optimization rule now has a different name  PushDownPredicates and there is an additional rule that is also responsible for pushing a filter PushPredicateThroughNonJoin so we actually need to exclude both of them to achieve the desired goalWe can see that through this technique Spark developers gave us the power to control the optimizer But with power comes also responsibility Lets list a couple of points that is good to keep in mind when using this technique: We have seen that being able to achieve the optimal performance may require understanding the query plan Spark optimizer does a very good job by optimizing our query using a set of heuristic rules There are however situations in which these rules miss the most optimal configuration Sometimes rewriting the query is good enough but sometimes it is not because by rewriting the query we will achieve a different logical plan but we do not have a direct control over the physical plan that will be executed Since Spark 24 we can use a configuration setting excludedRules that allows us to limit the optimizer and thus navigate Spark to a more custom physical planIn many cases relying on the optimizer will lead to a solid plan with a quite efficient execution however there are cases mostly in performance critical workloads where it might be worth to check the final plan and see whether we can improve it by taking the control over the optimizer
BuLZ9jVxsa5Rc8XwhBjWML,When starting to dive into the data world you will see that there are a lot of approaches you can go for and a lot of tools you can use It may make you feel a little overwhelmed at firstOn this post I will try to help you to understand how to pick the appropriate tools and how to build a fully working data pipeline on the cloud using the AWS stack based on a pipeline I recently builtThe pipeline discussed here will provide support for all data stages from the data collection to the data analysis The intention here is to provide you enough information by going through the whole process I passed through in order to build my first data pipeline so that on the end of this post you will be able to build your own architecture and to discuss your choicesIf you need to process stream data maybe Kinesis is a good thing for you but if you have some budget limitations and you do not mind about taking care of the infrastructure you can go for Kafka If you have to process historical data you wont need that stuff but Glue on the other hand can be a great friend In other words your needs will be the judge on what is best for you The important thing here is to understand your challenge and know your limitations in order to choose the right onesBy the time I got into the company there was a big problem: the data was too isolated Analyzing data was too slow and difficult that people could not find the motivation to do itThe data sources we had at the time were diverse There were some data that we had to collect from Facebook Ads API Ad Words API Google Analytics Google Sheets and from an internal system of the company In order to collect data from those sources I built a Node JS application since Node JS has the power of running asynchronously and it speed up things when it comes to collecting data in that scenarioThe proposed pipeline architecture to fulfill those needs is presented on the image bellow with a little bit of improvements that we will be discussingThe first step of the pipeline is data ingestion This stage will be responsible for running the extractors that will collect data from the different sources and load them into the data lakeIn order to run those Node JS scripts that do exactly this we were using an EC2 instance on AWS but a great improvement I recommend you to make is to use Lambda to run those scripts Lambda is a great serverless solution provided by AWS By using Lambda you will not need to worry about maintaining a server nor need to pay for a 24 hour server that you will use only for a few hoursBut where should I load that data? S3 is a great storage service provided by AWS It is both highly available and cost efficient and can be a perfect solution to build your data lake on Once the scripts extracted the data from the different data sources the data was loaded into S3It is important to think about how you want to organize your data lake For this pipeline once we would not have a team of scientists and analysts working on that data and once our data came from the sources pretty organized I created only a raw partition on S3 where I stored data in its true form (the way they came from the source) with just a few adjustments that were made in the Node JS scriptHowever if you would like to have data scientists and analysts working on that data I advise you to create other partitions in order to store data in a form that suits each one of those users You can create three directories here like that: Now you may ask: and how will I transfer data from one stage to another? And the answer is: it depends! If you have a small volume of data that will not exceed 3008M of memory and 15 minutes of execution time (those were the limits when I wrote that post check now if it still applies) a good solution could be Lambda You can create transforms and enrichment functions so that you can process data from one stage and load it into another However if your data exceeds this limit you may go for Glue Glue is a very useful tool for that On that pipeline I used Glue to perform the transformations on the data but since I did not implemented the transformed and enriched stages I used it to load data directly to the data warehouse But if your needs are of having those three (or more) stages Glue can also be a nice solution for it However if you need to handle a really large volume of data it can be a better solution to use an EMR cluster It will depend on the volume of data you are processing the velocity you have to process it and on how much you can spendNow that your data is already on your data lake transformed and enriched it is time to send it to a data warehouse! I have been using Redshift for a while now and I have been having a great experience with it It is a very performing and reliable solution with a fair price Redshift also provides a very great resource called Redshift Spectrum that makes it possible to query data directly from your data lake on S3For my solution since the volume of data was not a problem I stored all data on Redshift and gained on performance However if you have a large volume of data it can become expensive to maintain all historical data in Redshift so it is good for you to store only the most recent data on Redshift and let the historical data on S3 In addition to that a good thing to keep in mind is to store that historical data on S3 in a columnar format such as Parquet because it will decrease a lot the cost of querying it using Redshift SpectrumWhat is data worth for if people cannot access it? As the last step you will need to integrate a visualization tool to your pipeline The tool I chose to use for that was MetabaseMetabase is a great open source visualization tool It offers an intuitive and user-friendly interface so that users with no knowledge of queries SQL and those stuffs will be able to explore data and create graphs and dashboards to visualize their results Metabase also allows users to define notifications via email and slack to receive scheduled emails informing about defined metrics or analysis to create collections where you can group data by companys divisions to create panels to present your analysis to restrain access to user groups and so onOn this post we discussed about how to implement a data pipeline using AWS solutions I shared with you some of the things I used to build my first data pipeline and some of the things I learned from it Of course there are a lot more things you can use to improve it such as logs and so on but this is already a big step to start
a27FUPUmWq37qedciwBQ29,Apache Spark provides a few very simple mechanisms for caching in-process computations that can help to alleviate cumbersome and inherently complex workloads This article aims at providing an approachable mental-model to break down and re-think how to frame your Apache Spark computationsLet me first start by stating that a simple workload should fit into memory across a relatively low number of executors  because simple workloads are inherently simple: like loading one set of data and joining it with another in a common ETL flow The required overhead here tends to be low as you need to only fit both datasets in memory for a portion of the total processing time if tuned correctly This workload is the Data Engineers bread and butterAbove is a simple example of a batch based Extract Transform Load (ETL) workloadComplex workloads will more often than not never fit entirely in memory due to cost of the underlying data or due to the nature of the job like a streaming pipeline job Complex workloads tends to bite you in very interesting ways mainly due to the fact that what works locally in dev or stage environments rarely scales well on the first go around in productionProduction can be a scary place where data rates (events per second etc) bloom and grow way outside of a traditional normal distribution Things that are uncertain especially in the realm of humongous data tend to cause pipelines to scream to a halt spark jobs to die and for customers to get angry  all because we all must balance between cost of operation and the unfortunate Out of Memory ExceptionExamples of complex workloads would be the case of a typical time-series ingestion pipeline with streaming statistical aggregations for real-time metrics or operational dashboards or the good ol real-time machine learning pipeline (see example below) These workloads are typically a joint effort between Data Engineers and an applied Machine Learning or Statistical / Performance EngineerThe example above is a fake use case using what is called a Stream-Stream join using Apache Spark Structured Streaming This takes things further by including Ad Targeting to this streaming pipeline via an expensive real-time machine learning transformation secondary streaming join a filtered post-join query as well as downstream publishing to an Ad Recommendation stream In theory a pipeline like this one could be sending you the ads you see everyday onlineNow that we have a loose understanding of the differences between simple and complex workloads we can learn how to solve complex problems in more intelligent ways Lets goInterestingly enough the difference between most simple workloads and a complex workload tend to come down to intelligent patterns of decomposition given that a complex workload can be looked at as a series of simple workloads with a distributed finish line that all processes are independently racing towards together and whom can all be run either fully independently (eg: non deterministic workloads) or deterministically where prior phases of execution reduce a problem into more concise layersNext we will look at how Spark distributes workIn the spark world everything begins with a resource This resource has a Source and describes how data can be read from that given Source In the simplest file based spark applications we begin with a DataFrameReaderThe DataFrameReader is available for most of the simpler data reading tasks like json csv orc and others enables us to quickly load data into Spark to be evaluated as a series of transformations leading up to a final outcome (or action) It is this (load)->(transform)->(write) directed-acyclic-computational-graph (DAG) which is what makes Spark so damn fun to work with Starting with a simple file like we see belowWe are able to easily read json data into spark memory as a DataFrame This DataFrame wraps a powerful but almost hidden gem within the more recent versions of Apache Spark That is the RDDThe RDD is how spark beat Map-Reduce at its own game It stands for Resilient Distributed Datasets These datasets are are partitioned into a number of logical partitions For example say you have an Apache Kafka topic with 32 partitions Apache Spark would automatically turn the read stream on the Kafka resource into an RDD with 32 partitions to honor and preserve the original Kafka partitioning schemeThis means that the following is true of RDDsAn executor is Sparks nomenclature for a distributed compute process which is simply a JVM process running on a Spark Worker The sole job of an executor is to be dedicated fully to the processing of work described as tasks within stages of a job (See the Spark Docs for more details)Using cache appropriately within Apache Spark allows you to be a master over your available resources Memory is not free although it can be cheap but in many cases the cost to store a DataFrame in memory is actually more expensive in the long run than going back to the source of truth dataset If you have taken a look at the Spark UI (runs on port 4040 when sparkuienabled is set to true) and have determined that you cant squeeze performance out of the system then you are a candidate for applying cachingMainly if the most time taken during your computations is loading data from System A or B or C (say HDFS or MySQL or Redis) and you cant speed up anymore by scaling out Scaling out with spark means adding more CPU cores across more RAM across more Machines Then you can start to look at selectively caching portions of your most expensive computationsThe above configuration would allow you to take up 8 machines running 8 cores by 12gb ram *We use c5d2xl machines from amazon and allocate 2gb for off-heap and 2gb for Operating System as well Hence the 12g in the example aboveThe notion of the StorageLevel comes up when you are caching off the lineage of your DataFrameAbove we have an example of loading transforming and joining data with an additional data frame Given that this operation is lazy in that nothing will be computed until you provide an action we can then cheat and cache this lineage by calling head on our DataFrame to push Spark into actionOnce you have cached your computations in this case calling persist with the option of StorageLevelMEMORY_ONLY_SER you can proceed with as many operations against the results of your initial computations without having to restart each action at the source or your initial read commandStorageLevel has options which range from DISK to OFF_HEAP to NONE I suggest taking a look at the options there and testing different options that help to speed up your Spark ApplicationsWhen you are all done with your cached data you can remove it from Spark really easily by calling unpersist * tap with wand and say mischief managed  for the Harry Potter fans out thereThis will remove the cache either immediately or you can also use the blocking optionThen you will free up your resources for the rest of your necessary computationsThe approach I take to caching and persistence with Spark is to treat it more like the JVM garbage collector If I need to access the same data more than once then I store it in cache This allows me to reduce round-trips to my source of truth for the data which is either s3a:// or hdfs:// Given the type of data I work with which is mainly parquet and other FileSystem based data is heavy on the IO and the total loaded files side Once I have my data loaded it is often better to cache things off before even making another decision so I dont inadvertently make needless trips back to hdfs or s3 to load the same data againI work on the Voice Insights team at Twilio where we are ingesting millions of events every few seconds across a slew of Spark Streaming applications all running within our data lineage pipeline We are a hybrid Data Platform / Data Engineering and Data Sciences team working in CaliforniaSee my other post on Streaming Trend Discovery to see how we handled some of the more complex use cases with Spark
Y9EGxPVnspTXWusGwrg6Zj,I learned about MapReduce briefly pretty much a year ago when my job required a bit of Hadoop I then had not touched MapReduce let along doing it with Java So when an assignment asked me to implement multiple MapReduce jobs under one script it was a mess searching up Stack Overflow and YoutubeSo why not write something about it? Yes I amMapReduce is a computation abstraction that works well with The Hadoop Distributed File System (HDFS) It comprises of a Map step and a Reduce step Map performs filtering and sorting into another set of data while Reduce performs a summary operation In both steps individual elements are broken down into tuples of key and value pairsMapReduce gains its popularity by being able to easily scale data processing over multiple computing nodes behind the sceneextremely large datasetsTo illustrate how to chain multiple MapReduce jobs in one script I will be using the NYC Taxi & Limousine Commission dataset of around 76 million rows to compute the distribution of degree differences of locations The tsv file I am using has the following structure: Lets say we want to use MapReduce to obtain the following output of two columns: where diff is a location IDs out-degree minus its in-degree The out-degree of a location is the number of times that location is used for pickup and the in-degree is the number of times it is used for dropoff Then count is the frequency for a particular diffTherefore one way to accomplish this is to have two MapReduce jobs One is to calculate the diff for each location and the other is to turn that output of the first job into count as we want aboveThe Map procedure for Job #1 simply loops through and breaks the tsv input into 4 different values for each line: PickUpLocation ID DropOffLocation ID PassengerCount and TotalFare However only PickUpLocation ID and DropOffLocation ID are relevant for our task here Then for each element the class also creates an inDegree variable that is 1 and an outDegree variable that is -1Then it writes each (PickUpLocation inDegree) and (DropOffLocation outDegree) as a key-value tuple that will be processed further by the Reduce procedure which takes form like below where the first column is location ID and the second column indicates whether it is a pickup or dropoff from each location IDThen what the Reduce procedure does is simply grouping by location ID and aggregate by summing up the second column to achieve the diff Then it writes (Location ID diff) as a key-value tupleThe output of Job #1 is then passed in as the input for Job #2In order to group by diff to achieve the final output the Map procedure of Job 2 needs to swap the input key-value pairs (Location ID diff) into (diff Location ID) because Reduce procedure groups by keysEssentially the output of the Map procedure this time looks like: Finally that is then passed into the second Reduce procedure to finish the task: What happens here is that the Reduce procedure simply loops through the input and for each occurrence of a unique diff its count increases by 1The challenging step that I found is connecting the above two MapReduce jobs so that Job #2 could take the output of Job #1 as input without the need for Job #1 to physically write out a fileI had to look for and experiment with multiple suggestions online until I found the combination that is easiest to understandBasically the key is to create two different configurations for the two jobs as conf and conf2 where they also get two different instancesBut note that the system does not terminate here Then the output of Job #1 is the same as the input of Job #2: And thats it With this technique you are not limited to only two MapReduce jobs but can also increase to three five or even ten to fit your taskI hope this quick note helps whoever that are struggling to find a comprehensive and easy to understand guide on chaining MapReduce jobs
JkgEr7JfxVNsEr7KKurs6M,Lets start with the why as I move from one blog post to another I feel like Im missing something the more and more I become proficient as a Data Scientist the more convinced I am that Data Engineering is one of the most critical and foundational skills in any data scientists toolkit In my earlier posts in this 101 for dummies like Me series I talked about PyTorch & Deep Learning basics and how one can get started I believe every data scientist should know enough about data engineering to carefully evaluate how their skills are aligned with the stage and need of their company Despite its importance education in data engineering has been limited Given its nascency in many ways the only feasible path to get training in data engineering is to learn on the job and it can sometimes be too late I am very fortunate that I get to work with Data Engineers who are patient enough to help me get started in this nascent field but not everyone has the same opportunity As a result I have written up this 101 guide on Data Engineering to summarize what I learned to help bridge the gapThink of Artificial Intelligence as the top of a pyramid of needs Yes self-actualization (AI) is great but you first need food water and shelter (data literacy collection and infrastructure)This framework(left) puts things into perspective Before a company can optimize the business more efficiently or build data products more intelligently layers of foundational work need to be built first This process is analogous to the journey that a man must take care of survival necessities like food or water before he can eventually self-actualize Unfortunately many companies do not realize that most of our existing data science training programs academic or professional tend to focus on the top of the pyramid knowledge Even for courses that encourage students to scrape prepare or access raw data through public APIs most of them do not teach students how to properly design table schemas or build data pipelines As a result some of the critical elements of real-life data science projects were lost in translation We should not go as far as arguing that every data scientist needs to become an expert in data engineering However I do think that every data scientist should know enough of the basics to evaluate project and job opportunities in order to maximize talent-problem fit If you find that many of the problems that you are interested in solving require more data engineering skills then it is never too late then to invest more in learning data engineering which is something I started doing recentlyIn computing data is information that has been translated into a form that is efficient for movement or processing Relative to todays computers and transmission media data is information converted into binary digital form It is acceptable for data to be used as a singular subject or a plural subject Raw data is a term used to describe data in its most basic digital formatThe difference between structured and unstructured data is that structured data is objective facts and numbers that most analytics software can collect making it easy to export store and organize in typical databases like Excel Google Sheets and SQL You can also easily examine structured data with standard data analysis methods and tools like regression analysis and pivot tablesOn the contrary unstructured data is usually subjective opinions and judgments of your brand in the form of text which most analytics software cant collect making it difficult to export store and organize in typical databases You also cant examine unstructured data analysis methods and tools Most of the time you must store unstructured data in Word documents or NoSQL databases and manually analyze it or use the analysis tools in a NoSQL database to examine this type of dataIn a world where Google Analytics can spit out every metric under the sun you must remember that qualitative data like customer feedback is just as crucial for informing your marketing strategy as web metrics Without unstructured data you wont have a clear understanding of how your customers actually feel about your brand And thats crucial for every marketer to knowMaxime Beauchemin the original author of Airflow characterized data engineering in his fantastic post The Rise of Data Engineer: Data engineering field could be thought of as a superset of business intelligence and data warehousing that brings more elements from software engineering This discipline also integrates specialization around the operation of so called big data distributed systems along with concepts around the extended Hadoop ecosystem stream processing and in computation at scaleAmong the many valuable things that data engineers do one of their highly sought-after skills is the ability to design build and maintain data warehouses Just like a retail warehouse is where consumable goods are packaged and sold a data warehouse is a place where raw data is transformed and stored in query-able forms In many ways data warehouses are both the engine and the fuels that enable higher level analytics be it business intelligence online experimentation or machine learning Without these data warehouses every activity related to data science becomes either too expensive or not scalable For example without a properly designed business intelligence warehouse data scientists might report different results for the same basic question asked at best; At worst they could inadvertently query straight from the production database causing delays or outages Similarly without an experimentation reporting pipeline conducting experiment deep dives can be extremely manual and repetitive Finally without data infrastructure to support label collection or feature computation building training data can be extremely time-consumingThe biggest decision is whether to have a cloud-based or in-house server infrastructure While it may sound like a black-or-white selection there are many things to consider The first factor is how important uptime is to your business Cloud solutions are usually more expensive than in-house but the benefits of being in the cloud can far outweigh the costs for some businesses For example an online business that is reliant on web-based transactions will consider uptime an extremely important factor; therefore they will likely be willing to pay more for a cloud-based solution that can guarantee a certain level of uptime Other businesses not as dependent on uptime may be more suited to an in-house setupA third option however is hybrid data storage Rather than choosing between on-premises and cloud solutions why not use both? Hybrid data storage offers the best of both worlds by using both storage solutions A common example of hybrid data storage is when an organization stores its primary data on a local server but stores backup copies on the cloud Granted this is more costly than simply choosing a single storage option but it also protects against unforeseen disasters If a fire or flood occurs at the organizations server room it could destroy their locally stored on-premises data Since the organizations backups are stored on the cloud however they can restore their lost data with minimal interruptionThere are pros and cons associated with each data storage solution On-premises requires a substantial monetary investment but also offers the highest level of security; cloud storage is affordable scalable and can be accessed over the internet and hybrid brings them both together Consider your organizations short- and long-term needs and choose the data storage option thats best suitedBefore collected data can be analyzed and leveraged with predictive methods it needs to be organized and cleaned Data Engineers begins this process by making a list of what data is stored called a data schema Next they need to pick a reliable easily accessible location called a data warehouse for storing the data Examples of data warehousing systems include Amazon Redshift or Google Cloud Finally Data Engineers create ETL (Extract Transform and Load) processes to make sure that the data gets into the data warehouse These three conceptual steps are how most data pipelines are designed and structured They serve as a blueprint for how raw data is transformed to analysis-ready data To understand this flow more concretely refer below image: While all ETL jobs follow this common pattern the actual jobs themselves can be very different in usage utility and complexity For example we could have an ETL job that extracts a series of CRUD operations from a production database and derive business events such as a user deactivation Another ETL can take in some experiment configuration file compute the relevant metrics for that experiment and finally output p-values and confidence intervals in a UI to inform us whether the product change is preventing from user churn Yet another example is a batch ETL job that computes features for a machine learning model on a daily basis to predict whether a user will churn in the next few daysELT should be used instead of ETL in these various cases : So in short when using ETL the transformations are processed by the ETL tools while in ELT the transformations are processed by the target data sourceELT has the benefit of minimizing the processing on the source since no transforming is being done which can be extremely important if the source is a production system where you could be impacting the user experience as opposed to a copy of the source (via replication database snapshot etc) The negative of this approach is it may take longer to get the data into the data warehouse as with the staging tables you have an extra step in the process and you will need more disk space for the staging tablesOLTP is said to be more of an online transactional system or data storage system where the user does lots of online transactions using the data store It is also said to have more ad-hoc reads/writes happening on a real-time basis OLAP is more of an offline data store It is accessed a number of times in offline fashion For example Bulk log files are read and then written back to data files Some of the common areas where OLAP is used are Log Jobs Data mining Jobs etc Cassandra is said to be more of OLTP as it is real-time whereas Hadoop is more of OLAP since it is used for analytics and bulk writesData modeling is the process of producing a descriptive diagram of relationships between various types of information that are to be stored in a database One of the goals of data modeling is to create the most efficient method of storing information while still providing for complete access and reportingA data model describes information in a systematic way that allows it to be stored and retrieved efficiently in a Relational Database Management System (RDBMS) such as SQL Server MySQL or Oracle The model can be thought of as a way of translating the logic of accurately describing things in the real-world and the relationships between them into rules that can be followed and enforced by computer codeData normalization is the process of structuring a schema to avoid dependencies that create anomalies when new data is inserted or updated The idea behind normalization is to ensure that data can always be queried reliably Most commonly this involves reducing data stores to a point where no duplicate data is contained within them By ensuring the discrete storage of data points the model can offer more efficient storage of information while avoiding scenarios where contradictory data might be stored Although in terms of data storage a normalized model is inherently the most efficient for some operational purposes a fully normalized database can result in a sub-par performance So data modelers commonly also engage in strategic denormalization breaking normalization rules to optimize certain query or update processes to speed up the RDBMSThe implementation of a data warehouse and business intelligence model involves the concept of Star Schema as the simplest dimensional model Similar to every other dimensional model star schema consists of data in the form of facts and dimensions The reason behind the name Star Schema is that this data model resembles a star with ends radiating from the center where the center refers to the fact table and the radiating points are dimension tablesA relational database management system (RDBMS) is a collection of programs and capabilities that enable IT teams and others to create update administer and otherwise interact with a relational database Most commercial RDBMSes use Structured Query Language (SQL) to access the database although SQL was invented after the initial development of the relational model and is not necessary for its useIn general databases store sets of data that can be queried for use in other applications A database management system supports the development administration and use of database platforms An RDBMS is a type of DBMS with a row-based table structure that connects related data elements and includes functions that maintain the security accuracy integrity and consistency of the dataNumerous RDBMSes arose along with the use of client/server computing Among the competitors were Oracle Ingres Informix Sybase Unify Progress and others Over time three RDBMSes came to dominate in commercial implementations Oracle IBMs DB2 and Microsofts SQL Server which was based on a design originally licensed from Sybase found considerable favor throughout the client/server computing era despite repeated challenges by competing technologies As the 20th century drew to an end lower-cost open source versions of RDBMSes began to find use particularly in web applications Such systems included MySQL and PostgreSQLEventually as distributed computing took greater hold and as cloud architecture became more prominently employed RDBMSes met competition in the form of NoSQL systems Such systems were often specially designed for massive distribution and high scalability in the cloud sometimes forgoing SQL-style full consistency for so-called eventual consistency of data But even in the most diverse and complex cloud systems the need for some guaranteed data consistency requires RDBMSes to appear in some way shape or form Moreover versions of RDBMSes have been significantly restructured for cloud parallelization and replicationAs useful as they are DE frameworks are rarely born out of thin air An Airbnbs data engineer quoted in the source video mentioned below: There really is no magic when you have done certain task enough times you started to see patterns that can be automated When you see your work as workflows new possibilities ariseWhen thinking about which workflow to automate the framework designer needs to start by thinking about the end users experience There are generally three layers of a well-designed data engineering framework: the input layer the data processing layer and the output layerSo once you have your entire workflow automated using AriFLow and are handling big data you need to use Apache Spark to do Pre-processing on them On its website Apache Spark is explained as a fast and general engine for large-scale data processing But that doesnt even begin to encapsulate the reason it has become such a prominent player in the big data space Its adoption by big data companies has been on the rise at an eye-catching rate Understanding the reasons behind Sparks rise can help predict the trajectory of upcoming big data solutionsThe Spark core is complemented by a set of powerful higher-level libraries which can be seamlessly used in the same application These libraries currently include SparkSQL Spark Streaming MLlib (for machine learning) and GraphX each of which is further detailed in this article Additional Spark libraries and extensions are currently under development as wellIn this post I covered data engineering and the skills needed to practice it at a high level If youre interested in architecting large-scale systems or working with huge amounts of data then data engineering is a good field for you It can be very exciting to see your autoscaling data pipeline suddenly handle a traffic spike or get to work with machines that have terabytes of RAM Theres satisfaction in building a robust system that can work for months or years with minimal tweakingBecause data engineering is about learning to deal with scale and efficiency it can be hard to find good practice material on your own But dont give up hope  its very possible to learn data engineering on your own and get a job in the field
YqgPtuvtCYij7XGbcSgBgB,It is incredible how fast data processing tools and technologies are evolving And with it the nature of the data engineering discipline is changing as well Tools I am using today are very different from what I used ten or even five years ago however many lessons learned are still relevant todayI have started to work in the data space long before data engineering became a thing and data scientist became the sexiest job of the 21st century I officially became a big data engineer six years ago and I know firsthand the challenges developers with a background in traditional data development have going through this journey Of course this transition is not easy for software engineers either it is just differentEven though technologies keep changing  and thats the reality for anyone working in the tech industry  some of the skills I had to learn are still relevant but often overlooked by data developers who are just starting to make the transition to data engineering These usually are the skills that software developers often take for grantedIn this post I will talk about the evolution of data engineering and what skills traditional data developers might need to learn today (Hint: it is not Hadoop)Data teams before the Big Data craze were composed of business intelligence and ETL developers Typical BI / ETL developer activities involved moving data sets from location A to location B (ETL) and building the web-hosted dashboards with that data (BI) Specialised technologies existed for each of those activities with the knowledge concentrated within the IT department However apart from that BI and ETL development had very little to do with software engineering the discipline which was maturing heavily at the beginning of the centuryAs the data volumes grew and interest in data analytics increased in the past ten years new technologies were invented Some of them died and others became widely adopted that in turn changed demands in skills and teams structures As modern BI tools allowed analysts and business people to create dashboards with minimal support from IT teams data engineering became a new discipline applying software engineering principles to ETL development using a new set of toolsCreating a data pipeline may sound easy but at big data scale this meant bringing together a dozen different technologies (or more!) A data engineer had to understand a myriad of technologies in-depth pick the right tool for the job and write code in Scala Java or Python to create resilient and scalable solutions A data engineer had to know their data to be able to create jobs which benefit from the power of distributed processing A data engineer had to understand the infrastructure to be able to identify reasons for failed jobsConceptually many of those data pipelines were typical ETL jobs  collecting data sets from a number of data sources putting them in a centralised data store ready for analytics and transforming them for business intelligence or machine learning However traditional ETL developers didnt have the necessary skills to perform these tasks in the Big Data worldI have reviewed many articles describing what skills data engineers should have Most of them advise learning technologies like Hadoop Spark Kafka Hive HBase Cassandra MongoDB Oozie Flink Zookeeper and the list goes onWhile I agree that it wont hurt to know these technologies I find that in many cases today in 2020 it is enough to know about them  what particular use cases they are designed to solve where they should or shouldnt be used and what are the alternatives Rapidly evolving cloud technology has given rise to a huge range of cloud-native applications and services in recent years In the same way as modern BI tools made data analysis more accessible to the wider business several years ago modern cloud-native data stack simplifies data ingestion and transformation tasksI do not think that technologies like Apache Spark will become any less popular in the next few years as they are great for complex data transformationsStill the high rate of adoption of cloud data warehouses such as Snowflake and Google BigQuery indicates that there are certain advantages they provide One of them is that Spark requires highly specialised skills whereas ETL solutions on top of cloud data platforms are heavily reliant on SQL skills even for big data  such roles are much easier to fillBI / ETL developers usually possess a strong understanding of database fundamentals data modelling and SQL These skills are still valuable today and mostly transferable to a modern data stack  which is leaner and easier to learn than the Hadoop ecosystemBelow are three areas I often observe traditional data developers having gaps in their knowledge because for a long time they didnt have tools and approaches software engineers did Understanding and fixing those gaps will not take a lot of time but might make a transition to a new set of tools much smootherSQL code is a code and as such software engineering principles should be appliedI am a big fan of DBT  an open-source tool which brings software engineering best practices to SQL world and simplifies all these steps It does much more than that so I strongly advise to check it outWe tend to stick with the tools we know because they often make us more productive However many challenges we are facing are not unique and often can be solved today more efficientlyIt might be intimidating trying to navigate in the cloud ecosystem at first One workaround is to learn from other companies experiencesMany successful startups are very open about their data stack and the lessons they learnt on their journey These days it is common to adopt a version of a cloud data warehouse and several other components for data ingestion (such as Fivetran or Segment) and data visualisation Seeing a few architectures is usually enough to get a 10000-foot view and know what to research further when needed  eg dealing with events or streaming data might be an entirely new conceptAs much as I love Scala Python seems to be a safe bet to start with today It is reasonably easy to pick up loved by data scientists and supported pretty much by all components of cloud ecosystems SQL is great for many data transformations but sometimes it is easier to parse complex data structure with Python before ingesting it into a table or use it to automate specific steps in the data pipelineData processing tools and technologies have evolved massively over the last few years Many of them have evolved to the point where they can easily scale as the data volume grows while working well with the small data too That can significantly simplify both the data analytics stack and the skills required to use itDoes it mean that the role of a data engineer is changing? I think so It doesnt mean it gets easier  business demands grow as technology advances However it seems that this role might become more specialised or split into a few different disciplinesNew tools allow data engineers to focus on core data infrastructure performance optimisation custom data ingestion pipelines and overall pipeline orchestration At the same time data transformation code in those pipelines can be owned by anyone who is comfortable with SQL For example analytics engineering is starting to become a thing This role sits at the intersection of data engineering and data analytics and focuses on data transformation and data quality Cloud data warehouse engineering is another oneRegardless of whether the distinction in job titles will become widely adopted or not I believe that traditional data developers possess many fundamental skills to be successful in many data engineering related activities today  strong SQL and data modelling are some of them By understanding the modern cloud analytics data stack and how different components can be combined together learning a programming language and getting used to version control this transition can be reasonably seamless
dxAbLTwXyWkgxKXzUvKB7Y,There is a high demand for data engineers these days I can see job proposals flourishing over LinkedIn Yet recruiting a data engineer is quite hard I hear a lot say this has to do with this offer and supply balance Big data is in the trends all around the world and there is a high need for engineers able to tame this data This is a fact and it is undeniable The reason why I am writing this post today is that I strongly believe there is something else or rather something moreAfter looking at job proposals on the Internet I came up to the conclusion that the data engineer position is quite hard to define I could read various positions and skills requirements for the same so called data engineer and it really looked like every company had its own definition of what a data engineer is I have seen job descriptions that were very close to IT engineer Others were about development Surprisingly I have even read about companies looking for a data scientist as a data engineerThis fuzziness brings much confusion This is disturbing for me even though I have been working as a data engineer for some time nowI definitely think there is something missing in the definition of what a data engineer is and heres my takeMost job offers I can see are requiring technical skills such as ETL Spark Hadoop or NoSQL This is very very great but these are just tools actually What we are speaking about here is ways to achieve a purpose And this purpose is data processing which is  to me  exactly what this is all about I got interviewed many times for a job or a contractor mission and I always got the same questions: What are Scala companions? What is a DStream ? How would you configure partitions in Kafka? Those questions are really interesting (well I find them interesting as I love this field of informatics) but during those interviews people were checking that I knew the tools that I filled the grid and hardly no one spoke about the value I could extract from themTake this example Apache Spark: Spark has become one of the first choice tools to use when you want to process data Good but the take on Spark is not about writing jobs You can code the same Spark job with 4 different languages Writing some Spark has become really accessible but regarding your project and your end users does this ensure your data will be processed the most efficient way ? Actually it does not And this is where the data engineer comes into action The real value of the data engineer will be to bring his expertise to deploy and distribute a job in the cluster with the resources configuration that will use the cluster at its finestTo be concise Data engineerss value is about why when where and how the data will be processed So let me say the big word once in for all Yes in the end all of this is a matter of data and this is exactly what a data engineer is here for I believe that data engineers wont think in terms of tools and techniques but rather architecture and choose tools like books on a shelf Actually the real purpose of a data engineer is to understand the data design the way it could be processed and then help facilitate its processing by choosing the optimal tools configuration that will ensure reliability and performanceOf course data engineers need a good understanding of the environment and the context Much experience is then required to be legitimate to choose between all of those cutting edge tools But knowing the tools brings no guaranty over the performance of your data pipeline Also it can lead to misunderstandings of what kind of resource you really need Maintaining a Hadoop cluster is not data engineering it is IT Developing exclusively on Spark is not data engineering it is development Building data models with MLLib is not data engineering it is machine learningWhen I receive a candidate for a Data engineering job I try to focus on how he considers the interaction between his skills and the data he will have to process Learning a tool can take weeks maybe months Having the good mindset will take yearsThis is why I think finding data engineers is more than validating expertise over the traditional big data stack This is actually a matter of what you want to focus on If your goal is to find the expert among the experts the one then fine and I agree with you your research is going to be a long journey But if you focus on what the engineer can provide with because he focuses on the data rather than the tools well you may have picked a potentially very good candidateIn the end as clients are becoming more and more demanding and data is turning more and more complex to process I tend to believe that there could be much more data engineers than there actually are Many engineers get discarded when interviewed because they dont know enough about the inner details of how HDFS works or they dont know enough about scalaLet me share with your an experience of mine to explain why I believe that data engineers are all about data A couple of years ago in my former job I have faced quite a strange situation where I was working on a project where there was no data! I was ready to rock with my Kafka setup my Ansible scripts my Spark cluster and my jobs ready to run Thing is that there was no data At this moment my job was looking so meaningless that I realized that the most important thing above everything was dataWhy are data engineers all about data ? Because without data data engineers loose what makes them so important We call this relevance
TToCKsYc9Re64u2bnGFEZW,"There is a strange paradigm in Data Engineering when it comes to transformation code While we increasingly hold extract and load (EL) programming to production software standards transform code continues to be treated as a second-class citizen Ironically transform code often contains the complex business logic that could benefit greatly from being treated like softwareA code smell is  a surface indication that usually corresponds to a deeper problem in the system  In even simpler terms code smells are patterns in software that beg us to look a little closer Code smells in your application are not unlike actual smells in a refrigerator: a pungent odor may signal that something unsavory is present (like that carton of decade-old moo shoo pork) or it may be as innocuous as Limburger cheese Code smells do not guarantee that a problem exists and often it is the case that the best possible refactor resembles a different smell The value lies in that each occurrence prompts you to question what solution offers the most readable maintainable transform codeWhat follows is a collection of code smells specific to dimensional Data Warehouse transforms Encountering them should give you pause and present you with opportunities to leave the codebase better than you found itTranslated to English: If the receipt price isnt there try the label price if that isnt there try the catalog price and if all else fails figure the price at 0Why it smells: Fumbling through a handful of columns to grab the first available value indicates that data is not well understood Either the code does not know why one column value deserves preference or the resulting column is a mashup of several states that should be independentPossible refactoring: The nested coalesce above likely represents multiple independent states forced into a false condition Consider replacing with either an explicit decision tree (usually a CASE statement) or breaking each state into a distinct factTranslated to English: Name the user_sp_role column ROLE  people will know what that meansWhy it smells: A core tenet of Data Warehouse design is that the interface should err on the side of simplicity Using reserved words (even reserved words allowed by your particular dialect) will introduce complexity and opportunities for confusionPossible refactorings: Stick to verbose identifiers that are easy to use do not require quotes and will keep the Data Warehouse accessible to users of all SQL aptitudes ROLE could more intuitively be named web_application_role and avoid pointless confusionTranslated to English: If you want all the customers that do not have registered phone numbers just select where the phone number is NULL""Why it smells: NULL is a very important value in the Data Warehouse world If a join goes bad there will be NULL values If a group by fails or a window function isn't sliding as we expect there are NULL values When NULL plays double-duty as a legit data value debugging becomes nearly impossible Add to this that BI tools often behave inconsistently when presented with NULL values and you have a perfect place for bugs to hide""Possible refactorings: Dont use NULL values in dimensions; explicitly state each possible condition (ie use ELSE in CASE statements) so that any NULL value immediately draws scrutiny This will not only harden your transform code but contribute to the intuitive nature of the end product data NULL can mean a lot of things but 'No Phone Number Available' is crystal clearThis smell only applies specifically to dimension attributes NULL values are not only correct but important data points for additive facts (such as total_sale_value )Translated to English: We dont use customer units 8 or 13 anymore so we ignore them (Ted says 13 and 19 are all that matter) We also only care about the primary website customer value types (Bob says those are designated by a)Why it smells: Good code is self-documenting This generally means you can read the code and understand what it does without a decoder ring The example above isnt challenging due to complex business logic or technical intricacy but because it is overflowing with tribal knowledgePossible refactorings: CTEs are great tools for data mapping: When larger refactors are not possible comments are better than nothing Look for variables and constants that can be more descriptively named as a cheap way to greatly improve the codebase""Translated to English: website visits should always have a visit_id  so if they don't the record is bad and we should throw it outWhy it smells: The foundation of any Data Warehouse is truth Not just some but the whole truth which destructive transforms cannot provide A Data Warehouse missing records (even bad records) has no credibility and you will quickly find consumers asking for raw source accessPossible refactorings: Transform logic should be additive presenting greater value to the end-user In the example above a new column valid_record would filter to the same dataset in a BI layer while providing consumers with the confidence of having access to all the dataTranslated to English: Most of our web traffic is from the SF Bay area so if a web visit is missing a timestamp we update it to PSTWhy it smells: The job of the Data Warehouse is to provide users with the ability to make informed decisions not to make decisions for them Every time transform logic chooses a path for the data it inevitably removes options from the consumer in the processPossible refactorings: In the example above the original last_login_time would ideally render last_login_time_without_timezone along with last_login_time_with_timezone ; the end-user can then decide to make assumptions about the missing timezones at their own perilTranslated to English: The records with a created date greater than yesterday are the new recordsWhy it smells: Any time the same code can be run twice against the same data and return different results consider it a problem Good transformation logic is both idempotent and deterministic Unstable elements such as the current date or time make the code brittle and can easily land the system in an uncorrectable state if a transform job fails or is run twicePossible refactorings: Design transforms in a manner that is self-healing Using the same example: Translated to English: Unstructured grammar around identifiers erratic prefixing of column names and lack of a vocabulary systemWhy it smells: In a Data Warehouse the schema is the product interface Unpredictable lexis serve as undue friction for the user Is the table order or orders? Is the column sale_price or order_sale_price? Without a pattern this is all overhead to the usability of a Data WarehousePossible refactorings: Select conventions Document them Update the transform code to reflect them The same query with homogeneous language might look like: Translated to English: Any table view schema database or column where the name reflects the source system (ie postgres_user ) the extract-load medium (ie DATA_WAREHOUSESTITCHUSERS ) or any other mechanical component of the ELT process (ie cron_dailyusers )Why it smells: It can be hard for engineers to get out of our own headspaces This smell often results from designing a schema source down instead of end-user up The Data Warehouse must represent information in a way that reflects business domain objects; for example a hospital does not think of its consumers as billing users and chart system users and prescription users they are all simply patientsThis is a particularly hard smell to detect because the business domain often runs very close to the technology domain and users may have trained themselves to incorrectly align one with the other If a retailer has distinct eCommerce and physical point-of-sale systems it is very easy to think that the eCommerce system represents web_users and the POS system represents in_store_users  But this is not the case; the business has only CUSTOMERS who may shop in a store online or bothPossible refactorings: Think of your data product the way a UX designer would design an intent-driven application interface If you log into your Medium account you are asked for your username and password not your  dynamo_db username and password By the same logic your Data Warehouse userbase is interested in page visits not Google Analytics page visits or Adobe Analytics page visitsTranslated to English: Functions that are not part of the native SQL dialect for the target Data Warehouse and are not created as part of the codebaseWhy it smells: If we view the transform codebase as the blueprints by which our Data Warehouse is constructed stored procs (not created as part of the codebase) are off the books jobs The codebase no longer has all the elements of the machine and cannot effectively reproduce the warehouse This dangerous and brittle state leaves the warehouse open to catastrophic failure if (when) the instance goes downPossible refactorings: If you are using a SQL framework like DBT (or any SQL precompilation really) avoid stored procs and functions completely For those rare instances where a stored procedure or function is the only viable solution (or if you are using stored procs as your transform layer) include the definition of the proc in your code base with either a DROP CREATE or CREATE OR REPLACE pattern to ensure that it is recreated from your code with every run This will minimize the gap between the state of your code and the state of productionTranslated to English: Identifiers that are written case-sensitive or including special characters or reserved wordsWhy it smells: SQL is a 4th generation language and the intent of conventions like case folding (treating identifiers as case-insensitive values) is to more closely resemble human-to-human communication Quoted identifiers generally swim against the current of this intent forcing users to consider capitalization and potentially leading to confusing Leads_Prod vs leads_prod situations (these are 2 distinct tables!)Possible refactorings: Just dont quote identifiers ever Avoid the confusion and the overhead by using verbose descriptive names for databases tables/views and columns As a bonus your code will be portable this way (case folding is not consistent across different platforms so any quoted identifier is instantly non-portable)There was a valiant effort in the earlier days of data warehousing to quote everything making identifiers as pretty and report-ready as possible with column names like Monthly Report Status At the time this made a lot of sense as much of the consumption was directly from Data Warehouse tables into reports and spreadsheet extracts Today I would argue that BI tools are the best place for this kind of presentation polish and the Data Warehouse benefits more by keeping identifiers clean and verboseTranslated to English: Any timestamp that is not explicitly cast to UTC value especially the use of local time as a standardWhy it smells: Timestamps are the messiest of datatypes The implementation and handling of timestamps differ greatly from platform to platform language to language and especially tool to toolPossible refactorings: Explicitly convert all timestamps to UTC for storage Note that this is not the same as converting and then stripping the timezone (a weird yet painfully common practice that likely stemmed from a belief that timestamps without timezones are easier) Consistent use of UTC will streamline onboarding new datasets eliminate daylight savings time confusion and future-proof organizational knowledge past the point of a single timezone Let the BI tools worry about timestamp presentation (most will do it anyway and those helpful upstream conversions will likely do more harm than good)Translated to English: Schemata that reflect traditional BCNF that you would expect to find in transactional database designs In this example site_identifiers have been normalized out of site to protect referential integrityWhy it smells: Data warehouses are OLAP structures that fulfill a very different need from transactional databases Normalization and referential constraints are important parts of how OLTP systems do their job  but these tools are detrimental to the goals of a knowledge store Data Warehouses do not represent the desired state (ie that all page_views have a source_id that exists in the traffic_sources table) they represent the reality (ie that a bug associated 1 million page_views to a non-existent source) From a higher vantage point the presence of heavy normalization is probably a strong indicator that other OLTP conventions have been followed throughout the codebasePossible refactorings: Dimensional model design is outside the scope of this writing (for a greater understanding of how dimensional models differ from transactional models I highly recommend the Data Warehouse Toolkit by Ralph Kimball) In general these normalized values should be degenerated to wide flat dimensional tables like so: Translated to English: Complex transforms that are masked by seemingly stable identifiersWhy it smells: Squishy logic is arbitrarily sound business logic: in the example above the code decides that two sessions occurring less than a minute apart with extremely close thumbprints and originating from (nearly) the same location are likely the same user session This smell here is not the logic  whether or not this is an accurate way to merge browser sessions is up to the business; the smell is representing likely the same user session as the absolute value sessionPossible refactorings: Data Warehouse transform code represents what is known to be true In this example we know that each session exists while we hypothesize that certain sessions are actually the same session If the hypothesis is supported by the business it can easily be represented as additional information in the form of a likely_parent_session column Aggregations on top of this hypothesis can exist in additional materializations ie dim_collapsed_session and fact_collapsed_conversion etc Often more than one hypothesis is needed to support the range of business use cases In that event each hypothesis can either materialize further downstream in a domain-specific mart or be branded and used to enrich dim_session in the Data WarehouseTranslated to English: For a consumer to make use of the Data Warehouse they need input from the transform authorsWhy it smells: The Data Warehouse is both a business tool and a consumer product Like any complex tool intended for business use it must ship with comprehensive documentation Imagine if the only way to learn to use the VLOOKUP function in Excel was to call a Microsoft Engineer! Without consumer-facing documentation the product would be impractical to usePossible refactorings: There is a multitude of places documentation can live Nearly all Data Warehouse platforms support SQL comment meta for objects If you use a transformation framework like DBT then consumer-facing documentation is baked in with dbt docs Documentation can also be managed with tools like Sphinx Read The Docs or even simple markdown files The documentation solution must at a bare minimum: * be easy for consumers to access * be maintained as part of the data product * support effective search and navigationTranslated to English: The use of shorthand model aliases often one or two letters longWhy it smells: Abbreviated shorthand is very useful for writing quick ad-hoc queries But like all good software transformation code should be self-documenting and use object names that mean somethingPossible refactorings: Naming identifiers is one of the two hard things in software development Use alias names that are descriptive unique within the transform and convey the content of the represented table/CTE/dataset: Translated to English: Verticals refuse to agree on business logic around a KPI so we support multiple versions of the truthWhy it smells: Organizational Maturity is a critical element in any successful data initiative If the business is unwilling (or unable) to make sometimes-difficult decisions and move forward with a unified source of truth this indecision will be reflected in the Data Warehouse codebasePossible refactorings: The refactor for this smell is technically simple but practically difficult The business must evolve and declare a singular definition that all verticals will adopt In SQL this is as simple as: In the real world this can be a political minefieldOriginally published on https://yoyodynedatacom"
DoRCU6U6KTMYVrUzRUcH8r,Let me start by introducing two problems that I have dealt time and again with my experience with Apache Spark: Sometimes I solved above with Design changes sometimes with the introduction of another layer like Aerospike or sometimes by maintaining historical incremental dataMaintaining historical data is mostly an immediate solution but I dont really like dealing with historical incremental data if its not really required as(at least for me) it introduces the pain of backfill in case of failures which may be unlikely but inevitableThe above two problems are problems because Apache Spark does not really support ACID I know it was never Sparks use case to work with transactions(hello you cant have everything) but sometimes there might be a scenario(like my two problems above) where ACID compliance would have come in handyWhen I read about Delta Lake and its ACID compliance I saw it as one of the possible solutions for my two problemsDelta Lake Documentation introduces Delta lake as: Delta Lake is an open source storage layer that brings reliability to data lakes Delta Lake provides ACID transactions scalable metadata handling and unifies streaming and batch data processing Delta Lake runs on top of your existing data lake and is fully compatible with Apache Spark APIsDelta Lake key points: Consider the following piece of code to remove duplicates from a dataset: For my spark application running above piece of code consider a scenario where it fails on Line 4 that is while writing the data This may or may not lead to data loss
e35aPEXQKhS4SEKAvco44o,Druid is an open-source analytics data store designed for business intelligence (OLAP) queries on event data Druid provides low latency (real-time) data ingestion flexible data exploration and fast data aggregationLog in with your GCP account and create three virtual machines with SO DebianEnter each node with SSH and run: Zookeeper may be installed in an independent node although in this case we are going to install in Master node Log in using SSH and run the following scriptEdit file located in the route: apache-druid-0151-incubating/conf/druid/cluster/_common/commonruntimeproperties replicate for each node making the suggested changesThe following changes are written in orderIf you need to set up a basic authentication system add these properties in the common file for each node
FLcfdG5dpj4zSCbAdetZei,Dimensional modeling (DM) is part of the Business Dimensional Lifecycle methodology developed by Ralph Kimball which includes a set of methods techniques and concepts for use in data warehouse design The approach focuses on identifying the key business processes within a business and modelling and implementing these first before adding additional business processes a bottom-up approach
SkvsahGBY4HhsPj5zh9xrt,Building distributed systems for ETL & ML data pipelines is hard If you tried implementing one yourself you may have experienced that tying together a workflow orchestration solution with distributed multi-node compute clusters such as Spark or Dask may prove difficult to properly set up and manage In this article I want to show you how you can obtain a highly available scalable distributed system that will make the orchestration of your data pipelines for ETL & ML much more enjoyable and will free up your time to work with data and generate value out of it rather than spend time on maintaining clustersThe entire setup is to a large extent automated by AWS and Prefect Plus it will cost you almost nothing in the beginningIn December 2019 AWS launched a new Fargate feature which to many was considered a game-changer  they introduced an option to use AWS EKS on ECS Fargate which is a way of saying: AWS made the Fargate service an orchestrator not only for the ECS but also for the EKS Up to that point AWS Fargate was a serverless way of running containers only on AWS ECSEKS and Fargate make it straightforward to run Kubernetes-based applications on AWS by removing the need to provision and manage infrastructure for podsWhat does it mean for us? It means that we can now have a serverless Kubernetes cluster on EKS which is only charging us for running pods rather than their underlying EC2 instances It also means among other benefits: AWS takes care of all that for you All that you need to do is to write YAML files for your deployments and interact with the EKS via kubectl In short: your only task now is to write your ETL & ML code that adds value to your business and AWS takes care of the Ops ie operating maintaining and scaling your Kubernetes clusterConsidering that we are charged only for the actual vCPU and memory of running pods this provides a great foundation for a modern data platformOne possible disadvantage of using almost any serverless option is the issue of cold start when the container orchestration system needs to first allocate and prepare the compute resources (ao pulling the latest version of the image from the docker registry to the allocated worker node and building the image) which may add some additional latency before the container (or your K8s pod) will turn into a running stateIf your data workloads require a very low level of latency you may opt for the AWS EKS cluster with the traditional data plane and follow the instructions from this blog post to set up a non-serverless cluster on AWS EKS and to connect it to your Prefect Cloud environmentHowever you can have both! AWS allows you to mix the two: As you can see AWS designed EKS on Fargate with a lot of foresight which allows you to mix the serverless and non-serverless options to save your time money and maintenance efforts You can find out more about that in this video in which AWS introduced the serviceYou need to have an AWS account with either admin access or at least a user with IAM permissions for creating ECR EKS and ECS resources Then you must have AWS CLI configured for this account and eksctl installed as described in this AWS docsKubernetes on AWS works well with AWS ECR which is a registry for your Docker images To authenticate your terminal with your ECR account run: Note: <YOUR_AWS_REGION> could be ex us-east-1 eu-central-1 and moreIf you get Login Succeeded message you can create your ECR repositories for your data pipelines We will create two data pipelines: dask-k8 and basic-etl-prefect-flow  use the same names to follow this walkthrough but in general its easiest to give your ECR repository the same name as your Prefect flow to avoid confusionThen all you need to do is to run the following command which will deploy Kubernetes control plane and Fargate Profile in your VPC: I picked the name fargate-eks for the cluster but feel free to change it The --fargate flag ensures that we create a new Fargate profile for use with this cluster EKS allows you to create a custom Fargate profile if neededThe provision of all resources may take several minutes When finished you should see output similar to this one: Then if you check your context: You should get output similar to this one: This way you can see that you are connected to a serverless Kubernetes cluster running on AWS Fargate! To prove it further run: In the output you should see at least 1 fargate node waiting for your pod deploymentsNote: those nodes are running inside of your VPC but they are not visible within your EC2 dashboard You cannot SSH to those nodes either as they are fully managed and deployed by Fargate in a serverless fashionThe advantage of combining this AWS EKS cluster with Prefect is that the entire Kubernetes pod deployment and scheduling is abstracted away from you by Prefect This means that you dont even need to know much about Kubernetes in order to derive value from it In the next section we will connect this cluster to our Prefect Cloud account and start building distributed ETL & ML data pipelinesLets first sign up for a free Developer account on https://cloudprefectio/At first you will be welcomed by a clean UI showing you your flows agents and the general overview of the recent flow runs and the next scheduled jobs The flows themselves can be organized into several projects When you start building your data pipelines this main dashboard let you quickly identify the current status of all your data pipelines This dashboard is extremely helpful: imagine that you log into your account in the morning and see that all your pipelines are in a green status! You only need to dive deeper if you see any red bars indicating some failed data flowsI know companies who would dream of having this kind of dashboard and this level of transparency about their ETL & ML data pipelines and their status while at the same time being able to see that all agents executing the work are healthy: at the moment you can see that I currently have a Fargate agent ready to deploy the flows on AWS ECS For now we focus on AWS EKSLets continue with the setup and install prefect on your computer
28qNcw7mJhJihTH8VQeoLM,From many years Big Data has become widespread and trendy The Big Data technologies started to fill the gap between the traditional data technologies (RDBMS File systems … ) and the high evolution of the data and business needsWhile implementing these technologies is a must for many large-scale organization to ensure the business continuity many organization are aiming to adopt these technologies without really knowing if they can improve their businessBefore making your decision there are many things you should take into considerationBefore asking if your business needs Big Data technologies you have first to know what is Big Data I know this may looks weird but for true it happensMany times we were asked to develop a Big data ecosystem for many companies just because the size of their databases is close to 1 TB after years of storing data while they only need to optimize the data storage and to adopt some best practicesOne time I was asked to build a Big Data solution just to let a company manager able to search over hundreds of excel files that contains different information At the end the goal was achieved using some SQL Server integration services packages a small data warehouse and a Power BI projectJust googling the word Big Data will lead you to many articles describing it and it will removes any misunderstanding if existedBig Data is not only a data with a huge volume there are many other characteristics such as velocity veracity andvarietyIf you are sure that you have Big Data then you have to choose what is the right technology you are looking forThere are some myths about Big Data technologies; many people think that Big Data means Hadoop all the time while others think that Hadoop is a database management systemAs a data engineer before choosing the right technologies I have to study what type of data we are looking to handle; real-time or at rest online or offline structured or unstructured Then I need to specify the ecosystem layers and then choose the relevant technologies for each layer after studying other factors such as available resources cost and othersEven if most of Big Data technologies are open source and free of charge but they require expensive resources and a specialized staff while there is lack of a clear technical documentation for these technologies onlineYou should not only think about the technologies prices without taking other factors into considerationAt the end there are other factors that you may take into consideration but you have to know that Big Data technologies are not magic sticksSometimes your company only need to implement some best practices or to perform some optimization and the current system will serve you for many years before needing to adopt new technologiesOn the other hand when you really need Big Data technologies you will not take a lot of time to adopt
fEsTzR3k3tct7XYazUBNvB,Redshift is awesome until it stops being that Usually it isnt so much Redshifts fault when that happens One of the most common problems that people using Redshift face is of bad query performance and high query execution times It might be hard to digest but most of the Redshift problems are seen because people are just used to querying relational databases Although the look & feel of a traditional ACID compliant relational database & a MPP distributed relational database are the same but the underlying principles of storage & distribution completely change the gameTwo major things about Redshift change everything when comparing it to ACID compliant relational databases like MySQL PostgreSQL and so onWell now list down a couple of quick fixes and things to remember when getting started with Redshift workloads If youre not across Redshift going through the following document would be very helpfulUnlike relational databases Redshift doesnt support indexes Dont write the same kind of queries that you write in relational databases like MySQL or PostgreSQL Redshift is designed to perform the best when you select the columns that you absolutely most certainly need to query  the same way youre supposed to SELECT records in a row-based database youre required to select columns in a column-based databaseJoins in relational databases use indexing As we just established that indexes are absent from Redshift then what will you use? You will use a similar sounding construct to join tables That construct is called Distribution Key  a column based on which data is distributed across different nodes of the Redshift cluster How the data is distributed is defined by the distribution style selected for a given tableSimilar to any other database like MySQL PostgreSQL etc Redshifts query planner also uses statistics about tables Based on those statistics the query plan decides to go one way or the other when choosing one of many plans to execute the query Thats why its a good practice to ANALYZE the tables every now and then Theres no good frequency to run this that suits all Beware that ANALYZE can be a time consuming activity Do a quick cost-benefit analysis before deciding on the frequencyTheres a great article which mentions some of the things I talked about here Please refer this for more detailOne of the great things about columnar databases is that you can achieve high levels of compression because a lot of the same kind of data is stored contiguously  which means that many blocks on disk contain data of the same data typeWith row-based databases all the data for a single row is contiguously stored That is why it is better to reduce the number of rows scannedBecause of this compression is hard in row-based databases as a row may contain data of different data types which means that adjacent blocks on disk might have different data typesManaging disk space is usually a problem with all databases especially when youre dealing with analytical loads because analysts data engineers create a lot of tables for further analysis Even if thats not the case reduced free space affects query performance badly because it makes it difficult for the query execution engine to create temporary stuff on disk by inducing a lot of swappingIf you dont have any control over disk space it is better for a more elastic option in Spectrum  here you essentially move all your data to S3 which can scale as much as you want without any downtime for increasing disk spaceThese were some of the basic things that you can keep in mind There are many more ideas which you can find on AWSs documentation for Redshift The documentation isnt always helpful For that time you can refer to some of the links that have been shared through this post and below in the references sectionIf youre looking for an in-depth practical guide to Redshift theres nothing better than this one by Periscope
UdqSJopWM23qGsiTvRWssA,Spark is one of the most popular data analysis and engineering tools available today It has wide reach and adoption which has prompted major cloud providers to provide services based on it Theres Azure Databricks AWS Glue and Google Dataproc  all these services run Spark underneath One of the reasons Spark has gotten popular is because it supported SQL and Python both For the unique RDD feature the first Spark offering was followed by the DataFrames API and the SparkSQL API Since then it has ruled the marketVery many people when they try Spark for the first time talk about Spark being very slow It is obviously very slow because they dont understand the internals of Spark to use it the best way (or even in a good enough way) Im writing this to point out a couple of quick fixes for SparkSQL that you can employ without getting knee deep in SparkSQL internals Having said that the best way to tackle it is getting knee deep in the internals But here well just talk about the low hanging fruits of the non-performant SparkSQL problemsThis continues from one of my earlier pieces where I talk about quick and easy fixes for SQL queries for any relational database At some level many of the same concepts apply to SparkSQL too like  filter data early filter it correctly distribute data efficiently reduce skew and so on Lets go ahead and talk about these common problemsJust like databases the data storage for Spark will also have files that are small and files that are huge Just like in traditional distributed data warehousing systems we push the dimension tables to all the nodes of a cluster to avoid shuffles we do the same here by using the broadcast feature of SparkSQL Broadcast does exactly the same  it prematurely copies all the data of a small table to the worker nodes so as to reduce the shuffle between executorsWe can specify hints to instruct Spark what to do while performing an operation for one relation (table/file) to another relation Although its not universal but its commonly observed that more stages in a Spark job result in more shuffle across executors Lets talk about thatI keep coming back to this great analogy by Jim Gray where he talks about the sheer astronomical difference between the latencies in fetching from disk versus fetching from memory Fetching from disk if possible to avoid is almost criminal in this contextAs I mentioned in the introduction distributed systems like Redshift Snowflake Spark Hadoop etc face this common problem of sharing data over the network the job is running This shared data is fetched from disk Disk I/O as demonstrated by Jim Gray is extremely costly Obviously its not just the fetching of data  theres overhead work (like serde) for this data which makes it worseReducing the number of stages for a given Spark job will definitely help in alleviating some of the performance pain youre facing Use the rich Spark UI to identify how much data is being shuffled for every stage of the Spark jobOne of the positive side effects of reading through the Spark UI about shuffles is that youd get to know issues other than number of stages like the skewness of your dataFiltering pushdown predicates partition pruning  all are implementation of the same construct essentially The query execution engine wants you to try to discard as much data as possible to as to lower the cost of the query In traditional databases this is done on one level with indexing and then with partitioningPartitioning is the grouping of data to make it easy for the query execution engine to lookup data required for a query Pruning is the technique that enables rejecting whole partitions which arent required for a query You can use query hints to specify partitions in a SELECT statement tooAn alternative to partitioning is Bucketing Conceptually it tries to achieve the same performance benefit but rather than creating a directory per partition bucketing distributes data across a group of predefined buckets using a hash on the bucket valueThe first thing is to understand that like all transformations in Spark cache() operation is also lazily evaluated It doesnt invoke data movement to driver like actions do cache() should be used only when youre sure that the cached RDD will be used for further transformation down the lineDo not cache just because an RDD is small Do not cache just because its a dimension table (rather broadcast it) Caching has to be seen from the perspective of the data lineage the DAG of transformationsUnderstand the difference between caching & broadcasting  the former sends/broadcasts a copy of the cached data to an executors memory (and/or storage)Keep in mind that too much caching can create an overhead for the LRU algorithm as it will keep evicting things being cached and bringing new ones in to replace them And the cycle will continueYou can go ahead and look at the amount (percentage) of data cached in the Spark UI Observing the UI will definitely make your life earlierThese were some of the quickest and easiest ways to identify and fix performance issues with your SparkSQL
GbiQurwkLbcvYhXSm8Zcb6,Developers require test data during development and testing of an application Test data is essential to application development as its the only way developers can move one step ahead of mocking API responses Generating test data involves two different approaches Both should be used for making the application betterBad quality data will often consist of unsupported characters longer strings high accuracy floating point numbers and so on It is always helpful to find out what happens when bad data passes through the systemGenerating data in SQL is pretty easyRecursive CTEs are great They can be used to generate series data Similar stuff can be done in PostgreSQL using generate_series in Oracle by using connect by level and in MySQL by using session variables @id := @id + 1 Almost all databases have workarounds to generate numerical sequences series Some of these workarounds are more elegant than othersThe rand() function gives you a number between 0 and 1 If we wanted a number between 0 and 100 we would just multiply the output of the rand() function by 100 In the example shared below I we are generating a number between 12 and 100Use another CTE to use a list of city as a table An order by rand() on this CTE would give a random city out of the list of cities See that the result of the two subsequent queries is differentSimilarly two more CTEs one for common first names another for common last names Any combination of these two fields picked at random will generate a new name Obviously names can be repeated because of the order by rand() clause and the limited number of names supplied to the CTEIf you dont care about how real the names look you can actually generate random strings using substring(md5(rand())16) This would give you a random string with six charactersOnce youve wrapped your mind around generating random numbers in a given range generating a random date is easy The following example shows you how to generate a random date between 1st January 1919 to 28th December 2019 The only caveat is that this covers days between 1 to 28 for any given month We can definitely write a query to cover all dates by adding a few more conditions to thisFinally we come to a sample data set generated solely from a SQL query Gettings hundreds of thousands of records with auto-generated real looking data would only cost a couple of seconds of execution timeThe next step obviously is to take parameterize the query and make it a stored function or a stored procedure You could pass the number of records to be generated range for date of birth and so onYoull probably see an error as shown in the screenshot below Notice that I have tried to fetch 100000 records  defined in the recursive CTE in the beginning  id < 100000The configuration variable cte_max_recursion_depth needs to be set to 100000 or more to fetch those recordsAfter this you should be good to go
Mj7HdpjEtbJZacypQ8uKgs,As data science data engineering and DataOps teams are increasing in size by the day it is very important to improve the way they collaborate It is common to see data engineers and data analysts storing SQL queries Python scripts and Jupyter notebooks on their local machines It is highly recommended that none of that happens Although the work that these guys do differs a lot from the work of application developers theres no doubt that they share a lot of the common tools for writing code Also the workflows in most cases are also very similarThe most important tool for a team collaborating for writing code is a version control system like Git Mercurial or Subversion VCSs have been around for a number of years now but data teams have still to adopt to them fully into their workflow Here well talk about the tremendous benefits of using a VCS for data work and the best practices for working with a VSC for data work Lets goOrganization your code queries artefacts S3 buckets etc is very important Irrespective of your infrastructure being on the cloud or not you can devise methodologies to organise your code It starts with getting the folder structure right The folder structure for your application your database scripts and DevOps scripts  all depend on the application you are building and the tools you are usingVersion control systems provide a powerful way to ensure that everyone is on board with the folder structure and how the code is organizedAnd another one is about strategies for code organization The strategies dont apply the same way to database scripts SQL queries reports and stored procedures but the idea does The idea that code should be written and organized in a way that that it is readable searchable understandable and editableAlways push all the relevant work to remote branches even if the work is in progress I highly recommend using gitflow  the git development methodology using three levels of branching (with some exceptions)  master develop and feature The feature branch here would be any CR or fresh requirementStoring on local branches only is not really recommended unless it is something that is useless to others and is for your own experimentation and discovery Local branches not surprisingly are prone to data loss as your queries wont reside elsewhere in the universeHeres an example talking about using Git for SQL Server scripts Theres GitSQL  a dedicated source control solution for PostgreSQL databasesYou should have a mandatory reviewer for every merge request This makes sure that nothing suspicious or incorrect is going to production Manual checks and balances are really important All of the major version control systems have the option to mandate reviews by one or more peopleThere is a thorough post by Atlassians engineering team regarding this Do check it outImportant side-note  If your team is new to the review process they might be a bit cranky to implement it as it would seem like useless additional work but it hardly is that The extra effort on review pays off in the long runI cant stress enough how important it is to have an up to date staging environment For database testing purposes it is not necessary to have all the production data here at all times It really depends upon your testing requirements Whether the data is updated or not the database structure (tables views procedures functions and other objects) should be always in-syncIaC tools like Terraform and Pulumi have made it really easy to spawn and destroy complete environments in a matter of minutes Bringing up a staging environment if youre using all these tools shouldnt be hard and definitely shouldnt take much time Usually staging is understood more in the context of data warehousing  the layer where a lot of the preprocessing happens before the data is written to the main production data warehouse But here I mean staging for one of the usual development environments the others being testing development and production Staging is also commonly known as pre-production but the terminology isnt very strictAfter having a VCS having a CI/CD pipeline the next evolutionary step as it removes yet another step where a human error can result in the incorrect report being run Essentially such a pipeline would make sure that your latest code is continuously integrated and deployed to your environments automatically Tools for popular services like Redshift Tableau Informatica and most relational databases exist in the market to solve this However none of them are completely mature so beware of that factHaving a CI/CD pipeline also reduces a lot of time setting up environments and configurations for testing and development It is not necessary for you to use one of the tools available in the market as none of them are as evolved for data engineers as they are for developers You can create your own CI/CD tool for your use-case with a set of scripts and Lambda functions invoked by an event created by listening to a VCSLast but not the least creating a query library for your organization is one of the most important things for maintaining sanity This prevents rewriting of the same queries again by different teams I have done it for several teams but never across an organization Using a VCS for the query library makes sense but non-technical teams might find it difficult to operate using a VCSThere are open source tools like Metabase which can help you organize your queries in collections If not that you can use your BI tool to be the storage for the query library Although not the best option but it works if the teams are motivated to do thisAs SQL is a very widely used language across many development domains like application development data engineering data science and data analytics it is easy to find different patterns of usage storage and maintenance Having said that we shouldnt forgive that whatever pattern we follow it should make it easy for other people to understand explore and work on the code that has already been written Thats the only ask
jpSEvPrmoTusK8XQCTPQBF,In this guide Ill try to cover several methodologies explain their differences and when and why (in my point of view) one is better than the other and maybe introduce some tools you can use when modeling DWH (Data Warehouse) or EDW (Enterprise Data Warehouse)I started my BI (Business Intelligence) career in a small company that consulted with other companies on how to improve their processes or just helped them build a BI system so that they can make the decisions themselvesHow I imagined working during that time (almost straight out of the university): I would come to work get equipment someone would explain what I have to do I would start doing it I was so surprised when I came to work on the first day: Ive got the book The Data Warehouse Toolkit by Ralph Kimball and my manager told me to read and learn So I started reading and trying to understand it It took me some time I dont remember how long it took but maybe a week or so to get a proper understanding (at least in my view)Transformation area  transformed staging dataFact table  transactional or event-based data with some measures ie sale information warehouse products movementDimension table  all information for a particular thing in one place ie all product-related information customer informationI am starting with a technique that I learned first mostly because its easy to comprehendIt was created by Ralph Kimball and his colleagues (hence the name) This approach is considered to be a bottom-up design approach For a more general audience maybe its more familiar by dimensional modeling name Personally what I liked about this modeling  easy to design and analyze It was designed to answer specific questions or help understand particular areas (ie HR Sales) This approach allows for fast development but we lose some flexibility Usually we need to rebuild the DWH (or some of its parts) to apply the changes Most of BI reporting tools can understand this model and you can quickly drag and drop reports (ieKimball flow: Ill cover Star and Snowflake schemas in more detail in sections belowIn Star Schema we have a fact table and dimension tables (with all of the foreign keys in the fact table) You can find more details in this Wikipedia article In a nutshell it looks like this: Pros: Cons: Snowflake is a Star schema with more layers Ie we have an address in our shop dimension We can create an address dimension with address_PK which would point to the dim_shop You can read upon snowflake schema in this Wikipedia article Simplified view of it: Pros: Cons: Im not saying that one modeling is better than the other; it all depends on the use cases available resources and the end goal Weigh all of the options and think if youll be adding more groupings to the dimensions if you want to add more atomic layers later (updating fact table with foreign keys to dimension tables)Practically I havent used it so this will be a more theoretical overviewThis methodology was created by Bill Inmon and is considered to be a top-down approach We have to have a whole picture and model it accordingly to 3NF (Normal form) making this approach more complicated than Kimballs The downside is that you need skillful people to design this data model and to integrate all of the subject areas into it Compared to Kimballs it takes more time to get it running but its way easier to maintain and its more of an enterprise-grade approachInmon Flow: To compare these to it all comes down to the company/business area data we want to model From my personal experience the first project I had was from a retail client We did a model on Kimball with a Star schema because we knew requirements and their data questions If it would want some more integrations to their DWH like join the employees who were working those days inventory management it would be more appropriate with the Inmon approachIn my opinion if a company is small and they want to track and improve only particular elements  usually its easier and faster to go with Kimballs approachMoving from small company to a big corporation had its toll on me as well I got a better understanding that sometimes we need a better and more simplified EDW Thats when I started to work with Data Vault As far as I see it  its a combination of Kimballs Star Schema and Inmons methodology Best of both worlds If you want a more detailed view check out Dan Linsteds site hes the creator of this approachIm going to cover several most essential components of Data VaultHub is a collection of all of the distinct entities ie for account hub wed have an account account_ID load_date src_name So we can track from where the record originally came from when it was loaded and if we need a surrogate key generated from the business keyNot to sound funny but the link is a link between different hubs Ie we have employees and they belong to teams Teams and employees have different hubs so we can have team_employee_link which would have team_employee_link team_id employee_id load_date src_nameSlowly changing dimension of particular entity attributes Ie we have a product as an entity We have multiple product information columns name price So we load this information as a slowly changing dimension with information product_id open_date close_date is_deleted product_name product_price Capturing all the changes lets us re-create snapshots of data and see how it evolvedAs well as these most basic entities we have Satellite Links Transactional Links I wont go into details for these; if you want to know more about it  check Dan Linstedts site or WikipediaComing from a simple data background I came across a Data lake term Which stores all the information (structured and non-structured) we have Nowadays terms like Big data Data lake are getting tons of attention I didnt understand why we needed to store that much data of different types until I started working with vast amounts of it Data is todays and futures gold In my experience for all data-driven companies data lakes are almost a must Store as much as you can then do analysis look for insightsStoring data from multiple sources in raw formats comes with its own costIf you wont keep tabs and manage your data lake properly  it might become a data swampFrom my point of view its an additional layer before creating your EDW You have data engineers working on bringing raw data to the data lake and building an EDW on top of it Analysts can work and rely on the pre-processed and cleansed dataDatabricks company introduced this term at the end of January 2020 The idea here that we do everything straight on the source data Most of the ML/AI tools are more designed for unstructured data (texts images videos sound) Processing them to DWH or Data Lake will take some time and for sure will not be near real-time Im a bit skeptical about this approach Its like creating a vast data swamp and letting people drown in it Too much data that is not managed cleansed might lead to false assumptions and will not be one source of truth in a big company At least now while streaming is not that big of a thing I think its worth waiting for this methodology to mature more Unless youre some tech startup that wants to be way ahead of your competition and use bleeding-edge technology but its just my assumptionsIn my opinion all of these methodologies will co-exist for a long timeIf its a large enterprise  the data lake probably is a must if you want to stay competitive and provide excellent service to your customers In my opinion you still will have to create a pre-processed layer which would be some sort of one source of truth for reports with more aggregated/cleansed data The best fit here (in my opinion) is Star or Snowflake schemas It will enable us to look for general patterns and trends more quickly In case we need to do a deep dive and DWH/Datamart is too aggregated  we could always go to the data lake and check raw dataOr maybe your employees are tech-savvy and youre a next-gen tech startup and you want to beat your competition by giving insights while using bleeding-edge technologies  perhaps you need a lakehouse? Unfortunately I havent seen real use cases cant think of how it can work with the old school methods for cleaner and smoother analysis
PYYwABJdVFixpMkZrtyN2f,As an expat in Malaysia I needed to use the available transportation facilities to move around the country The public transportation facilities in Malaysia are very good indeed as long as you stay nearby them By default it will be a place that is more expensive than in further areasMy problem was that the route from my workplace and the affordable housing area was a little bit far from public transportation So I needed to use a taxi or walk for 30 minutes Walking in Malaysia is good as long as it is not raining (It is always raining in Kuala Lumpur)As a tech guy I directly decided to use the available technology to solve my problem and there is nothing more helpful in this situation than the ridesharing servicesI arrived in Malaysia 2016 and at that time there were only two service providers for the ridesharing services Uber and GrabFor me both were new… and a few factors in my consideration: So my criteria in the selection process were simple and all based on my situation as a new foreigner working for a small IT company that did not provide flexible working hours Thats right I went to the office during the morning rush hours and left in the evening rush hoursThere are a few reasons that led to my decision to prefer Grab over Uber during my stay in Malaysia That does not mean that Uber was bad or anything but my lifestyle and my persona as a customer were more related to Grab than UberNow after four years of living in Malaysia with an average daily use of Grab Ride service I have enough data to understand my spending on transportationUnfortunately Grab doesnt provide a report about your trips and expenses together in a direct way it just sends you a receipt for your recent trip but they dont send a weekly or monthly summary of your usage and spending… So if you want to know how much in total you spent on Grab Rides services there is no direct way to have itThere used to be one way to know your trips details in the last 179 days (the last 6 months) from https://hubgrabcom/ which allows you to check and download your latest trips… So if you were aware of that website you could get a good insight into your expenses… Grab no longer provides this service at the time of writing this article I tried to use the same website in 2020 but it is no longer possible It redirects me to create a business account (which I already have) and shows summary inside the app itselfPart of my job as a Data Engineer is finding a smart way to collect data that is required for the Analysis process Real-life data is scattered everywhere and not easy to be extracted and be used in a direct format Add to that my passion for Solution Architecture (I am a certified AWS Solution Architect  which I studied as a hobby in the beginningThe key point to collect my Grab trips data was my email I connected my email account with my Grab account so I received emails for most of the activities I completed using the Grab appAs long as I receive a receipt in my email I can process all the emails to extract the data of my transportation since the first trip until now The email usually contains: There are two ways to do it batching and streamingStreaming is a more real-time process to have more insights about what you are tracking For Data Engineers having a streaming pipeline is always a higher level than having scheduled patch jobs After I implemented the patching technique I decided to convert it all to a serverless streaming process using AWS servicesLambda (Serverless Function) will parse the email file and it will extract all the dataIt extracts: Then Lambda function stores this data in DynamoDB table (or any RDP if you want)Then the visualization tool or the reporting tool gets this data from the database table and visualizes it in my dashboardCharts always give the big picture of what you want to know Business analysts always deliver charts to deliver aggregated information One picture is worth more than a million wordsHere is a representation of the trip from my workplace to my apartment during the first half of the year in 2017Here is a representation of the same trip from my workplace to my apartment during the first half of the year in 2018You can notice that the pickup point service enhanced and became more accurate compared to 2017 and that was noticeable as we became less struggling in finding the driverHere is my total spending for a whole year in 2017Here is my total spending for a whole year in 2018And my total spending for a whole year in 2019During that time Grab changed their email format multiple times That required me to update my Lambda function code multiple times and redeploy it to handle the old and the new format All Data Engineers can relate; when the source schema changes you expect some mismatching and errorsOf course I started with the batching I downloaded the emails and processed them on my local machineThen I created a Chrome Extension which extracts the data directly from the email The extension then sent an API request to AWS APIGateway which redirected the data to Lambda function Lambda then inserted the data to DynamoDBThat was not dynamic as I needed to open the mail and run the extension to parse the email and confirm sending the data to the endpointI started my streaming process to store the data into a DynamoDB table The decision was based on how simple it is to create a table on DynamoDB and use the boto library to ingest data into it But after a few months of adding data to the table I regretted using it for this application especially when I was retrieving the data I know the schema I know what I wanted to store and I had full control over it that all means a rational database should be better I think Aurora Serverless should be a better choiceThe data extraction process is not an easy process The data around us usually not designed to be extracted but to be presented When the Data Engineer starts his\\her job there are questions that required answers: Answering these questions will lead to the next level of questions which are focused on the Architecture of his\\her solution: There are always more questions that will appear in your way which will make your journey more fun Enjoy the Data journey
SgbX5cMrVFhFZBc9mzqHxo,Over the past few years many companies have embraced data platforms as an effective way to aggregate handle and utilize data at scale Despite the data platforms rising popularity however little literature exists on what it actually takes to successfully build oneBarr Moses CEO & co-founder of Monte Carlo and Atul Gupte former Product Manager for Ubers Data Platform Team share advice for designing a data platform that will maximize the value and impact of data on your organizationYour company likes data A lot Your boss requested additional headcount this year to beef up your data engineering team (Presto and Kafka and Hadoop oh my!) Your VP of Data is constantly lurking in your companys Eng-Team Slack channel to see how people feel about migrating to Snowflake Your CEO even wants to become data-driven whatever that means To say that data is a priority for your company would be an understatementTo satisfy your companys insatiable appetite for data you may even be building a complex multi-layered data ecosystem: in other words a data platformAt its core a data platform is a central repository for all data handling the collection cleansing transformation and application of data to generate business insights For most organizations building a data platform is no longer a nice-to-have but a necessity with many businesses distinguishing themselves from the competition based on their ability to glean actionable insights from their data whether to improve the customer experience increase revenue or even define their brandMuch in the same way that many view data itself as a product data-first companies like Uber LinkedIn and Facebook increasingly view data platforms as products too with dedicated engineering product and operational teams Despite their ubiquity and popularity however data platforms are often spun up with little foresight into who is using them how theyre being used and what engineers and product managers can do to optimize these experiencesWhether youre just getting started or are in the process of scaling one we share five best practices for avoiding these common pitfalls and building the data platform of your dreams: For several decades data platforms were viewed as a means to an end versus the end as in the core product youre building In fact although data platforms powered many services fueling rich insights to the applications that power our lives they werent given the respect and attention they truly deserve until very recentlyTo answer this question you have to put on your data platform product manager hat Unlike specific product managers a data platform product manager must understand the big picture versus area-specific goals since data feeds into the needs of every other functional team from marketing and recruiting to business development and salesFor instance if your businesss goal is to increase revenue (go big or go home!) how does data help you achieve these goals? For the sake of this experiment consider the following questions: By answering these questions youll have a better understanding of how to prioritize your product roadmap as well as who you need to build for (often the engineers) versus design for (the day-to-day platform users including analysts) Moreover this holistic approach to KPI development and execution strategy sets your platform up for a more scalable impact across teamsIt goes without saying that receiving both buy-in upfront and iterative feedback throughout the product development process are necessary components of the data platform journey What isnt as widely understood is whose voice you should care aboutYes you need the ultimate sign-off from your CTO or VP of Data on the finished product but their decisions are often informed by their trusted advisors: staff engineers technical program managers and other day-to-day data practitionersWhile developing a new data cataloging system for her company one product manager we spoke with at a leading transportation company spent 3 months trying to sell her VP of Engineering on her teams idea only to be shut down in a single email by his chief-of-staffConsider different tactics based on the DNA of your company We suggest following these three concurrent steps: At the end of the day its important that this experience nurtures a community of data enthusiasts that build share and learn together Since your platform has the potential to serve the entire company everyone should feel invested in its success even if that means making some compromises along the wayUnlike other types of products data platforms are not successful simply because they benefit first-to-market Since data platforms are almost exclusively internal tools weve found that the best data platforms are built with sustainability in mind versus feature-specific winsRemember: your customer is your company and your companys success is your success This is not to say that your roadmap wont change several times over (it will) but when you do make changes do it with growth and maturation in mindOur suggestion: choose solutions that make sense in the context of your organization and align your plan with these expectations and deadlines Sometimes quick wins as part of a larger product development strategy can help with achieving internal buy-in  as long as its not shortsighted Rome wasnt built in a day and neither was your data platformIt doesnt matter how great your data platform is if you cant trust your data but data quality means different things to different stakeholders Consequently your data platform wont be successful if you and your stakeholders arent aligned on this definitionTo address this its important to set baseline expectations for your data reliability in other words your organizations ability to deliver high data availability and health throughout the entire data life cycle Setting clear Service Level Objectives (SLOs) and Service Level Indicators (SLIs) for software application reliability is a no-brainer Data teams should do the same for their data pipelinesThis isnt to say that different stakeholders will have the same vision for what good data looks like; in fact they probably wont and thats OK Instead of fitting square pegs into round holes its important to create a baseline metric of data reliability and as with building a new platform feature gain sign-off on the lowest common denominatorWe suggest choosing a novel measurement (like this one for data downtime) that will help data practitioners across the company align on baseline quality metricsOne of the first decisions you have to make is whether or not to build the platform from scratch or purchase the technology (or several supporting technologies) from a vendorWhile companies like  you guessed it  Uber LinkedIn and Facebook have opted to build their own data platforms often on top of open source solutions it doesnt always make sense for your needs While there isnt a magic formula that will tell you whether to build vs buy weve found that there is value in buying until youre convinced that: One VP of Data Engineering at a healthcare startup we spoke with noted that if he was in his 20s he would have wanted to build But now in his late 30s he would almost exclusively buyI get the enthusiasm he says But Ill be darned if I have the time energy and resources to build a data platform from scratch Im older and wiser now  I know better than to NOT trust the expertsWhen it comes to where you could be spending your time  and more importantly money  it often makes more sense to buy a tried and true solution with a dedicated team to help you solve any issues that ariseBuilding your data platform as a product will help you ensure greater consensus around data priorities standardize on data quality and other key KPIs foster greater collaboration and as a result bring unprecedented value to your companyIn addition to serving as a vehicle for effective data management reliability and democratization the benefits of building a data platform as a product include: Building a data platform might seem overwhelming at first blush but with the right approach your solution has the potential to become a force multiplier for your entire organizationWant to learn more about building a reliable data platform? Reach out to Barr Moses and the Monte Carlo TeamThis article was co-written by Barr Moses and Atul Gupte
YKGmdNSRdWapEVrKbW5Mbf,Came across a Linkedin post that mentioned a dataset that had the city Philadelphia spelled in 57(!) different ways and asked data scientists on their views to approach this data quality issueI came across two other posts which again was talking about this issue comments ranging from there should be better validation in the application that collects this data to people should learn how to spell Well sometimes in your job as a data scientist/data engineer you do come across such data quality issues and you just need to deal with it Maybe you can implement better controls so that you get better data in the future but the data that is already collected has to be cleaned if you want to use it In this post I will show how to fix this issue with OpenRefine a free data wrangling toolOpenRefine(formerly known as Google Refine) is a powerful tool that can transform data easily without any coding And if you are a programmer you can use GREL(Google Refine Expression Language similar to Excel formulas) to do some complex data transformations The interesting aspect about this tool is that it is free open-source and can be installed locally on your laptop and all the data processing is done locally The data never leaves your computer I have used OpenRefine in data migration projects where the legacy applications had free text fields containing an address names etc that needs to be standardized and transformed before it can be migrated to a target system and it worked very wellWhen we have text data that is messy like in this example (top\xa0entry\xa0is\xa0the\xa0correct\xa0one) the same word spelled incorrectly a mix of lower and upper case characters some having leading or trailing spaces white space characters extended western characters first name and last name order mixed up and dates are written in different ways\u200a\u200awe could use Open Refine to clean it upCreate a projectThe tool opens up with the option to create a Project We can import data from different file formats(JSON CSV fixed-width etc) and sources(locally from our computer as well as directly from the web)OpenRefine reads in the file contents and tries to automatically identify the different columns and headers and shows a preview We can modify the settings if needed Once we are happy with the preview we can click on Create ProjectOpenRefine then loads the data and shows a view similar to a spreadsheet We can click on the down arrows at the top of the column to see the various data transformations that we can do on the dataLet us take a look at the column city We are interested to see the entries where Philadelphia is spelled incorrectly To easily view the distribution we could filter the rows that start with ph(assuming that at least the first two characters are spelled correctly :-)) We can use a regular expression ^ph The character ^ ensures that only the cities that start with ph are shown This is not strictly necessary but this helps to limit the variations in entries that we can easily see in the facet windowWe then add a text facet which shows the number of rows per distinct values in the column city As you can see there are quite a number of variations in the way Philadelphia is spelledEdit the text inlineIf there is a single entry that needs correction we can go ahead and directly click on edit next to the entry and replace the value This would be fine if we are replacing a few valuesCluster and editBut if we have too many variations that need to be replaced with a single value we have an option to cluster the entries based on some in-built methods and then edit them at one-goClicking on the Cluster button opens a window with the entries clustered together based on a default method key collision and function fingerprint We can change the keying function and see the entries clustered differentlyOnce we are happy with the cluster we could go ahead and provide a new value for these clusters If key collision doesnt give the desired results we could try out another method nearest neighbor We can also recluster and edit iteratively We dont need to know all the inner workings of the functions to use it though the documentation is an interesting read if you are curiousFeel free to play around with the different methods and choose the one that works best for the given data For this dataset Levenstheins distance with a radius 8 gives the best resultsAfter a couple of re-clustering we have corrected all the incorrect entries of PhiladephiaAll the manual cleaning actions that we have done so far are recorded We can see them listed under Undo/Redo as a sequence of steps We can go back to any step and see the results exactly at the end of that step If we are not happy with a particular step we can just do another cleaning action and that overwrites the stepOnce we are happy with our results we could export the cleaned dataSure you can!OpenRefine has some client libraries that we could use I will write a post on how to use Python and OpenRefine to set up a data cleaning pipelineWe covered only a small feature of OpenRefine It has a lot more to offer The University of Illinois and Duke have good tutorials to get you started
YtfSX84Lebe3PUhMrJQNcG,Azure Storage always stores multiple copies of your data When Geo-redundant Storage (GRS) is used it is also replicated to the paired region This way GRS prevents that data is lost in case of disaster However GRS cannot prevent data loss when application errors corrupt data Corrupted data is then just replicated to other zones/regions In that case a backup is needed to restore your data Two backup strategies are as follows: In this blog it is discussed how snapshots and incremental backups can be created from a storage account see also overview depicted belowTo support the creation of automatic snapshots and incremental backup of your storage account three types of scripts are used and discussed in the remaining of this blog: Notice that blob snapshots are only supported in regular storage accounts and are not yet supported in ADLSgen2 (but is expected to become available in ADLSgen2 too) Scripts are therefore based on regular storage accounts detailed explanation of the scripts can be found below Also notice that scripts deal with the creation of snapshots/backups not with restoring itIn a data lake data is typically ingested using Azure Data Factory by a Producer To create event based triggered snapshots/incremental backups the following shall be deployed: Now every time when new data is ingested using ADFv2 an Azure Function is called that creates a snapshot and sends an incremental backup request for new/changed blobs see also belowThe internal working of script HttpSnapshotIncBackupContainerProducer can be explained as follows: The core of the script is as follows: Notice that only the Producer ADFv2 Managed Identity and this Azure Function Managed Identity have write access to this container Blob triggers do not work in this scenario since no events are fired when blobs are modifiedThe asynchronous incremental backup creation is discussed in the next chapterOnce a new incremental backup request is added to the storage queue this message shall be processed such that incremental backup is created In this the following shall be deployed: Now every time a new incremental backup request message is received on the storage queue an Azure Function is triggered that calls an ADFv2 pipeline that creates an incremental backup see also belowThe working of script QueueCreateBlobBackupADFv2 can be explained as follows: In step 2b it can also be decided to run blob_lease mode which exclusively locks the file and guarantees the correct version of the file is added to backup storage account Whether or not using blob lease depends on a lot of factors (eg max lease time allowed file size number of jobs immutability)The core of the script is as follows: In the previous two chapters it was discussed how snapshots and incremental backups can be created when a producer adds new data to the data lake using ADFv2 However there can also be a need to trigger snapshots/incremental backups that are time-based This will be discussed in the next chapterIn chapter 2 it is discussed how a Producer can create event-based snapshots/incremental backup requests However there is also a need for a reconciliation script that can create missing snapshots and/or missing backups This can happen when a Producer forgets to add the Azure Function to its ingestion pipeline and/or the script failed to run To create a time based function the following shall be deployed: Now an admin can configure the reconciliation script to be run periodically such that snapshots are created and sends an incremental backup request see also belowThe internal working of script HttpSnapshotIncBackupStorageReconciliation can be explained as follows: The core of the script is as follows: Azure Storage always stores multiple copies of your data and Geo-redundant Storage (GRS) additionally stores copies in a paired region However GRS storage does not protect data being corrupted because of application errors Corrupted data is then just replicated to the other zones and regions Two measurements that can protect against data corruption are as follows: In this blog it is discussed how synchronous snapshots and asynchronous incremental backups can be created using scripts in this github See also extended high level overview depicted below
WjNwYkTEYcNFdzpCsbaA23,Disclaimer: this post assumes basic knowledge of Airflow AWS ECS VPC (security groups etc) and Docker I suggest an architecture that may not be perfect nor the best in your particular case In that case make what you want from this lectureWhere I work we use Apache Airflow extensively We have approximately 15 DAGs that may not seem like a lot but some of them have many steps (tasks) that involve downloading big SQL backups transforming some values using Python and re-uploading them into our warehouseAt first we started using the Sequential Executor (no parallelism just 1 task running at a time) due to the easy setup and lack of many DAGs As time went on DAGs kept increasing and some of them presented the opportunity to make use of parallelism so with some configurations we started using the Local ExecutorWell both cases had only 1 container deployed in AWS ECS doing everything: serving the web UI scheduling the jobs and worker processes executing them This wasnt scalable the only option we had was scaling vertically (you know adding more vCPU more RAM and such) There was not another optionFurthermore if something in the container fails the whole thing fails (no high availability) Also the whole service must be public for the webserver to be accessible through the internet If you want to make certain components private (such as the scheduler and workers) this is NOT possible hereThere isnt any guide talking about how to deploy Airflow in AWS or making use of their extensive offer of servicesThis whole diagram might be complicated at a first glance and maybe even frightening but dont worry What you have to understand from this is probably just the following: Principally AWS ECS and Fargate are the stars in this
4UiLrzi8Hz3AsJEHfJFpMj,Despite all the talks about big data and data science it seems that at many companies we are still in the era of data monkey Hours are wasted copy/pasting cells from tools to Excel and making manual reports Few insights if any are generated Most of my time is spent churning out a bunch of fairly standard reports on a monthly/quarterly basisThis is not business intelligence and it is not creating valuable insights This is wasting time plain and simpleLets go back to the roots of the problems: Lets see how those problems can be handledUsually the solution is to throw the problem to the IT department (or more precisely the definition of the data you want) Some hours/days/weeks later they provide a way to get the data through a query (which gives you a CSV file or an Excel/Google Sheet) Still a bit of manual labor but the data is already in good shapeDays later a new issue arises a new query is the solutionSoon after again a new problem shows up It is close to be solved by the first two queries but not exactly You still need IT to create a new one for this specific problem And maintain itThe downside of this solution is that soon you end up with 10s of queries that provide what should be comparable measures (lets say revenues) but they dont match because they werent designed for the same issue or by the same personAfter some level of complexity you start to see queries that are built above the previous queries that no one want to touch anymore It doesnt feel rightShould you avoid such solution? Probably not Its cheap at the beginning and allows you to move fast But when the burden is too heavy to carry you need to clean this data debt The logical way is to go for solution 2Data Warehousing is an old concept dating from early 1980 at least The idea is to get all the corporate data in one place modeled with care and easy to query Clean and ordered Sounds too good to be true? Well it isThe main issue is that up to now those data warehouse are IT domain and business users have few powers over it Every change takes months and direct access is usually forbidden (for safety reasons) It is sad that so much power stay sleepingIn 2000 the data warehouse was depicted by the number 3 : 3 years 3 million 3 reportsIn the search of perfection the corporate data warehouse goes too far the opposite direction of solution 1 It tries to define business reality as an immutable thing (good luck in this fast-paced environment) It needs corporate wide consensus (where the middle ground satisfies no one) It assumes data governance (probably a synonym of political games or IT despotism) In short its too ambitious to be achievedThis is why after spending millions dollars of budget people still end up using Excel instead of the onerous Business Intelligence tools : export to excel is the third most common button in data and business intelligence app … after OK and CancelTechnology is getting better and better On the hardware side you have more power on your laptop that most data warehouses two decade ago On the software side believe it or not Excel contains a good part of what run data warehouses (and not two decade ago … right now SQL Server Analysis Services) Check out my article to learn about Excel super powersOther tools are available with Tableau Looker … on one side (trying to abstract anything technical) and Jupyter Metabase Mode on the other side (staying technical but still remote from software development)Its now easy to manipulate huge amount of data and to connect to data sources And by easy I mean people outside of the IT department with a slim computer background make it happen it everydayIs it easy? Heck no you need a real data sensibility Some people have it some dont But you no longer need a computer degree to make Business Intelligence All you need is curiosity and business understandingStill two gap remains: The technical complexity is (somewhat) easy to manage but the data complexity is here stay Remember the awkward excel database you maintain with great pain (15 sheets 1000s of formulas 10s of bugs)? Well most databases are like thatThats why you still need a data warehouse to remove this complexityEven the most brightest Excel and Power BI users recognize that adding a database for data shapping is invaluable But you dont want a monolith data warehouse you want agility You want to keep ownership in the business department My views on the subject are best synthetized in my Data Warehouse Manifesto where I introduce the module-based data warehouseYou can rely on SQL the language of databases Here again I can quote Excel experts saying that it is easy and valuable to learn some simple SQL code SQL is widely used for 30 years ago You learn the basics and use contractors for more complex parts That way you will have a common languageWith a rock solid (but always evolving) data warehouse you command and empowering tools on top expect to produce great resultsThose previous solution can be divided by a profound choice you have to make Who handle data complexity for insight generation? You can answer IT in which case solution 1 and 2 are for you You can also answer Business and focus on solution 3 and 4 Business Intelligence is neither IT neither Business Its a blend And someone need to take care of it for your company to thriveMy take is obviously toward the business side Im a techy data guy and I havent work in IT departments for 10 years now (but still enjoy working with them) I have seen firsthand what is possible when the people in pain takes care of their destiny Less technical focus and more business valueWhat is your take on this? Let me knowOriginally published at https://dataintoresultscom on July 16 2019
2AcJegznA7csRepfFGHxds,At Twilio we handle millions of calls happening across the world daily Once the call is over it is logged into a MySQL DB The customer has the ability to query the details of the Calls via an APIOne of the tasks I recently worked on is to build a system allowing the customers to export their historical calls dataAt first glance this seems trivial but if we go deeper and think about how a system would scale for some of our biggest customers who have been with us since the inception this problem could be classified as building a system to scale Our typical customer scale can range from making a few 100 calls a day to millionsSuddenly this problem becomes a big data problem too when a large customer making 1 million calls a day requests the last 5 year worth of data The total calls can be in the range of 1000000 * 5 * 365I had to think about how to optimize reading from the Mysql and efficiently write to S3 such that the files could be available to download • Write a cron job that queries Mysql DB for a particular account and then writes the data to S3 This could work well for fetching smaller sets of records but to make the job work well to store a large number of records I need to build a mechanism to retry at the event of failure parallelizing the reads and writes for efficient download add monitoring to measure the success of the job I would have to write connectors or use libraries to connect with MySql and S3I decided to use Apache Spark for handling this problem since on my team (Voice Insights) we already use it heavily for more real-time data processing and building analytics Apache Spark is a popular framework for building scalable real-time processing applications and is extensively used in the industry to solve big data and Machine learning problems One of the key features of Spark is its ability to produce/consume data from various sources such as Kafka Kinesis S3 Mysql files etcApache Spark is also fault-tolerant and provides a framework for handling failure and retry elegantly It uses a checkpointing mechanism to store the intermediate offsets of the tasks executed such that in the event of a task failure the task could be restarted from the last saved position It works well to configure the job for horizontal scalingAssuming the readers have a basic understanding of Spark (Spark official documentation is a good place to start) I am going to dive into the codeLets look at how to read from MySQL DBThe other way to connect with JDBC could be by proving the config as a MapThere is a subtle difference between using query and dbtable Both options will create a subquery in the FROM clauseThe big take away here is query does not support the partitionColmn while dbTable supports partitioning which allows for better throughput through parallelismLets say if we have to export the CallLog for one of our huge customers we would need to take advantage of a divide and conquer approach and need a better way to parallelize the effortWhat it means is that Spark can execute multiple queries against the same table concurrently but each query runs by setting a different range value for a column(partitioned column) This can be done in Spark by setting a few more parameters Lets look at a few more parameters: numPartitions option defines the maximum number of partitions that can be used for parallelism in a table for reading This also determines the maximum number of concurrent JDBC connectionspartitionColumn must be a numeric date or a timestamp column from the table in question The parameters describe how to partition the table when reading in parallel with multiple workerslowerBound and upperBound are just used to decide the partition stride not for filtering the rows in the table So all rows in the table will be partitioned and returned This option applies only to readingfetchSize The JDBC fetch size which determines how many rows to fetch per round trip This can help performance on JDBC drivers which default to low fetch size (eg Oracle with 10 rows) This option applies only to readingIn the above example partitionColumn can be CallId Lets try to connect all the parametersThe above configuration will result in running the following parallel queries: The above parallelization of queries will help to read the results from the table fasterOnce Spark is able to read the data from Mysql it is trivial to dump the data into S3Conclusion: The above approach gave us the opportunity to use Spark for solving a classical batch job problem We are doing a lot more with Apache Spark and this is a demonstration of one of the many use cases I would love to hear what are you building with the Spark
VpJF6cmnA63MWcvnYzwJqZ,**Disclosure: Some of the links below are affiliate links meaning at no additional cost to you I will earn a commission if you click through and make a purchase* This article covers the most recent exam syllabusSo youve probably googled the title above and now youre here Dont worry though youre not the only one I also did the same thingHaving recently graduated with a degree in physics I spent the summer of 2019 interning at an early-stage startup called Zenu This was the first time I was exposed to the role of a Data Engineer and the Google Cloud PlatformYou can read more about my internship hereI joined Zenu as an aspiring Data Scientist and a firm believer in the companys vision to later find out that they didnt have any data or the infrastructure to store it Since I wanted to work closely with data I took on the responsibility for the design and delivery of Zenus analytical databaseOne technical solution to the business requirement was to use BigQuery as our data warehouse which supports fast querying of nested rows using the familiar standard SQL syntax Furthermore it allows simple integration with other Google Cloud products such as Data Studio for analytical reports and AI platform for machine learningComing from a non-technical background I learnt the data engineering skills required for the job from scratch During this time I came across the Google Cloud Professional Data Engineer Certification and took several online courses to gain a better understanding of the technology and the wider role it has in the future of data analyticsI spent about two months casually watching a couple of videos a day and then an intensive one month of practice papers and nailing the key points I took the exam in early NovemberCoursera was the first online course I took and is taught by Google employees They offer a combination of presentations hands-on labs and demosI found this course to be quite advanced for someone without any prior commercial experience I wasnt aware of current technologies such as those in the Hadoop ecosystem and was overwhelmed with many unfamiliar terminologiesI finished watching all the videos but to be honest I forgot most of it immediately However I highly recommend taking the Preparing for the Google Cloud Professional Data Engineer Exam Course about a week or two before taking the examination This is like the revision lecture at university where they did a quick cover of the key products that can you can expect to see in the examAs any broke student would do I used the 1-week free trial and downloaded all the videos for offline viewingThis was the second online course I took and covers about 70% of the content that came up in the exam This course also offers a combination of presentations hands-on labs and demosThis course gives a high-level overview of each Google Cloud service and covers key concepts as well as Googles best practices for using each oneThe course was structured well starting from the foundational concepts to the different types of databases architecting pipelines machine learning and data visualisationI found this one very easy to follow and was kind on the novice Matthew Ulasiens (the course instructor) expertise on the Google Cloud Platform meant his explanations were very clear and concise In his videos he often highlights key facts and concepts that you can expect to come up in the examThere are a variety of hands-on labs walkthroughs and quizzes that help consolidate your understanding and provide an opportunity to test the material youve learnt in the videos directly on the Google Cloud PlatformOne of the most helpful resources provided by this course was the Data Dossier which is like a digital set of course notes It highlights the key points for each service displays clear comparison tables diagrams and workflows as well as tips to remember for the examI recommend taking this course first if youre new to data engineering as it is slightly easier to follow than the Coursera courseIf I were to recommend just one paid subscription this would be the one I paid for the 3-month student subscription and it was worth it Not only do you get access to the amazing resources on the GCP course but youll also have full access to all the other courses offered by Linux Academy (which I used)Cloud Academy was the third and last online course I used to prepare for the exam I started this course about a week before taking the exam and used it more as a refresher than a learning guideThe most useful thing about this course was that it covered topics such as Security and Networking Data Encryption and Compute Engines which were not covered in the other courses but can be tested according to the official exam syllabusApart from the topics mentioned above I watched all the other videos at x15 the speed to check if there were any new information I was previously unaware of I found the hands-on labs a bit harder and more copy-pasting long lines of pre-written commandsI used the 1-week free trialHere are some of the other resources I found very useful but are optional depending on your experienceThis course is a comprehensive introduction to the world of Big Data It covers the principles of Big Data infrastructures and its integration with cloud computingThe most useful thing about this course for me was that it provided a high-level overview of the most popular Big Data technologies including core Hadoop the Hadoop ecosystem (Hive Pig Kafka etc) and Apache Spark For the exam you will be expected to know what each of these does and their GCP equivalent eg Kafka -> PubSub Hive -> BigQueryThis course was recommended at the start of the Google Cloud Certified Professional Data Engineer course for those unfamiliar with SQL It covers basic SQL statements and functions including aggregations joins and sorts I recommend this course if you havent used SQL before or dont use it very often as you will need it to write query statements in BigQueryThis course is Googles fast-paced practical introduction to machine learning I used this course a quick refresher since I had already covered some of the popular algorithms and concepts in a university course However I think its well structured and will give you a good foundation in machine learningQwiklabs is a platform that provides temporary credits to use on various cloud platforms in the format of tutorials and demos You can use Qwiklabs to get extra hands-on experience with the Google Cloud Platform I took the Baseline: Data ML AI / Data Engineering / BigQuery for Data Warehousing courses I found the last one quite useful as it allows you to practise writing query statements in BigQueryThere are many individuals that passed the Professional Data Engineering exam and shared their experience online about their preparation and exam experience I feel like most of them come from a technical background so they skim over some obvious knowledge (since the recommended experience to take the exam is 3+ years of industry experience including 1+ years designing and managing solutions using GCP)One pattern that seemed common having read online forums comments and hearing about people that have taken the exam and failed was that they underestimated the difficulty of the exam because they do similar things in their day jobI can relate to this overconfidence for the machine learning part of the exam since I was already familiar with the algorithms having covered them in a university module I spent less time on this part of the syllabus and as a result was less confident in some of my answersAs students the one advantage we have is that we are used to studying In my case I just finished my final year of exams and was still in study mode so preparing for this exam wasnt too unfamiliar On top of your busy schedules I understand that it might have been years since you last studied for an exam so I will outline some of the key areas you should focus on and how I prepared for itThe first mistake I made was to not read the official Google exam guide properly and naively assume that the online courses would cover this What the online courses do well is that they cover each GCP service in detail outline its common use cases Googles best practices for using the service and how they fit in the overall data engineering roleWhat the exam is testing you on is your ability to come up with a feasible technical solution to the business requirement Although there will never be one perfect solution a great solution should take the four points mentioned above into considerationIn the exam you can expect to see several options that may meet the business or technical requirement but one is better than the restIf youve watched some of the online courses you may recognise these termsLift and Shift is simply migrating existing processes or infrastructure to the cloud without redesigning anything Some companies might take this approach because they dont want to spend time and money modifying their current infrastructure but still want to use the benefits of the cloud For example hosting a MySQL database on compute engines rather than migrating all the data to Cloud SQLLift and Leverage means to move your existing processes over to the cloud and make them better using some of the services the cloud has to offer For example you can use Dataproc to run your Hadoop and Spark workloads but store the data in Cloud Storage as opposed to storing it in HDFS This is more cost-efficient since you only pay for the time the job is running and then you can shut down the cluster when you arent using it without losing all its data since it is now stored in Cloud StorageAfter doing all the online courses and the practice questions you might think that most of the answers involve choosing the most appropriate Google Cloud service for a particular business or technical requirement After all Google is selling their superior low-cost fully managed no-ops capabilities of its Cloud ServicesHowever this is not always the case and its important to read the question carefully as some questions in the exam have a technical requirement where they dont want to make any modifications to their existing processesThe hardest part of the exam for me was the trade-off between cost and performance (This is probably where your 3+ years of industry experience might come in handy!) The business people think in terms of minimising costs and the technical people think in terms of increasing performance
LxVHJQ3PyQmvg94hzfhDwk,Azure Data Factory (ADFv2) is a popular tool to orchestrate data ingestion from on-premises to cloud In every ADFv2 pipeline security is an important topic Common security aspects are the following: In the remainder of this blog it is discussed how an ADFv2 pipeline can be secured using AAD MI VNETs and firewall rules For more details on security of Azure Functions see my other blog For a solution how to prevent data loss in your Data lake using snapshots and incremental backups see this blogUpdate 2020 07 26: It is now possible to run ADFv2 Azure hosted Integration Runtime in a Managed VNET  This is in public preview and this blog has not yet been updated yet with this featureIn this ADFv2 pipeline data is read from SQLDB transformed using an Azure Function and written to an ADLS gen2 account The architecture of the project can be found belowThe Azure services and its usage in this project are described as follows: The following security aspects are part of this projectIn the next chapter the ADFv2 pipeline architecture is realizedIn this chapter the following steps are executed to create and deploy the ADFv2 pipelineIn this tutorial deployment of resources will be done using code as much as possible The following resources need to be installed: After the preliminaries are installed the basic resources can be installed Open your Visual Studio Code create a new terminal session and execute commands belowIn this part the self-hosted integration runtime in ADFv2 is configured This will be done using VMs running in the VNET that was created earlier Execute the Azure CLI script belowThe scripts can take 15 minutes before it is finished After the script is run you can verify in your ADFv2 instance whether it is succesfully deployed see also belowIn this part the ADLS gen2 account will be securedAfter the last step access to your storage account from the Azure portal is refused (since it is not part of the VNET) To enable access from the portal you you need to whitelist your IP addressIn this part the SQLDB will be secured This is done as follows: Azure PowerShell is used to configure the rulesTo enable access from the computer for test purposes you you need to white your IP addressIn this part the Azure Function will be deployed This is done as follows: First step is to create an Azure Function in python using this quickstart Subsequently update the requirementtxt __init__py and add cdmschemapy from this git repository Then publish your function to the $funname that was created in step 3a To enhance security add a firewall rule using the following Azure CLI script: Finally AAD access to the Azure Function can be configured by added an app registration to the Azure Function and only allow MI of ADFv2 to access Azure Function Refer to this tutorial and this PowerShell script how to do thisIn this part the ADFv2 pipeline will be configured and run Go to your Azure Data Factory Instance select to set up a code repository and import the following GitHub repository rebremer and project adfv2_cdm_metadata see belowOnly the pipeline BlogADFv2sec is relevant for this blog Customize the related linked services of this pipeline for ADLS gen2 Azure Function and SQLDB see also belowSee this link how to be enabled logging in your Azure Function using Applications Insights An example of logging can be found belowIn the final step the pipeline can be run in ADFv2 Go to your pipeline and click debug When everything goes well all green checks will appear in the output see belowIn this blog an architecture is created that deploys a secure ADFv2 pipeline in which data is read from SQLDB transformed using an Azure Function and written to an ADLS gen2 account In this AAD access control Managed Identities VNETs and firewall rules are used to secure the pipeline see also architecture belowNote from Towards Data Sciences editors: While we allow independent authors to publish articles in accordance with our rules and guidelines we do not endorse each authors contribution You should not rely on an authors works without seeking professional advice See our Reader Terms for details
h3NbztGzNY662WLsHCvciP,Early in my career when I had to hire for my team I made the mistake of asking the interviewee things that I had worked on regardless of whether they had worked on or not As an interviewer you are in a position of privilege With that privilege comes a responsibility  of going through the interviewees resume in detail of preparing some basic questions of understanding what kind of work theyre coming from and what kind of work theyre looking forI read somewhere how Elon Musk hires his team Of course hiring a data engineer is not the same as hiring a rocket scientist but theres definitely some overlap in hiring for any field of workWhat Im really looking for is evidence of exceptional ability Did they face really difficult problems and overcome them? And of course you want to make sure if there was some significant accomplishment were they really responsible or was someone else more responsible Usually someone who really had to struggle with a problem they really understand it and they dont forgetAccording to research anxious people usually get a low performance rating on an interview So before you get into any technical stuff be sure to make them feel calm and comfortable Ask if they want water A thirty-second chit chat doesnt hurt Simple things helpFor data engineers and data analysts one can obviously assume that theyll know SQL and maybe a little bit of basic Python (or something similar) If youre interviewing someone in the early stages of their career they might not know the different flavours of SQL so be open to accepting T-SQL PL/SQL SQL-92 use of session variables in MySQL use of CTEs and whatnotMany years ago I interviewed for a campaign management firm in Gurgaon The technical interviews were mostly about SQL They were working on MS SQL Server I was well versed in Oracle 11g They had asked something specific to MS SQL Server but then saw that I was going in the right direction and just kept observing where I was going They asked me to explain what I was trying I did clear that interviewIf they understand how to query if they are able to think on the spot chances are theyll grasp whatever new flavour of SQL you want them to work on Unless youre hiring for someone wholl fix everything theres wrong with your architecture you wont need an expert who has 10 years of experience on a specific ETL toolNever forget to ask something very basic about databases and applications Youll get an idea if theyve just developed a skill with sheer brute force or do they have some understanding of how things work For instance a database engineer should always be able to answer questions about transactions types of databases Similarly a SQL data analyst should always be able to write simple queries like the standard third highest salaried people in every department of a given company and so onIf they get stuck with a query or a piece of code help themWhile asking them to write a query or some piece of code do remember to help them along the way I appeared for an interview for one of the FAANGM companies long ago I didnt get in but I remember the interviewers being so very helpful They were helping me out where I was making a mistake or when I was going the wrong way The whole experience was greatNever forget the fact that as you are interviewing multiple people for the same position theyre also interviewing for other positions Many people want the job you have to offer Many people want the interviewee whos in front of you Get inside the room with a positive approving mindset Look for the positives rather than the negatives firstOnce thats taken care of try to sell the job to them even before you ask your first question Its important that this message gets across  that you are taking this interview as seriously as they are Finding good data engineers and data analysts is hard anyway Dont make it harder on yourself by being an ignorant interviewerWhat I have shared is not just from taking loads of interviews but also appearing for them Sitting on both sides of the table gives you perspective The questions I talked about were mostly about assessing the technical skillset of an interviewee For behavioural questions follow a more generic guide on conducting interviewsI hope some of this is useful
9FjqADHZVwPTEdtbzq9WPX,Consider a scenario where data objects are continuously being ingested at raw-data bucket This data is periodically processed and stored at processed-data bucket Our interest is to find a way how to track which new objects are unprocessed and process them This in order to escape processing the same objects many times Consider the below setup of AWS services as a response to this scenarioWith a cloud I have abstracted all different AWS services (EMR Lambda EC2 etc) that you can use to process data Squares represent data objects and their color represents their state as described in the figure belowIntroduce an SQS queue to the setup Ill call this queue raw-data-object-creation-event-queue A message event will be sent to this queue whenever a new object is created in the raw-data bucketTo accomplish this set up an event listener at raw-data bucket and listen for All object create events happening In case an object is created (ie uploaded at this bucket) send a notification event at the SQS queue createdAt your processing scripts poll events from the SQS service and parse json messages to take the object keys of the unprocessed data objects Read those objects from the raw-data bucket and process them At the end of processing execution if processing is successful delete the events you polled from the queueGoing with SQS is definitely the easiest solution to implement and it is also quite cheap You can make 1 million free SQS requests per SQS queue for per month This means that you can make ~22 requests per minute and pay nothingKeeping track of unprocessed objects is necessary to escape processing the same object many times The solution proposed with SQS is easy to implement monitor and cheap at the same timeOther solutions might be considered depending on the use case as: keeping track of the timestamp of last file processed; having raw data objects first stored in a temp-raw-data bucket and moving them to archived-raw-data when they get processed successfully; use bucket versioning and delete raw files whenever they are processed
dTkW3CQ7MPePPfkudhrLtY,Data Clinics the time our team puts aside daily for working with stakeholders on any walk-in requests offer the best ROI for the data teams time This is because on average these are the lowest complexity but highest leverage tasks we work on in a given weekBefore we had Data Clinics our work was all over the place: one day would be entirely focussed on ad-hoc and the next entirely on longer-term projectsWe needed a way to prioritize both sides: ad-hoc and plannedOver the past two and a half years  as weve scaled from 1 to 4 Data team members 50 to 150 non-medical team members and 5K to 2M covered Canadians  Data Clinics are how weve handled the growing demand for ad-hoc workA Data Clinic  or Office Hours for all the non-medical companies out there  is a time of day when you set aside the weeks plan and welcome walk-ins with any and all types of data requestsData Clinics are not a new idea the idea has been employed by many companies  like Ro!* At Dialogue weve spent some time perfecting the way we run these clinics  this article is not the definitive guide to Data Clinics but rather a collection of what weve learned along the wayBack when Dialogue started running Data Clinics the Data Team was only one person and Matt ran an as-needed clinic for 30 minutes everyday Now that the data team is four people each team member runs 2 one-hour Data Clinics per week  for a total of 8 hours of data clinic  spreading them out so each day has at least oneSome examples of common data clinic requests include: The goal isnt always to fully answer these questions but more often than not the goal is to get the ball rollingAt Dialogue were firm believers in applying Scrum software development principles to what we do and by extension we believe the 80/20 rule is all around us: the first bit of work the first 20% will be the highest value on a project accounting for 80% of the benefit While not everything of course is so simple  some projects only bring benefit once fully complete for example  many projects we work on follow this principle In all of the above example questions there is a lot of benefit to be had just from taking 15 minutes to analyze train or brainstormWhile most of the data teams time is dedicated to large projects  with greater uncertainty and complexity and greater strategic value  this is a refreshing opportunity to focus on the small things that can help move the needle todayNow with the introduction out of the way these are the more concrete reasons why we run Data Clinics and why we think you should tooAs mentioned above Data Clinics are great for getting big returns on small amounts of time Even if the task cant be fully completed or even if its just time for planning the task youve already gone from zero to a plan This is the top reason we run our Data ClinicsInterruptions are terrible for productivity and Data Clinics are a convenient way for us to timebox those interruptions Rather than handling a pile of direct messages we route them all to a single channel on Slack for handling asynchronously as data clinic requestsFor me I know Im most productive on analytical tasks in the morning so the Data Clinic is a great way of preserving that time and giving me a more interpersonal task to break up the remainder of the day in the afternoonTask backlogs can easily feel out of touch when theyre filled with large projects Data Clinics in contrast let you know what the data needs are todayIn some cases data requests even help us understand how we should change the long term planA less obvious benefit of the Data Clinic is that it has pushed our Data team to make our work more easily maintained by non-authors In Data Clinics requesters can ask a question about any part of the business so depending on the day you may be answering questions about subjects you have worked on or subjects you have barely touchedNow more than ever we keep this in mind when reviewing codeBy extension this is also a great way to onboard new team members Pairing on a Data Clinic is a great way to go broad but not too deep into the data pipeline helping explain the general approach without getting too caught up in the detailsSometimes our priorities for a given sprint or even quarter skew to one or a handful of teams If were onboarding a lot of new clients for example we will need more support on Medical Operations and Customer Success Even in these times a smaller department like Talent can still make requests during a Data ClinicNow that weve been running these Data Clinics for over two years we have a few pointers about what has worked best for us and whyData Clinics should happen often Requests can be timely so its important to not load up all the Data Clinic time into one day Also its a good tool for breaking up larger tasks so they can be used to help manage personal productivity this wayThe calendar and if possible the space should be public and easily accessible and known to all We have a Google Calendar for our Data Clinics that you can find that will let you know whos doing Data Clinic when and where We also try to keep them at roughly the same time (often this is at the end of day) to help minimize miscommunications and help set expectations around how long a question will take to be answeredOur data clinics work best when every team member does their fair share This helps with cross-maintainability of our work with keeping various parts of the pipeline familiar but also with keeping expectations around ad-hoc time and interruptionsUnfortunately some team members will always be pulled into ad-hoc projects more frequently but structuring Data Clinics these ways can at least help minimize the total effect of those distractions Ideally for planning and for peace of mind interruptions and distractions should be shared across the teamWeve tried doing only in-person and it just doesnt work If you want to be able to handle all requests your Data Clinics need to be flexible enough to handle synchronous  that is to say in-person  and asynchronous requestsFor this we maintain a Slack channel where anyone can leave us a question and well respond during our Data Clinics If we can we will sometimes schedule a time slot during the data clinic to run through the problem live but sometimes thats not possible and well handle the request in a thread This helps ensure that everyone regardless of their schedule can get the help they needBy using a public space this also helps with visibility for other team members You can tag someone who might also be interested or you can tag an older thread that might be relevant for a new questionAnother tip is that to ensure something has been fully resolved we leave a ✅ once the request is closed When scanning the channel this helps you to identify which questions are still open Sometimes we will also leave a 👀 on a request to let our other data team members know that while not yet resolved someone is looking into itThis is the toughest one We do our best to respect the time limit of 15 minutes per request but sometimes we will go as far as 30 minutes We do this to be able to serve as many people as possible and in the same vein to be fair to our other requestersIf we spend too much time on one request it takes time from other people In these situations the focus should be on making a plan not on making the deliverable itselfData Clinics are going really well at Dialogue but theres always room for improvement! A couple improvements that were thinking about are how to: We want to automatically track who is making requests what type of requests and how long it takes us to resolve them Ideally we would use this to improve documentation and solve simple requests before they need to be brought to the Data ClinicWell use that data to then identify the best opportunities for improving self-serve Simple opportunities for this include better documenting key tables and better training for common self-serve subjects we just need to know where to start for the biggest impactWe will also use that data to determine who is using Data Clinics the least We want to encourage the underrepresented departments to ask more questions and as a result get more benefit from the Data TeamBefore we had monthly requests from 3 departments  Product Customer Success and Medical OperationsThe biggest benefit by far however is that Data Clinics have increased Data Literacy both in terms of depth and breadth People are more likely to use data in their day-to-day more likely to understand the data and more likely to help their team members understand it tooIf you end up adopting Data Clinics I hope that they help you as much as theyve helped us* My colleague Matt had come up with the idea to call our office hours Data Clinics back in the fall of 2017
2jgHAET6TUmPFvQ8Rhxyxx,Slow queries are one of the main problems in any data or software team Usually the first knee jerk reaction to fasten your queries is to create all the indexes you can think of Although indexes are supposed to make queries faster they cannot absolve you from the sins you committed while writing the queries Also creating indexes comes with a cost which is something if not thought about can kill database performance in all aspects and not just reading dataSlow query issues are seen more often with large tables We can see slow queries in two different kinds of setups  one is not time critical and the other one is Lets talk about what are slow queries for the one is critical first If youre developing an application a good rule of thumb is to write your frequently run queries in such a way that they return a response within 500 ms For analytical systems (non time-critical) it is highly subjective Reports and BI tools can be tricky Someone once told me that it took over a day to run a single BI dashboard and they were not really bothered by thatHaving worked with large tables (> 1 billion records) quite a lot I have realized that there are a couple of things that we can take care of while doing indexes on themCreating and maintaining an index on a very large table is costlier than on smaller tables Whenever you create an index a copy of the indexed column + the primary key is created on disk and the index is kept in memory as much as possible If an index has all the data required by a query it will never go to the actual table For example if you have filtered data on customer_id and grouped by year and the index is on (customer_id year) and you arent querying anything else in the SELECT statement then the query will not go to the disk table to fetch records as the index satisfies all the data requirements for the queryYou should strive to have optimal indexes for all queries ie indexes that cover most if not all columns that should use an index in operations like filtering grouping sorting selecting and so on These indexes are called covering indexes They cover the requirements for your queriesMore indexes means slower inserts; more indexes mean more disk & memory space occupiedIndex design is tricky If your system has a variety of queries a single or even a couple of indexes might not fulfil the requirements of all those queries The first reaction as mentioned earlier is to create an index that suits your queries not using indexesThe cost overhead of indexes is great Every time you add an index it writes on the indexed table become slower as it has to balance more B-tree structures every time a record comes in The cost is definitely higher when the B-tree writes are not sequential which is why more often than not it is better to pick and existing index and modify it by adding another column or changing the column order or bothReduce your footprint by modifying an existing index instead of adding more indexes mindlesslyFor this to happen you must have deep insight into your queries Itd be great if you have a catalog that tells you which query requires which index so that the modifications you do to the index dont have an unintended effect on the rest of the queriesMySQL supports online changes for index operations  so if you create or delete an index reads and writes on the table will not be impacted during the creation of the index Thats what the documentation says but I have seen issues with that For better performance with index operations use Perconas pt-online-schema-change tool Its tried & testedBalancing a larger B-tree is more resource consuming than balancing a smaller B-tree Partitioning splits your table into smaller sub-tables under the hood but you dont get to know about it unless go to the disk and see that there are different table files created instead of one for your table Same goes for indexes on that tableThe main motive of indexing & partitioning is the same  discard data for queries Apart from discarding the data using partition pruning partitions also have the positive side effect that I just mentioned  it gives us smaller trees to balance smaller indexes to recomputeAnother rule of thumb is to use partitioning for really large tables ie tables that have at least a 100 million rows Once partitioning is done all of your queries to the partitioned table must contain the partition_key in the WHERE clause otherwise it will be a full table scan  which is equivalent to a linear search on the whole tablePartitions result in smaller B-trees/indexes hence theres less work to recompute those indexes on insertsFor indexing to work you should have a suitable field that all your queries can use Theres a host of different types of partitions you can use on your data Find more about them hereAlthough this should be covered by creating covering indexes I think that it deserves a separate mention because indexes can only help you so much to hide bad query writing Disk operations are costly I keep bringing up Jim Grays analogy to demonstrate Disk I/O as it is so important to understand this in the current contextSort operations not using an index are performed on disk Disks are slow To avoid disk operations make sure that you look out for hints & information in the EXPLAIN PLAN of your query When you see filesort understand that it will try to fit the whole table in the memory in many chunks If the table is too large to fit in-memory it will create a temporary table on disk and do the same thing there Look out for a using filesort with or without a combination of using temporary Whenever MySQL cannot perform the sort using an index using filesort will show up in the planTo understand this in-depth please go through this very old post by someone working on the MySQL optimizer Although the post is old I think its still relevant and not much has changed to this partThere are a couple of things that you should take care of while sorting  do remember that you cant skip columns in an index For an index on (x y z) you cannot write a query that orders by x z and expect it to fully use the index The index wont also work if you have a range filter or an IN clause on the first column It will also not work when two columns (x y) are sorted in different order in the query ie ORDER BY x ASC y DESC If you dont take care of any of these points the index will not be used while executing the query\u200eThe essence of this point was to get across the idea of making sure that you use indexes for sorting too as otherwise its going to be a very costly operationHandling large tables in databases is fun Thats really when you get to understand the intricacies of the database system its strength and its weaknesses I would advise you to create a test database generate test data to populate a large table and run queries on that to understand how it all works Practice trumps theoryShoutout to Kai Sassnowski for this talk which validated and reinforced my years of learning about indexing in MySQLThis is part of a series of articles about database performance & SQL for Towards Data Science
QYUYaS4Qj4xhMwsCiTExTr,Virevol ai is a collaborative visual shopping app We run a remote-first lean team have been for 2+ years To do our magic we swim with a lot of dataWe get millions of rows from 100s of sources every day run our models on them and serve them in a variety of ways We are on the Google stack and have GCP cloud functions services containers you name it They trigger other events on file drops queue messages hooks schedules Everything is event-based and just provisioned in time (We are stingy)There are too many data paths to keep in our headsWith observable ELT we can trace the provenance of data down to each row with matching logs inputs versions of every job that touched it We can do ELT accounting in dollars a pipe dream for most CTOs This enables us to rework parts of the pipe that are cost us more not all legacy codeETL is a series of batch jobs When one finishes the next one is triggered Sometimes two sometimes they are Y-joined back to oneYou get a directed graph Graphs are easy™Graphs go from top to bottom (or left to right if you like)The complexity arises from handling when some step in the middle breaks or takes too long Think Rube Goldberg machineOr worse if nothing breaks And you find the error days down the road while making sense of dataTo manage this complex pipeline coordination people use all sort of great tools  Luigi dbt Airflow etcIts a very top-down way of instrumentingThe end-user cares only about the last step If the report needs to go out at 9am and the data isnt ready youll get a call Thats your SLAIf a service needs refreshed recommendations for a user every 10 minutes thats your SLAThe end-user sadly is not impressed by DAGs or fancy toolsWhen things break a tearful junior developer tries to debug why at 3am She joined The Company to get her hands dirty with neural nets But was baited and switched to doing ETL Worse it is called Data Janitorial Work by some not Data Lorax as it shouldShe toils through some documentation of what should have happened If the person who wrote that still works there a bit of Slack Then going through the logs to make sense of things If she is really lucky there might even be a mostly up to date runbook for this job She hops from Pagerduty to JIRA to wiki to github to logsBirds sing in a distance to welcome the break of dawnWe introduce a new concept called a Global Trace Identifier or tid for shortEvery function batch job or service that will transform data needs to get a tid for that specific runIt provides all old tids its version and instance variables (eg X-AppEngine-QueueName X-Appengine-Taskname) These are logged in BigQuery in service_tracker tableIt has to log that run with this tidWhen done it has to append that rows last_tid column to this) If it is a new row it has to also populate tid column to that row Scroll down to Slowly Changing Dimensions section for optimizationsHere is the deployed Google function that does all we need for thisLets draw a graph outlining these cases: Here A B C…F are all different processes Each takes a thing wakes up sends a thing and shuts down But we do not need to have a documented graph like this one anywhere
T8BWCMHPbjRKx65LNbdySi,At the beginning of my data engineer path I was lost in so many new terms and tools I have been struggling to know where to start what skills I should learn and what sample projects I can practiceThis weekend I recall my 6 months journey and summarize my experience I questioned myself that if I start again what should I pick? Hereby with an intent to help other juniors saving time I recommend you take a look at these books tools and a courseThe course Data Warehouse Concepts Design and Data Integration by the University of Colorado System introduce the work of a data engineer will work with You will go through the concept and architecture of Data Warehouse Data-Mart Dimensional Model ETL Data Pipeline as well as the tool to build themWhat I like about the course is it covers a lot of topics in data engineering and from that I can use the keywords to drill and research further Dont worry if you cannot get the idea for the first time it requires real experience to completely understand it I saved all the slide decks and constantly look up it when I face a conceptOn the other hand I dont like the tools introduced in the course It requires using Pentaho Pivot4j WebPivotTable to finish the peer-graded assignment However the tools UX is awful and it is also challenging to install The DBMS they use in the course is Oracle and it is not easy to set up too; I use Macbook and I have to run it inside a virtual machine It makes me frustrated every time I have to touch them I hope they will upgrade the course material to use more modern tools with easy setup and we can use the tool in a real project For example with DBMS they can choose Postgres so we can quickly start a Docker containerOverall I recommend taking the course to get used to several data engineering topics and we can collect the keywords for researching further Besides the course results in a certificate I see it is valuable too Especially when I switch from software engineer to data engineer I need a certificate to strengthen my resumeStar Schema The Complete Reference is my go-to book when I want a sample for my architecture Like the course it covers most of the topic in data engineering However it comes with a very high level of detail and addresses real-world complexityThe book begins with the fundamentals of star schema design and slow change processing Then it introduces the 2 famous data warehouse architectures of legends WH Inmon and Ralph KimballWhile reading the book I realized that I have many fallacies about data engineering Like its name it is nice if all the team members read this book so when in discussions all the members know what they are referring toOther people also recommend the book The Data Warehouse Toolkit: The Definitive Guide to Dimensional Modeling I have not read this book so I wont comment on thisMy team and I have made a mistake that we try to implement what already has commercial tools We spent a lot of time implementing data pipelines using the tool Airflow and it is not time effectiveAirflow is a workflow schedule and monitor tool You define an operator to execute a task like transforming a Pandas data frame or extracting data from a database An operator can be a Python/bash scripts and Airflow also provide a lot of pre-defined operators Then you set up a scheduler to execute these operators Literally with that concept you can implement a whole data pipelineOn the dark side it is time-consuming to implement when we want to introduce new kinds of operators and managing the infrastructure is also painful We have planned time to implement dimensional models or deliver a new data-mart however we end up fixing errors from the old operators or a server downLastly we decided to change our approach With data integration we use Stitch to move data from many data sources into 1 place called Data Lake After that we do the transforms using SQL on the Data Lake (the Data Lake is basically a DBMS) I am investigating time to explore the tool DBT and it is a potential candidate to implement the transformsAbout BI we are using Google Data Studio to build the dashboard It has many drawbacks but I like its drag&drop features as well as it is a web application so we can quickly build and share dashboardsTo choose the right tool you need to consider your organization context (resources business needs) I recommend you to read these 2 articles before deciding the tools: The things I list are from my mistakes/experiences I hope it can save you a little time or at least it servers as a hint for your journeyDespite what materials you learned what tools you know how to use the most important thing to master a topic is a real experience Reading the book or practice the tool samples show you only the surface of reality My strategy to learn new things is jumping into a project and dive deep into it You will realize what you need along with the process
QbJ8pbA9637HBWLajaxtLz,In the 110 release Airflow introduced a new executor to run workers at scale: the Kubernetes executor In this article well look into: Airflow has a new executor that spawns worker pods natively on Kubernetes Theres a Helm chart available in this git repository along with some examples to help you get started with the KubernetesExecutorThe data engineering space rapidly evolves to process and store ever-growing volumes of data Lots of technologies exist today to store and query petabytes of raw data in so-called data lakes or data warehouses Think of open-source platforms  Hadoop Kafka Druid Ceph  or cloud-native solutions  Amazon Redshift and S3 Google BigQuery and GCS Azure Data Warehouse and Data Lake You can use tools such as Spark Flink Storm or Beam to process these humongous amounts of data Or run a query on a data warehouse and store the results in a new table Data pipelines often consist of many steps and many tools to move data aroundA workflow scheduler manages dependencies between tasks and orchestrates their execution Well-known schedulers are Airflow Luigi Oozie and Azkaban Airflow gained a lot of support from the community because of its rich UI flexible configuration and ability to write custom extensions Airbnb started the project and open-sourced it as an Apache incubatorImagine we have a pile of data sitting somewhere in Google Cloud Storage (GCS) waiting to be processed We want to use Spark to clean the data and move it to BigQuery for analysis The budget is tight so we dont have a Hadoop cluster running 24/7 We first want to create a new Dataproc cluster (Dataproc is Hadoop on Google Cloud) Then the Spark job fetches the data from GCS processes it and dumps it back to GCS Finally a BigQuery job loads the data from GCS into a table At the same time we can close the Dataproc cluster to clean up our resources Heres how such an Airflow workflow looks likeAirflow natively supports all the steps above and many more Airflow also takes care of authentication to GCS Dataproc and BigQuery This was a simple illustrative example of how Airflow creates workflows between tasks If youre new to Airflow I highly recommend to check out this curation of sources on Github This article assumes you already understand Airflows essentialsAirflow can run more than just data pipelines It can trigger any job with an API That opens a world of opportunities! Before you know your Airflow installation becomes crowded with pipelines Its time to scale Kubernetes is one of the leading technologies to scale applications Individual pieces of an application run as isolated containers in so-called pods Kubernetes duplicates those pods when the application needs more powerAirflows architecture fits perfectly in this paradigm At its core a scheduler decides which tasks need to run next The web server enables users to interact with DAGs and tasks A database keeps track of the state of the current and past jobs Finally Airflow has workers that run the tasks The scheduler web server and database usually dont need to scale Its the worker nodes that do the heavy duty Airflow has two popular executors available that deploy workers at scale  the CeleryExecutor and KubernetesExecutorCelery is a distributed task queue that balances the workload across multiple nodes Using Celery to schedule jobs on worker nodes is a popular approach to scale Airflow Here you can find a Helm chart to automate the deployment with the CeleryExecutor But if using Celery works so well then why would we need another executor for Kubernetes? Because celery is quite complex You need to deploy Celery as an extra component in your system And Celery requires a broker such as RabbitMQ or Redis as back-end Additionally you probably want Flower a web interface to monitor Celery Perhaps these many components add too much overhead for the task at hand? Cant we have something more simple? Yes! Airflow 110 introduced a new executor to scale workers: the Kubernetes executorWith Celery you deploy several workers up front The queue will then schedule tasks across them In contrast the KubernetesExecutor runs no workers persistently Instead it spawns a new worker pod for every job Airflow cleans up the resource as soon as the job finished Now we leverage the full potential of Kubernetes Theres no more need for additional components Scaling is only limited to the size of the cluster As long as you have enough CPU and memory available Airflow can keep scheduling more tasks When Airflow has no more jobs to run only the scheduler web server and database remain alive Your cluster can use its resources for other applicationsLets dive into the practical details The remainder of the post walks through a simple example Well use a Helm chart to set up Airflow on minikube but you can deploy it on any cloud provider if you want Our deployment will sync the DAGs regularly from Airflows git repository Lets start by cloning the git repositoryBefore you continue make sure that minikube is up and running You might have to wait a minute after minikube has started to spin up the dashboardFirst of all we need a docker image that contains Airflow version 1102 or higher Make sure that you have Docker installed and that minikube is up and running Follow these guides to install and set up docker and minikube The repository contains a script to build the docker imageThe script builds a new image from docker-airflow with the Kubernetes dependencies When the script has finished the image will be available as airflow:latest in the registry of minikubeHelm charts allow developers to create reusable Kubernetes deployments Helm reads a valuesyaml file and generates a set of yaml files These files describe a Kubernetes deployment We only have to create a new yaml file that overwrites the default settings to create a custom deployment You can find all the default values and their explanation here Well cover the necessary changes one by one Start by creating a new valuesyaml fileAirflow uses a fernet key to encrypt passwords in the database We dont want to store them as plain text of course Heres how you can generate such a keyPaste the generated key in your valuesyaml fileNext we have to tell which image to use If you built the image on minikube then the configuration looks like this: The pull_policy does not matter on minikube because it uses its local registry to get the Airflow imageWe have to tell Airflow were to get the DAGs from and how to store them Our example fetches the example DAGs from the Airflow repository at regular intervals and copies them to the DAGs folder The following snippet shows you how to do this: Persistence must be disabled to pull DAGs from git In most cases the DAGs dont live in the root git folder In our case we have to look for them in the subdirectory /airflow/example_dags Well pass this directory to subpath parameter Its default value is dags You can configure how frequently to pull DAGs from git using the wait fieldMake sure your valuesyaml file looks like the snippet below before continuing to the deploymentThe remainder of the tutorial is childs play With only a few commands well have Airflow up and running But first we need to install Helm on minikube By the way we call the part of Helm that runs on Kubernetes Tiller Well also install the required Helm dependenciesUse Helm to generate the yaml files and deploy AirflowWithin a couple of minutes Airflow should be up and running Dont worry if it takes a little while Usually the web server and scheduler try to connect to the Postgres database before its ready The connection needs to time out before the pods fail and restart To speed up the process restart the web server and scheduler pods manually as soon as the database is readyWhen all the pods are ready you can use this command to port-forward the web UI to http://localhost:8080Now try to run example_bash_operator by unpausing it The DAG should run twice now Watch how Airflow starts new pods and cleans up finished ones in the minikube dashboard You can look at the logs in the graph and tree view Unfortunately you cannot view the logs from the task instances panel Thats still a bug in the KubernetesExecutorAirflow 110 introduced a new executor to run Airflow at scale: the KubernetesExecutor The scheduler interacts directly with Kubernetes to create and delete pods when tasks start and end As a result only the scheduler and web server are running when Airflow is idle That frees up resources for other applications in the cluster Airflow is now able to scale natively on Kubernetes without the need for additional components such as Celery The deployment is much simpler and straightforwardHowever the new executor is still pretty new and sometimes behaves a bit unexpected Version 1102 resolves most of the issues so I recommend to use only this version or above Theres one tricky part remaining though The volume for the logs needs to allow read and write access from all Airflow pods Cloud providers dont natively support ReadWriteMany volumes so you have to provide a solution yourself The repository contains an example with an NFS serverNonetheless the executor works as expected and is stable enough to be put to production The development team of Airflow did a great job Kudos! Thank you for sticking to the end I hope this post can be useful for your next deployment
JbN5sArJywGzw54Z6cXkNE,A minute of silence for the bugs we have all faced in productionOkay the minute is over Back to the use case I expected around 100 records/second on average on my pipeline Also I wanted to find the threshold of my pipeline: how much load can it take before breakingThe tool I used for load testing was JMeter I learned how to use JMeter during my internship days as a tester and its so amazing how I was able to apply the skills learned again as a Big Data Engineer Truly knowledge never goes to wasteOn a higher level my pipeline accepted input via an API so my load testing reduced to the load of the API and heres what I did in simple steps: Currently I am using a Windows machine(dont judge okay? Its all due to this COVID situation that I dont have access to my laptop) So heres how did the setup on my Windows:- Install & setup Java if not already done- Download JMeter from here- Unzip- Launch JMeter by running Unzipped folder > bin > jmeterbat- Voila GUI will be launchedHappy testing!!Ciao
a6wVhr9QK58WtXNCq6nLis,"Hadoops MapReduce is not just a Framework its also a problem-solving philosophyBorrowing from functional programming the MapReduce team realized a lot of different problems could be divided into two common operations: map and reduceBoth mapping and reducing steps can be done in parallelThis meant as long as you could frame your problem in that specific way there would be a solution to it that could easily be run in parallel This will usually result in a big performance boostThat all sounds good and running things on parallel is usually a good thing especially when working at scaleIn order to understand the MapReduce framework we need to understand its two basic operations: Map and ReduceTheyre both high order functions: Meaning they are functions that can take other functions as their argumentSpecifically when you need to convert a certain sequence of elements of type A into a result or series of results of type B you will: To make this clearer lets see an exampleSuppose youre working for an e-commerce company and they give you a log file of this form: Then they ask you to tell them how many fruits each customer boughtIn this case after parsing this file to turn it into an actual format like CSV you could easily go through each line and add the number of bought fruits on a dictionary under each nameYou could even solve it with a bit of Bash scripting or load the CSV on a Pandas DataFrame and get some statisticsHowever if the log file was a trillion lines long bash scripting wouldnt really cut it Especially not if youre not immortalYou would need to run this in parallel Let me propose a MapReduce-y way of doing it: If youre familiar with SQL and relational databases you may have thought of a similar solutionNotice how the mapper doesnt need to see the whole file just some of the lines The reducer on the other hand only needs to have the lines that have the same Name (the ones that belong to the same group)You could do this with many different threads on the same computer and then just join the resultsOr you could have many different processes running the map jobs and feeding their output to another set running the reducing jobIf the log was big enough you could even be running Mapper and Reducer processes on many different computers (say on a cluster) and then joining their results on some lake in the endThis kind of solution is very common in ETL jobs and other data-intensive applications but I wont delve any further into applicationsIf you wish to learn more about this kind of scalable solutions I recommend you check this OReilly book on designing applications at scaleNow that you have an understanding of what MapReduce is and why MapReduce scales lets cut to the chaseFor this first article we will program two different implementations of the Map functionOne of them will be single-threaded to introduce a few concepts and show a simple solution The other one will use the pthread library to make an actually multi-threaded and much faster version of Map Finally we will compare the two and run some benchmarksAs usual all the code is available on this C GitHub projectFirst of all lets remember what Map doesThe Map function receives a sequence and a function and returns the result of applying that function to each element in the sequenceHowever theres a catch C is statically typed and we would like our Map function to be as generic as possible We want it to be able to map over a sequence of elements of any type (provided they all share a type Lets not get carried away here boys)How do we solve this? There are probably a few different solutions to this problem I chose the one that looked like the most simple one but feel free to pitch in with other ideasWe will use sequences of void* and cast everything to this type This means every element will be represented as a pointer to a random memory address without specifying a type (or size)We will trust whatever function we are calling over all these sequence elements knows how to cast them to the right type before using them Were effectively delegating that problem awayA smaller problem we need to solve is sequence length A pointer to void doesnt carry the information of how many elements the sequence has It only knows where it starts not where it endsWe will solve this other problem by passing sequence length as a second argument Knowing that our Map function becomes pretty straightfowardYou see the function receives a void** to represent the sequence it will map over and a void* (*f)(void*) function that transforms elements of some generic type to another (or the same) oneAfter that we can use our Map function on any sequence We only need to do some awkward wrapping and pointer arithmetic beforehandHeres an example using a function that returns 1 for prime numbers and 0 for the othersAs expected the resulting pointer points to a sequence of integers: 1 corresponds to prime numbers 0 to composite onesNow weve gone through the single-threaded Map function lets see how to make this run on parallel in CIn order to use parallel execution in C we can either turn to processes or threadsFor this project we will be using threads as theyre more lightweight and in my opinion their API is a bit more intuitive for this kind of tutorialThreads API in C is quite intuitive if only a bit obscure at first""To use them we will have to #include <pthreadh> Pthreads' man page explains their interface quite nicely However for this tutorial all we will use is the pthread_create functionpthread_create takes four arguments: After calling on pthread_create a parallel thread of execution will begin running the given functionOnce we call pthread_create for each of the chunks we wish to map we will have to call pthread_join on each of them which makes the parent (original) thread wait until all the threads it spun finish runningOtherwise the program would end before the mapping was doneNow lets feast our eyes on some codeTo code MapReduces Map function in C the first thing we are going to do is define a struct that can store the generic inputs and outputs for it as well as the function we will be mapping withSince parallel execution requires some manner of slicing and partitioning we will store that logic inside this structure as well using two different indices for the start and end of our slice""Next we will code the function that actually does the mapping: it will cycle the inputs from start to end storing the result of applying the mapped function to each input in the outputs' pointerFinally the star of the show the function that starts the threads assigns a map_argument to each of them and waits for all the map jobs to run finally returning the resultsNotice how this function allows us to choose how many threads we want and partitions the data accordingly It also handles pthreads  creation and joiningFinally the way we would call this function in main looks something like this: Where NTHREADS is the number of threads we want and N is how many elements numbers hasIn order to measure performance improvements from using parallel Map I tested some single-threaded algorithms against their multi-threaded counterpartsFor my first test I used the slow_twice function which simply multiplies each number by 2You may be wondering why is it called slow? The answer is simple: we will double each number 1000 timesThis makes the operation slower so we can measure time differences without having to use so many numbers that initialization takes too long It also lets us benchmark the case of many memory writesSince execution time for each number is constant the non-parallel algorithms time grows pretty much linearly on input sizeI then ran it with 2 4 and 8 threads My laptop has 4 cores and I found that to be the optimum number of threads to use as well For some other algorithms Ive found using a multiple of my quantity of cores to be optimum but this hasnt been the caseI ran each benchmark 10 times and took the average just in caseHere are the results: For both test cases using 4 threads was about three times faster than the single-threaded implementation This proves using Parallel Map is a lot faster than using a common single-threaded versionThere was also a cost to adding more than 4 threads probably due to the overhead of initialization and context switchingFor this benchmark I coded a naive prime testing function: it simply iterates through all the numbers smaller than the input and returns 1 if any divides it 0 otherwiseNotice how this function takes O(n) instead of O(1) for each element so a few partitions of our data (which is ordered) will be a lot slower than the othersIn this case again the parallel algorithm beats the single-threaded one No big surprises thereI think this is because when partitioning our inputs dividing it into smaller chunks makes the slowest partition take less time thus making our bottleneck smallerI had a lot of fun running this experimentPicking how many threads to use turns out to be a lot harder than just use the same amount as cores and depends a lot on our input even for very dumb algorithmsThis may help us understand why optimizing a clusters configuration can be such a daunting task for a big applicationIn the future I may add a parallel reduce implementation to complete this little frameworkA few other benchmarks that mightve been fun and I may run in the future are Map in C vs Python List Comprehensions and C vs SIMD-AssemblyIf you want to level up as a Data scientist check out my best Machine Learning books list and my Bash tutorialFeel free to contact me on Twitter Medium or devOriginally published at http://wwwdatastufftech on October 19 2019"
eDG6CoYdmSBfuvWHS28VfV,The most popular and widely used open-source database finally saw its arrival on the cloud trinity two days ago when Google Cloud announced that it supports MySQL 80 GA on Cloud SQL The first one to offer upgrades to MySQL 80 in the cloud was obviously AWS then Azure and now GoogleMySQL 8 comes with a fantastic set of features like online DDL the much awaited window functions enhanced JSON features recursive common table expressions (CTEs) for hierarchical querying default UTF8MB4 collation & encoding soft deletes invisible indexes descending indexes staged rollouts and more Cloud SQL does provide the standard security features with an option to encrypt the data with your own keys Both encryption in-transit and encryption at rest are supportedTo explore the exhaustive list of features of MySQL 80 please visit this blogpost by MySQL Server TeamOther standard features to support BCP such as High Availability by the way of making the system fault tolerant and having an option to replicate the same data to multiple regionsThe Cloud SQL Proxy allows a user with the appropriate permissions to connect to a Second Generation Cloud SQL database without having to deal with IP whitelisting or SSL certificates manually It works by opening unix/tcp sockets on the local machine and proxying connections to the associated Cloud SQL instances when the sockets are usedIf you havent already upgraded your on-prem or cloud MySQL instance to MySQL 8 now is the timeThe closest competitor of MySQL is PostgreSQL If you try and understand the product roadmaps for both the databases youll find that both of them are aiming to do something what the other does PostgreSQL is probably trying to implement a layer of storage engines MySQL on the other hand is trying its best to have great JSON array and geospatial support MySQL finally introduced window functions PostgreSQL has had them for yearsTheres a controversial blogpost by Ubers Engineering team about why they shifted to MySQL from PostgreSQL a couple of years ago Its a very interesting read but has had its share of criticismGoogle has accelerated its cloud expansion in the last couple of years with the support of great open-source tools like Airflow Kubernetes etc and exposing Googles own highly scalable services like BigTable With acquisitions like Kaggle and Looker Googles outlook is looking great in the years to come All I want to say is  feel safe when you run MySQL 80 on Google Cloud Platform
nQz2FMGLRhmaNRT8YTanyF,In this post Ill share my first impression of the product This is in no way meant to be a complete review  I am barely scratching the surface Hopefully youll get an idea of what Cloud Data Fusion is and whether a closer look is worth your timeBefore diving into the details lets consider the historical context of Cloud Data FusionIf you worked in the data / analytics space before data scientist became the sexiest job of the 21st century you might know that graphical ETL (extract-transform-load) tools have been widely used in corporate data warehousing environments In fact if you visit the headquarters of any large bank or retailer theres a good chance their data warehouse environment is still locked into some proprietary graphical ETL tool like DataStage or SSISWith the rise of startups and big tech  both heavily reliant on data for product development  code (often open source) became more important for building data pipelines One example of this is the prevalence of Airflow for orchestration of data pipelines  both in startups and big tech companiesGoogle has already capitalized on the emergence of Airflow with Cloud ComposerThe divide between the traditional corporate world on the one hand and the brave new tech world on the other is revealed in the language of the two camps Corporate folks speak of data integration while techies use the term data pipelinesIn the marketing of Cloud Data Fusion Google uses both terms but its no coincidence that data integration features so prominently: Google is going for the corporations with Cloud Data Fusion That their enterprise pricing starts at ~$3000/month is further evidence of thisAt the same time even among startup hipsters theres a big movement towards #NoCode (simply check Twitter for lots of examples) So Googles timing for launching Cloud Data Fusion as a no-code data-pipeline product seems appropriateAs with most Google Cloud Platform (GCP) products its very easy to get started with Cloud Data Fusion You simply enable the Cloud Data Fusion API create an instance and configure the proper IAM permissions In my case it took about 15 minutes to spin up a Basic edition instance in the asia-east-1 zoneOnce your instance is up make sure you set up permissions for the service account as described hereCloud Data Fusion conveniently uses Google authentication and you get a tour of the interface when you log inQuick side-note: the fact that the term data governance is used in the onboarding is further evidence that Google is targeting corporations: Cloud Data Fusion has a Hub where you can get both individual components or complete data pipelines An example of a simple pipeline is their quickstart which reads articles from the New York Times (NYT) stored in Cloud Storage parses and cleans the data and loads top-rated books below $25 into BigQueryAfter selecting this pipeline (and optionally customizing it) you click Deploy to make the pipeline executableFrom there theres a Run button you can use to execute the job As youd expect you can also schedule jobs to run at regular intervalsBelow are some of the issues I ran into while playing around with Cloud Data Fusion If I had read the documentation theres a good chance I would have found out what was actually going on here If someone working on Cloud Data Fusion reads this post these errors might serve as example issues that people could run intoOverall Cloud Data Fusion really feels like a product targeted at larger corporations who might want to move their ETL jobs into Google Cloud PlatformI personally doubt that Cloud Data Fusion will get traction among startups but it seems clear that this is not Googles target audience for this productFor traditional corporations there are at least two reasons why it might make sense to move over to Cloud Data FusionFirstly theres a good chance the $1-3000 monthly price is lower than many alternatives in the enterprise data integration space Have a look at DataStage to get an idea of what competitors might chargeSecondly since Cloud Data Fusion is built on Cask Data Application Platform (CDAP) theres a lower risk of lock-in Specifically you get portability in that you can export your data pipelines and bring them with you to another platform that supports CDAP Side note: Google acquired Cask (the main contributor to CDAP) about a year agoBy Alex Svanevik Data Scientist at D5  The Data Science DAO
E6NAzdptNtbwXTP8eHvExp,There has been no dataset for FRIENDS or any of the popular sitcoms available till today except the web transcripts ( For this demonstration I am using transcripts from this link-https://fangjgithubio/friends/) So I have created this dataset for the use of everyone who is interested in exploring this dataset and create similar datasets using the method described hereA peek into the final dataset created is as below The csv file for this is available at https://rawgithubusercontentcom/shilpibhattacharyya/Friends_Analysis/master/friends_datasetThe complete code for this methodology and dataset creation is available in my github repo (https://githubcom/shilpibhattacharyya/Friends_Analysis) This data has a shape (93335 5) with 93335 rows and 5 columns I am pasting a snippet here to give an idea of how I created this datasetA peek into the web transcript link (https://fangjgithub
j9XdC3bXcTVfDJ96ztUGXG,I hope anyone out there that needs to pipe data from salesforce to MySQL without external connectors because your superior asks you to do so can be saved by this articleSo if you dont already know  salesforce stores items such as accounts  cases into objects Objects consists of field and have a type for each field What we are going to do today is to import all the accounts data into MySQLThe security token is later needed to connect to salesforce using nodeIf you are using mySQL now you will know that there is a username and password as well as a host So you will need these information to connect to your account + the database you want to connect toto view your connection parametersNow let us create a new database test_db using create database test_db and subsequently create table test_table values (where we will store the test_table to load the salesforce dataCreate a directory of your choice and do npm init to get all the necessary files to be set upNow create a configjs file to put these credentials inWith that we will install jsforce using nps install jsforceNow let us do a test connection to jsforceBased on the documentation  using connlogin  connlogin(username  password + salesforce security token) You should then be able to get no error code (: It is time to put everything together The idea is that we want to first connect to mySQL and ensure that it is connected Next we will then connect to salesforce and using jsforce wrapper to query data like using SQL langauge and then import it to mySQL (: With that you should get this
F8Es7wGVFyjDJhHDRyAN4H,In this article I will present the characteristics of the publisher/subscriber model for data engineers and application developers I will start by explaining the definition of the concept and its architecture what the Google Cloud Pub/Sub cloud service is and finish with a simple demo that will work as a usage exampleThe programming model is based on the orchestration of workflows organized by a system of transmission of records on channels called topicsThere are two distinct roles in this model producers and consumers (which will later be called publishers and subscribers) The producers are responsible for releasing the records on the topics where a data center that is generally composed of a distributed cluster receives the record stores it and makes it available to consumers who obtain it by accessing the record queue for the topicReal-time streams occur when there are agents acting in both roles usually starting as consumers receiving the records sent to a topic applying some kind of processing or task with the information received and finally publishing it again on a different topicA transmission system has the following essential characteristics: The streaming programming model then encapsulates the data pipelines and applications that transform or react to the record streams they receiveA topic is a category or feed name to which records are published It can be understood as a communication channel that receives information through some kind of protocol (FTP TCP/IP UDP HTTP SMTP etc) and whose information is retained in order of arrival and can be accessed by more than one source of consultation (multi-subscriber)The structure of a topic can vary depending on the streaming system that we implement if you use Apache Kafka you will have more than one partition that ensures replication and increases the availability of information on the topicWhat is common to all systems is that topics are composed of ordered immutable sequence of records that is continually appended to  a structured commit log The records in the topics are each assigned a sequential id number called the offset that uniquely identifies each record within the topicOn the other hand the publishers and subscribers are external entities to the streaming system that can communicate with it through the same protocols mentioned One of the advantages of the model is that publishers and subscribers can be cloud systems jobs coded in any language and even licensed third-party applications Most systems of this type expose libraries and templates to build custom clients if neededThe Google Cloud Platform offers Pub/Sub as an asynchronous messaging service that decouples services that produce events from services that process events Basically it allows us to create topics and subscription channels without worrying about the data center infrastructure needed for storage and distributionUsing the Google Cloud SDK we can build our clients for the publication and consumption of subscriptions counting on a native integration with the rest of the services of the platform which evidently increases the potential of our system under the streaming modelIn Google Cloud Pub/Sub a publisher application creates and sends messages to a topic Subscriber applications create a subscription to a topic to receive messages from it Communication can be one-to-many (fan-out) many-to-one (fan-in) and many-to-manyGoogle Cloud Pub/Sub through its APIs sends traffic using the HTTP protocol via the internet allowing flexibility in the way records are sent (batch or online) and in the way they are received (asynchronous or synchronous)The architecture of any solution with this service must send the record (called message) in base 64 format (so it can be any type of data) to a topic via HTTP where once received it is stored and replicated for a configurable time from the creation of the topic The message is then received by the subscribers of the topic until the maximum storage time is reached or until the message is confirmed with an ack by the subscriber whichever comes firstIn order to explain with an example of application the functioning of the Pub/Sub model for applications and data flows I will build a simple architecture of a single topic and a single subscription of Google Cloud Pub/SubThe demo will consist of building the solution in the following steps: Finally I will be showing how to expand the architecture to include a data ingestion flow and real-time analytics using Google Cloud Dataflow and TableauCreating topics and subscriptions using the GCP Console is a very simple process First look for Pub/Sub in the menuCreate a topic providing a Topic ID (you can encrypt the topic using your own AES 256 keys)When the topic is ready create a simple subscription for itAnd thats it Take note of the topic and subscription name since those will be needed laterIn order to connect our applications with our Google Cloud project an authentication is required In this case to simplify the process and ensure security we will create two service accounts obtain their credentials in JSON format and use them as a authentication methodThe first service account will be for a publisher it allows only publish to topics permissions Go to IAM > Service AccountsCreate a new service account and provide a name and a description (the service account ID will be automatically created)In the role section search for Pub/Sub PublisherFinally create a key and download it in JSON formatThatis it for the publisher service account for subscriber repeat the steps but changing the selected for from Pub/Sub Publisher to Pub/Sub SubscriberBoth service accounts are now ready to be usedFor the construction of both customized clients it is necessary to have a python environment with the google-cloud-pubsub library installedAdditionally we must create the environment variable GOOGLE_APPLICATION_CREDENTIALS whose value will be the path to the JSON file with the credentials we have previously downloadedIn my case I created an environment using Anaconda and installed the library using pip The official installation documentation can be found at this linkNow that the environment is ready we can start coding our clientsUsing the library and the documentation a simple publishing script was coded this will publish a message from the system arguments or a default in case no arguments are provided Finally it will be printing the message ID as a confirmation of success Make sure in replacing topic_name with the name of your Google Cloud Pub/Sub topicThe subscriber takes the other JSON credentials and simply loops over the messages of the topic one by one printing its data and responding with the expected ack for each message receivedNow we are ready to run our clients using the console for testing purposesAs can be seen at the time the publisher client send its message the subscriber client responds printing the messageAt this moment we have our local clients who fulfill the basic functionalities of a publisher and a subscriber However it would be interesting to expand our options using any type of client since we are currently restricted to programming languages where the connection API existsTo solve that we will make use of Google Cloud Functions a serverless solution that will allow us to expose our publishing client as a web service that receives the messages through the HTTP protocol and sends them to the topic using the API just like the client we just builtFirst look for Cloud Functions in the GCP ConsoleCreate a new function type a name select an appropriate memory allocation and select HTTP as the trigger Also you can allow unauthenticated invocations for testing purposes in a production environment you will expect an authentication header in each HTTP triggerFor source code you can zip a complete environment and uploaded as a direct file a file in Cloud Storage or as a repository in Cloud Source For this demo the inline editor will be enough select Python 3X as the runtimeIn the MAINPY tab we will be providing our function code taking into account that the parameters will be received in a JSON request An accurate transformation of our local publisher client will be something like the followingFor the other tab we need to provide the name of the libraries needed to execute our code In this demo we only need the google-cloud-pubsub libraryIn advanced options it isnt necessary to provide the credentials in the JSON file as we can set directly in the GCP Console the service account that will be used during the execution of each triggerAlso do not forget to provide the correct function name in the function to execute field Thats it when the function is ready we can test the connections with any REST consumer using the url of the function I will be using SOAP UI in this caseAs you can see our client subscriber is able to receive the messages sent from SOAP UI consuming the Google Cloud Functions web service Make sure the media type is application/jsonAs a demonstration of how our ability to integrate with any system has been expanded I decided to use Salesforce CRM and build a lightning application in it that consists of a simple text field and a button that when pressed sends the text to the web service via HTTPIn order to avoid going in depth with the construction of this tool I will simply present the source code that is executed when you click on the buttonAs you can see its a matter of creating an object that sends a PUT type request to the Google Cloud Function URL whose body is a JSON and the content-type is application/jsonWith the current architecture we can connect our publishers and our subscribers to any App but we are not taking into account the case where one or more cleaning processes of the received records are needed nor are we storing the records in timeUsing Google Cloud Dataflow we can expand our current architecture by creating a cleansing flow from templates (or create our own from scratch) to store the data in a data warehouse such as BigQuery which can be accessed by analytics clients like TableauIn the topic detail page in GCP Console we can create a job to export the messages of a topic to Google BigQuery using Google DataflowUsing the Cloud Pub/Sub Subscription to BigQuery template we can create a pipeline that ingests a Cloud Pub/Sub stream of JSON-encoded messages from a Pub/Sub Subscription performs a transform via a user defined JavaScript function and writes to a pre-existing BigQuery tableThen once in BigQuery we can build real-time reports and dashboards with Google data studio or 3rd party clients like TableauThis article explains the architecture and concepts of the streaming-based programming model explains its architecture and use with the Google Cloud Pub/Sub tool and finally presents a demo in which the integration functionality that this model offers can be corroborated whether it is for creating applications that are fed from a streaming data center or for ETL streams of data engineering and real-time reporting
XaqVjK7QRRbY9zETJHLXf3,I recently found myself writing and referencing Saved Queries in the AWS Redshift console and knew there must be an easier way to keep track of my common sql statements (which I mostly use for bespoke COPY jobs or checking the logs since we use Mode for all of our BI)Turns out there IS an easier way and its called psql (Postgres terminal-based interactive tool)! Getting started with psql is super easy and youll rejoice in the amount of AWS console clicking it cuts outStart by installing postgresNext connect to your Redshift clusterSince you will be using psql ALL the time I recommend creating an alias in your ~/bash_profile so you can easily establish your database connection with a single word I use redshift as my alias You can use the credentials youve defined in your bash_profile: You will need to ensure your cluster security group is set to receive inbound traffic from whatever IP address youll be using in development If your cluster is in a custom VPC you can do this from the command line using the CLIs authorize-security-group-ingressSiobhán is the Data Engineering Lead at Landed Landed is on a mission to help essential professionals build financial security near the communities they serve and we invest alongside educators when theyre ready to buy homes in expensive markets
dL4GNKUdwSkjyg5KzgTcNM,When we think about data-parallel pipelines Apache Spark immediately comes to mind but there are also promising and fresher models able to achieve the same results and performancesThis is the case of Apache Beam an open source unified model for defining both batch and streaming data-parallel processing pipelines It gives the possibility to define data pipelines in a handy way using as runtime one of its distributed processing back-ends (Apache Apex Apache Flink Apache Spark Google Cloud Dataflow and many others)Apache Beams great capabilities consist in an higher level of abstraction which can prevent programmers from learning multiple frameworks Currently the usage of Apache Beam is mainly restricted to Google Cloud Platform and in particular to Google Cloud DataflowHowever when it comes to moving to other platforms it can be tricky to find some useful references and examples that could help us running our Apache Beam pipelineThats why I would like to tell you my experience on how to run an Apache Beam Pipeline on DatabricksNote: the code of this walk-through is available at this Github repositoryI decided to start off from official Apache Beams Wordcount example and change few details in order to execute our pipeline on DatabricksThe official code simply reads a public text file from Google Cloud Storage performs a word count on the input text and writes the output to a given path In order to simplify this process we will replace these operations by simply reading the input text from an in-code mocked string finally printing the word count results to the standard outputThe input string will be defined as a List: We then create a simple Beam custom DoFn Transform to print our results: Our final pipeline will look like this: We now have a working Beam Pipeline that can be executed in local mode If you try to run it you should see this printed to your standard output: Now we would like to execute our pipeline on our Databricks instance To achieve this we need to modify few more things in our code First of all we modify our WordCountOptions which has to extend the SparkContextOptions class These Beam options are necessary in order to manipulate Beams SparkContext A Databricks cluster has its own SparkContext which is crucial to retrieve in order to scale out the application Once we retrieve the SparkContext we can directly inject it into Beams SparkContextOptions as shown below: With this final version of our Beam code we are now ready to launch our Databricks workspace in Azure and to proceed by creating a new Job We package our project into a fat jar (in this example I will be using the standard maven life cycle to package my application) and we upload it to our Job by clicking on Upload JarNotice that if you have any Spark dependencies in your pomxml file remember to mark them as provided since our Databricks cluster will provide them to our application through the execution contextAfter specifying the main class its essential to use these parameters that will be parsed by SparkContextOptions: Finally we can setup the cluster that will be associated to our Job by clicking on edit: In this way we define a New Automated Cluster with 2 workers on Databricks runtime version 64 If you prefer you can also create an Interactive Cluster which gives you more control over clusters executionNow were good to go! If your job looks similar to the one below just click on run now and wait for its terminationPlease note that if you have written your Beam pipeline in python the procedure to make it work on Databricks should look more or less the same: just remember to inject Databricks SparkContext into Beam and execute your Pipeline with the right set of parameters
9X3oAicpeLWvEUi8VGgNgJ,"Recently I was looking into how to deploy an Apache Flink cluster that uses RocksDB as the backend state and found a lack of detailed documentation on the subject I was able to piece together how to deploy this from the Flink documentation and some stack overflow posts but there wasnt a clear how-to guide anywhere My goal with this post is to show a step by step guide for how to do this as of Flink 1110 This guide can be followed with minimal modifications for AWS deployments I will note the changesBefore you start on this guide it is expected that you have an Azure Subscription with AKS and a Storage Account with a blob store running If you need help getting that up and running see the links section at the bottom of this article with links to the Azure guides for how to set those services up""It is also expected that you have flink the azure-cli and docker downloaded see the bottom for links to those if you don't have them installedUsing Azure Blob storage or AWS S3 storage for Flink backend state requires an extra plugin jar that is not built into the base Flink docker image To add the needed jar you first need to download the apache/flink-docker repo from GitHubOnce you have downloaded the repo navigate to the folder with the version of Flink you want to deploy In my case I want to use 111 then if you plan on using scala code navigate to the directory that has the version of scala you are using I use scala 212 so for me that folder is scala_212-debian If you use java or python I believe you can use either version Inside that folder you should see two files: Dockerfile and docker-entrypointsh The file we will be editing is Dockerfile Open that up in a text editor and add the following lines just above the COPY docker-entrypointsh lineFor Azure Blob Storage: For AWS S3 you have 2 options Presto and Hadoop: Presto: Hadoop: Next from the Flink 1110 folder navigate to the /opt/ directory and copy the jar you need for your backend to the same directory as the Dockerfile you edited For Azure that jar is flink-azure-fs-hadoop-1110Now Build and Push the Docker Image to DockerHub using the following command replacing hunterkempf with your DockerHub username and changing the tag to reflect the version of flink scala and the plugin you added to it: First go to the Flink Kubernetes setup page and create the following yaml files on your computer using a text editor and copying/pasting from the Appendixflink-configuration-configmapYou will need to add the following lines to the flink-confyaml section: This example is for a blob store named: flinkblob in a storage account named: flinkstorage You will also need to get the Access Key to your storage account and paste it after the fsazureaccountkeyflinkstorageblobcorewindowsnet: jobmanager-session-deploymentyaml and taskmanager-session-deploymentReplace the image: flink:1110-scala_211 with the name of the docker image you created in part 1 for both jobmanager-session-deployment and taskmanager-session-deploymentNow you should have a Flink Cluster running on AKS using Azure Blob Storage as a RocksDB Backendyou should see something similar with your settings listed out in key value notationTo test the Deployment you can upload one of the test jars from the Flink 111Running the Streaming WordCount job took 1 second and returned no errors for me If you get an Internal Server Error check the Job Manager Logs to see the error stack trace"
PeLcT3prjPGcRfHZraGUZX,It doesnt matter if you are running background tasks preprocessing jobs or ML pipelines Writing tasks is the easy part The hard part is the orchestration Managing dependencies among tasks scheduling workflows and monitor their execution is tediousEnter Airflow Your new workflow management platformA couple of years ago In Scaling Effectively: when Kubernetes met Celery I wrote about my own implementation of a workflow engine using Flask Celery and Kubernetes I considered available solutions  including airflow With no satisfying solution in sight I decided to implement my own framework Since then airflow had come a long way Here is why I switched to Airflow: When using the right setup the one we are about to see Airflow is both scalable and cost-efficientThough the UI is not perfect it is one of Airflows core competencies And in this case a picture is worth a thousand words: Airflow has plenty of integrations both in the form of Operators and in the form of ExecutorsAnd an experimental yet indispensable REST API for workflows which implies you can trigger workflows dynamicallyWith so many companies using Airflow I can rest assured knowing it is going to continuously improveUsing helm and some premade commands we can destroy and re-deploy the entire infrastructure easilyWe use kubernetes as the tasks engine Airflow scheduler will run each task on a new pod and delete it upon completion Allowing us to scale according to workload using the minimal amount of resourcesAnother great advantage of using Kubernetes as the task runner is  decoupling orchestration from execution You can read more about it in Were All Using Airflow Wrong and How to Fix ItWe use Git-Sync containers Those will allow us to update the workflows using git alone No need to redeploy Airflow on each workflow change➕ Decoupling of orchestration and execution➖ Extra pods for celery workers redis and flower monitoring➕ No extra pods➖ Weak-Decoupling well have to define execution code and dependencies inside the DAGs➕ No extra pods➕ Decoupling of orchestration and execution➖ Unsupported  currently causes recursion of pod startupMake sure you have: It is also recommended to set up Kubernetes DashboardTo fill in the cookiecutter options check out scalable airflow template github repoI use docker images since then I can decouple airflow from the actual tasks it runs I can change the underlying task without changing anything in airflow configuration code or deploymentWhen constructing the image I start with python-cli-template  which provides a fast and intuitive CLI experienceMedium: https://mediumcom/@talperetz24Twitter: https://twittercom/talperetz24LinkedIn: https://wwwlinkedin
eoZUA3hd93hvW7pMVBbtd4,I dont use SQL very often and every time I need it I find myself googling for the syntax of even the most basic operations To help myself out I have put together a cheat sheet of useful queries all in one place I hope you will find it useful tooThe queries are in Postgres but the patterns are translatable to other SQL flavors The notes are based on the great DataCamp courses such as Introduction to SQL Joining Data in SQL and Introduction to Relational Databases in SQL as well as on my own StackOverflow searchesThanks for reading! I hope you will find this basic cheat sheet useful If you liked this post try one of my other articles
ahrF6thQnof4BzYvm7jNQT,Moving from Pandas to Spark with Scala isnt as challenging as you might think and as a result your code will run faster and youll probably end up writing better codeIn my experience as a Data Engineer Ive found building data pipelines in Pandas often requires us to regularly increase resources to keep up with the increasing memory usage In addition we often see many runtime errors due to unexpected data types or nulls As a result of using Spark with Scala instead solutions feel more robust and easier to refactor and extendFor a visual comparison of run time see the below chart from Databricks where we can see that Spark is significantly faster than Pandas and also that Pandas runs out of memory at a lower thresholdSpark provides a familiar API so using Scala instead of Python wont feel like a huge learning curve Here are few reasons why you might want to use Scala: In general well use Datasets where we can because theyre type safe more efficient and improve readability as its clear what data we can expect in the DatasetTo create our Dataset we first need to create a case class which is similar to a data class in Python and is really just a way to specify a data structureFor example lets create a case class called FootballTeam with a few fields: Now lets create an instance of this case class: Lets create another instance called manCity and now well create a Dataset with these two FootballTeams: Another way to do this is: The second way can be useful when reading from an external data source and returning a DataFrame as you can then casting to your Dataset so that we now have a typed collectionMost (if not all) of the data transformations you can apply to Pandas DataFrames are available in Spark There are of course differences in syntax and sometimes additional things to be aware of some of which well go through nowIn general Ive found Spark more consistent in notation compared with Pandas and because Scala is statically typed you can often just do myDatasetLets start with a simple transformation where we just want to add a new column to our Dataset and assign it constant value In Pandas this looks like: Theres a small difference in Spark besides syntax and thats that adding a constant value to this new field requires us to import a spark function called litNote that weve created a new object as our original teams dataset is a val which means its immutable This is a good thing as we know that whenever we use our teams Dataset we always get the same objectNow lets add a column based on a function In Pandas this will look like: To do the same in Spark we need to serialise the function so that Spark can apply it This is done using something called UserDefinedFunctions Weve also used a case match as this is a nicer implementation in Scala than the if-else but either will workWe will also need to import another useful spark function col which is used to refer to a columnNow that weve added a new column that isnt in our case class this will convert it back to a DataFrame So we either need to add another field to our original case class (and allow it to be nullable using Options) or create a new case classAn Option in Scala just means the field is nullable If the value is null we use None and if populated we use Some(value)  An example of an optional string: To get the string from this we can call optionalStringget()  and this will just return something  Note that if were not sure whether it will be null or not we can use optionalStringgetOrElse(nothing) which will return the string nothing if nullFiltering a Dataset is another common requirement which is a good example of where Spark is more consistent than Pandas as it follows the same pattern as other transformations where we do dataset dot transformation (ie datasetfilter() )We are likely to need to perform some aggregations on our dataset which is very similar in Pandas and SparkFor multiple aggregations we can again do something similar to Pandas with a map of field to aggregation If we want to do our own aggregations we can use UserDefinedAggregationsOften we also want to combine multiple Datasets which may be with union: … or with a join: In this example we have also created a new Dataset this time using a case class called Player Note that this case class has a field injury which can be nullNotice that weve dropped the player_name column as this will be a duplicate of top_goal_scorerWe may also want parts of our code to just use Scala native data structures such as Arrays Lists etc To get one of our columns as an Array we need to map to our value and call collect()Note that were able to use the case class inbuilt getters to return the name field and this wont compile if name is not a field in our class FootballTeamAs an aside we can add functions to our case classes too and both values and functions will come up as options for autocompletion when using an IDE such as IntelliJ or vs code with Metals pluginTo filter our Dataset based on whether it exists in this Array we need to treat it as a sequence of args by calling _*At this point hopefully youre keen to have a go at writing some Spark code even if just to see whether my claim that its not too different from Pandas stands upTo get started we have a couple of options We can use a notebook which is a quick way to get some data and start playing around Alternatively we can set up a simple project Either way youll need Java 8 installedFor this example were going to use a spylon kernel in a Jupyter notebook https://pypiorg/project/spylon-kernel/ First run the following commands to set up your notebook which should open up your notebook in a browser Then select the spylon-kernel from your available kernelsLets check we have the correct Java version by adding the following to a cell: The output should be: If not check JAVA_HOME in your bash profile and ensure its pointing to Java 8The next step is to install some dependencies To do this we can add the following code snippet to a new cell This sets up some spark config and also allows you to add dependencies Here Ive added a visualisation library called vegasTo connect to our data source we can define a function perhaps something like this: This is a connection to a csv file but there are lots of other data sources we can connect to This function returns a DataFrame which we may want to convert to a Dataset: We can then start working with this data and have a go at some of the data transformations we discussed and many moreNow that youve had a go at playing around with some data you might want to set up a projectThe two main things to include: Example buildsbt: Example SparkSession: We can then extend objects with this wrapper which gives us a spark sessionTo conclude Spark is a great tool for fast data processing and is growing every more popular in the data world As a result Scala is also becoming a more popular language and due to its type safety can be a good choice for data engineers and data scientists who may be more familiar with Python and Pandas Spark is a great introduction to the language because we can use familiar concepts such as DataFrames so it doesnt feel like a huge learning curveHopefully this has given you a quick overview and perhaps enabled you to start exploring Spark either within your notebook or within your new project
JGMuB7cChLYfYjvLkeXFmX,Across the world we are producing and consuming data at exponential rates This is just a fact With the advent of 5g networks we should be capable of handling up to 10/Gbs up from 300 Mbs from 4g LTE This means we have the bandwidth to send and consume more event data and metrics from across all kinds of connected devices and embedded systems than ever before in human historyWhen you consider companies that are data-first/data-driven and succeeding within their respective industries (think Google Facebook Microsoft) versus companies that are collecting data with the end goal of hopefully using it to solve business goals at a point further down the road  but who are only collecting for future consumption; the goals and mindsets couldnt be more differentIf you are actively producing consuming and putting to use the data generated within your organization then there is typically a rich accompanying data culture within the business as well as a direct company-wide mandate to emit and store usable data from across the verticals or business units Alternatively companies who are not using the data that they produce have no idea if the structure of the data will be a direct benefit to the company down the line and this will hamper the future ability of that company to make informed decisions from their data (See my other post on Preventing the Data Lake Abyss for more details on data hygiene)If you consider how intelligent life communicates then you can distill that down to a common vocabulary which transforms words into meaning Given that the meaning of words are informed by the context of their setting then it makes sense to represent the data encapsulated within an event or metric with a common set of vocabulary and defined within a structure that can be easily understood and maintained for the foreseeable future to comeTake initiatives like the CloudEvents open-specification which is trying to build an open-schema for event-based infrastructure or the Elastic Common Schema which is trying to unify the way that organizations write data into ElasticSearch or even the older HTML microdata specification which enables a common set of entities and relationships to exist in the markup of HTML All of these initiatives have overlapping and common goals to create specifications that enable unified vocabulary and structure for dataWhether you use any of these specifications or simply reference them as a cheatsheet for how to model your data it is invaluable to document an agreed upon format that can be reused within your company or projectShared libraries and common formats help to reduce the burden of figuring out how to model new data This can be achieved in many ways but I am a huge proponent for compiled structured data (like Googles Protocol Buffers) Protocol buffers allow the reuse of data structures across many composable types with the use of imports They also compile down to many languages (java /obj-c/javascript/scala/etc) so the same data structures can be used across an organization by importing a library This post is not about protobuf but about how to think about modeling data so Ill get back to itImagine for a second the seemingly simple use case of modeling User data at a company with no data warehouse no data lake and who even lacks a traditional Database Startups face this kind of problem all the time and if you have started to think about solving this problem as well then you have probably started to think of a User in terms of say your Netflix or Facebook account It helps though to ask yourself what a general User is and if the notion of a User has the same meaning across more than one context as well as the additional step that asks what is it exactly that encapsulates User dataI like to start this process off by working backwards What are the use cases you would like this data to solve for This process is no different than when a product manager (PM) goes through a similar exercise that most Engineers are now intimately familiar with You know As a user of this feature I would like to… One can draw many corollaries to the PM user stories and use that process as a mental model for working with data  given it helps to break down complex problems into snackable contextual ideasSay we all work at a new startup that is focusing on delivering the next great local-farm-fresh-to-front-porch-in-a-box concept Now that we have context and given context is king we can now start to consider the User and User Data modeling journey on the same pageIf we start with concrete examples of how our data will be used then the process of breaking things down in reverse order becomes an easier exercise and gives the data modeler a clear idea of what is necessaryGiven the three use cases from above we can distill the need for a User (Customer) a Vendor (provider / farm in this case) a Product (Box of Farm Fresh Food) which contains a collection of one or more Items (Produce/Goods) which are controlled by a Schedule (Interval of Deliveries) which dictates when a box of farm fresh food will show up on someones doorstep as a DeliveryNow imagine these relationships as an over arching data story: A User of our Service receives our Product on a given Schedule and the Items in each Delivery contain seasonal selections from local VendorsThis reminds me that we need to attribute GeoRegion as well as the notion of some Temporal data to the User/Vendor and the User and Vendor Activity/ This data can be encapsulated directly in the main records (User/Vendor) as well as in an Event Events could be increase or decrease frequency of a Delivery schedule or a Vendor declaring they are out of an Item Events can be created for almost anything and enable you to cross-correlated Users and Vendors behavior in the system to start to add more Insights into the layers of event dataWhile building out the use cases we have to also account for a Users Satisfaction and that could be solved with the notion of a Rating that would be associated with each Product Delivery This could be sourced through direct requests via the services App or Website eg the typical Rate your experience you get when you step out of a Lyft or get some Doordash delivered The positive benefit here is that you can begin to learn from each User based on their prior satisfaction with each of their Deliverys This opens the door to building new learning models on top of all of this data Say you want to keep track of the best and worst vendors by region based on direct customer feedback then this metric could be used to discount partnership fees with the best performing Vendor and is also a metric to use when ditching the poor performing VendorsBreaking down complex ideas and delivering meaning from words is the responsibility of any great storyteller It is even more important from a data storyteller given that their success literally makes or breaks a company and their ideas and meaning are composition of easy to follow metrics clearly defined events and insightful behavior driven accounting of piles of otherwise boring data! Now go make this happen for yourself
CFVT8pX8WkuwrbJpL7zmUx,"Fast search capability realtime integrated data nowadays is a necessity if we want to build something like e-catalog or e-commerce We dont want our users to get angry because they spend a lot of time just to get information from our portal We also want the product information that inputted by our product team in the different applications immediately ready to search by our users or customersSay the product team using MySQL as the main data source And we will use ElasticSearch as a search engine service in our portal We need every change in MySQL will immediately affect the ElasticSearch IndexIn this my first article I will demonstrate how can we stream our data changes in MySQL into ElasticSearch using Debezium Kafka and Confluent JDBC Sink Connector to achieve the above use case requirement Credit to my master Erfin Feluzy that introduce me to Debezium and give me the inspiration to write my first article on mediumDebezium is a distributed platform that turns your existing databases into event streams so applications can quickly react to each row-level change in the databases Debezium is built on top of Kafka and provides Kafka Connect compatible connectors that monitor specific database management systems Debezium records the history of data changes in Kafka logs so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running ensuring that all events are processed correctly and completelyDebezium is open source under the Apache License Version 2Apache Kafka is an open-source stream-processing software platform developed by LinkedIn and donated to the Apache Software Foundation written in Scala and Java The project aims to provide a unified high-throughput low-latency platform for handling real-time data feedsThe Kafka Connect Elasticsearch sink connector allows moving data from Apache Kafka® to ElasticsearchIn this tutorial we will use a separate container for each service and dont use persistent volume ZooKeeper and Kafka would typically store their data locally inside the containers which would require you to mount directories on the host machine as volumesStart zookeeper in the container using debezium/zookeeper imageStart Kafka in the container using debezium/kafka docker imageIn this demonstration I will use a pre-configured Docker image that also contains sample data provided by DebeziumStart MySQL in a container using debezium/example-mysql imagethe above command will create a container named mysqldbz""Next let's execute the container and enter the interactive bash shell on the containerTo use capture the CDC on MySQL Debezium needs bin_log configuration in our MySQL enabled Thanks to Debezium since were using a pre-configured MySQL Docker image we dont need to configure it anymore Lets check the configuration""As you can see bin_log enabled by default it's disabledhere we will use single-node elastic and elastic version 77 The container will run with the name elasticdbzThis service exposes REST API to manage the Debezium MySQL connector The container will run with the name connectdbzdont forget to link this container with kafkadbz zookeeperdbz elasticdbz since this service needs to communicate with kafkadbz zookeeperdbz elasticdbz servicescheck the status of Debezium Kafka Connect Service using CURL from the response we will see were using version 24After deploying Debezium MySQL connector it starts monitoring inventory database for data changes eventsTo watch the dbserver1inventorycustomers topic we will need to start Kafka console consumers The container will run with the name watcherafter running the watcher we can see that Debezium starts monitoring the inventory database and put the result as dbserver1inventorycustomers topic""Let's compare with table inventoryLets try to update the customer table""until the step we have achieved to integrate MySQL-Debezium-KafkaWe will get streamed data from Kafka's topic when there is new or changed data in MySQLTo make an integration with Elastic Search we need Kafka Connect Elastic Sink Connector installed on our Debezium Kafka connect containerStep 7 Download Kafka Connect Elastic Sink Connector https://wwwconfluentLets check if the databases and the search server are synchronizedas we can see now all data in MySQL is synchronized All the data in MySQL can be found in the above elastic indexLets insert new data into the Customers table and see what happens in the elastic indexThanks to the article written by my master Erfin Feluzy article https://mediumcom/@erfinfeluzy/tutorial-streaming-cdc-mysql-ke-kafka-dengan-debezium-3a1ec9150cf8 that inspired me to create this articleFinally we made integration between MySQL and ElasticSearch using Debezium I hope this demonstration can help you to solve the data latency problem between MySQL DB and ElasticSearch Now all changed in MySQL DB will immediately be affected in Elastic Index You can try to use another DB such as PostgreSQL Oracle DB2 MSSQL etcReference : https://mediumcom/@erfinhttps://debeziumio/documentation/reference/11/tutorialhttps://docsconfluentio/current/connect/kafka-connect-elasticsearch/index"
2pKgmhGQZ5btPBXJ6e4S6A,A working example of how to use the Snowpipe REST API calls to load a file into a table This is a small tutorial of how to connect to Snowflake and how to use Snowpipe to ingest files into Snowflake tables So for the purpose of delivering this tutorial to our dear readers we opened a free trial account with Snowflake This lesson consists of three steps first one loading data from your computer into the internal stage second loading data manually from S3 Bucket into the external stage and last step loading data into the external stage using AWS Lambda functionThe reason behind this process is to understand better how S3 Buckets and Snowflake tables can work together in harmony So you can enjoy more with less work minimum effort maximum satisfaction But before we start playing with Snowflake and S3 lets check the requirementsIn this section of the document you prepare the environment inside Snowflake to be ready to create tables stages and pipes by following these statements you will be able to make it happen: So we created this program simple_ingest_snowflakepy to access Snowflake using user account host port pipe_name a private key in pem format check these parameters in the file below: In the python program commented above we used these parameters to build JWT Lets begin with an abstract definitionA JSON Web Token (JWT) is a JSON object that is defined in RFC 7519 as a safe way to represent a set of information between two parties The token is composed of a header a payload and a signatureIn Snowflake the generation of JWTs is pre-built into the python libraries that Snowflake API provides (and which are documented in Snowflake docs) so ideally we would simply write a simple script that uses these libraries to automatically take care of JWTs for usAfter we run the program for a couple of seconds we succeed in ingest the file from the internal staging area to the table area of Snowflake and you can check the next logs that everything went smoothlyIf you check the log above you can see that first of all is a POST operation and second we have success in the response code and this is positive feedbackHowever this second log is a GET operation invoked by an expression inside our code for checking the file size time received errors seen if its completed and status type which is in our case was loadedIn this section we talk about on how to ingest the data using the new feature auto-ingest of Snowflake and notification events we succeeded in streaming the data from S3 Buckets to Snowflake tables but we did it manually why manually? because our free account doesnt have the option of auto-ingest activated this is the reason why we did it without notifications and auto-ingest We accomplish this step by following the next tasks: For this task you must follow the good practices of AWS Security that take into consideration the following rules: We have two types of users one of them is the administrator user almalikz and the other one is normal user zaid_user so this task is only possible to activate using the administrator user: In this task we used the normal user account which is zaid_user instead of the administrator userIn this section we will guide you through the process of automating Snowpipe with AWS Lambda functionsAWS Lambda is an event-driven serverless computing platform provided by Amazon as a part of the Amazon Web Service It runs when triggered by an event and executes code that has been loaded into the system You can adapt the sample python code provided in this topic and create a Lambda function that calls the Snowpipe REST API to load data from your external stage S3 The function is deployed to your AWS account where it is hosted Events you define in Lambda (eg when files in your S3 bucket are updated) invoke the Lambda function and run the python codeThis section describes the steps necessary to configure a Lambda function to automatically load data in micro-batches continuously using Snowpipe So lets get started following the next steps: In order to complete this guide you need to first create a deployment package containing your code and its dependencies An easy way to do it is grabbing a t2micro instance and set up an execution environment that mimics the one the python Lambda will run in This is the step youll have to do it in your EC2 micro instance following the next commands: At this point you can then test fire the Lambda and see the output in the AWS Web Console UI Navigate to Lambda > Functions > lambda_functionYou accomplish the mission my dear friend Well Done!You have created a Lambda function to stream data from S3 Buckets to Snowflake tables this is a fantastic first step for you towards becoming a Data Engineer!I have been creating quite a few tutorials to show you how to do streaming data All of these start from the very basics We will start to put more articles in the next weeks So keep in touch with us and discover an easy way to learn streaming dataCheck our free course on Udemy
bEP47G2mcL9booqRaZEHPd,Disclaimer: this post assumes some understanding of programmingWhen we first started to get to know AWS Redshift we fell in love for the fast aggregated query processing This strong advantage meant sky-rocketing our productivity and speed when performing statistical studies or simply data-extractions So of course we turned it into our main data warehouse for all the data sources we managed (Adwords Salesforce FBAds Zendesk etc)  all of this using Stitch as our main ETL toolStitch holds a nice subscription plan of $100 offering process capacity for 5M rows and $20 per additional million rows Stitch logs and billing invoices tell us we barely reached $180 on a very busy month using all the data sources mentioned above Most of those months were just plain $100 (no additional million rows used)Our company develops maintains and sells its core SaaS product IncreaseCard Hosted in the AWS Cloud our production database resides in AWS RDS storing between 10 and 20 million new rows every month and considerably growingSo typically using Stitch to sync our production database to a Redshift warehouse would turn out to be painfully expensive for a third-world startup company Also considering our countrys currency lost a lot of its value against US dollars recentlyWe also had another restriction The companys developing its second major product IncreaseConciliación This one uses the NoSQL Apache Cassandra database to store and process its huge data The problem is both products must be synced in order for Conciliación to use transactions extracted by Card In other words we had to build a data-lake accessible for consumption by any service to perform syncing operations on-demandBeing a small team of 2 people the mighty Data Team we get it easy to try and test new things especially architecturesWe started by using AWS Data Pipeline a UI based service to build ETLs between a bunch of data sources Although the process of building an ETL was rather easy there were a bunch of workarounds that we had to take in order for it to be effective  remember that we have to update every change whether it be an insertion a deletion or an update Since you cant use code here it became unmaintainable quickly Furthermore there isnt much detailed documentation or clear examples for this service IMOWe tried to set a Lambda to consume every 15 minutes the Wal log of a replication slot of our Postgres database and send it to a Kinesis Firehose data stream It seemed to be all safe and sound until a production process updated more rows than usually expected We found out that in these cases the way the records came from the logical decoding were huge rows full of chunks of changes of the tables involved causing the function to die of lack of memory every time it tried to load it We solved this by setting true to the property write_in_chunks of the logical decoding plugin we used (wal2json) enabling us to partition the incoming JSON log Long story short the function could still be terminated unsuccessfully due to not enough time to process the huge transaction No buenoOk so lets head to itOur current architecture consists of the following: For this post to be more democratic Ill divide it into 2 sections The first one will be the steps to replicate changes directly to Redshift The second one building the S3 data-lake for other services to use Feel free to read one or the other or even better both 😄Since I dont consider myself smarter than people that write AWS documentation Ill copy-paste some of their instructions below 🤷\u200d♂Find your RDS instance look up the parameter group that this instance applies Either duplicate or modify the parameter group directly with the following • - Set the rdslogical_replication static parameter to 1 As part of applying this parameter we also set the parameters wal_level max_wal_senders max_replication_slots andmax_connections These parameter changes can increase WAL generation so you should only set therdslogical_replication parameter when you are using logical slots • - Reboot the DB instance for the static rdslogical_replication parameter to take effectAfter rebooting we should be good to go A good way to test if everythings gucci is by running the following line on your database consoleFirst we must create the resources that the task is going to use Create the endpoints (source and target) and the replication instance (basically an EC2 instance thats in charge of all the dirty work)The process of creating a replication instance is super straightforward so I wont go over it just make sure to use a VPC that has access to the source and target you intend to useFor the source endpoint tick the option that says something like Select a RDS instance and effectively select your source database and fill the fields for the credentialsFor the target endpoint select Redshift and fill in all the textboxes with Host Port and credentialsGreat now we have everything we need lets create the task thats going to migrate and replicate the stuffIn DMS press the Create Task button put it a fancy name and start selecting all the resources we created earlier For Migration type choose Migrate existing data and replicate ongoing changes this performs a full migration first and then starts replicating on-going CRUD operations on the target databaseOn the selection section select whatever schema/table you want to use in my case Ill just replicate every change within a schema  also check if the tables have primary keys DMS may mess up if they dont have a PK Use % character as a wildcard (ie all tables containing the word foo  would be table = %foo%The task can also have transformation rules it enables you to for example change a schema or table name on the target destinationReady to unleash it? Hit that create task button and wait for it to start  or start it if you chose not to start when created The task now is going to fully load the tables into your destination and then start replicating changes You should see Load complete replication ongoing as its status when finished migratingVoila done! Your Redshift warehouse is now enriched by your production RDS data good job! At this point if anything went wrong you can always enable the logging feature of the DMS task to see what happened 😄 just take into account Cloudwatch billingYou can also check some useful charts that the DMS task presents to usNext section is going to be about the creation of the S3 data-lake for other services to consume database changes If youre leaving now thanks for reading and I hope this post was useful to youSame as before create a task that instead of Redshift as a target destination its an S3 bucketFor the target endpoint previously create an S3 bucket and simply put the name of the bucket and the corresponding role to access it Take into account the service role should have the following policyOn the additional parameters section put the following addColumnName=true; including the previous parameters or others that you may want to useIn this case choose to Replicate data changes only as migration type we dont want to infest our S3 bucket of previous dataFor more info about using S3 as a target for DMS click hereOk the moment of truth run your little Frankenstein babyThe state of the task should be Ongoing replication If otherwise check the logs related to the replication instance for errors in the Cloudwatch serviceFrom now on if youre using a staging database instance (which I hope you are) create update and delete some rows in order for changes to be replicated by the task This will hopefully conclude in a csv file in the S3 bucket you specified in your destination endpointThis file should have something like: If youre not what here we call as salame youd realize I stands for Insert D for delete and U for UpdateThe moment youve been waiting for (or not) This is where we get to code the function thats going to parse the CSV document thats being created on our bucketFor this part I suggest you use Serverless a nice tool to easily deploy Lambda functions using your AWS CLI credentials Its as simple as writing a yml file hitting serverless deploy and voilaNow lets create our Lambda function and later add the S3 bucket event to trigger the function on every object creation Also you may want to add a prefix if youve specified that the files are going to be created within a specific folder just put folder/ on the prefix textbox and youre good to goLets code part of the function thats going to receive and extract the data from the file uploaded in the bucketSo what we first want to do is build our handler function you know the usual main() but Lambdably speakingThis main function is going to receive as a parameter the JSON event that Cloudwatch handles to it basically saying Yo theres a new file created in the bucket heres its key to access itHeres the sample event that AWS gives usAnd below I paste the Python code that I use to get the final content of the file created in S3Now you have the data of the CSV in the file_content variable If you have experience with parsers this should be a piece of cake if thats not the case for you I recommend you check this linkFrom now on its on your hands Use a python driver to handle the CRUD operations processing every row of the CSV impacting the changes on Redshift For this case I recommend using functions like execute_values() (psycopg2) to minimize execution time as explained here Use Lambda environment variables to handle credential secrets for the function to use remember its not a good idea to hardcode themWhat I present to you is just one of the thousand possibilities to achieve the goal of synchronizing databases If you dont mind spending some hundreds 💸 on services that handle the ETL process for you go for it No really just go for itThis architecture serves us with 2 main positive side effects IMO First one having tracking information about the changes in your production database this can never be needless Nowadays with services like AWS Athena and Glue you can query that data directly via the console The second one is the ability to connect/trigger any procedure via S3 bucket object creation event  in our case the IncreaseConciliación team replicating changes in their own Cassandra database
8Ni8vvEDVhhe5qAFkxLZix,Airflow is an open-source workflow management platform developed by Airbnb and now maintained by Apache
CJF5WitbRkkBco5sz5XKto,With the increasing number of open-source frameworks such as Apache Flink Apache Spark Apache Storm and cloud frameworks such as Google Dataflow creating realtime data-processing jobs has become quite easy The APIs are well defined and the standard concepts such as Map-Reduce follow almost similar semantics across all frameworksHowever still today a developer starting in the realtime data processing world struggles with some of the peculiarities of this domain Due to this they unknowingly create a path that leads to rather common errors in the applicationLets take a look at a few of the odd concepts which you might need to conquer while designing your realtime applicationThe timestamp at which the source generates the data is called Event time whereas the timestamp at which your application processes data is known as Processing time Not distinguishing between these timestamps is the cause of most common pitfall in the realtime data-streaming applicationsLets elaborate on itData queues are prone to delays due to several issues such as high GC in brokers or too much data leading to backpressure Ill denote an event as (EP) where E is event timestamp (HH:MM:SS format) and P is processing timestamp In an ideal world E==P but that doesnt happen anywhereNow lets assume there is a program that counts the number of events received each secondAs you can see both of these are entirely different resultsMost of the realtime data applications consume data from a distributed queue such as Apache Kafka RabbitMQ Pub/Sub etc The data in the queue is generated by other services such as the clickstream from the consumer app or logs from a databaseThe issue queues are susceptible to delays An event generated can arrive in your job even in some tens of milliseconds or can take more than an hour (extreme back pressure) in the worst caseNot assuming there will ever be a delay in data is a pitfall A developer should always have tools to measure latency in the data eg In Kafka you should keep a check on the offset lagYou should also monitor the backpressure as well as the latency (ie the difference between event time and processing time) in your jobs Not having these will lead to unexpected misses in data eg a 10 min time window can appear to have no data and the window 10 min after it to have twice as much the expected valuesIn a batch data-processing systems joining two datasets is relatively trivial In a streaming world the situation becomes a bit cumbersomeWe now join both of the data streams on their keys For simplicity well be doing an inner joinKey A  Both value A & value A arrive at the same timeKey B  value B comes 1 second earlier than value B` Hence we need to wait for at least 1 second on datastream 1 for the join to work Thus You need to consider the following: Key C  value C arrives 4 seconds later than value C This is the same situation as before but now you have an irregular delay in both datastream 1 and 2 with no fixed pattern of which stream will give the value 1Key D  value D arrives but no value D is observed Consider the following: The answer to all of the above questions will depend upon your use case The important part is to consider all of these questions rather than ignoring the complexity of streaming systemsIn a standard microservice the configuration is present inside the job or in a DB You can do the same in data streaming applications However you need to consider the following before going on with this approachIf the config needs to be accessed for each event and the number of events is a lot (more than a million RPM) then you can also try alternative approaches One is to store the config inside your job state This can be done in Flink and Spark using stateful processing The config can be populated in-state using a file reader or another stream in KafkaIn a streaming world making a DB call for each event can slow down your application and lead to backpressure The choice is to either use a fast DB or eliminate network calls by storing state inside the applicationIf the config is very large you should only use an in-application state if the config can be split across multiple servers eg A config that holds some threshold value per user Such a config can be split across several machines bases on the user id key This helps in reducing the storage per serverPrefer a DB if the config cant be split across nodes Otherwise all the data will need to be routed to a single server which contains the config and then re-distributed again The only server containing the config acts as a bottleneck in the scenarioDesigning real-time data streaming applications can seem easy but developers make a lot of mistakes like the ones mentioned above especially if they come from the microservices worldThe important part is to understand the basics of the data streams and how to process a single stream and then move on to the complex applications dealing with multiple joins realtime configuration updates etcOne of the most important books in this domain is Designing Data-Intensive Applications: The Big Ideas Behind Reliable Scalable and Maintainable SystemsConnect with me on LinkedIn or Facebook or if you have any queries drop a mail to kharekartik@gmail
bQXQ9YcmRHAUv7h5MYutZd,Apache Spark is an open-source distributed computing framework In a few lines of code (in Scala Python SQL or R) data scientists or engineers define applications that can process large amounts of data Spark taking care of parallelizing the work across a cluster of machinesSpark itself doesnt manage these machines It needs a cluster manager (also sometimes called scheduler) The main cluster-managers are: As the new kid on the block theres a lot of hype around Kubernetes In this article well explain the core concepts of Spark-on-k8s and evaluate the benefits and drawbacks of this new deployment modeYou can submit Spark apps using spark-submit or the using the spark-operator  the latter is our preference but well talk about it in a future tutorial post This request contains your full application configuration including the code and dependencies to run (packaged as a docker image or specified via URIs) the infrastructure parameters (eg the memory CPU and storage volume specs to allocate to each Spark executor) and the Spark configurationKubernetes takes this request and starts the Spark driver in a Kubernetes pod (a k8s abstraction just a docker container in this case) The Spark driver can then directly talk back to the Kubernetes master to request executor pods scaling them up and down at runtime according to the load if dynamic allocation is enabled Kubernetes takes care of the bin-packing of the pods onto Kubernetes nodes (the physical VMs) and will dynamically scale the various node pools to meet the requirementsTo go a little deeper the Kubernetes support of Spark relies mostly on the KubernetesClusterSchedulerBackend which lives in the Spark driverThis class keeps track of the current number of registered executors and the desired total number of executors (from a fixed-size configuration or from dynamic allocation) At periodic intervals (configured by sparkkubernetesallocationbatchdelay) it will request the creation or deletion of executor pods and wait for that request to complete before making other requests Hence this class implements the desired state principle which is dear to Kubernetes fans favoring declarative over imperative statementsThis is the main motivation for using Kubernetes itself The benefits of containerization in traditional software engineering apply to big data and Spark too Containers make your applications more portable they simplify the packaging of dependencies they enable repeatable and reliable build workflows They reduce the overall devops load and allow you to iterate on your code fasterMy favorite benefit is dependency management since its notoriously painful with Spark You can choose to build a new docker image for each app or to use a smaller set of docker images that package most of your needed libraries and dynamically add your application-specific code on top Say goodbye to long and flaky init scripts compiling C-libraries on each application launchDeploying Spark on Kubernetes gives you powerful features for free such as the use of namespaces and quotas for multitenancy control and role-based access control (optionally integrated with your cloud provider IAM) for fine-grained security and data accessIf you have a need outside the k8s scope the community is very active and its likely youll find a tool to answer this need This point is particularly strong if you already use Kubernetes for the rest of your stack as you may re-use your existing tooling such as the k8s dashboard for basic logging and administration and prometheus + grafana for monitoringOn other cluster-managers (YARN Standalone Mesos) if you want to reuse the same cluster for concurrent Spark apps (for cost reasons) youll have to compromise on isolation: On the other hand with dynamic allocation and cluster autoscaling correctly configured Kubernetes will give you the cost benefits of a shared infrastructure and the full isolation of disjoint container sets It takes about 10s for Kubernetes to remove an idle Spark executor from one app and allocate this capacity to another appThe efficient resource sharing described above will generate significant cost savings compared to traditional YARN deploymentsBut for a given Spark workload running on the same fixed-size resources on YARN and Kubernetes there is no performance difference between the two setups as shown by our benchmark (Apache Spark Performance Benchmarks show Kubernetes has caught up with YARN)If youre new to Kubernetes the new language abstractions and tools it introduces can be frightening and take you away from your core mission And even if you already have expertise on Kubernetes theres just a lot to build: These problems are mostly unaddressed by the managed Kubernetes offerings from the cloud providersShuffles are the expensive all-to-all communication steps that take place in Spark Executors (on the map side) produce shuffle files on local disk that will later be fetched by other executors (on the reduce side) If a mapper executor is lost the associated shuffle files are lost and the map task will be re-scheduled on another executor which hurts performanceOn YARN shuffle files can be stored in an external shuffle service such that when dynamic allocation is enabled the mapper executor can be safely removed on a downscaling event without losing the precious shuffle files The same architecture is not possible on Kubernetes As a result dynamic allocation must operate with one additional constraint: executors holding active shuffle files (as tracked by the Spark driver) are exempt from downscaling This shuffle-aware dynamic allocation is available since Spark 30 and produced great results for our customersThere is exciting ongoing work to go further and build a shuffle architecture where compute resources (k8s nodes and containers) are entirely separate from temporary shuffle data storage Once completed this work will go much further than the external shuffle service as it will enable hard dynamic allocation as well as make Spark resilient to the sudden loss of executors (which is a frequent problem when using spot/pre-emptible VMs)As the founder of Data Mechanics a managed Spark platform deployed on Kubernetes I am biased in this answerThe shift towards containerization and Kubernetes has taken over the traditional software engineering world over the recent years enabling key benefits like simpler dependency management cloud agnostic interfaces declarative infrastructure powerful CI/CD and testing toolsIf youre kicking off a new Spark project we strongly recommend running on KubernetesDoes it mean that every data team should become Kubernetes experts? No Weve built Data Mechanics precisely for that reasonOur platform addresses the main drawback weve outlined earlier: weve done all the setup work stitching together the best open source projects to build the Spark platform youd build for yourself with dynamic optimizations on top that make Spark faster and more stable
B3bNtKbkUDZP8jqn8uR28r,In 2016 while I was leading a team at Gainsight fondly called Gainsight on Gainsight (GonG) I became all too familiar with these issues We were responsible for our customer data and analytics including key reports reviewed weekly by our CEO and quarterly by our board of directors Seemingly every Monday morning I would wake up to a series of emails about errors in the data It felt like every time we fixed a problem we found three other things that went wrong Whats worse we werent catching the issues on our own Instead other people including our very patient CEO were alerting us about these issues Recently I talked to a CEO of another company who told me he used to go around the office and put sticky notes saying this is wrong on monitors displaying analytics with erroneous dataAt Gainsight I had the privilege to work with hundreds of companies that were leading industries in their approach to customer data and deriving insights to improve business outcomes But I saw firsthand that while many companies were striving to become data-driven they were falling short As they encountered pains and challenges similar to the ones I had they lost faith in their data and ultimately made suboptimal decisions A founder I met recently told me how persistent data issues in his companys data and dashboards negatively impacted their culture and hindered their ambition to become a data-driven companyIve come to call this problem data downtime Data downtime refers to periods of time when your data is partial erroneous missing or otherwise inaccurate It is highly costly for data-driven organizations today and affects almost every team yet it is typically addressed on ad-hoc basis and in a reactive mannerI call it downtime to harken back to the early days of the Internet Back then online applications were a nice-to-have and if they were down for a while  it was not a big deal You could afford downtime since businesses were not overly reliant on them Were now two decades in and online applications are mission-critical to almost every business As a result companies measure downtime meticulously and invest a lot of resources in avoiding service interruptionsSimilarly companies are increasingly reliant on data to run their daily operations and make mission critical decisions But we arent yet treating data downtime with the diligence it demands While a handful of companies are putting SLAs in place to hold data teams accountable to accurate and reliable data it is not the norm yet In the coming years I expect there will be increased scrutiny around data downtime and increased focus on minimizing itData downtime may occur for a variety of reasons including for example an unexpected change in schema or buggy code changes It is usually challenging to catch this before internal or external customers do and it is very time consuming to understand root cause and remediate it quicklyHere are some signs that data downtime is affecting your company: Tomasz Tunguz recently predicted that Data Engineering is the new Customer Success In the same way that customer success was a nascent discipline and is now becoming a prominent function in every business data engineering is expected to similarly grow I couldnt agree more and would add that the problem of data downtime spans many teams in data-driven organizations As companies become more data-driven and sophisticated in their use of data along with business functions increasingly becoming heavier consumers of data I expect the problem of data downtime to amplify and grow in importance I also expect better solutions will emerge to help teams across the industry mitigate data downtime In a future post Ill describe how we attempted to solve this problem at Gainsight  and the benefits companies achieve when they regain trust and confidence in their dataIf data downtime is something youve experienced Id love to hear from you! Reach out via email barrmoses at gmail dot com
UyjjATv2GGfa6AmAhBT8TW,"Ccasting from one type to another is very common in SQL queries The use stems basically from bad database design changing requirements legacy applications and lazy database maintenance Not intentionally but these things make sure that the needs of a data consumer are never met directly A consumer has to figure out workaround solutions to get their data Type casting is one of the main although less talked about tools used for thatWere going to use the following sample dataset to go through different examples of casting in a queryFor the scope of this article assume that when were talking about casting it includes all casting-related functions in all flavours of SQL like to_date in Oracle cast(something as int) in MySQL and so on However the examples are based on MariaDB (MySQL)""All the queries in the example below would give the exact same output  which means that the database engine is implicitly understanding the where clause input were giving it It internally casts '1' to 1 and vice-versa in the where clause as per its requirements All four queries in the example below will result in the same outputThis works the same for string to floating point numbers and vice-versa However not all databases support implicit type casting to the same degree For the scope of this post I have used MariaDBWhen a database is able to implicitly cast values we can assume that it would have an impact on the performance Implicit conversion is not cost-free as it by definition adds more work for the query engine As a matter of fact implicit conversion can end up costing more than explicit conversion because in explicit conversion you are in control of the cost whereas in the case of implicit conversion the database is in control and it decides what action to take  which in turn incurs costThis is where a human specifies what to cast and what to cast it into Because string  integer string  floating point conversions are taken care of by the database engine itself the next most important use case for casting comes when we talk about dates Dates are the most disparately stored information in database systems There are so many variants here the two main of which are  time zones and date formats Most of the times dates arent stored as dates or timestamps but theyre stored as strings Thats where it all gets too interestingHeres an example showing a number of options that the database engine provides for conversion of a string to a date""See carefully that the database is extremely reliable and understanding when it comes to storing dates in strings It deciphers almost all of the date formats with the exception of one  2019 12 12  where it is a clear case of the database not identifying the space I guess no one writes dates like that either But you cant be so sure anyway If your dates are in this format you can do a replace(_date' ''') and convert it to a dateNotice that the database has return the result set but with 26 warnings Lets see what they areWarnings are not errors A warning here is a notification that the database suspects that something might have been wrong Something similar happens when you try to insert ignore a duplicate into a table If you insert it wont throw a warning it will throw an error With warnings the database kind of assumes that youll look at those if required With errors it cannot proceed unless you fix themJust like we converted dates from string a whole lot of other conversions are possible but with careAs mentioned earlier any work added to the query engine will have some impact on the performance For reasonably sized datasets implicit and explicit conversions (casting) might not contribute a lot to slowing down the query by themselves Although its completely possible that your query is slow because of other issues arising from type conversionA type conversion (casting) can render one or more indexes of the query useless You can go ahead and check if an index is being used or not in the query plan using explain or explain extended If the index is not being used we can use explicitly type cast the column in the where clause or the join clause and so onLets see how this impacts indexesRunning the following two queries tells us that when we pass a string to be compared to an integer field in the where clause it does pick up the index idx_val_int but in the next query when we pass an integer to be compared to a string field in the where clause it doesnt use the index It does show the name of the index in possible keysTheres additional information on why the index was not picked""The only way out is to provide the value as a string ie '1' rather than 1 or cast(1 as char) 1 being the input of an application hence parameterizableEvery database has a number of options to convert one datatype to another which solve for most of the use cases we face on a day-to-day basis Although type conversion is a great feature it does come with a performance cost be the conversion implicit or explicitAlthough not always practical a great way to avoid all this is to design and maintain the database and the queries in such a way that minimal type conversion is required If thats not possible do make sure that your queries are using the right indexes so that performance doesnt take even a bigger hit Theres more about character sets and collation Well talk about that in another time"
mDM9GY3hKBEji25GgMEvPt,In November 2019 The Azure Cosmos DB team announced a bunch of cool features that makes developing and administrating Cosmos DB applications a lot easierMy favorite feature by far is the new Autopilot modeIn Azure Cosmos DB you provision throughput to manage your workload Before Autopilot you had to set a hard maximum limit on how many Request Units per second (RU/s) a container or a database could have This was difficult to do as you would have to guesstimate how much throughput you would need and hope for the bestEven if you didnt reach the maximum limit you were still charged for reserving it So you might set the maximum throughput level to 10K RU/s but only use that for 15 minutes out of the dayThere are tools that we can use to change the amount of throughput provisioned We can use the portal (like a caveman) or use the NET SDK to update the amount programmatically (which just adds complexity to our code 😥)Enter Autopilot mode 😃 Instead of manually setting throughput ourselves we can set Autopilot on our containers and databases in Cosmos DB which will automatically and instantly scale the throughput provisioned without us needing to do anything nor affecting the availability of our applicationAutopilot currently works on a tiered system where you set the maximum level of throughput that you dont want to exceed on a container or database Containers or Databases can then scale between 01(max) to the maximum value instantlyFirst off its a lot more flexible! I cant tell you how many times Id have to crack out the old abacus and figure out many RUs Id have to provision for Cosmos (and for certain times as well)It also makes your code a lot tidier as well In order to get around the 429 errors the NET SDK does allow you to upgrade the throughput on your Databases or Containers if youre reaching the max allocated throughput But this wouldnt avoid me having to work out how many RUs I needed to meet the demand of my applicationsAutopilot can also save you some serious cash! Ive been in situations where one engineer in our team would do some testing in Cosmos and wack up the provisioned RUs to 20KWith Autopilot you only pay for the resources that your Cosmos DB account uses on a per hour basis So using that 20K example if we hit that level of RUs required using Autopilot we would have only have only been charged for the hours that we needed the RU level to be at 20K rather than the whole week in which that engineer forgot to bring it back down againIf the workload that your Cosmos DB account receives is varied or unpredictable having Autopilot enabled ensures that youll have enough throughput provisioned to meet the demands that your workload requires This removes the need from having to change the amount of throughput provisioned yourself since Autopilot scales the amount up or down within a tier for youIf youre developing a new application or youre just doing a bit of testing having Autopilot enabled can help you assess how much throughput youll need to provision once you deploy your applicationsCreating Databases and Containers with Autopilot on is really straight forward Im going to assume that you know how to provision a Cosmos DB account so Ill skip that just to save some timeLets create a container with Autopilot enabled You can also enable Autopilot at a Database level if you want but Ill just use container as an example In your Cosmos DB account click on New ContainerLike we would normally do when creating new containers in Cosmos we would give the container a name partition key value and either create a new database or assign it a pre-existing databaseIf this youll have to pick an autopilot tier for your container Just pick the 4K tier for now and then click OkIf you need to change the tier later go into your containers Scale and Settings tab and you can pick a new tier You can go up or down a level depending on what demands affect your containerAs things stand there are a couple of catches you need to be aware of when it comes to Autopilot Hopefully when the feature becomes generally available these issues will be resolved but if you dont want to wait that long…The only way you can enable Autopilot onto your containers and databases in Cosmos is through the portal If youre deploying these via CI/CD pipelines using CLI Powershell or REST APIs you currently wont be able to do thatOne way Ive managed to get around this is to provision the auto-piloted containers in the Portal and then test my deployment scripts to see if they revert back to manually provisioned containers The good news is that the Autopilot feature remainedYou cant enable autopilot on existing containers/databases 😥 Much like if you want to change your partition key for a container youll have to recreate the container againYou can turn off Autopilot and switch to manual but you cant go back again Again bit of a limitation but when Im pretty hopeful that when Autopilot goes GA well be able to chop and change as neededAutopilot also takes into consideration how much storage your container/database is currently storing For example if we create a container that has a maximum throughput level of 4000RU/s we will also have a maximum storage limit of 50GB Once we have exceed 50GB of storage within that container we will automatically be upgraded to 20000 RU/s If you dont need to store this data in Cosmos and you want to keep your costs down you may have to consider how you will archive data to keep within the storage limit for this Autopilot tierHopefully in this article you saw the benefits of using Autopilot in your Cosmos DB databases and containersI cant stress how much of a game changer this is when developing and administration applications for Cosmos DB
DhTbbBXdsoxfVQe68yY9jP,The process of moving data from one system to another be it a SaaS (Software as a Service) Data Warehouse or Database is done by Data EngineersData Engineers are focused on providing right kind of data at the right time by ensuring that the most pertinent data is reliable transformed and ready to use We use the process called ETL - Extract Transform Load to construct the Data Warehouse There are several tools available to support the process of ETL like AWS Glue Informatica etc In this article I will be focusing on AWS Glue as the ETL tool and challenges faced in achieving certain requirements Follow this link to read more about AWS Glue as the ETL toolDespite its importance education in data engineering has been limited Given its inception in many ways the only feasible path to get training in data engineering is to learn from the job and it can sometimes be too late As a result I have written up this guide to summarize what I learned to help you with some critical aspects in ETLIn AWS Glue we cant perform direct UPSERT query to Amazon Redshift and also cant perform a direct UPSERT to files in s3 buckets In this article I will cover on how to overcome these challenges from AWS Glue using numerous approachesNote: This guide is for anyone who is curious on solving ETL challenges using AWS GlueAlthough you can create primary key for tables Redshift doesnt enforce uniqueness and also for some use cases we might come up with tables in Redshift without a primary key So performing UPSERT queries on Redshift tables become a challengeIn order to achieve this we can get use of the postactions option in Redshift JDBC sink as followsIn the above code implementation we use a staging table to insert all rows and then perform a upsert/merge into the main table using a post actionAnother scenario is where there is a primary key exist for Redshift tables In this scenario we can change the post action as shown belowWe cant perform merge to existing files in S3 buckets since its an object storage A workaround is to load existing rows in a Glue job merge it with new incoming dataset drop obsolete records and overwrite all objects on s3 There are two ways to do this using DataFrames in Spark and I recommend the first approach since it involves few lines of codeFollowing is the second approach which fits only for dataset with lower number of columnsWe can go with a different approach by using Dataset API in Apache Spark as followsBut there is a limitation in using the above approach since there is an upper limit for number of fields in case classesIn this post we came across numerous approaches to handle the SQL UPSERT from AWS Glue Hope you liked this article If you would like to share any suggestion on any approach then feel free to say so in commentsSee you soon with a new article
LRT3P5qJznNXFh8oDSAv4H,Apache Kafka is a streaming platform that allows for the creation of real-time data processing pipelines and streaming applications Kafka is an excellent tool for a range of use cases If you are interested in examples of how Kafka can be used for a web applications metrics collection read my previous articleKafka is a powerful technique in a data engineers toolkit When you know how and where Kafka can be used you can improve the quality of data pipelines and process data more efficiently In this article we will look at an example of how Kafka can be applied for more unusual cases such as storing data in Amazon S3 and preventing data loss As you may know fault-tolerance and durability in data storing are among the crucial requirements for most data engineering projects So it is important to know how to use Kafka in a way that meets those needsHere we will use Python as the programming language To interact with Kafka from Python you also need to have a special package We will use the kafka-python library To install it you can use the following command: Kafka can be used for storing data You may be wondering whether Kafka is a relational or NoSQL database The answer is that it is neither one nor the otherKafka as an event streaming platform works with streaming data At the same time Kafka can store data for some time before removing it This means that Kafka is different from traditional message queues that drop messages as soon as they are read by the consumer The period during which the data is stored by Kafka is called retention Theoretically you can set this period to forever Kafka also can store data on persistent storage and replicates data over the brokers inside a cluster This is just another trait that makes Kafka look like a databaseWhy then isnt Kafka used widely as a database and why arent we addressing the idea that this might be a data storage solution? The simplest reason for this is because Kafka has some peculiarities that are not typical for general databases For example Kafka also doesnt provide arbitrary access lookup to the data This means that there is no query API that can be used to fetch columns filter them join them with other tables and so on Actually there is a Kafka StreamsAPI and even an ksqlDB They support queries and strongly resemble traditional databases But they are like scaffolds around Kafka They act as consumers that process data for you after its consumed So when we talk about Kafka in general and not its extensions its because there isnt a query language like SQL available within Kafka to help you access data By the way modern data lake engines like Dremio can solve this issue Dremio supports interactions using SQL with data sources that dont support SQL natively So for example you can persist data from Kafka streams in AWS S3 and then access it using Dremio AWS editionKafka is also focused on the paradigm of working with streams Kafka is designed to act as the core of applications and solutions that are based on streams of data In short it can be seen as a brain that processes signals from different parts of the body and allows an organ to work by interpreting those signals The aim of Kafka is not to replace more traditional databases Kafka lives in a different domain and it can interact with databases but it is not a replacement for databases Kafka can be easily integrated with databases and cloud data lake storage such as Amazon S3 and Microsoft ADLS with the help of DremioKeep in mind that Kafka has the ability to store data and the data storage mechanism is quite easy to understand Kafka stores the log of records (messages) from the first message up till now Consumers fetch data starting from the specified offset in this log of records This is the simplified explanation of what it looks like: The offset can be moved back in history which will force the consumer to read past data againBecause Kafka is different from traditional databases the situations where it can be used as a data store are also somewhat specific Here are some of them: Later in this article we will look at an example of how Kafka can be used as a data store in a use case similar to the first one described aboveA lot of developers choose Kafka for their projects because it provides a high level of durability and fault-tolerance These features are achieved by saving records on disk and replicating data Replication means that the same copies of your data are located on several servers (Kafka brokers) within the cluster Because the data is saved on disk the data is still there even if the Kafka cluster becomes inactive for some period of time Thanks to the replication the data stays protected even when one or several of the clusters inside the broker are damagedAfter data is consumed it is often transformed and/or saved somewhere Sometimes data can become corrupt or lost during data processing In such cases Kafka can help restore the data If needed Kafka can provide a way to execute operations from the beginning of the data streamYou should be aware that the two main parameters used to control the data loss prevention policy are the replication factor and the retention period The replication factor shows how many redundant copies of data for the given topic are created in the Kafka cluster To support fault-tolerance you should set the replication factor to a value greater than one In general the recommended value is three The greater the replication factor the more stable the Kafka cluster You can also use this feature to place Kafka brokers closer to the data consumers while having replicas on geographically remote brokers at the same timeThe retention period is the time during which Kafka saves the data It is obvious that the longer the period the more data you will save and the more data you will be able to restore in case something bad happens (for example the consumer goes down due to power failure or the database loses all data as the result of an accidental wrong database query or hacker attack etc)Heres an example of how Kafkas storing capabilities can be very helpful when the business logic of data processing changes suddenlySuppose we have an IoT device that collects weather metrics such as temperature humidity and the concentration of carbon monoxide (CO) The device has limited computation power so all it can do with the metrics is to send them somewhere In our setup the device will send data to the Kafka cluster We will send one datapoint per second measuring every second as a day (in other words the IoT device collects information on a per-day basis) The diagram that visualizes this flow is demonstrated below: A consumer subscribes to the topic to which the producer sends its messages The consumer then aggregates the data in a specified way It accumulates data during the month then calculates the average metrics (average temperature humidity and the concentration of CO) Information about each month should be written into the file The file contains only one line and the values are separated by commas Here is an example of what the file might look like: Lets start from the creation of the Kafka producer The producer will be located in the producerpy file At the beginning of the file we should import all the libraries well need and create the KafkaProducer instance which will work with the Kafka cluster located on the localhost:9092: Below you can see the code that generates data and sends it to the Kafka cluster At the top we defined the initial values for temperature humidity and CO concentration (prev_temp prev_humidity prev_co_concentration) The counter i is used for record indexing For this simulation we want generated values to be random but we also want to avoid very inconsistent results (like changing the temperature for 40 degrees in just one day) So we dont just generate random values; we also need to take into account values from the previous day when generating values for the next day Also on each iteration we check whether the generated numbers are in acceptable intervalsAfter generating all of the required data for the current timestamp the script creates the weather_dict variable from the dictionary with data After that the producer sends the JSON-encoded weather_dict to the specified topic of the Kafka cluster We assign the current values to the corresponding variables that represent the data from the previous timestamp In the end we wait for one second before executing the next iteration of the loopNow lets explore the consumerpy file At the top of the file we define the consumer with several parameters The first and second parameters are the names of the topics for subscription and the URL where the Kafka server is located The auto_offset_reset argument defines the behaviour when the OffsetOutOfRange error occurs The earliest value means that the offset should be moved to the oldest available record So all messages (records) will be consumed once again after the error The consumer_timeout_ms parameter is responsible for turning off the consumer when there are no new messages during the specified period of time The -1 value means that we dont want to turn off the consumerYou can read more about these and other parameters of KafkaConsumer in the documentationLets move on to the most important part of the consumerpy file In the beginning we define the counter i and the lists where we will store data during the month On each iteration we will decode the message (extract weather_dict from it) and append values for the current day to the corresponding lists If the i counter is equal to 30 we calculate the average values of the metrics for the monthNext we open the file weather_aggregationtxt and write data into it The data is written in a single line without line breaks So the programs that will need to read the file should be aware that every 5th value is the start of the new data pointBefore running the producer and consumer you should run the Kafka cluster You can do this using the following command (assuming that you have already installed Kafka): Here is how the output file looks: Suppose now that the time flew and after some period of time the business requirements have changed Now we need to process the weather data in a different wayFirst the aggregation metrics (averages) should be calculated on a per-week basis rather than on a per-month basis as per the previous requirements Secondly we need to convert Celsius temperature degrees into Fahrenheit scale Finally we want to change storing logic Instead of creating a txt file and writing all data into one file we need to create a CSV file with columns and rows Each row should represent one data point (one weeks data) In addition we want to save the same data into an AWS S3 bucketChanging the code to implement these changes is not a problem But we collected a lot of data earlier and we dont want to lose it In the ideal situation we want to recalculate all the metrics from the very beginning So in the result we need to receive data in the new format but ensure it includes those time periods for which we used a different processing approach earlier Kafkas storing capabilities will help usLets explore the changes in the code we needed to make (file consumerpy) First we need to import the boto3 library specify AWS credentials and instantiate the resource (S3) Also we changed the names of the variables with lists to make them reflect the fact that they accumulated weekly data rather than monthly The next change is that we are looking at each 7th record now in order to execute aggregation (earlier we waited for each 30th record) Also we implemented the conversion from Celsius to Fahrenheit formula ((c * 18) + 32)Other changes are related to the saving of processed data Now we work with CSV files and if it is the first week we write the column names to the file Also the script adds a new line character after each data point to write information about each week on a new row Finally we insert the weather_aggregationcsv file into AWS S3 storage as well The bucket s3-bucket-name was created earlierTo recalculate aggregations from the start of our IoT device life (when it started to send data to Kafka) we need to move the offset to the beginning of the message queue In kafka-python it is as simple as using the seek_to_beginning() method of the consumer: In other words if we place the calling method before the code that we described above we move the offset to the beginning of the queue for the consumer This will force it to read messages again that it has already read and processed This demonstrates the fact that when Kafka stores messages it doesnt remove data after the consumer reads it once Here is the weather_aggregationcsv file generated by the updated consumer: This example shows that Kafka was useful as a data storing system The benefits that Kafka gives in terms of data loss prevention are easy to see Suppose that the server where our consumer is located was down for some time If Kafka didnt have data storing capabilities all messages sent by the producer would be lost But we know that when the consumer will be alive again it will be able to fetch all messages that were accumulated by the Kafka cluster during the consumer downtime No additional actions are required to use this feature You dont need to move the offset It will be located on the message that was consumed the last time the consumer was live; it will start to consume data right from the place where it stoppedThe example we demonstrated is simple but it allows us to understand how useful Kafka is At the moment we have the CSV file with the weekly-aggregated weather data We can use it for data analysis (for example look at the Integrating Tableau with Amazon S3 tutorial) machine learning model creation (Creating a Regression machine learning model using ADLS Gen2 data) or for internal purposes of an application Dremio also allows us to join data sources in the data lake and work with it using SQL (even if the original data sources have no support for SQL  see Combining Data From Multiple Datasets or A Simple Way to Analyze Student Performance Data with Dremio and Python tutorials) Dremio is a useful instrument in the data engineering toolkitIn this article we explored how Kafka can be used for the storing of data and as a data loss prevention tool for streaming applications We provided an overview of these features listed the use cases where they are useful and explained why Kafka isnt a replacement for traditional databases We demonstrated a case where a different approach for data processing and transformation was implemented At the same time stakeholders wanted to have the results of data processing computed according to the new approach on all data that we processed starting from the very beginning With Kafka this issue was solved easily
6D7bYDxgjYdtmvEuWumJNb,Metrics are the indicators (values) that reflect the state of a process or a system When we have a sequence of values we can also make a conclusion about trends or seasonality In summary metrics are indicators of how the process or the system evolves Metrics can be generated by applications hardware components (CPU memory etc) web servers search engines IoT devices databases and so on Metrics can reflect the internal state of the system and even some real-world processes Examples of real-world metrics include an e-commerce website that can generate information about the number of new orders over any given period air quality devices that can collect data about the concentration of different chemical substances in the air and CPU load which is an example of the metrics pertaining to the internal state of the computer systemThe collected metrics can be analyzed in real time or stored for batch analysis later Collected metrics can also be used to train machine learning modelsCollecting metrics can be a complex process because it depends on many parameters and conditions The source of the metric produces the values then those values are either delivered to a cloud data lake storage or used in real time The method in which metrics are delivered from source to storage as well as the approach for storing can vary significantly from one case to anotherOne of the tools that can help with the collection of metrics is Apache KafkaApache Kafka is a tool used for building real-time data processing pipelines and streaming applications Kafka is a distributed system which means it can operate as a cluster from several sources The Kafka cluster is the central hub used by different data generators (called producers) and data consumers (called consumers) Applications (desktop web mobile) APIs databases web services and IoT devices are all typical examples of producers The producer is the entity that sends data to the Kafka cluster The consumer is the entity that receives data from the Kafka cluster The Kafka cluster can consist of one or more Kafka brokersKafka uses topics to organize data The topic is the category for streams of data Each data point in the topic has its own unique timestamp key and value Producers can write data into specific topics while consumers can subscribe to the desired topics to receive specific sets of data Kafka supports data replication which is the creation of copies of the same data on different brokers This prevents data loss when one of the brokers is damaged or out for some reasonKafka is one of the most popular event streaming platforms and messaging queues It is used by many large companies to manage their data pipelines Here are several of the most important advantages that Kafka provides: Lets take a look at how Kafka can be used for collecting metricsUsually collecting metrics is done in real time This means that the source of the metrics constantly generates data and can send it as a data stream As we know Kafka is a good tool for handling data streams which is why it can be used for collecting metricsIn this example we will use a simple Flask web application as a producer It will send metrics about its activity to the Kafka cluster The consumer will be a python script which will receive metrics from Kafka and write data into a CSV file This script will receive metrics from Kafka and write data into the CSV file On its own the Python app can enrich data and send metrics to cloud storage At this stage the data is available for a range of best-of-breed data lake engines like Dremio to query and processHeres a tip: If you want to perform metrics monitoring you can use tools like Prometheus Grafana Kibana etc The pipeline is the same: the web application sends data into the Kafka cluster after which the metrics should be delivered to the aforementioned platforms where they are visualized and analyzed It is also possible to set up alerts and notifications if some events occurLets look at the example of metrics collection with the help of Kafka We will use a Flask web application as a source of metrics Using the app people can create orders and buy essential goods Here is the main page of the website: It is very simple: when the user clicks on the New order button they will go to the next page where they can place the orderWhen the user checks the checkbox field this means they want to pay for the order immediately If not the goods will be supplied under credit conditions After the user clicks on the Make an order button the next page is loaded: On this page the user can review the details of the created order The new element here is the total price which is calculated by multiplying the price for the 1 unit times the ordered amountLets now look at the code of the application It has several files including forms templates configs database (SQLite) etc But we will demonstrate only the files that play a role in generating and sending metrics to the Kafka clusterIt is important to note that for this article we will use the kafka-python package It allows us to work with Kafka directly from Python code We installed it using the following command: Below you can see the code from the modelspy file This file describes the structure of the database We have just one table there called Order It is represented by a Python class where each attribute is the column in the database But the most interesting part of this file is the send_order_info_to_kafka() functionThis function is enhanced by the eventlistens_for() decorator (imported from the sqlalchemy library) This decorator monitors the event when the record about the new order is inserted into the database When this occurs the function creates a KafkaProducer instance (which points to the URL where the running Kafka cluster is located) and specifies the name of the Kafka topic  new_orders Then the function creates the body of the message We want to send statistics about the orders to Kafka Thats why for each order we create the dictionary with information about the order amount its total price and whether it is prepaid or not Then we transform this dictionary into JSON format encode it and send it to Kafka using the producers methods send() and flush() So this function is triggered every time that users create a new order The purpose of the function is to send information about the created order to the Kafka clusterWe want to collect one more set of metrics  the number of requests for a certain period of time It is a common metric to monitor for any web application So each time someone visits a page on our website we need to send the notification about this to our Kafka cluster Here is how we can implement this behavior In the file utilspy we define the function called ping_kafka_when_request() The body of this function is very similar to the function that we saw before It creates the instance of the producer defines the name of the topic where the producer should commit messages (web_requests) and then uses the send() and flush() methods to send messages This function is a little bit simpler because we dont need to create a complex body for the message We just send value=1 each time a new request occursTo make this function work we need to call it in the view functions for each of our pages This can be done in the routespy file (see the code below) There are three functions: (index() create_order() and order_complete()) Each of these functions are responsible for executing some logic while rendering pages on the website The most complex function is the create_order() function because it should process form posting and the insertion of new records into the database But if we talk about interaction with Kafka you should pay attention to the fact that we import the ping_kafka_when_request() function from the utils file and call it inside each of the view functions (before executing all remaining code in that function)Those were the producer sides of our architecture We explained that the code needed to be located inside the web application in order to send metrics to the Kafka cluster Now lets look at another side  consumersThe first file is consumer_requestspy Lets examine it by chunks At the beginning of the file we import all the packages well need and create the instance of Kafka consumer We will apply several parameters so it can work the way it was intended You can read about them in the documentation The most important parameters are the names of the topics to which we want to subscribe the consumer (web_requests) and the bootstrap_servers parameter that points to the server where the Kafka cluster is locatedNext we need to create a function which will poll the Kafka cluster once a minute and process the messages which Kafka will return The name of the function is fetch_last_min_requests() and you can see it in the code sample below It needs two parameters as inputs The next_call_in parameter shows when the next call of this function should occur (remember that we need to fetch new data from Kafka every 60 seconds) The is_first_execution parameter is not required By default it is equal to FalseAt the beginning of the function we fix the time when the next call of this function should occur (60 seconds from now) Also we initialize the counter for requests Then if it is the first execution we create the file requestscsv and write a row with headers to it The structure of this dataset will be simple It should have two columns  datetime and requests_num Each row will have the timestamp in the datetime column as well as the number of requests that were processed by the website during the given minute in the requests_num columnIf this is not the first execution of the function we will force the consumer to poll the Kafka cluster You should set the timeout_ms parameter of the poll() method to a number greater than zero because otherwise you can miss some messages If the poll() method returns the non-void object (batch) we want to loop over all fetched messages and on each iteration increment the count_requests variable by 1 Then we open the requestcsv file generate the row (string from the current datetime and counter_requests values joined by comma) and append this row to the fileThe last line in the given function is the timer setup We insert three parameters into the Timer object First is the period of time after which the function (the second parameter) should be triggered The period of time is calculated dynamically by subtracting the current timestamp from the time stored in the next_call_in variable which we computed at the beginning of the function The third parameter of the Timer object is the list with arguments which should be passed into the function which we want to execute We immediately start the timer using its start() methodWhy do we need such a tricky way of defining the time where the next function call will occur? Cant we just use the more popular timesleep() method? The answer is no We used this approach because the execution of the logic that is located inside the function takes some time For example the Kafka cluster polling will take at least 100 milliseconds Moreover we then need to count requests and write the result into the file All these things could be time consuming and if we simply pause the execution using timesleep() the minute period will drift every next iteration This can corrupt the results Using the threadingTimer object is a slightly different and more suitable approach Instead of pausing for 60 seconds we compute the time when the function should be triggered by subtracting the time that was spent on the execution of the code inside the functions bodyNow we can use the defined function Just initialize the next_call_in variable by the current time and use the fetch_last_minute_requests() function with this variable as the first parameter and the True flag as the second (to mark that this is the first execution)That is all for the consumer_requestspy file But before you can execute it you should run the Kafka cluster Here is how you can do it locally from the Terminal (assuming that you already have it installed): Now you can run the file Then go to the web application (you can run the Flask application using the command flask run) in your browser and try to browse it  visit its pages After a while you should have the file requestscsv in the folder where your consumer file is located The file could look like this (actual data will be different and depends on the number of times you visited the pages of your app): What we did was to build the pipeline allowing us to collect a web application metric (number of requests) using Kafka and PythonNow lets look at another consumer We called this file as consumer_orderspy The first part of the file is very similar to the previous file The one difference is that we import the json library because we will need to work with JSON-encoded messages Another difference is that the Kafka consumer is subscribed to the new_orders topicThe main function is the fetch_last_minute_orders() The difference from the function with the previous consumer is that this function has six counters instead of just one We want to count the total number of orders created over one minute the total amount of items ordered the total cost for all orders as well as how many orders are prepaid how many items were in prepaid orders and the total price for all prepaid orders These metrics could be useful for further analysisAnother difference is that before starting the calculation of the aforementioned values we need to decode the message fetched from Kafka using the json library All other logic is the same as for the consumer that works with requests This file to which the data should be written is called orderscsvThe last part of the file is the same: getting the current time and triggering the function defined above: Given that you already have the Kafka cluster running you can execute the consumer_orderspy file Next go to your Flask app and create some orders for several minutes The generated orderscsv file will have the following structure: You can see that our Python scripts (especially those that work with order data) perform some data enrichmentData engineers can customize this process For example if the app is very large and high-loaded the Kafka cluster should be scaled horizontally You can have many topics for different metrics and each topic could be processed in its own way It is possible to track new user registrations user churns the number of feedbacks survey results etcAlso you can set up a collection of some low-level metrics like CPU load or memory consumption The basic pipeline will be similar It is also worth mentioning that writing data into CSV files is not the only option you can also make use of open data formats such as Parquet and land all this data directly on your data lakeOnce you have the metrics collected you can use Dremio to directly query the data as well as to create and share virtual data sets that combine the metrics with other sources in the data lake all without any copiesIn this article we built a data pipeline for the collection of metrics from the Flask web application The Kafka cluster is used as the layer between data producers (deployed in the web application) and data consumers (Python scripts) Python scripts act as apps that fetch metrics from the Kafka and then process and transform data The examples given are basic but you can use it to build more complex and diverse pipelines for metrics collection according to your needs You can then use Dremios data lake engine to query and process the resulting datasets
9E3nNawAH2kqoUFtKgjQrm,Whenever data science or deep learning comes up in a conversation the room is immediately filled with an intellectual aura and peoples curiosity on how youve made a business million dollars by using a modern crystal ball  a predictive model If you are the data scientist you wont bore them with the significant number of hours wrestling with data to make sure its clean semantically correct and appropriately formatted for whichever model you used You will jump to the last 20% of your work and start explaining mathematics that will leave the room of people in awe (and confusion)Nah the point is youve made the business million dollarsWell while I am sure you are most likely a decent data scientist for having a model selected by the companys decision-makers to go ahead with the implementation I am even more sure that you have a strong data engineering team that enabled you and stably sourced all that variety of data for you in the first place Oh their job is not done either Now they have a whole new project down the pipeline because they and the software engineers need to make your model production-ready by integrating it with the current system (or spinning up a whole new service for it)Previously I had an article outlining how to make the most out of a data analytics certificate program The very first big hurdle I ran into when I was finishing the program with a capstone project was  importing data I did extra learning to understand how to read popular types of flat files into pandas Dataframe and even authenticate application program interface (API) to ping data Many hours of getting acquainted with the data meant cleaning data with a dictionary of incorrect values mapped to correct values molding the data narrow or wide depending on the visualization or report format and tweaking the visualization so readers can understand instead of guess the implications Unlike some of my classmates who were more obsessed with developing a predictive model I was fascinated by the finesse invested to just get the right data for quality analysisTwo companies that I got hired after the program both had the same motivation They wish to improve their business processes so they can leverage quality data and obtain meaningful analytical results They (and any business) would always be interested in reducing admin/manual work getting a step ahead of the competitor and accessing secrets to boost their profitability faster However the real questions I had when I started working were if the business process exists at all and if it does how resistant it is to be transformed digitallyPerhaps the term business process sounds as vague to you as it was to me If I get an attempt to define it based on my experience I believe it is this: A digitally transformed business process would have a defined and standardized protocol that details how data is produced (data collection) transformed (data pipeline or commonly known as ETL) and consumed (data analytics & data visualization)The bottleneck that prevents a business process to be digitized is usually either at the data collection or at the data pipeline stage For example (modified from a true story) if different teams all have their little versions of a spreadsheet that report costs does it make more sense for you to centralize the place they report costs so you only need to collect data from one place? Lets say you are a team rock star and managed to centralize the cost data you start noticing that one team reports costs in a completely different format from the other team This is seriously throwing off your dashboard to visualize the cost reporting Now you would need to review your data pipeline that sends data to the dashboard to normalize this discrepancyI was hired as an analyst but I was having serious imposter syndrome since I didnt think I have done enough cool analytics to drive business decisions (it is also when I did more research around the job title data engineer and Robert Chang brought it home with his trilogy P1/P2/Finale) Nevermind the cutting-edge machine learning models behind a recommendation system sometimes it is a struggle to even put together a report with overarching fields that do not make sense put together (well the requester may not know that) or to find the data source at allLuckily in the latter company they are much less resistant to transform their business processes so I am saved from having to wrestle with piles of flat files Still new challenges started bubbling up Instead of flat files I was able to manage centralized mission-critical operational data in a MySQL database via ETL scripts hosted on AWS Lambda Many systems that the entire operations use do not integrate so the MySQL database is a bypass to bridge the data Having this centralized database allows us to live up to the data-driven mandate The interest in pulling data from this MySQL database grew from a few analysts to managers whose decisions are pertinent to the historical data or master data it housesIf you are an experienced engineer you will have many questions popping up right about nowAll very valid questions This is when I genuinely resonate with a passage in The Rise of Data Engineer written by the author of Airflow Maxime BeaucheminThe modern data warehouse is a more public institution than it was historically welcoming data scientists analysts and software engineers to partake in its construction and operation Data is simply too centric to the companys activity to have limitation around what roles can manage its flow While this allows scaling to match the organizations data needs it often results in a much more chaotic shape-shifting imperfect piece of infrastructureYou work with what you are given or are not blocked to do and uncontrollable factors will affect how you design/manage the data ecosystem This company was a start-up about two years ago and I was hired at a very early point in its expansion phase At the time so much was happening  company merger people turnover new system onboarding data integration etc Though I was fairly excited that in some way it means we get to define a new business process with the new atmosphere it could also mean some great existing business processes may be lost before it is documented or if it does not fit in the companys road-mapTaking a step back you would notice most businesses often undergo the evolution of data pipelines before they can leverage the power of big data tools Buzzwords like big data machine learning and AI gained a lot of public attention for analytics What is often missing from a cutting-edge tech conference to a business-as-usual office is the application of appropriate technology to optimize the businesss current stateFinally understanding and redesigning the entire data infrastructure for business places you in an extremely invaluable position to many stakeholders A decision-maker will have to consult you for a report that needs to be created from scratch A software engineer will have to ask you for the ripple effect to other data services upon a new deployment An analyst will have to brainstorm with you on how (often) to capture data so their analysis sustains a business need Bonus point? Should one day you change your mind about being a data engineer you will know the data and their use case by heart so much that you can transition to any of the roles described above with relative ease
XKvEiYRQ7xRPw92rEriv4X,Handling late arriving events is a crucial functionality for Stream Processing Engines A solution to this problem is the concept of watermarking And it is supported by the Structured Streaming API since Spark 21Watermarking is a useful method which helps a Stream Processing Engine to deal with lateness Basically a watermark is a threshold to specify how long the system waits for late events If an arriving event lies within the watermark it gets used to update a query Otherwise if its older than the watermark it will be dropped and not further processed by the Streaming EngineIn distributed and networked systems theres always a chance for disruption  nodes going down sensors are loosing connection and so on and so forth Because of that its not guaranteed that data will arrive in a Stream Processing Engine in the order they were created For the sake of fault tolerance its therefore necessary to handle such Out-of-Order dataTo deal with this problem the state of an aggregate must be preserved In case a late event occurs the query can then be reprocessed But this means that the state of all aggregates must kept indefinitely which causes the memory usage to grow to indefinitely too And that is not practical in a real world scenario unless the system has unlimited resources (resp an unlimited budget) Therefore watermarking is a useful concept to constrain the system by design and prevent it from exploding at runtimeSince Spark 21 watermarking is introduced into Structured Streaming API
GF3cUi6LyY2i6vh7vtU3zK,"Aalthough reading about database theory and database documentation is of the utmost importance it is also really important to get used to a system by just playing around To get a very strong grasp of SQL start by experimenting writing stupid queries to no end solving mathematics puzzles using queries creating a random game of chess using a query and whatnot Of course youll probably not get a chance to do any of this while at work so take time out especially if youre early stages of your career as a data person And obviously do it on your own machineUnderstanding how where clauses are evaluated how subqueries work how ordering works and how nulls behave in a database are some of the things youll get a deeper understanding of if you experiment Doing this will also prepare for any trick questions that you get asked during interviews However it is not very fashionable to ask tricky questions in interviews these daysAs a start look at this example  all of SQL comes down to the efficient use of WHERE clause It is important to understand how it behavesWhen you use integers in the WHERE clause it represents integers but when you use integers in GROUP BY and ORDER BY clauses the integers represent the positional number of the column in your result set (not in the table) Its another lazy trick to write the queries faster However it is recommended that you not use GROUP BY 1 3 ORDER BY 1 DESC as it leaves room for inducing errors while someone else edits the query in the future especially if it is a complex queryFor most databases youd find a dedicated piece of documentation talking about how to handle nulls Heres a piece from MySQLs official documentation talking about working with nulls and another one with information about problems/issues with nulls Reading these you can find out that you cannot use equality or comparison operators with null valuesMoving on Understanding logical operators is really important to write SQL queries Most SQL engines support all major local operators like AND OR NOT XORI made some interesting discoveries here Using tilde with an integer will just subtract that number from the biggest integer possible in the database and return the result But if you select ~ alone it will give you an error Regarding the interchangeability of AND and && it is recommended that you use AND instead of && or vice-versa throughout the codebase so as to have decent readability It could be confusing otherwise Also everyones favorite tool search & replace wont work properly""Another point about logical operators is that they support all the logical math that comes with them For instance SELECT NOT NOT 1 will give us 1 What happens when we do SELECT NOT NOT 'Nice' Here are a couple of examples to demonstrateWhen used with mathematical operators logical operators should be separated by parentheses or the database engine wont recognise the logical operator and consider it as a string and will throw an errorI find this method of exploring database behaviour really fascinating Most of the queries that have been written use the aforementioned principles which are built into the database engine most of the databases deal with null values but do we understand how the database is actually reading and interpreting our queries? I submit that to understand databases deeper one has to go through such random queries They are like math puzzles but obviously astronomically simplerThis is true for almost all databases that support SQL"
jGMwvoLNi5G7DcBuVTK2fD,Big Data is a buzzword circulating everyday in articles news interviews and where not Its one of the terms in IT that you can get quite philosophical about Ask everyone and everyone has something different to say With the most frequent answers being it-depends and others mainly defining Big Data after its qualities there isnt much much on defining it from other points of viewWhenever you are asked this question break down the answer and define Big Data from the perspective of qualities it posses (the most common definition) from the physical resources perspective and from data analysis perspectiveThis is the most common approach on defining Big Data It describes a couple of qualities that in coincidence all of them start with V They were four on the beginning: Volume Velocity Veracity Variety and later many others were introduced These qualities come at an increased amount Bigger volume higher velocity more challenges in veracity and more variety than you can think about This increase challenges the current state of technological advancements and requires further research and developments to deal with it The definition derived out of this is: Big Data is referred to the data that comes in increased volume velocity variety and more difficulties in veracity than we usually handle Data might have all of these qualities some of them or only one of themDefining Big Data on this way its a good starting point Ending here its okay but you havent yet punctuate the true meaning behindBig Data is an evolutionary term This because its tightly coupled to the current state of technological developments What was named Big Data at 2010 probably wouldt be called Big Data at 2020 And probably in years to come this term will fade away because we will have tools and resources that will not be challenged anymore Here you might see different definitions of what Big Data is and what it isnt that in some way or another relate with the physical resources used to process itIf traditional tools are challenged when handling data with increase in volume velocity veracity variety and you-name-it that data is called Big DataData that can entirely be processed in a single common machine no matter what qualities it posses that is not Big DataFor traditional tools you might think of the ones we usually install in our machines to process/analyse data easily Microsoft Excel is one of themBig Data is here because we want to know more about certain phenomenons and less speculate less about what they might be If we look back statistical analysis was there even though tech wasnt there Due to the difficulty of analysing the whole data for a phenomenon people extracted samplesThere are many ways how you can extract samples aiming to represent the entire dataset as close as possible Even though you can approximate on this you will never achieve an 100% correct representationFrom the analysis perspective in Big Data we want to have a full grasp of what is going on We want to consider every single data point during the analysis From here derives another definitionBig Data is about considering the entire data when analysing a phenomenon and not only part of itSure Big Data is more about getting general directions than accurate results but this doesnt nullify the fact that its better to go through every single data point than skipWhenever explaining what Big Data is sum up everything at the end by mentioning the tech developments that directly impact what we know as Big Data Cloud IoT and recently 5G all are big factors that enforce or relax the meaning behind Big DataCloud in one side is a big factor in relaxing Big Data challenges The benefits that come out of it especially elasticity of resources (infinite amount of resources for your needs) help enormously on easing the burden of dealing with Big BataIoT and recently 5G on the other side are constantly presenting Big Data challenges More data is generated and transmitted at higher speed that ever before These tech factors going head to head with one another ensure that Big Data is here to stay for a long timeBig Data is a term that encompasses various meanings Breaking up the answer in the parts mentioned above and explaining them not only provides a better understanding of what Big Data really is but ensures that you have also done an out-of-the-box thinking around it
BBhW5ZDSh6okpouLLaTuzy,Apache Airflow is an open-source scheduler to manage your regular jobs It is an excellent tool to organize execute and monitor your workflows so that they work seamlesslyApache Airflow solved a lot of problems that the predecessors faced Lets first understand the architecture and then well take a look at what makes Airflow betterDAGs (Directed Acyclic Graphs) represent a workflow in Airflow Each node in a DAG represents a task that needs to be run The user mentions the frequency at which a particular DAG needs to be run The user can also specify the trigger rule for each task in a DAG eg You may want to trigger an alert task right after one of the previous tasks failsLet us try to understand the various components of AirflowAirflow primary consists of the following components : It is responsible for scheduling your tasks according to the frequency mentioned It looks for all the eligible DAGs and then puts them in the queue If a DAG failed and retry is enabled the scheduler will automatically put that DAG up for retry The number of retries can be limited on a DAG levelThe webserver is the frontend for Airflow Users can enable/disable retry and see logs for a DAG all from the UI Users can also drill down in a DAG to see which tasks have failed what caused the failure how long did the task run for and when was the task last retriedThis UI makes Airflow superior to its competitors eg In Apache Oozie seeing logs for non-MR (map-reduce) jobs is a painIt is responsible for actually running a task Executor controls on which worker to run a task how many tasks to run in parallel and update the status of the task as it progressYou can run your task on multiple workers managed by Celery or Dask or KubernetesThe tasks are pulled from a queue which can be either Redis or RabbitMQBy default Airflow uses SerialExecutor which only runs one task at a time on a local machine This is not advised to be done in productionAirflow uses MySQL or PostgreSQL to store the configuration as well as the state of all the DAG and task runs By default Airflow uses SQLite as a backend by default so no external setup is required The SQLite backend is not recommended for production since data loss is probableAirflow provides various methods of monitoring You can see the status of the tasks from the UI It sends an mail in case a DAG fails You can also send the email if a task breaches the defined SLA The logs for a task can also be viewed from the Airflow UI itselfThis feature came pretty recently in Airflow v110 Lineage allows you to track the origins of data what happens to it and where it moves over time such as Hive table or S3/HDFS partitionIt comes pretty handily when you multiple data tasks reading and writing into storage The user needs to define the input and output data sources for each task and a graph is created in Apache Atlas depicting the relationship between various data sourcesSensors allow a user to trigger a task based on a certain pre-condition The user needs to specify the type of sensor and the frequency at which they need to check for the condition eg You can use the HDFS Partition sensor to trigger a task when a particular partition such as date is availableAirflow also allows users to create their operators and sensors in case an already rich ecosystem of existing ones doesnt satisfy your requirements I wrote a SparkOperator because the official one didnt allow me to tweak all the parameters All the code is written in Python which makes it easy for any developer to integrateApart from all the benefits mentioned above Airflow also has seamless integration with all the services in big data ecosystems such as Hadoop Spark etc Since all the code is written in Python getting started with Airflow will only take a couple of minutes You can take a look at the official quickstart guideYou can also explore https://databandai/ for a much more powerful setup to monitor your data pipelines powered by Apache Airflow
5aSZTd7MFUtSz4jp9LZZQw,Spark does things fast That has always been the frameworks main selling point since it was first introduced back in 2010Offering a memory-based alternative to Map-Reduce gave the Big Data ecosystem a major boost and throughout the past few years it represented one of the key reasons for which companies adopted Big Data systemsWith its vast range of use cases its ease-of-use and its record-setting capabilities Spark rapidly became everyones go-to framework when it comes to data processing within a Big Data architectureOne of Sparks key components is its SparkSQL module that offers the possibility to write batch Spark jobs as SQL-like queries To do so Spark relies behind the scenes on a complex mechanism to run these queries through the execution engine This mechanisms centerpiece is Catalyst: Sparks query optimizer that does much of the heavy-lifting by generating the jobs physical execution planEven though every step of this process was meticulously refined to optimize every aspect of the job There is still plenty you could do from your end of the chain to make your Spark jobs run even faster But before getting into that lets take a deeper dive into how Catalyst does thingsSpark offers multiple ways to interact with its SparkSQL interfaces with the main APIs being DataSet and DataFrame These high-level APIs were built upon the object-oriented RDD API And they kept its main characteristics while adding certain key features like the usage of schemas (For a detailed comparison please refer to this article on the Databricks blog)The choice of the API to use depends mainly on the language youre using With DataSet being only available in Scala / Java and replacing DataFrame for these languages since the release of Spark 20 And each one offers certain perks and advantages The good news is that Spark uses the same execution engine under the hood to run your computations so you can switch easily from one API to another without worrying about whats happening on the execution levelThat means that no matter which API youre using when you submit your job itll go through a unified optimization processThe operations you can do within your Spark application are divided into two types: The DataFrame and DataSet operations are divided into the same categories since these APIs are built upon the RDD mechanismThe next differentiation to make is between the two types of transformations which are the following: So when you submit a job to Spark what youre submitting is basically a set of actions and transformations that are then turned into the jobs logical plan by Catalyst before it generates the ideal physical planNow that we know how Spark sees the jobs that we submit to it lets go through the mechanisms that turn that list of actions and transformations into the jobs physical execution planFirst of all an important concept to remember when working with Spark is that it relies on lazy evaluation That means that when you submit a job Spark will only do its magic when it must to  ie when it receives an action (like when the driver asks for some data or when it needs to store data into HDFS)Instead of running the transformations one by one as soon as it receives them Spark stores these transformations in a DAG (Directed Acyclic Graph) and as soon as it receives an action it runs the whole DAG and delivers the requested output This enables it to optimize its execution plan based on the jobs DAG instead of running the transformations sequentiallySpark relies on Catalyst its optimizer to perform the necessary optimizations to generate the most efficient execution plan At its core Catalyst includes a general library dedicated to representing trees and applying rules to manipulate them It leverages functional programming constructs in Scala and offers libraries specific to relational query processingCatalysts main data type is a tree composed of node objects on which it applies a set of rules to optimize it These optimizations are performed via four different phases as indicated in the diagram below: One distinction that may not be very clear at first is the usage of the terms logical plan and physical plan To put it simply a logical plan consists of a tree describing what needs to be done without implying how to do it whereas a physical plan describes exactly what every node in the tree would doFor example a logical plan simply indicates that theres a join operation that needs to be done while the physical plan fixes the join type (eg ShuffleHashJoin) for that specific operationNow lets go through these four steps and delve deeper into Catalysts logicThe starting point for the Catalyst optimization pipeline is a set of unresolved attribute references or relations Whether youre using SQL or the DataFrame/Dataset APIs SparkSQL has no idea at first on your data types or even the existence of the columns that youre referring to (this is what we mean by unresolved) If you submit a select query SparkSQL will first use Catalyst to determine the type of every column you pass and whether the columns youre using actually exist To do so it relies mainly on Catalysts trees and rules mechanismsIt first creates a tree for the unresolved logical plan then starts applying rules on it until it resolves all of the attribute references and relations Throughout this process Catalyst relies on a Catalog object that tracks the tables in all data sourcesIn this phase Catalyst gets some help With the release of Spark 22 in 2017 a cost-based optimizer framework was introduced Contrarily to rule-based optimizations a cost-based optimizer uses statistics and cardinalities to find the most efficient execution plan instead of simply applying a set of rulesThe output of the analysis step is a logical plan that then goes through a series of rule-based and cost-based optimizations in this second step Catalyst applies all of the optimization rules on the logical plan and works with the cost-based optimizer to deliver an optimized logical plan to the next stepJust like the previous step SparkSQL uses both Catalyst and the cost-based optimizer for the physical planning It generates multiple physical plans based on the optimized logical plan before leveraging a set of physical rules and statistics to offer the most efficient physical planFinally Catalyst uses quasiquotes a special feature offered by Scala to generate the Java bytecode to run on each machine Catalyst uses this feature by transforming the jobs tree into an abstract syntax tree (AST) that is evaluated by Scala which then compiles and runs the generated codeSpark SQL relies on a sophisticated pipeline to optimize the jobs that it needs to execute and it uses Catalyst its optimizer in all of the steps of this process This optimization mechanism is one of the main reasons for Sparks astronomical performance and its effectivenessNow that we examined Sparks sophisticated optimization process its clear to us that Spark relies on a meticulously crafted mechanism to achieve its mind-boggling speed But to think that Spark will give you optimal results no matter how you do things on your side is a mistakeThe assumption is easily made especially when migrating from another data-processing tool A 50% shrink in processing time compared to the tool that youve been using could make you believe that Spark is running at full-speed and that you cant reduce the execution time any further The thing is you canSpark SQL and its optimizer Catalyst can do wonders on their own via the process we discussed above but through some twists and techniques you can take Spark to the next levelThe first thing to keep in mind when working with Spark is that the execution time doesnt have much significance on its own To evaluate the jobs performance its important to know whats happening under the hood while its running During the development and testing phases you need to frequently use the explain function to see the physical plan generated from the statements you wish to analyze and for an in-depth analysis you could add the extended flag to see the different plans that Spark SQL opted for (from the parsed logical plan to the physical plan) This is a great way to detect potential problems and unnecessary stages without even having to actually execute the jobCaching is very important when dealing with large datasets and complex jobs It allows you to save the datasets that you plan on using in subsequent stages so that Spark doesnt create them again from scratch This advantage sometimes pushes developers into over-caching in a way that makes the cached datasets a burden that slows down your job instead of optimizing it To decide which datasets you need to cache you have to prepare the totality of your job and then through testing try to figure out which datasets are actually worth caching and at which point you could unpersist them to free up the space they occupy in memory when cached Using the cache efficiently allows Spark to run certain computations 10 times faster which could dramatically reduce the total execution time of your jobA key element to getting the most out of Spark is fine-tuning its configuration according to your cluster Relying on the default configuration may be the way to go in certain situations but usually youre one parameter away from getting even more impressive results Selecting the appropriate number of executors the number of cores per executor and the memory size for each executor are all elements that could greatly influence the performance of your jobs so dont hesitate to perform benchmark testing to see if certain parameters could be optimizedFinally an important factor to keep in mind is that you need to know the data that youre dealing with and what to expect from every operation When one stage is taking too long even though its dealing with less data than other stages then you should inspect whats happening on the other side Spark is great when it comes to doing the heavy-lifting and running your code but only you could detect business-related issues that may be related to the way you defined your jobIf you apply all of these rules while developing and implementing your Spark jobs you can expect the record-breaking processing tool to reward you with jaw-dropping resultsThese recommendations are merely a first step towards mastering Apache Spark In upcoming articles well discuss its different modules in detail to get an even better understanding of how Spark functionsThis article was first published on INVIVOOs tech blog
24bZgC7iKRtRj8M25Ps7A3,Youve been asked to build a new data platform for your organisation Exciting right? Except youre more likely to fail than succeedGartner estimates that 60% of data projects fail with some analysts suggesting that is a conservative estimateHere are some of the reasons I believe cause data projects to failData has become one of the most important business assets but most organisations place no value in it Data is often seen as an offshoot of functionality with little thought given to how it can be used to drive value in the organisationData is the new oil has become an oft-used phrase to describe the value of data Crude oil is unrefined and whilst it has some intrinsic value it requires further processing before its true value is realised Too many organisations are content to just collect crude data with little thought given to its refinement into useful productsConways law usually applied to software development states: organizations which design systems … are constrained to produce designs which are copies of the communication structures of these organizationsThis law holds and is probably more accurate in the data space It is common for each area of the organisation to store its data in a structure and place that only works for its use cases with little thought given to how it may be shared or utilised by other areas of the organisationWithout a centralised function working to define a organisation-wide solution data silos are likely to occur with data stored in separate datastores Even in the better scenario of data being stored in a centralised location the data structures are not approached with an open-mindset This leads to less obvious but just as problematic silosImmature organisations often see data as a technical problem that can be solved with technology usually suggesting data is produced by technical systems and stored in technical solutionsThis leads to data solutions being worked on by technical teams in isolation from the rest of the organisation Architecture of the data solution is often performed by software architects and developers with little context or understanding of the nuances of the dataNaturally this leads to sub-optimal solutions which either results in delivery of a platform that doesnt work for the majority of end users or a long and tortuous delivery process resulting in lots of re-design and re-workOrganisations and their leaders are being seduced by the promise of increased performance born of data-driven decision making This leads to statements from the top such as: Data is our number 1 priorityEvery area of our organisation must utilise data in their decision makingNo new product or system can be delivered without capturing dataWhilst these sound like useful edicts they can have unexpected consequences Often it instills a sense of data frenzy in organisations which results in ill-defined requirements and the capturing of data with no thought of how it can and will be usedData frenzy can also manifest itself in a demand to do everything at once because of the need to be seen to be doing data This results in decisions being made with little focus on how this will benefit the organisation and more of a box-tick to ensure you are not accused of not doing the right thingOrganisations often have long-established datastores already in place Data warehouses have been around a long time and they are used frequently to deliver analysisHowever the skills used to deliver these and to analyse the data held within them are not the same as building a modern data platform and to provide deep and meaningful insight from the data held withinMost organisations fail to understand that the transition to a modern data platform will require large skill shifts in the existing workforce and struggle to plan for the closure of the gapThe demand for highly-skilled data-literate resources has surged in recent years and most organisations are not willing to invest in recruiting these individuals and are content to try to leverage their existing workforce This is a false economy Assuming you have a great data platform you will get nowhere near the true value you could from it without great people working with itData projects most often fail due to people problems Technical problems can usually be solved fairly easily but people are a much harder and nuanced issueIt is important to realise that for your data project to succeed it has to be a holistic approach including people from all areas of your organisation working with an open collaborative mindsetWithout this your data project will (probably) fail
LvgYGF9QbZjiTwDkEmaAAT,I assume that you are already familiar with AWS Cloud platform especially with Lambda and EMR services I assume that you already have an EMR cluster running and know how to set up a lambda function This article covers an end-to-end topic of creating a PySpark project and submitting it to EMR At the end this article results in a PySpark project called pyspark-seed that is ready to be cloned and further developed on top of itMy go to IDE for Python projects is PyCharm Jupyter Notebook is always there for quick checks To build the Python virtual environment I will go with venv Venv is simple and comes preinstalled with your Python interpreter For Python I will go with version 37; for PySpark I will go with version 245 A continuous integration configuration file is provided and is ready to get executed via GitLab Runner I do also host the source code of this pyspark-seed project in GitHub due to larger communityBefore even creating the project let us discuss about the deployment! Yes deployment happens the last but its importance should be discussed at the beginning of the project Why? Because most of the times the way how you deploy the project affects the project structure and code organisationIn a PySpark project you can deploy stand-alone scripts or you can deploy packed/zipped projects Deploy stand-alone scripts if you have few simple jobs that do not share functionality among each other Deploy a packed/zipped project when you have multiple jobs that share functionality among each otherWhen packing spark jobs written in Java or Scala you create a single jar file If packed correctly submitting this single jar file in EMR will run the job successfully To submit a PySpark project in EMR you need to have two things: I have created a Python project called pyspark-seed The structure of the project is as follows: I use venv to create the isolated Python environment Python binaries inside this environment are identical to the ones in the Python you used to create this environment Modules installed in this virtual environment are independent from the ones installed in your local/system Python To create a Python environment type the following commands in your project root directory (ie /pyspark-seed)The first command creates a Python environment This will make a directory named venv in your project structure The second command will activate the Python environment created The last command will run setuppy and install/setup the projectThis files responsibility is to properly setup your Python project You can specify the name of your project version provide a description of your project author name packages and much more A simple setuppy is as follows: Specifying the project version in __version__ variable allows you to access it during CI and use it to generate a path where you will store artefacts (ie seed_module and mainpy) in s3 Accessing this variable is as simple as: A CI pipeline for a PySpark project usually has three base stages: build test and deployPython 37-stretch is used as base image This version is required if you want to install PySpark and run PySpark tests during the test phase This pipeline is linear Every stage runs after the previous one finishes successfully Stages build and test run automatically whereas deploy should be triggered manually for the deployment to happen AWS credentials are stored as environment variables in GitLab When storing artifacts in s3 we always overwrite the latest path content with the latest changesEvery job should be written in a separate Python file To keep things simple every job should have one single function called process that receives at least spark_session input_path and output_path as parameters These parameters the job name we want to run and other are specified in the lambda function discussed at the next section A simple template of a job is shown belowAWS Lambda is a serverless service You can schedule lambda code runs through AWS CloudWatch trigger code runs as response to an event or even trigger lambda functions on-demand through API calls A lambda function that submits jobs to EMR is presented below This lambda function runs in Python 37I have defined main_path and modules_path that by default point at the latest version of the artefact Arguments specific to your main function are passed in spark-submit after the main_path I prefer setting all parameters in a dictionary cast dictionary to string and pass this whole string as a single parameter to the main When received this string dictionary i use ast module to extract the dictionary out of itThis dictionary of parameters then is passed to runpy function which sets up the spark session with configs provided and runs the job stated in parametersConsider a use case where a single data file (size~3GB) is dumped in s3 raw data bucket every 20 min Consider that an event listener is set to this bucket properties This event listener listens for all object creation events If an object is uploaded in this bucket the listener catches it and triggers the target lambda function This lambda function gets the object path from the event message provides the job parameters and submits a job in EMR The following diagram shows this workflow from the beginning to the endIn this article I have described the process of starting a PySpark project creating a CI configuration file deploy artifacts in S3 and submitting jobs in EMR through Lambda Functions Most of the advice provided are taken directly from my personal experience using PySpark in production and research done I have hosted the source code for this pyspark-seed project in Github Lots of other details can be learned by exploring this repository yourself Feel free to clone it and make it betterOf course that many aspect of this project can be done differently I intended to provide this seed project as a starting point that can furthermore be developed All questions feedback and critiques are welcomed I believe in the world where critiques drive change
SKHPbZVKZAsoVyTh2yFtjF,At WorkMarket like many other growth-stage startups were in the midst of a transition from a monolithic architecture to a microservices architectureEvery team has had to plan for the transition in its own way For us the Data Team this meant reevaluating our current ETL (Extract Transform Load) processes and pipelines in order to plan for the increasing complexity that comes with maintaining more data sources and repositories than can be handled manuallyCurrently we use AWS Data Pipeline to move data from our MySQL monolith database to our Redshift warehouse We have a Github repo that houses our Redshift create table DDL (Data Definition Language) and a pipeline initialization script When theres a new pipeline to add you submit a PR that includes the DDL for the Redshift table youd like added Our DBA (Database Administrator) then runs the pipeline initialization script using your DDL Finally the pipeline goes live and your data begins publishing to the warehouseIts worked okay for us so far While it was the appropriate solution with a data engineering team of one there are issues with this setup going forward: Ultimately given the chance to try something new with our ETL we were curious to see what other tools were out there to help meet our needs going forwardMonoliths mean singular databases and for our current pipelines are well and good A microservices architecture however neednt be limited to such strict engineering paradigms Further while we as an engineering team have standardized on Java + MySQL for our language and database for now thats not necessarily going to be true forever or for all microservices (Parts of our data stack for instance rely on Python and PostgresOur Platform team uses Kafka to publish logs and metrics from new microservices as they come onlineWe started with the source: Confluent Confluent was started by the people behind Apache Kafka and who now develop a popular streaming platform built on Kafka It seemed like an obvious choice
