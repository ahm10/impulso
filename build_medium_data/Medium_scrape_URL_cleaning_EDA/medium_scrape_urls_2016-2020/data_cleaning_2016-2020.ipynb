{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medium Data Data Cleaning\n",
    "Clean the data scrapped from medium archive pages with the scrape_master.py file. \n",
    "\n",
    "Each archive was <b>scraped for each day between Jan 2016 and Sep 2020.</b>.\n",
    "\n",
    "#### The following Tags are Scraped\n",
    "['r','python', 'data-science','machine-learning', 'artificial-intelligence','deep-learning',data-engineering', 'data-analytics', 'statistics', 'reinforcement-learning']\n",
    "\n",
    "## Purpose of the Data\n",
    " 1. To <b>create a performance metric for Medium's authors</b>, so they can compare their work to the rest of Medium.\n",
    " 2. To <b>compare the performance of authors and publications</b> on Medium.\n",
    " 3. To <b>create a leaderboard</b> of the top performing authors and publications in each tag .\n",
    " \n",
    " 4. To <b>find the differences that distinguish well-received articles.</b>\n",
    " \n",
    " \n",
    "\n",
    "\n",
    "## Structure of the data\n",
    "- Title\n",
    "- Subtitle \n",
    "- Image (yes/no)\n",
    "- Author\n",
    "- Publication\n",
    "- Year - Month - Day\n",
    "- Tag\n",
    "- Reading Time\n",
    "- Claps\n",
    "- Comment (yes/no)\n",
    "- Story Url\n",
    "- Author URL\n",
    "\n",
    "<img src=\"img/card.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Subtitle</th>\n",
       "      <th>Image</th>\n",
       "      <th>Author</th>\n",
       "      <th>Publication</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Reading_Time</th>\n",
       "      <th>Claps</th>\n",
       "      <th>Comment</th>\n",
       "      <th>url</th>\n",
       "      <th>Author_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Per Harald Borgen</td>\n",
       "      <td>Learning New Stuff</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>artificial-intelligence</td>\n",
       "      <td>7</td>\n",
       "      <td>1.6K</td>\n",
       "      <td>0</td>\n",
       "      <td>https://medium.com/learning-new-stuff/how-to-l...</td>\n",
       "      <td>https://medium.com/@perborgen?source=tag_archi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016: The Future of User Experience</td>\n",
       "      <td>Whats happened with UX?</td>\n",
       "      <td>1</td>\n",
       "      <td>Krish Ramineni</td>\n",
       "      <td>Technology, Invention, App, and More</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>artificial-intelligence</td>\n",
       "      <td>5</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>https://medium.com/technology-invention-and-mo...</td>\n",
       "      <td>https://medium.com/@krishramineni?source=tag_a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Computing all the Feels</td>\n",
       "      <td>#PostsFromTheNearFuture</td>\n",
       "      <td>1</td>\n",
       "      <td>Clayton d'Arnault</td>\n",
       "      <td>Digital Culturist</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>artificial-intelligence</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>https://digitalculturist.com/computing-all-the...</td>\n",
       "      <td>https://digitalculturist.com/@cjdarnault?sourc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trying to Muse Rationally About the Singularit...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>/r/21dotco</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>artificial-intelligence</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>https://medium.com/@emergingtechnology/trying-...</td>\n",
       "      <td>https://medium.com/@emergingtechnology?source=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Being Good Enough</td>\n",
       "      <td>November 2nd, 2008</td>\n",
       "      <td>1</td>\n",
       "      <td>/r/21dotco</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>artificial-intelligence</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://medium.com/@emergingtechnology/being-g...</td>\n",
       "      <td>https://medium.com/@emergingtechnology?source=...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title                 Subtitle  \\\n",
       "0                                                NaN                      NaN   \n",
       "1                2016: The Future of User Experience  Whats happened with UX?   \n",
       "2                            Computing all the Feels  #PostsFromTheNearFuture   \n",
       "3  Trying to Muse Rationally About the Singularit...                      NaN   \n",
       "4                                  Being Good Enough       November 2nd, 2008   \n",
       "\n",
       "   Image             Author                           Publication  Year  \\\n",
       "0      1  Per Harald Borgen                    Learning New Stuff  2016   \n",
       "1      1     Krish Ramineni  Technology, Invention, App, and More  2016   \n",
       "2      1  Clayton d'Arnault                     Digital Culturist  2016   \n",
       "3      1         /r/21dotco                                   NaN  2016   \n",
       "4      1         /r/21dotco                                   NaN  2016   \n",
       "\n",
       "   Month  Day                      Tag  Reading_Time Claps  Comment  \\\n",
       "0      1    1  artificial-intelligence             7  1.6K        0   \n",
       "1      1    1  artificial-intelligence             5    42        0   \n",
       "2      1    1  artificial-intelligence             7    30        0   \n",
       "3      1    1  artificial-intelligence            17    12        0   \n",
       "4      1    1  artificial-intelligence            16     0        0   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://medium.com/learning-new-stuff/how-to-l...   \n",
       "1  https://medium.com/technology-invention-and-mo...   \n",
       "2  https://digitalculturist.com/computing-all-the...   \n",
       "3  https://medium.com/@emergingtechnology/trying-...   \n",
       "4  https://medium.com/@emergingtechnology/being-g...   \n",
       "\n",
       "                                          Author_url  \n",
       "0  https://medium.com/@perborgen?source=tag_archi...  \n",
       "1  https://medium.com/@krishramineni?source=tag_a...  \n",
       "2  https://digitalculturist.com/@cjdarnault?sourc...  \n",
       "3  https://medium.com/@emergingtechnology?source=...  \n",
       "4  https://medium.com/@emergingtechnology?source=...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# medium = pd.read_csv('Medium_Scrapermedium_artificial-intelligence_2009-2016.csv')\n",
    "scraped_files = glob.glob(\"scraped_tags/*.csv\")\n",
    "\n",
    "frames =[]\n",
    "for file in scraped_files:\n",
    "    #all of the seperate scrapes from different tags\n",
    "    df = pd.read_csv(file)\n",
    "    frames.append(df)\n",
    "medium = pd.concat(frames)\n",
    "medium.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles scraped (before cleaning):  508028\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of articles scraped (before cleaning): \",medium.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# Converting Strings to Floats\n",
    "\n",
    "Before we can work with the data we need to <b>convert the \"Claps\" column from string to float values</b>. Note that the Object datatype is non-numeric. There is also an issue with <b>Claps in the form of \"5.5K\", rather than \"5500\".</b>\n",
    "\n",
    "### Preview of DataTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Title                           object\n",
       "Subtitle                        object\n",
       "Image                            int64\n",
       "Author                          object\n",
       "Publication                     object\n",
       "Year                             int64\n",
       "Month                            int64\n",
       "Day                              int64\n",
       "Reading_Time                     int64\n",
       "Claps                          float64\n",
       "url                             object\n",
       "Author_url                      object\n",
       "Tag_artificial-intelligence      uint8\n",
       "Tag_data-analytics               uint8\n",
       "Tag_data-engineering             uint8\n",
       "Tag_data-science                 uint8\n",
       "Tag_deep-learning                uint8\n",
       "Tag_machine-learning             uint8\n",
       "Tag_nlp                          uint8\n",
       "Tag_python                       uint8\n",
       "Tag_r                            uint8\n",
       "Tag_reinforcement-learning       uint8\n",
       "Tag_statistics                   uint8\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medium.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reformatting Clap Information to Floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clap dtype:  float64\n"
     ]
    }
   ],
   "source": [
    "#Claps entries higher than 999 are written \"5.5K\"\n",
    "# here we remove the \"K\", convert the string to float, then multiply by 1000.\n",
    "numeric_claps = []\n",
    "for x in medium.Claps:\n",
    "    if \"K\" in str(x):\n",
    "        numeric_claps.append(float(x[:-1])*1000)\n",
    "    else:\n",
    "        numeric_claps.append(x)\n",
    "medium[\"Claps\"] = numeric_claps\n",
    "medium[\"Claps\"] = pd.to_numeric(medium[\"Claps\"])\n",
    "print(\"Clap dtype: \", medium.dtypes[\"Claps\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# Removing Comment Entries\n",
    "Comment entries have been encoded into the data with the Comment column. Since these entries are not articles, I remove them in the following script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Entries to be removed:  18229\n",
      "Percentage of remaining data:  3.59 %\n"
     ]
    }
   ],
   "source": [
    "no_comm = medium[medium.Comment==0]\n",
    "no_comm = no_comm.drop([\"Comment\"], axis=1)\n",
    "print(\"Number of Entries to be removed: \", medium.shape[0]-no_comm.shape[0])\n",
    "print(\"Percentage of remaining data: \" ,round(((medium.shape[0]-no_comm.shape[0])/medium.shape[0])*100,2), \"%\")\n",
    "medium = no_comm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning up  Urls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://medium.com/learning-new-stuff/how-to-learn-neural-networks-758b78f2736e?source=tag_archive---------0-----------------------\n",
      "https://medium.com/technology-invention-and-more/2016-the-future-of-user-experience-6d1b7ef3481f?source=tag_archive---------1-----------------------\n",
      "https://digitalculturist.com/computing-all-the-feels-ed567049a4?source=tag_archive---------2-----------------------\n"
     ]
    }
   ],
   "source": [
    "#before\n",
    "for i in range(3):\n",
    "    print(medium.url.values[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "medium.url = medium.url.str.split(\"?\", expand=True)\n",
    "medium.Author_url = medium.Author_url.str.split(\"?\", expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://medium.com/learning-new-stuff/how-to-learn-neural-networks-758b78f2736e\n",
      "https://medium.com/@perborgen\n",
      "https://medium.com/technology-invention-and-more/2016-the-future-of-user-experience-6d1b7ef3481f\n",
      "https://medium.com/@krishramineni\n",
      "https://digitalculturist.com/computing-all-the-feels-ed567049a4\n",
      "https://digitalculturist.com/@cjdarnault\n"
     ]
    }
   ],
   "source": [
    "#after\n",
    "for i in range(3):\n",
    "    print(medium.url.values[i])\n",
    "    print(medium.Author_url.values[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Checking for Non Entries in the Data\n",
    "\n",
    "\n",
    "### All NaNs in Each Column\n",
    "We only have missing values in Title, Subtitle, or Publication. <b>NaNs in publication column because not all articles are published. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaNs\n",
      "Title                15758\n",
      "Subtitle            185827\n",
      "Image                    0\n",
      "Author                1149\n",
      "Publication         248750\n",
      "Year                     0\n",
      "Month                    0\n",
      "Day                      0\n",
      "Tag                      0\n",
      "Reading_Time             0\n",
      "Claps                    0\n",
      "url                      0\n",
      "Author_url            1149\n",
      "\n",
      "Total Entries:   489799\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of NaNs\")\n",
    "for x in range(13):\n",
    "    print(\"%-15s %10d\" % (medium.columns.values[x], medium.iloc[:,x].isna().sum()))\n",
    "print()\n",
    "print(\"Total Entries:  \", medium.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove NaN Authors\n",
    "Medium is doing something weird with adding existing articles from sites like pcmag.com. The cards on the archive timeline have neither author nor publication. Since there are only a coulple hundred entries withou an author, I choose to remove these from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# medium = medium[medium.Author.notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NaN Title and Subtitle Entries\n",
    "Sometimes when scraping the archive page, Titles are in weird formats. The result, <b> some articles titles are scraped as subtitles</b>.\n",
    "\n",
    "Here is a breakdown of the NonEntries in Title/SubTitle Columns. I choose to keep these in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total NaN Title Entries:  15758\n",
      "Entries with NaN Title but existing SubTitle:  7035\n",
      "Entries with neither title nor subtitle:  8723\n"
     ]
    }
   ],
   "source": [
    "#Total entries with no Title\n",
    "print(\"Total NaN Title Entries: \", medium[medium.Title.isnull()].shape[0])\n",
    "\n",
    "#Entries with no title but with a subtitle\n",
    "print(\"Entries with NaN Title but existing SubTitle: \",medium[(medium.Title.isnull() & medium.Subtitle.notnull())].shape[0])\n",
    "\n",
    "#Neither Possible explanations?\n",
    "print(\"Entries with neither title nor subtitle: \", medium[(medium.Title.isnull() & medium.Subtitle.isnull())].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaNs\n",
      "Title                15758\n",
      "Subtitle            185827\n",
      "Image                    0\n",
      "Author                1149\n",
      "Publication         248750\n",
      "Year                     0\n",
      "Month                    0\n",
      "Day                      0\n",
      "Tag                      0\n",
      "Reading_Time             0\n",
      "Claps                    0\n",
      "url                      0\n",
      "Author_url            1149\n",
      "\n",
      "Total Entries:   489799\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of NaNs\")\n",
    "for x in range(13):\n",
    "    print(\"%-15s %10d\" % (medium.columns.values[x], medium.iloc[:,x].isna().sum()))\n",
    "print()\n",
    "print(\"Total Entries:  \", medium.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Duplicate Articles with duplicated URLs and Multi-tagged\n",
    "Medium allows an  author to include 5 tags for each story.\n",
    "\n",
    "When we scraped the archive page, we scraped each individual tag. <b>As a result, stories will appear multiple times in our data (with different tags)</b>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are:  283166 Duplicated URL entries.\n",
      "Unique posts with duplicate urls:  110214\n",
      "Total unique urls:  316847\n"
     ]
    }
   ],
   "source": [
    "#multi_urls is all entries in the dataset that have duplicates (includes all duplicates)\n",
    "multi_urls = medium[medium.duplicated(subset=[\"url\"], keep=False)]\n",
    "print(\"There are: \", multi_urls.shape[0], \"Duplicated URL entries.\")\n",
    "print(\"Unique posts with duplicate urls: \", multi_urls.shape[0]- medium[medium.duplicated(subset=[\"url\"], keep=\"last\")].shape[0])\n",
    "print(\"Total unique urls: \", medium.shape[0]- medium[medium.duplicated(subset=[\"url\"], keep=\"last\")].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are:  260230 Duplicated tag entries.\n",
      "Unique posts with multiple tags:  109508\n"
     ]
    }
   ],
   "source": [
    "#one hot encode the tags \n",
    "medium = pd.get_dummies(medium, columns = [\"Tag\"])\n",
    "\n",
    "#multi_tags is all entries in the dataset that have duplicates (includes all duplicates)\n",
    "multi_tags = medium[medium.duplicated(subset=[\"url\", \"Year\", \"Month\",\"Day\"], keep=False)]\n",
    "print(\"There are: \", multi_tags.shape[0], \"Duplicated tag entries.\")\n",
    "print(\"Unique posts with multiple tags: \", multi_tags.shape[0]- medium[medium.duplicated(subset=[\"url\", \"Year\", \"Month\",\"Day\"], keep=\"last\")].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Remove all but one of each duplicate entry, then sort by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "316847"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#keep only one entry of each duplicated article\n",
    "sort_url = medium[~medium.duplicated(subset=[\"url\"], keep=\"last\")]\n",
    "\n",
    "#sort the entry to put it in the exact same order as the groupby above\n",
    "medium_clean = sort_url.sort_values([\"url\",\"Year\",\"Month\",\"Day\"]).reset_index().drop(\"index\",axis=1)\n",
    "\n",
    "# medium_clean.shape[0]\n",
    "# medium_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "How much data do we have after cleaning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of after cleaning:  316847\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of after cleaning: \", medium_clean.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_clean.to_csv(\"Medium_scrape_urls_multi-tag _clean_2016-2020.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
