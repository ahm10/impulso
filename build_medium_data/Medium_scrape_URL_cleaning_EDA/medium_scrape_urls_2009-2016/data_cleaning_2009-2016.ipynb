{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medium Data Data Cleaning\n",
    "Clean the data scrapped from medium archive pages with the scrape_master.py file. \n",
    "\n",
    "Each archive was <b>scraped for each day between Jan 2009 and Sep 2016.</b>.\n",
    "\n",
    "#### The following Tags are Scraped\n",
    "['r','python', 'data-science','machine-learning', 'artificial-intelligence','deep-learning',data-engineering', 'data-analytics', 'statistics', 'reinforcement-learning']\n",
    "\n",
    "## Purpose of the Data\n",
    " 1. To <b>create a performance metric for Medium's authors</b>, so they can compare their work to the rest of Medium.\n",
    " 2. To <b>compare the performance of authors and publications</b> on Medium.\n",
    " 3. To <b>create a leaderboard</b> of the top performing authors and publications in each tag .\n",
    " \n",
    " 4. To <b>find the differences that distinguish well-received articles.</b>\n",
    " \n",
    " \n",
    "\n",
    "\n",
    "## Structure of the data\n",
    "- Title\n",
    "- Subtitle \n",
    "- Image (yes/no)\n",
    "- Author\n",
    "- Publication\n",
    "- Year - Month - Day\n",
    "- Tag\n",
    "- Reading Time\n",
    "- Claps\n",
    "- Comment (yes/no)\n",
    "- Story Url\n",
    "- Author URL\n",
    "\n",
    "<img src=\"img/card.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Subtitle</th>\n",
       "      <th>Image</th>\n",
       "      <th>Author</th>\n",
       "      <th>Publication</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Reading_Time</th>\n",
       "      <th>Claps</th>\n",
       "      <th>Comment</th>\n",
       "      <th>url</th>\n",
       "      <th>Author_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Machine Learning is Fun! Part 4: Modern Face R...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Adam Geitgey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>artificial-intelligence</td>\n",
       "      <td>13</td>\n",
       "      <td>28K</td>\n",
       "      <td>0</td>\n",
       "      <td>https://medium.com/@ageitgey/machine-learning-...</td>\n",
       "      <td>https://medium.com/@ageitgey?source=tag_archiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Machine Learning is Fun! Part 3: Deep Learning...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Adam Geitgey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>artificial-intelligence</td>\n",
       "      <td>16</td>\n",
       "      <td>25K</td>\n",
       "      <td>0</td>\n",
       "      <td>https://medium.com/@ageitgey/machine-learning-...</td>\n",
       "      <td>https://medium.com/@ageitgey?source=tag_archiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Deep Learning Is Going to Teach Us All the Les...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Scott Santens</td>\n",
       "      <td>Basic Income</td>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>artificial-intelligence</td>\n",
       "      <td>14</td>\n",
       "      <td>9.4K</td>\n",
       "      <td>0</td>\n",
       "      <td>https://medium.com/basic-income/deep-learning-...</td>\n",
       "      <td>https://medium.com/@2noame?source=tag_archive-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How to build your own Neural Network from scra...</td>\n",
       "      <td>A beginners guide to understanding the</td>\n",
       "      <td>1</td>\n",
       "      <td>James Loy</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>artificial-intelligence</td>\n",
       "      <td>7</td>\n",
       "      <td>43K</td>\n",
       "      <td>0</td>\n",
       "      <td>https://towardsdatascience.com/how-to-build-yo...</td>\n",
       "      <td>https://towardsdatascience.com/@jamesloyys?sou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Complete Beginners Guide To Chatbots</td>\n",
       "      <td>Everything you need to know.</td>\n",
       "      <td>1</td>\n",
       "      <td>Matt Schlicht</td>\n",
       "      <td>Chatbots Magazine</td>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>artificial-intelligence</td>\n",
       "      <td>11</td>\n",
       "      <td>5.6K</td>\n",
       "      <td>0</td>\n",
       "      <td>https://chatbotsmagazine.com/the-complete-begi...</td>\n",
       "      <td>https://chatbotsmagazine.com/@mattprd?source=t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  Machine Learning is Fun! Part 4: Modern Face R...   \n",
       "1  Machine Learning is Fun! Part 3: Deep Learning...   \n",
       "2  Deep Learning Is Going to Teach Us All the Les...   \n",
       "3  How to build your own Neural Network from scra...   \n",
       "4           The Complete Beginners Guide To Chatbots   \n",
       "\n",
       "                                 Subtitle  Image         Author  \\\n",
       "0                                     NaN      1   Adam Geitgey   \n",
       "1                                     NaN      1   Adam Geitgey   \n",
       "2                                     NaN      1  Scott Santens   \n",
       "3  A beginners guide to understanding the      1      James Loy   \n",
       "4            Everything you need to know.      1  Matt Schlicht   \n",
       "\n",
       "            Publication  Year  Month  Day                      Tag  \\\n",
       "0                   NaN  2009      1    1  artificial-intelligence   \n",
       "1                   NaN  2009      1    1  artificial-intelligence   \n",
       "2          Basic Income  2009      1    1  artificial-intelligence   \n",
       "3  Towards Data Science  2009      1    1  artificial-intelligence   \n",
       "4     Chatbots Magazine  2009      1    1  artificial-intelligence   \n",
       "\n",
       "   Reading_Time Claps  Comment  \\\n",
       "0            13   28K        0   \n",
       "1            16   25K        0   \n",
       "2            14  9.4K        0   \n",
       "3             7   43K        0   \n",
       "4            11  5.6K        0   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://medium.com/@ageitgey/machine-learning-...   \n",
       "1  https://medium.com/@ageitgey/machine-learning-...   \n",
       "2  https://medium.com/basic-income/deep-learning-...   \n",
       "3  https://towardsdatascience.com/how-to-build-yo...   \n",
       "4  https://chatbotsmagazine.com/the-complete-begi...   \n",
       "\n",
       "                                          Author_url  \n",
       "0  https://medium.com/@ageitgey?source=tag_archiv...  \n",
       "1  https://medium.com/@ageitgey?source=tag_archiv...  \n",
       "2  https://medium.com/@2noame?source=tag_archive-...  \n",
       "3  https://towardsdatascience.com/@jamesloyys?sou...  \n",
       "4  https://chatbotsmagazine.com/@mattprd?source=t...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# medium = pd.read_csv('Medium_Scrapermedium_artificial-intelligence_2009-2016.csv')\n",
    "scraped_files = glob.glob(\"scraped_tags/*.csv\")\n",
    "\n",
    "frames =[]\n",
    "for file in scraped_files:\n",
    "    #all of the seperate scrapes from different tags\n",
    "    df = pd.read_csv(file)\n",
    "    frames.append(df)\n",
    "medium = pd.concat(frames)\n",
    "medium.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles scraped (before cleaning):  114500\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of articles scraped (before cleaning): \",medium.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# Converting Strings to Floats\n",
    "\n",
    "Before we can work with the data we need to <b>convert the \"Claps\" column from string to float values</b>. Note that the Object datatype is non-numeric. There is also an issue with <b>Claps in the form of \"5.5K\", rather than \"5500\".</b>\n",
    "\n",
    "### Preview of DataTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Title           object\n",
       "Subtitle        object\n",
       "Image            int64\n",
       "Author          object\n",
       "Publication     object\n",
       "Year             int64\n",
       "Month            int64\n",
       "Day              int64\n",
       "Tag             object\n",
       "Reading_Time     int64\n",
       "Claps           object\n",
       "Comment          int64\n",
       "url             object\n",
       "Author_url      object\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medium.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reformatting Clap Information to Floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clap dtype:  float64\n"
     ]
    }
   ],
   "source": [
    "#Claps entries higher than 999 are written \"5.5K\"\n",
    "# here we remove the \"K\", convert the string to float, then multiply by 1000.\n",
    "numeric_claps = []\n",
    "for x in medium.Claps:\n",
    "    if \"K\" in str(x):\n",
    "        numeric_claps.append(float(x[:-1])*1000)\n",
    "    else:\n",
    "        numeric_claps.append(x)\n",
    "medium[\"Claps\"] = numeric_claps\n",
    "medium[\"Claps\"] = pd.to_numeric(medium[\"Claps\"])\n",
    "print(\"Clap dtype: \", medium.dtypes[\"Claps\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# Removing Comment Entries\n",
    "Comment entries have been encoded into the data with the Comment column. Since these entries are not articles, I remove them in the following script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Entries to be removed:  1169\n",
      "Percentage of remaining data:  1.02 %\n"
     ]
    }
   ],
   "source": [
    "no_comm = medium[medium.Comment==0]\n",
    "no_comm = no_comm.drop([\"Comment\"], axis=1)\n",
    "print(\"Number of Entries to be removed: \", medium.shape[0]-no_comm.shape[0])\n",
    "print(\"Percentage of remaining data: \" ,round(((medium.shape[0]-no_comm.shape[0])/medium.shape[0])*100,2), \"%\")\n",
    "medium = no_comm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning up  Urls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78?source=tag_archive---------0-----------------------\n",
      "https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721?source=tag_archive---------1-----------------------\n",
      "https://medium.com/basic-income/deep-learning-is-going-to-teach-us-all-the-lesson-of-our-lives-jobs-are-for-machines-7c6442e37a49?source=tag_archive---------2-----------------------\n"
     ]
    }
   ],
   "source": [
    "#before\n",
    "for i in range(3):\n",
    "    print(medium.url.values[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "medium.url = medium.url.str.split(\"?\", expand=True)\n",
    "medium.Author_url = medium.Author_url.str.split(\"?\", expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78\n",
      "https://medium.com/@ageitgey\n",
      "https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721\n",
      "https://medium.com/@ageitgey\n",
      "https://medium.com/basic-income/deep-learning-is-going-to-teach-us-all-the-lesson-of-our-lives-jobs-are-for-machines-7c6442e37a49\n",
      "https://medium.com/@2noame\n"
     ]
    }
   ],
   "source": [
    "#after\n",
    "for i in range(3):\n",
    "    print(medium.url.values[i])\n",
    "    print(medium.Author_url.values[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Checking for Non Entries in the Data\n",
    "\n",
    "\n",
    "### All NaNs in Each Column\n",
    "We only have missing values in Title, Subtitle, or Publication. <b>NaNs in publication column because not all articles are published. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaNs\n",
      "Title                 6356\n",
      "Subtitle             36237\n",
      "Image                    0\n",
      "Author                 488\n",
      "Publication          39303\n",
      "Year                     0\n",
      "Month                    0\n",
      "Day                      0\n",
      "Tag                      0\n",
      "Reading_Time             0\n",
      "Claps                    0\n",
      "url                      0\n",
      "Author_url             488\n",
      "\n",
      "Total Entries:   113331\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of NaNs\")\n",
    "for x in range(13):\n",
    "    print(\"%-15s %10d\" % (medium.columns.values[x], medium.iloc[:,x].isna().sum()))\n",
    "print()\n",
    "print(\"Total Entries:  \", medium.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove NaN Authors\n",
    "Medium is doing something weird with adding existing articles from sites like pcmag.com. The cards on the archive timeline have neither author nor publication. Since there are only a coulple hundred entries withou an author, I choose to remove these from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# medium = medium[medium.Author.notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NaN Title and Subtitle Entries\n",
    "Sometimes when scraping the archive page, Titles are in weird formats. The result, <b> some articles titles are scraped as subtitles</b>.\n",
    "\n",
    "Here is a breakdown of the NonEntries in Title/SubTitle Columns. I choose to keep these in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total NaN Title Entries:  6356\n",
      "Entries with NaN Title but existing SubTitle:  1285\n",
      "Entries with neither title nor subtitle:  5071\n"
     ]
    }
   ],
   "source": [
    "#Total entries with no Title\n",
    "print(\"Total NaN Title Entries: \", medium[medium.Title.isnull()].shape[0])\n",
    "\n",
    "#Entries with no title but with a subtitle\n",
    "print(\"Entries with NaN Title but existing SubTitle: \",medium[(medium.Title.isnull() & medium.Subtitle.notnull())].shape[0])\n",
    "\n",
    "#Neither Possible explanations?\n",
    "print(\"Entries with neither title nor subtitle: \", medium[(medium.Title.isnull() & medium.Subtitle.isnull())].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaNs\n",
      "Title                 6356\n",
      "Subtitle             36237\n",
      "Image                    0\n",
      "Author                 488\n",
      "Publication          39303\n",
      "Year                     0\n",
      "Month                    0\n",
      "Day                      0\n",
      "Tag                      0\n",
      "Reading_Time             0\n",
      "Claps                    0\n",
      "url                      0\n",
      "Author_url             488\n",
      "\n",
      "Total Entries:   113331\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of NaNs\")\n",
    "for x in range(13):\n",
    "    print(\"%-15s %10d\" % (medium.columns.values[x], medium.iloc[:,x].isna().sum()))\n",
    "print()\n",
    "print(\"Total Entries:  \", medium.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Duplicate Articles with duplicated URLs and Multi-tagged\n",
    "Medium allows an  author to include 5 tags for each story.\n",
    "\n",
    "When we scraped the archive page, we scraped each individual tag. <b>As a result, stories will appear multiple times in our data (with different tags)</b>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are:  110856 Duplicated URL entries.\n",
      "Unique posts with duplicate urls:  2145\n",
      "Total unique urls:  4620\n"
     ]
    }
   ],
   "source": [
    "#multi_urls is all entries in the dataset that have duplicates (includes all duplicates)\n",
    "multi_urls = medium[medium.duplicated(subset=[\"url\"], keep=False)]\n",
    "print(\"There are: \", multi_urls.shape[0], \"Duplicated URL entries.\")\n",
    "print(\"Unique posts with duplicate urls: \", multi_urls.shape[0]- medium[medium.duplicated(subset=[\"url\"], keep=\"last\")].shape[0])\n",
    "print(\"Total unique urls: \", medium.shape[0]- medium[medium.duplicated(subset=[\"url\"], keep=\"last\")].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are:  16150 Duplicated tag entries.\n",
      "Unique posts with multiple tags:  7875\n"
     ]
    }
   ],
   "source": [
    "#one hot encode the tags \n",
    "medium = pd.get_dummies(medium, columns = [\"Tag\"])\n",
    "\n",
    "#multi_tags is all entries in the dataset that have duplicates (includes all duplicates)\n",
    "multi_tags = medium[medium.duplicated(subset=[\"url\", \"Year\", \"Month\",\"Day\"], keep=False)]\n",
    "print(\"There are: \", multi_tags.shape[0], \"Duplicated tag entries.\")\n",
    "print(\"Unique posts with multiple tags: \", multi_tags.shape[0]- medium[medium.duplicated(subset=[\"url\", \"Year\", \"Month\",\"Day\"], keep=\"last\")].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Remove all but one of each duplicate entry, then sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Subtitle</th>\n",
       "      <th>Image</th>\n",
       "      <th>Author</th>\n",
       "      <th>Publication</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Reading_Time</th>\n",
       "      <th>Claps</th>\n",
       "      <th>...</th>\n",
       "      <th>Tag_data-analytics</th>\n",
       "      <th>Tag_data-engineering</th>\n",
       "      <th>Tag_data-science</th>\n",
       "      <th>Tag_deep-learning</th>\n",
       "      <th>Tag_machine-learning</th>\n",
       "      <th>Tag_nlp</th>\n",
       "      <th>Tag_python</th>\n",
       "      <th>Tag_r</th>\n",
       "      <th>Tag_reinforcement-learning</th>\n",
       "      <th>Tag_statistics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Artificial Intelligence and Big data for Sourc...</td>\n",
       "      <td>What do the buzzwords mean? How to leverage th...</td>\n",
       "      <td>0</td>\n",
       "      <td>Predikt</td>\n",
       "      <td>Don't Panic, Just Hire</td>\n",
       "      <td>2015</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>M.G. Siegler</td>\n",
       "      <td>500ish</td>\n",
       "      <td>2015</td>\n",
       "      <td>6</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "      <td>40.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Artificial Intelligence gets a Makeover</td>\n",
       "      <td>With cognitv algorithms, Swag cold-cocks IBM W...</td>\n",
       "      <td>1</td>\n",
       "      <td>Graham Anderson</td>\n",
       "      <td>Adventures of Swag</td>\n",
       "      <td>2015</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Formalizing indirect normativity</td>\n",
       "      <td>How do you precisely specify a utility functio...</td>\n",
       "      <td>0</td>\n",
       "      <td>Paul Christiano</td>\n",
       "      <td>AI Alignment</td>\n",
       "      <td>2012</td>\n",
       "      <td>10</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>43.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A possible stance for AI control</td>\n",
       "      <td>I suggest that AI control research focus on fi...</td>\n",
       "      <td>0</td>\n",
       "      <td>Paul Christiano</td>\n",
       "      <td>AI Alignment</td>\n",
       "      <td>2015</td>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "      <td>9</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  Artificial Intelligence and Big data for Sourc...   \n",
       "1                                                NaN   \n",
       "2            Artificial Intelligence gets a Makeover   \n",
       "3                   Formalizing indirect normativity   \n",
       "4                   A possible stance for AI control   \n",
       "\n",
       "                                            Subtitle  Image           Author  \\\n",
       "0  What do the buzzwords mean? How to leverage th...      0          Predikt   \n",
       "1                                                NaN      1     M.G. Siegler   \n",
       "2  With cognitv algorithms, Swag cold-cocks IBM W...      1  Graham Anderson   \n",
       "3  How do you precisely specify a utility functio...      0  Paul Christiano   \n",
       "4  I suggest that AI control research focus on fi...      0  Paul Christiano   \n",
       "\n",
       "              Publication  Year  Month  Day  Reading_Time  Claps  ...  \\\n",
       "0  Don't Panic, Just Hire  2015     10   18             4    4.0  ...   \n",
       "1                  500ish  2015      6   29             6   40.0  ...   \n",
       "2      Adventures of Swag  2015     11   13             9    6.0  ...   \n",
       "3            AI Alignment  2012     10   31            31   43.0  ...   \n",
       "4            AI Alignment  2015     11   30             9    6.0  ...   \n",
       "\n",
       "  Tag_data-analytics Tag_data-engineering  Tag_data-science  \\\n",
       "0                  0                    0                 0   \n",
       "1                  0                    0                 0   \n",
       "2                  0                    0                 0   \n",
       "3                  0                    0                 0   \n",
       "4                  0                    0                 0   \n",
       "\n",
       "   Tag_deep-learning  Tag_machine-learning  Tag_nlp  Tag_python  Tag_r  \\\n",
       "0                  0                     0        0           0      0   \n",
       "1                  0                     0        0           0      0   \n",
       "2                  0                     0        0           0      0   \n",
       "3                  0                     1        0           0      0   \n",
       "4                  0                     0        0           0      0   \n",
       "\n",
       "   Tag_reinforcement-learning  Tag_statistics  \n",
       "0                           0               0  \n",
       "1                           0               1  \n",
       "2                           0               0  \n",
       "3                           0               0  \n",
       "4                           0               0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#keep only one entry of each duplicated article\n",
    "sort_url = medium[~medium.duplicated(subset=[\"url\"], keep=\"last\")]\n",
    "\n",
    "#sort the entry to put it in the exact same order as the groupby above\n",
    "medium_clean = sort_url.sort_values([\"url\",\"Year\",\"Month\",\"Day\"]).reset_index().drop(\"index\",axis=1)\n",
    "\n",
    "medium_clean.shape[0]\n",
    "# medium_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "How much data do we have after cleaning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of after cleaning: \", medium_clean.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_clean.to_csv(\"Medium_scrape_urls_multi-tag _clean_2009-2016.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
