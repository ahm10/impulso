UUID,url,topic,title,subtitle,tags,author,h1_headers,h2_headers,paragraphs,blockquotes,bold_text,italic_text
KU6Kv3kaYZX6wJvW2MYyCN,https://medium.com/@tenaciouscb/new-year-new-user-journeys-c07880f147f2?source=tag_archive---------0-----------------------,data-science,"New Year, New User Journeys",None,"['Open in app', 'Open in app', 'Github', 'User Research', 'Data Science']",Chrissie Brodigan,"['Whats hard to measure', 'New User Journeys & Demographics', 'Three New User Studies', '1.) The New User Quiz', '2.) New User Panel Data NAC Longitudinal', '3.) New User Exit Study The GitHub 365', 'Wrapping up']","['Lets talk about you:', 'Inactive but not abandoned']","['Most web services experience seasonal lift in new user sign-ups and traffic. This is especially true in January when people return to work and school and resolve to make changes to behaviors. Specifically many new users arrive to apps with their highest potential to learn something do something. However new users readiness is often quickly lost when your products experience doesnt present them a compelling path forward.', 'RealTalk™ … Over the past three years as our newest users arrived to GitHub who were different than our early adopters we failed to engage the majority and lost them.', 'This large leak of our newest community members has led GitHubs user research team to conduct a series of exploratory and evaluative studies over the past three years. Today we have a richly designed a 3-part research program born out success and failure which Ill present in this post. The sum of these efforts now helps us to better understand and design for this fast-changing ever-growing and often-failing newcomer group.', 'From delving into the behaviors of Millennials in the workforce to measuring motivations behaviors and activity across global communities our efforts reveal important stories about trends that characterize what our product team has been referring to as the new developer:', 'Dashboards and metrics power many decisions in product development. I find these tools to be impressive and useful they tell us some things but I can almost never get what I need to know about our users as humans by looking at graphs alone. Ines Sombra eloquently shared a memorable statement in her Monitorama 2015 talk:', 'The gap from metric/graph to insight can be huge.', 'Example: Do you know whats going on here?', 'Heres my short list on whats hard to measure with traditional dashboards analytics and KPIs:', 'Lets explore the three ways were studying hard-to-measure things about GitHubs newest users.', 'Our research approach to studying new users and their journeys is backed by three survey instruments listed individually below. The approach is portable so you can recreate it almost anywhere. In addition to required questions about product experience weve slowly added optional personal demographic questions (human age education sex/gender). When youre working on a growing app/community that has reach into both hobbyist and professional communities you can learn a lot from human age. For example many people on GitHub transform over time from hobbyists into professionals. How do people age with your product?', 'Responsibly gathering and using personal demographics wisely is a game-changer.', 'GitHubs user research team is tasked to go beyond machine reporting in dashboards connecting the what with numbers to the why with human stories. We take a mixed methods research approach with closed-answer and open text fields and interviews. Three instruments power three studies:', 'Lets do a deeper dive into each:', 'We designed and launched a short 5-question quiz (written in English) presented to 5% of new users from the U.S. upon inception. When people sign up for a service you have a willing audience and their first run is a great time to ask a few helpful questions (helpful to both you and to the end user). At GitHub we dont ask users survey questions if we cant use the data to improve their experience. Every question is deliberate  we use the data to surface insights that make life on GitHub better.', 'Never ask a question if you arent going to be able to make use of the data.', 'Accumulating information in and of itself is not useful. Information cant possibly serve a purpose until we first identify whats meaningful and then manage to make sense of it. Even once we understand the information it remains inert until we actually do something with it. The true promise of the information age isnt tons of data but decisions and actions that are better because theyre based on an understanding of whats really going on in the world.  Stephen Few', 'The new user quiz provided insights into which attributes were predictors of success as well as those that conversely lead towards churn (people failing to get value out of GitHub and leaving) within the first 30-days of a new account. We learned what core skill-level people had with git and programming and what they were looking for from the product. A few key takeaways included:', '• Knowing git   25% of users but 70+% self-identified as new to git.• Writing some code   60%.• Wanting to get involved in open source   90%.', 'There are some holes Im not including which reveal more about failure (thats a post for another time!).', 'Examples: Jessica Suttles co-founder/engineer is doing this with Currents early access program and Etsys shop creator survey. And of course if youre looking for a new user quiz stop by OKCupid and create an account (at your own peril).', 'Where the new user quiz provides instant insights upon inception our new user panel data project is a longitudinal study. As we gather data for an entire year well be doing prospective work tracking variables and identifying outcomes as they occur (e.g. a persons first merged pull request). However at the end of the year well do a retrospective analysis looking back and identifying the variables that contributed to the outcomes weve documented (e.g. it took three months before a persons first merged pull request).', 'Longitudinal studies are complex and can be arduous to complete  in our technical field there seem to be very few research teams investing in them. Gathering panel data a continuous effort: a person may participate in the first two surveys but then miss the third while another one of their peers misses the first two but does complete the third. Longitudinal studies yield incredibly rich data on user experiences that demonstrate how behaviors evolve over time.', 'In the case of GitHub we know that it takes a long time for most new users to gain confidence with git and learn and integrate GitHub into their workflows. We also know that we lose the vast majority of new GitHub account creators. The GitHub NAC (new account creators) study is an effort to follow this process closely among a single cohort of new users who signed up in September 2015  September 2016 to help us better understand the most current journey from sign up to established user.', 'Among longitudinal studies there are two general types:', 'Born out of failure . . .', 'GitHubs NAC study is a panel study and interestingly it was born out of failure. In August  September 2015 we conducted our annual Tools & Workflows survey which presented users with 35 questions. When we looked at the more than 3000 responses we realized that we were so thin on new user respondents (123 total). Those new users who did respond to the T&W survey represented outliers so we couldnt draw statistically meaningful conclusions from their participation. Simply by participating they were demonstrating behavior different from their cohort.', 'Our main recruitment mode had been an in-app prompt so we tried sending out an email to 5000 new user accounts since this group was less likely to be on the site and see the prompt but again that didnt work. We realized the survey was too long and asked questions better-aligned for people further along in their GitHub journey.', 'We divided the main survey up into a series of smaller surveys and began to send them out to a cohort of 90k new account creators. The result was higher statistically relevant (and repeat) participation with more than 4000+ respondents organized by Explorers (people browsing the site) and Creators (people taking maker actions).', 'In addition to the difficulty of collecting the data analysis of panel studies is particularly complicated because repeated observations of the same individual violate assumptions about the independence of observations that are necessary for many statistical methods so this type of data requires specialized models. Additionally attrition rates between waves must be examined for systematic bias since higher attrition rates among certain types of respondents will introduce bias into the final data set. This is a long-term study without immediate insights but well share what we learn around the mid-point in March 2016.', 'Studying people over time is a special treat for researchers. If youve ever been sucked in by a where are they now tabloid headline you know the intuitive appeal of finding out how things turned out. The enduring popularity of the Up series the profound insights from the Harvard Grant Study and the critical policy findings from the Panel Study on Income Dynamics demonstrate the cultural and scientific power of this type of study.', 'I recently shared some of this studys results and its structure and include it here because its connected to both the New User Quiz and the GitHub NAC.', 'While were waiting on the NAC end-of-year (EOY) is an excellent time for a cross-sectional study and retrospective on your apps growth  specifically looking at where you didnt grow. At GitHub we conduct a short annual survey dubbed The GitHub 365. We use the 365 to examine what happened with people who signed up for an account but at some point ceased to return.', 'A cross-sectional study captures and depicts a snapshot of activity. In this case we were studying why new users went inactive within their first year of account creation. From a personal perspective the 365 is our opportunity to listen to people who left us in other words these are not our superfans.', 'A 365 study is an instrument that enables you to look back at a years worth of inactive account data and can be used to help your organization hit the ground running in January when youre most likely to have a burst in seasonal sign-ups (those new year resolutions!). The study was driven by a short well-designed survey instrument and paired with account data including activity.', 'When we think of inactive users we think of abandoned accounts. In the 365 study we reached out to 100000 inactive accounts created from December 2014  December 2015. More than 3000 people responded.', 'Q: Which best describes why you stopped using GitHub?', 'People shared thoughts and experiences through closed-response options and open-ended responses depicting that more than 50% of inactive account respondents (who are humans and not bots) indicated that they intend to return to GitHub someday.', 'The largest responding population identified Ill be back Ive been busywhich challenges the notion that inactive accounts are abandoned accounts. We looked for differences in the respondent group who identified that they would be back finding that they are instead best characterized by their heterogeneity. They are a bit of everything which indicates that the phenomenon of starting something and then not having time to follow through is both common and universal.', 'Our biggest takeaway is that listening to inactive account holders tells us that they might more fruitfully be thought of as dormant rather than abandoned and could use a gentle git push.', 'Studying new users is a multi-phase approach:', 'When this research is paired with analytics dashboards and KPIs it can become your organizations greatest superpower in achieving and sustaining growth.', 'How will you study your newest users this year maybe even shine a light into unexplored areas charting and creating journeys?', '', 'Research is better when you get to discover new things about the world with someone who has a different perspective  someone who is willing to challenge you.']","['The gap from metric/graph to insight can be huge.', 'Responsibly gathering and using personal demographics wisely is a game-changer.', 'Accumulating information in and of itself is not useful. Information can’t possibly serve a purpose until we first identify what’s meaningful and then manage to make sense of it. Even once we understand the information, it remains inert until we actually do something with it. The true promise of the information age isn’t tons of data but decisions and actions that are better because they’re based on an understanding of what’s really going on in the world. –Stephen Few', 'Q: Which best describes why you stopped using GitHub?']","['January', 'changes to behaviors.', 'path forward.', '3-part research', 'new developer', ':', 'Awareness', 'Motivations', 'Workarounds', 'Behaviors  ', 'Emotions  ', 'what', 'why', 'New User Quiz  ', 'Longitudinal New Account Creators (NAC) Survey  ', '365 Inactive Accounts Exit Survey', '1.) The New User Quiz', 'What', 'How', 'Where', 'What', 'Currents early access program', 'Etsys shop creator survey', 'prospective', 'retrospective', 'The GitHub NAC', 'September 2015  September 2016', 'Cohort studies', 'Panel studies', 'Born out of failure . . .', 'GitHubs NAC', 'study', 'Tools & Workflows survey', '90k new account creators.', 'assumptions about the independence of observations', 'bias', 'Up series', 'Harvard Grant Study', 'Panel Study on Income Dynamics', 'The GitHub 365.', 'Who', 'Why', 'Whats', 'How', 'A', 'cross-sectional study', 'Inactive but not abandoned', 'Ill be back Ive been busy', 'they are instead best characterized by their heterogeneity.', 'dormant', 'rather than abandoned', 'git push.']","['path forward.', 'RealTalk™ … Over the past three years as our newest users arrived to GitHub who were different than our early adopters we failed to engage the majority and lost them.', 'Ines Sombra', 'hard to measure', 'Core', 'Future', 'Aspirational', 'Responsibly gathering and using personal demographics wisely is a game-changer.', 'what', 'why', 'Never ask a question if you arent going to be able to make use of the data.', 'Accumulating information in and of itself is not useful. Information cant possibly serve a purpose until we first identify whats meaningful and then manage to make sense of it. Even once we understand the information it remains inert until we actually do something with it. The true promise of the information age isnt tons of data but decisions and actions that are better because theyre based on an understanding of whats really going on in the world.', 'There are some holes Im not including which reveal more about failure (thats a post for another time!).', 'What', 'questions would you ask your newest users upon sign-up?', 'How', 'would those questions help you predict success/failure?', 'Where', 'would they uncover intent/motivation?', 'What', 'would you be able to learn about people signing up for your service today as opposed to your early adopters?', 'Examples:', 'Jessica Suttles', 'co-founder/engineer is doing', 'this', 'with', 'Currents early access program', 'and', 'Etsys shop creator survey', '. And of course if youre looking for a new user quiz stop by OKCupid and create an account (at your own peril).', 'prospective', 'retrospective', 'most current journey from sign up to established user.', 'Cohort studies', 'Panel studies', 'Born out of failure . . .', 'GitHub journey.', 'assumptions about the independence of observations', 'bias', 'Up series', '', 'Harvard Grant Study', '', 'Panel Study on Income Dynamics', 'ceased to return.', 'Who', 'are your newest users that dont come back?', 'Why', 'did they leave?', 'Whats', 'one thing you could have done differently to help them succeed?', 'How', '(and should) you try to bring them back?', 'A', 'cross-sectional study', 'Ill be back Ive been busy', 'they are instead best characterized by their heterogeneity.', 'dormant', 'rather than abandoned', 'git push.', 'Research is better when you get to discover new things about the world with someone who has a different perspective  someone who is willing to challenge you.']"
LXKChyvLWYK25d53FTaMHE,https://medium.com/@alexRutherford/how-do-i-become-a-data-scientist-b060831a3183?source=tag_archive---------1-----------------------,data-science,How do I become a Data Scientist?,None,"['Open in app', 'Open in app', 'Data Science', 'Big Data']",Alex Rutherford,None,"['Understand the Big Picture', 'Not all Data Scientist Roles are equal', 'Learn Python or R', 'Get Artistic', 'I Studied X Can I Become a Data Scientist?', 'Start a Pet Project', 'Learn to Communicate', 'Get Acquainted with End-to-End Development', 'Data Science for Social Good']","['[This post is a reboot of a post originally published on my personal blog on 28th June 2014]', 'I often get asked by people who are coming to the end of their Masters degree or PhD or who are looking to change career direction; How can I become a Data Scientist?. This question has been answered in some form by others already (the latest from the folks at Insight is great and DataTau has some riff on the same idea seemingly every week). However I find the question and the different answers most interesting and find myself repeating the same information again and again so I decided to turn it into a blogpost.A few caveats a priori;', 'The full set of material that could reasonably be in the scope of data science jobs is huge (more on that later). I hope the following will help you in your search for the foundational material that is a shortcut I can offer you. However you should still be prepared to put in a lot of hours working through this material in your own time.', 'You should understand what Big Data means on an intellectual level. There are two extremes of thought to consider; theoretical model driven approaches or empirical data-driven approaches. Generative approaches such as Cellular Automata and Agent Based Models (ABMs) are examples of the former and Machine Learning and Data Mining the latter. While there is no shortage of cheerleaders for empiricism these days you should still get to know the work done by Stephen Wolfram Josh Epstein and Doyne Farmer whose discussion of the use of ABMs in finance actually argues against the use of data.', 'Often this tension comes down to the question does observation of a correlation negate the need for understanding the underlying causative mechanism? The right answer is not yes or no. The example of machine translation tells us that we can have much more success with an empirical statistical approach combined with vast amounts of data to learn from compared to a hugely complex model based approach. But this debate is a very fiery one with very smart people such as Noam Chomsky and Peter Norvig (Head of Research for Google) taking opposite views (see links below).Part of being a good Data Scientist involves knowing when to be healthily skeptical of each result which pops out of your model. In some cases all that matters is that there is a correlation other times it matters exactly what the correlation is with; that Pearson coefficient of 0.9 between rainfall in Tasmania and comment activity on Reddit is probably a fluke. The possible criticisms of Big Data along these lines are common and you should be prepared to answer them.', 'A Data Scientist working at the World Bank Open Data division compared to one working in the Data Science Team and another at Yahoo! on Computational Advertising effectively all have different jobs. Some Data Scientists will work very hard to incrementally improve a very well defined metric (click through rate successful movie recommendations) others will have the freedom to explore a very rich multi-dimensional dataset and some will be in between. The point is that even within industries the role is not very well defined and depends a lot on the company or organisation. So do your homework; read everything you can about what the company does; reports academic papers blog posts and Twitter accounts.', 'Forget FORTRAN C STATA SPSS and Mathematica. For exploratory data analysis machine learning reusable libraries and sharing of code; you will need to learn Python or R. But dont worry! The ROI on these languages is huge.', 'Python offers a very simple syntax and very broad capabilities; the ability to program and deploy web applications from data collection to exploratory analysis all the way to database interaction and web hosting. However the basic data structures are slow official documentation is lacking and the use of whitespace can be downright weird. The main libraries for data science are', 'R on the other hand has an esoteric syntax (common gotchas here and basic tutorials here) and more complex data structures at first. However the community is very lively and can deliver extremely succinct code with practise. Good packages to look at are', 'The relative merits of each are discussed in more detail here. As a biased Pythonista I would also point out that Python is now the official teaching language of MIT.', 'Very often people at the end of a long pipeline of analysis simply stop as soon as they have a plot or figure that meets the minimum requirements i.e. that the points go in the right direction and the axes are labeled (optional). Although this is understandable since it can often take months to arrive to this point it is a mistake. The importance of presenting information in the right way and in a straightforward and informative way is a science in itself. Nathan Yaus blog and books are essential reading and the godfather of visualisation Edward Tufte should also be high on your list.', 'But in general take some time with your figures. They might make sense to you but can the same be said for someone who hasnt been thinking hard about it for as long as you have? Finally dont be afraid to simply adjust the default settings of your plots; soften colours (no-one enjoys stark pure reds and blues) and make text labels larger.', 'Very few people qualify to be Data Scientists emerging from formal education programs (although there are now some programs looking to turn out fully-functioning Data Scientists). The Insight white paper deals with this dearth of qualified data scientists very well.Traditionally computer scientists and electrical engineers were the hoarders of knowledge about natural language processing dimensionality reduction and other seemingly esoteric things. More recently though these skills have become important to academic biologists geneticists astrophysicists financiers applied mathematicians as well as others. All of these fields have a strong data science component. In general the harder the science the better. Aspects which are best learned through a long formal education include a rigorous scientific method (hypothesising checking testing reflecting concluding) and an ability to translate complex ideas into actionable models and code. That said data science is often not for the purist or the researcher frightened of the occasional hack or sub-optimal result. Consider the problem of geolocating an arbitrary string (i.e. going from West London to 51.5 N 0.1 W) even Google Maps with all of its training examples cannot get this right every time. Sometimes solutions can only be so good and not perfect. If you are used to the physical sciences where the gravitational constant has been measured with an accuracy of 10^-11) a little mental adjustment is needed. Finally an inquisitive nature is essential. The rush of finding something interesting in a data set is more important than the what you did to get there.That said I believe that all disciplines are capable of making the transition and your diverse background can bring a much needed new perspective to a team. Data science invariably deals with the actions of people so for that reason data-minded social scientists are being put to good use at places such as Facebook. One other distinction is between people who do analytics to feed into human decision making rather than computer decision making. People who are hiring do understand that to some extent they must grow their data scientists but despite that you will invariably need to meet them halfway and fill in some gaps yourself. The best way to show you can make the transition to applied data science is to show that you have already made the transition to doing applied data science. Which brings me to my next point…', 'There are so many lessons to learn in data analysis and after a point one cannot learn these from a book or a blog post. Therefore just start analysing something anything to get some practise. Learn the common operations in your own time and start to build an intuition for what to look for in different kinds of data. I am strongly of the opinion that there is value in reinventing the wheel and coding up some common operations yourself even if they exist already. You will learn a lot for yourself as well as appreciating the work that goes into making a robust and optimised boilerplate library. Here are some of my favourite free open places to get data', 'Before I left academia if someone asked me what should be added to a data science curriculum I would answer that a particular class of classification or optimisation algorithms are essential or this suite of command line tools. After just a short time spent in The Real World I realised that once you have a decent grounding any new topic or technique can be picked up very easily in an arbitrarily short time. But what is more important and less easily learned is how to communicate the value and insight in common data science techniques to non-experts.In The Real World people are busy have limited budgets and their own targets to reach. Unlike academia they dont have time to indulge in exploring something because it looks interesting. Likewise you cannot rely on people to read your carefully written report especially if they feel that they dont understand it. So you must explain what it means that you used the first principle component of a set of metrics as the covariate rather than a simple univariate linear regression on the spot to a business major. What is worse is that the business major will not take your word for it. On the contrary the business major will have a good education will ask you tough questions and will generally be wary of you telling her/him to work in a different way.', 'Most of the time data doesnt exist in neat cleaned CSV files and often a static visualisation wont suffice. More likely you have to pull data by making an HTTP request to an API clear out all the emoticons and mis-spellings put it in a database and then make a dynamic webpage which interacts with it and presents the data graphically using some flavour of JavaScript. Therefore these are all things that you should at least have an appreciation of even if you are lucky enough to have someone else to hand off these back-end/hardware duties to.', 'It has been most interesting to see Data Science impact different sectors at different times. Advertising logistics and marketing caught the bug early on Google Translate changed linguistics forever quantitative finance has steadily become more empirical in recent years not to mention law bibliometrics and election campaigning. But one of the recent additions to the fold is the development sector.', 'Where I work at United Nations Global Pulse we consider real-time data sources such as social media and anonymised mobile phone records to augment the more traditional data sources used by development practitioners such as costly surveys and censuses. We have labs in Kampala and Jakarta and we often advertise for interns as well as collaborations with industry and academia.', 'Further challenges that may arise in this context include potentially low volumes in places that have low internet penetration as well as questions that are being answered with data for the first time (or at least in data with high frequency). Needless to say an interest in current affairs the intersection of technology with society and global development are a must in addition to the above hard technical skills.Other organisations that look at similar problems include Datakind who also deal with domestic charitable problems through the lens of Big Data. Datakind chapters are currently expanding into cities globally and have a lively community in New York. Finally Rayid Ghani (the former Chief Scientist for the Obama campaign) is running the Data Science for Social Good program in Chicago.']",[],None,"['[This post is a reboot of a post originally published on my personal blog on 28th June 2014]', 'against', 'does observation of a correlation negate the need for understanding the underlying causative mechanism?', 'with', 'human', 'computer', 'have already made']"
WoyEJADs4A7HVS4Ye6Dg5W,https://medium.com/@Classy/using-data-science-to-improve-social-solution-design-in-edtech-8cb8329ea43b?source=tag_archive---------2-----------------------,data-science,How Data Science Improves EdTech Design,None,"['Open in app', 'Open in app', 'Education', 'Technology', 'Data Science']",Classy,None,None,"['By Edward Zhong Data Science Fellow Bayes Impact', 'Educational video games like Oregon Trail and Math Blaster are as old as the first home computers. Introduced in classrooms in the 1980s these games promised to revolutionize education and randomized control studies (RCTs) showed that some games were pretty effective at teaching kids.', 'But RCTs long the gold standard for measuring causal effects are unable to keep pace with the increased speed with which games are developed and updated. The ideal RCT is to randomly assign half a class to play the game and half the class to a placebo treatment and then measure the difference in their test scores before and after the experiment. This is well beyond the resources available to start-up game developers.', 'As a result the Bill & Melinda Gates Foundation proposed a project to evaluate games faster. Motion Math creator of delightful and challenging games that help students master fundamental math concepts with over 3.8 million downloads and expert critical acclaim was one of the grantees of the Gates request. Motion Math pitched the idea of creating a game platform: a technology that would allow even hobby game developers to plug their games in and get data analytics especially learning analytics back.', 'The platform tracks student performance in the apps and on embedded micro-assessments over time and across different apps providing a warehouse of data to developers teachers and the students themselves. The promise of this type of data is obvious: you can start to correlate individual student performance over time as a function of engagement in particular games. While this evidence is not causal in the sense of a RCT high positive correlation between student math score growth and app engagement is pretty persuasive.', 'Our mission at Bayes Impact is to use data science to improve social initiatives. We take seasoned Silicon Valley engineers and data scientists to work with nonprofits and governments. We partnered with Motion Math to provide the statistical and data science skills to complement their EdTech and game engineering expertise.', 'Our work is on-going were collecting data on students common core performance in order to evaluate apps. In our preliminary analysis weve been looking at in-game data to provide feedback for teachers and students. Once weve collected enough data on individual students for a longer period of time well be able to embark on the secondary phases of the project.', 'The promise of this method is that it can scale with potentially hundreds of games  we dont need to create custom models for each game.', 'Teachers as Consumers of Data While teachers are the most informed about their students academic skills they still only get one lens to understand their students progress and motivations. When we presented them in-game data in an easy-to-digest visualization (a large and largely underappreciated part of data science) the teachers response was eye-opening.', 'Figure 1 shows an example of a students game play for Hungry Fish where players add numbered bubbles together to feed a fish that only desires one specific number. The x-axis is the number of levels played  for example 5 corresponds to the fifth level played. The y-axis represents the level of the game chosen. Finally games can end in quick wins mere wins losses and quits.', 'The student described in figure 1 started at level 13 and after a few hiccups progressed to level 12. As the player navigated through the game he became increasingly more adept at addition until his 32nd game when he mastered the highest level of the game something that even adults find difficult.', 'His teachers reaction affirmed why this data is valuable for educators. I love it. This is good for me to see. I dont think of him as a student that perseveres and pushes through. Look at what a high level hes at. Hes the sweetest kid but he gets easily distracted in class. He just wants to play and when hes playing he has the ability to push through instead of giving up and jumping down [in difficulty]. Im proud of him!', 'We think that this data can be incredibly powerful for teachers to have both real-time surveillance of their students skills competency and to provide a unique window for teachers to engage with students.', 'Students as Consumers of Data We want students to take control of their learning  to feel ownership of their competencies and to allow them to improve on areas where they feel weak. In our view this is the ultimate manifestation of personalized learning. But part of allowing kids to personalize their growth is giving them data about their progress.', 'To that end weve created a scalable method to score progress on measures of competency perseverance and risk aversion to present to students. For example weve created a model which indicates whether students are taking an appropriate amount of risk (neither too much nor too little) as they navigate through the game. When kids perform below a challenge appropriate level the games will provide notifications that encourage them to try more difficult levels.', 'To make the score for a students competency for example we first subjectively hand-scored several hundred students. In order to convince ourselves that the hand-scored values were reasonable we had 6 different reviewers from different backgrounds try to come up with scores for students on the question what do you think the students competency is. Reassuringly these scores were highly consistent: reviewers scores were about 80 percent correlated on average suggesting that hand-scoring made sense.', 'However its not scalable to have someone hand-score thousands of students in real-time. Instead we created a statistical model that replicates what a human-scorer would have given [1]. We train the model on scores we already produced and the game-play data. For future students we only need to run their game-play data through the model to output results that are like hand-scores. The promise of this method is that it can scale with potentially hundreds of games  we dont need to create custom models for each game.', 'Future Directions', 'We continue to work with Motion Math to fulfill the original Gates mandate of short-cycle evaluation. Ultimately well get enough data to start asking interesting questions about the apps themselves what engages different types of students what helps them learn and how different apps can be recommended at different stages of comprehension. At Bayes our central thesis is that data when used intelligently can multiply impact for social good. We think this is especially true of educational games.', 'Bayes Impact deploys teams of full-time data scientists and software engineers to build data-driven software solutions for the social sector. Bayes Impacts data science projects are focused on building implementable solutions to a wide range of societal problems including healthcare disaster recovery and criminal justice. Bayes has worked with organizations like the Michael J. Fox Foundation eSpark Youth Villages has been funded by the grantmakers like the Bill & Melinda Gates Foundation Tablea Foundation Microsoft.', '[1] For technical readers we first run hand-scores and lots of features of the game-play data through LASSO to do feature selection. Finally we non-parametrically regress the hand-scores on the relevant features selected in the prior stage. The model is then used on future students. Note that this is scalable because LASSO automatically generates relevant features for different games so no engineer needs to select features by hand.', 'Originally published at www.classy.org.']",['“The promise of this method is that it can scale with potentially hundreds of games — we don’t need to create custom models for each game. “'],"['Teachers as Consumers of Data', 'Students as Consumers of Data', 'Future Directions', 'Bayes Impact']","['By Edward Zhong Data Science Fellow Bayes Impact', 'The promise of this method is that it can scale with potentially hundreds of games  we dont need to create custom models for each game.', 'Figure 1. Example 3rd Grader', 'Bayes Impact', 'deploys teams of full-time data scientists and software engineers to build data-driven software solutions for the social sector. Bayes Impacts data science projects are focused on building implementable solutions to a wide range of societal problems including healthcare disaster recovery and criminal justice. Bayes has worked with organizations like the Michael J. Fox Foundation eSpark Youth Villages has been funded by the grantmakers like the Bill & Melinda Gates Foundation Tablea Foundation Microsoft.', '[1] For technical readers we first run hand-scores and lots of features of the game-play data through LASSO to do feature selection. Finally we non-parametrically regress the hand-scores on the relevant features selected in the prior stage. The model is then used on future students. Note that this is scalable because LASSO automatically generates relevant features for different games so no engineer needs to select features by hand.', 'Originally published at', 'www.classy.org']"
KPkG2Rrcad2eNipLYMmPVY,https://medium.com/@alexRutherford/fake-real-data-or-real-fake-data-eb6622d2ab2e?source=tag_archive---------3-----------------------,data-science,Fake real data? Or real fake data?,None,"['Open in app', 'Open in app', 'Data Science', 'Big Data', 'Data Privacy']",Alex Rutherford,None,None,"['This post is a reboot of a post originally published on my personal blog on 23rd March 2014]', 'I came this great blog post from Redrow Analytics recently that looked at fake data. It reminded me of a question I often ask myself: What makes real data real? It is a question worth asking since real data has a nasty habit of being abused.This is a game with echoes from my lectures on solid state physics many years ago. It is also a game that ex-physicist Albert-László Barabási and co-authors played to great effect in Small But Slow:How Network Topology and Burstiness Slow Down Spreading. The authors start with a timed record of 325 million calls between 4.6 million users of a phone network. This is like a video recording that you can play back to see the patterns in how people call each other. The authors simulated an infection just like a phone virus that spreads from person to person as one calls another.In order to show the importance of the structure of the network and the timing of the calls Karsai et al shuffled the network at random destroying the cliques and communities in the network. Then they shuffled the times of all of the calls and so on eventually arriving at a completely synthetic call network that looks exactly the same as the original from a distance but had all the patterns and correlations washed out. What they found is that the time for the virus to take over decreased steadily as each aspect of the data became uncorrelated. In other words wiping out the correlations in the real data leads to a network which looks like the original but underestimates a key characteristic.', 'Call Detail Records (CDRs) are an extremely rich source of data detailing each call or SMS between 2 SIMs along with a timestamp and a rough location. While the content of the call is obviously sensitive this meta-data of the call is also sensitive as it reveals locations and relationships between people. It was shown in a recent high profile paper that even when names phone numbers and any other personal information is removed only a minimum amount of information is needed to elucidate the identity of a person in a CDR. What this means on a practical level is that if I know 4 times and places of when someone made calls I could go to the anonymised CDR and figure out which anonymous user represents that person and then see all their other calls in 95% of cases.Given the proven value of CDR analysis for transport planning mapping poverty tracking malaria and disaster relief but in light of the liability of intrinsically privacy invading real data the prospect of realistic synthetic CDR data is enticing. This was the admirable motivation behind DP-WHERE: Differentially private modeling of human mobility by Mir et al.In this paper the authors mapped out the home and work locations based on commuting patters. They then measured the distribution of distances that people travel and the distributions of the number of calls make per day the length of the calls and the time of day that people call. From this they were able to create synthetic user behaviours based on real user behaviours by sampling these distributions. While the new data looks like the old data in many ways it isnt actually descriptive of any real person so it cant violate anyones privacy. Of course this is a great achievement and represents a potentially sustainable way for researchers to gain insights from CDR datasets without using data from which someone could be identifiable.But what happened in the process is that each characteristic of the data is now independent of any other. So if people who live in this part of town make more calls this will be smoothed out. Or it might be that people who commute farther make more calls that are shorter but in the synthetic data this pattern is not preserved.Of course this isnt realistic there are all kinds of biases and correlations in the real world. These correlations and bursty behaviours represent the datainess of data; we dont spread our calls or movements out throughout the day evenly if half of people travel 4 miles to work and the other half travel 2 miles its not accurate to talk of an average person traveling 3 miles.The question is how un-realistic are synthetic mobility traces? The authors measure the effect of this looking at the average of the maximum range of users in a day among other quantities. This metric is found to decrease from 3.2 to 1.9 miles. To some people this is an acceptable level of accuracy and to others not and it is certainly application specific. This is by no means a criticism of Mir et al; this is certainly a valuable proof of concept of an idea with great potential. It is by the authors own admission that the context must be considered.As with any pre-processing step in a Big Data pipeline biases can creep in and I would prefer that at least partially realistic data be available than none at all. But that said humans dont bounce around at random like the molecules of the ideal gas that young physicists all learn about. It could be that we are missing an important piece of the puzzle if we pretend that they do.']",[],['wiping out the correlations in the real data leads to a network which looks like the original but underestimates a key characteristic'],"['This post is a reboot of a post originally published on my personal blog on 23rd March 2014]', 'What makes real data real?', 'Small But Slow:How Network Topology and Burstiness Slow Down Spreading', 'et al', 'real', 'synthetic', 'DP-WHERE: Differentially private modeling of human mobility', 'et al', 'independent', 'et al']"
AkUKjbhw6VsidoFWTRq6cT,https://medium.com/@alexRutherford/if-i-like-x-how-did-you-know-i-like-y-and-would-i-have-liked-z-more-cae135153dd1?source=tag_archive---------4-----------------------,data-science,"If I like X, how did you know I like Y? (and would I have liked Z more)",None,"['Open in app', 'Open in app', 'Data Science', 'Machine Learning']",Alex Rutherford,None,"['Filtering Content Together', 'The Netflix Algorithm', 'Compressing Culture']","['If you have ever found a new film that you enjoyed via Netflixs If you enjoyed X try Y suggestions or connected with a colleague via LinkedIns People you might know (PYMK) feature then you have experienced the power of Big Data to improve your online experience. These algorithmically generated suggestions essentially augment the users experience by offering a personalised navigation through an otherwise overwhelmingly large set of choices be they other users films books or musical artists.', 'What really appeals to me about these suggestions which are now baked into 21st century cultural consumption is their efficacy in spite of their simplicity. The PYMK feature was one of the LinkedIn data teams first high profile features which is now ubiquitous. The idea is that if Alice and Bob are friends and Alice and Charlie are friends then it is likely that Charlie and Bob also know each other. This is known as transitivity in network science and it describes the tendency for people to form cliques and communities where everyone knows everyone else. If social connections were made between two people at random and independently of any third party then transitivity would be very low and the PYMK feature wouldnt be of much use.', 'But the world is not random. Things in the real world happen in bursts and display strong patterns even though they may be sometimes counter-intuitive; for example the link between high IQ and enjoying curly fries found in this infamous paper analysing Facebook likes. The human race is not immune from this predictability and information retrieval takes full advantage of this to offer personalised routes through otherwise intractably large datasets.', 'I find the other model of If you like X try Y suggestions much more troubling than the innocuous PYMK model. Such features are examples of collaborative filtering whereby a picture is built up of relationships between different items of interest by examining what others have previously expressed an interest in. By observing these past connections that others have made these same connections can be suggested to other newly observed people about whom there is incomplete information. In the simplest terms lets suppose I observe a large number of people who enjoyed both Flight of the Concords and the Mighty Boosh (since they are both comedy shows with satirical musical interludes). If I find new people who liked the Mighty Boosh but hadnt expressed an opinion on Flight of the Concords then I would bet that they would enjoy it and recommend it to them. This continues for large numbers of items so that full profiles of different kinds of people can be constructed describing all of the items they will statistically want to consume.', 'Perhaps the most high profile example of a collaborative filtering system is the Netflix recommendation algorithm and associated challenge and reward. The Netflix challenge winning team did for Singular Value Decomposition (SVD) what Googles PageRank did for eigenvector centrality; namely take an abstract piece of linear algebra and not only make it relevant but also deploy it extremely successfully. For simply bringing to life important topics taught to me badly 10 years ago I applaud the Netflix challenge.', 'But perhaps I can explain my unease with collaborative filtering with an examination of the early applications of the SVD; image compression. An image is a matrix of pixels each with a colour. The most verbose way to describe an image is to provide a list of the form pixel at row 2 and column 3 is 10% red 50% green and 0% blue.', 'The idea of image compression is to find a way to describe the image more simply than the exact colour of each of its many pixels. The compressed image will most likely lose some detail compared to the original but the trick is to make the lost detail as negligble as possible while making the compressed image as small as possible. There are many different ways to compress an image JPEGs and GIFs each use different methods which suit different kinds of images. My personal favourite is fractal image compression which takes advantage of the fact that images of nature such as coastlines and river basins often resemble themselves on a smaller scale. (Technical difficulties in implementation did however lead to the following tongue in cheek algorithmic recipe for fractal image compression: (1) Lock graduate student in his office (2) dont let him or her leave until they have arrived at the algorithm)', 'The SVD looks at the image and tries to find rows of pixels that best approximate many other rows in the image. As a simplified example if the image was of a smooth sand on the bottom half and a plain sky on the top the image could be compressed by storing a mostly blue row representing the sky and a mostly brown row for the sand. A crude approximation of the image could be formed by approximating every line in the original with one of these two rows. Using 2 different rows to approximate a detailed image would lead to an underwhelming approximation of the original but as more rows are used to build up the image more detail can be captured (the affect of including more rows or singular values can see in this demo). The process works by recognising structure in an image and then using a smaller number of typical rows to describe all the rows in an approximate way.', 'You might have guessed whats coming next; you are a row in an image to be compressed. The image is the mass of humanity with their individual preferences quirks unique nurture and tastes and a recommendation engine somewhere wants to simplify this image to a few simplified rows. Once these representative rows have been derived someone could look at the things that it is known that this person likes and fill in the blanks by comparison with the representative rows. Admittedly the maths does make it possible to classify someone as 50% a sports person and 50% a comedy person and to make suggestions accordingly. Despite that this process fundamentally proceeds by comparing each person to some smoothed and averaged vision of people who are like them. The danger is that these handy representative vectors of taste that the SVD spits out become used to make suggestions that we cannot escape from. So these simplified caricatures of taste start to define our taste rather than describe it.', 'How often has it happened to you in a conversation about a band or film that you enjoy with a like-minded person that the similarity of your tastes stifle any new avenues or discoveries? Shared enjoyment of one thing is enough to immediately and fully know the other person for they are the same as you according to the algorithm. Of course there are real and organic relations; Democrats are truly more likely to enjoy Colbert than are Republicans and broadly speaking they would opt for more Colbert than Glenn Beck. But as the average characteristics of others are served up to us continually something different happens. A vicious cycle ensues whereby our simplified classes of behaviour become more prescriptive than descriptive. Add in the peer pressure of those who have already imbibed the suggestions of the recommendation algorithm and the basin of attraction deepens.', 'This kind of phenomenon has not gone unnoticed. Ali Parisi has expounded on the Filter Bubble the phenomenon whereby personalised web experiences limit the diversity of opinions and news that can be accessed stifling debate in the process. Zeynep Tufekci has also commented insightfully on the role of algorithmic filtering in news coverage.', 'It may seem churlish to complain that many platforms that simplify our lives dramatically allow us to connect with friends and families and to easily enhance our career networks are doing this in a slightly imperfect way. But the modern debate on how artificial intelligence can feasibly be integrated into society demands of us a thorough and honest dissection of all its implications.']",[],None,"['pixel at row 2 and column 3 is 10% red 50% green and 0% blue', 'define', 'describe']"
