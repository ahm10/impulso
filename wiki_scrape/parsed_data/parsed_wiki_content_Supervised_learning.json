{"General": ["Presence of interactions and non-linearities.", " If each of the features makes an independent contribution to the output, then algorithms based on linear functions (e.g., linear regression, logistic regression, Support Vector Machines, naive Bayes) and distance functions (e.g., nearest neighbor methods, support vector machines with Gaussian kernels) generally perform well.", " However, if there are complex interactions among features, then algorithms such as decision trees and neural networks work better, because they are specifically designed to discover these interactions.", " Linear methods can also be applied, but the engineer must manually specify the interactions when using them.", "When considering a new application, the engineer can compare multiple learning algorithms and experimentally determine which one works best on the problem at hand (see cross validation).", " Tuning the performance of a learning algorithm can be very time-consuming.", " Given fixed resources, it is often better to spend more time collecting additional training data and more informative features than it is to spend extra time tuning the learning algorithms."], "Steps": ["In order to solve a given problem of supervised learning, one has to perform the following steps:"], "Algorithm choice": ["There are four major issues to consider in supervised learning:"], "Bias-variance tradeoff": ["   if it predicts different output values when trained on different training sets.", " The prediction error of a learned classifier is related to the sum of the bias and the variance of the learning algorithm.", " Generally, there is a tradeoff between bias and variance.", " A learning algorithm with low bias must be \"flexible\" so that it can fit the data well.", " But if the learning algorithm is too flexible, it will fit each training data set differently, and hence have high variance.", " A key aspect of many supervised learning methods is that they are able to adjust this tradeoff between bias and variance (either automatically or by providing a bias/variance parameter that the user can adjust)."], "Function complexity and amount of training data": ["The second issue is the amount of training data available relative to the complexity of the \"true\" function (classifier or regression function).", " If the true function is simple, then an \"inflexible\" learning algorithm with high bias and low variance will be able to learn it from a small amount of data.", " But if the true function is highly complex (e.g., because it involves complex interactions among many different input features and behaves differently in different parts of the input space), then the function will only be able to learn from a very large amount of training data and using a \"flexible\" learning algorithm with low bias and high variance.", "There is a clear demarcation between the input and the desired output."], "Dimensionality of the input space": ["A third issue is the dimensionality of the input space.", " If the input feature vectors have very high dimension, the learning problem can be difficult even if the true function only depends on a small number of those features.", " This is because the many \"extra\" dimensions can confuse the learning algorithm and cause it to have high variance.", " Hence, high input dimensional typically requires tuning the classifier to have low variance and high bias.", " In practice, if the engineer can manually remove irrelevant features from the input data, this is likely to improve the accuracy of the learned function.", " In addition, there are many algorithms for feature selection that seek to identify the relevant features and discard the irrelevant ones.", " This is an instance of the more general strategy of dimensionality reduction, which seeks to map the input data into a lower-dimensional space prior to running the supervised learning algorithm."], "Noise in the output values": ["In practice, there are several approaches to alleviate noise in the output values such as early stopping to prevent overfitting as well as detecting and removing the noisy training examples prior to training the supervised learning algorithm.", " There are several algorithms that identify noisy training examples and removing the suspected noisy training examples prior to training has decreased generalization error with statistical significance."], "Other factors to consider": ["Other factors to consider when choosing and applying a learning algorithm include the following:"], "Algorithms": ["The most widely used learning algorithms are:"], "Support Vector Machines": ["Similarity learning"], "How supervised learning algorithms work": ["   is the input space and"], "  ": ["  ."], "Empirical risk minimization": ["to memorize the training examples without generalizing well.", " This is called overfitting."], "Structural risk minimization": ["   is a linear function of the form"], "Generative training": ["   can be regarded as a generative model that explains how the data were generated.", " Generative training algorithms are often simpler and more computationally efficient than discriminative training algorithms.", " In some cases, the solution can be computed in closed form as in naive Bayes and linear discriminant analysis."], "Generalizations": ["There are several ways in which the standard supervised learning problem can be generalized:"], "Semi-supervised learning": [" In this setting, the desired output values are provided only for a subset of the training data.", " The remaining data is unlabeled.", "Learning to rank: When the input is a set of objects and the desired output is a ranking of those objects, then again the standard methods must be extended."], "Approaches and algorithms": [" In this setting, the desired output values are provided only for a subset of the training data.", " The remaining data is unlabeled.", "Proaftn, a multicriteria classification algorithm"], "Applications": [" In this setting, the desired output values are provided only for a subset of the training data.", " The remaining data is unlabeled."]}