UUID,sent
eit72x9f7yyEb8G2oaFQ6b,This article describes a way to periodically move on-premise Cassandra data to S3 for analysis Were been using this approach successfully over the last few months in order to get the best of both worlds for an early-stage platform such as 1200aero: The cost effectiveness of on-premise hosting for a stable live workload and the on-demand scalability of AWS for data analysis and machine learningOur end goal was to upload data to S3 organized as follows: To that effect we performed the following steps described in detail in this article: For your reference we used Cassandra 311 and Spark 231 both straight open source versions • Building a Django POST face-detection API using OpenCV and Haar Cascades • Understanding and building Generative Adversarial Networks(GANs) • Learning from mistakes with Hindsight Experience Replay • Predicting buying behavior using Machine LearningThe data we want to analyze consists of millions of low-altitude air traffic messages captured from ADS-B receivers at a number of regional airports The original table definition is: Notice the primary key is (icao gentime) Cassandra data models are designed with a specific data retrieval use case in mind Our applications query data by aircraft identifier so our partitioning column is the aircrafts icao (a 6-character hex transponder code) followed by gentime (a timestamp) as the clustering column that orders records chronologically within each partitionProblem: In order to periodically export a day/week/months worth of data we needed to be able to retrieve records by date Since such attribute did not originally exist we had to: Defining a new date column in Cassandra is straightforward: However populating it proved more complex than anticipated: We had to update several hundred million records and did not find a suitable CQL UPDATE statement to do so in place (ie deriving day from the gentime timestamp) After evaluating all alternatives we ended up doing this: Export the primary key for every record to a CSV file using CQL: Output file: Update the file in Unix to create a new field based on the existing timestamp: The above yielded a file with the new attribute: Import the modified file into the same table which updates every record with just the new column value leaving all other fields intact: The import took several hours but worked flawlessly In hindsight its still far from ideal and required some research to guarantee the safety of the data but given the lack of support in Cassandra its as good a solution we could findPrior to creating our first Cassandra materialized view we considered secondary indexes However the many caveats and words of caution from experienced users led us in the direction of materialized viewsMaterialized view creation ran in background for ~12 hours as it duplicated and organized the data This query reports any builds in progress: To date we have not observed a significant performance overhead associated with the materialized view DataStax found that every materialized view takes away ~10% of the insert performance on a given tableOnce we had the means to retrieve the data by date we had to ensure that the Spark Cassandra connector issued the right queries to retrieve the data efficientlySpark configuration parameters for the Cassandra connector (passed when creating a SparkConf via spark-submit sparkmagic in Jupyter etc): Import the right packages define a dataset on the Cassandra table: Filter data for a specific date: …or for a number of dates (discrete dates must be specified in an IN condition): To verify that your filter will be applied to the Cassandra table print the physical plan with dfexplain() There should be predicates (ie actual values) being pushed down to the data source as in these examples: Important: Attempts to filter the dataset that dont conform to the connector documentation will cause Spark to issue a full table scan to Cassandra (and thus defeat the purpose of reorganizing your data as done in Step 1)As a reminder our goal was to lay the data in S3 partitioned by date example: So rather than partitioning the dataset by the whole date string (eg 2018 11 06) we needed to partition by its components (year month day) Hence we had to first convert the day column into year month and day columns: At long last we had our data available in Spark with the necessary attributes and partitioned by the criteria we neededSetting up Spark to push data to S3 was pretty easy Required AWS libraries are included in the Hadoop distribution Spark depends upon All we had to configure were the following five entries in $SPARK_HOME/conf/spark-defaultsconf: Now we were ready to partition and push the data to S3 in a single instruction: Performance of the above statement is surprisingly fast (in part due to Parquet files being much smaller than their CSV equivalents) Loading a days worth of records (~500000 rows) completes in less than one minuteWe use SaveModeAppend to make it possible to load additional days without deleting existing dataNote: The s3a URL prefix has desirable performance and capacity implications for large file operations such as ParquetFrom here sky is the limit You can spin an EMR cluster and query S3 directly with an arbitrary number of cluster nodes Alternatively you can also launch a Sagemaker notebook and point it to EMR Spark following similar steps to those outlined in my previous article And as of the time of writing Boto3 the AWS SDK for Python now makes it possible to issue basic SQL queries against Parquet files in S3Future articles will describe how 1200aero is using these data to predict potentially hazardous situations for general aviation aircraft
FBbr4ipgZWYUJmvhk8iDX2,Betterment has built a highly available data processing platform to power new product features and backend processing needs using AirflowBetterments data platform is unique in that it not only supports offline needs such as analytics but also powers our consumer-facing product Features such as Time Weighted Returns and Betterment for Business balances rely on our data platform working throughout the day Additionally we have regulatory obligations to report complex data to third parties daily making data engineering a mission critical part of what we do at BettermentWe originally ran our data platform on a single machine in 2015 when we ingested far less data with fewer consumer-facing requirements However recent customer and data growth coupled with new business requirements require us to now scale horizontally with high availabilityOur single-server approach used Luigi a Python module created to orchestrate long-running batch jobs with dependencies While we could achieve high availability with Luigi its now 2017 and the data engineering landscape has shifted We turned to Airflow because it has emerged as a full-featured workflow management framework better suited to orchestrate frequent tasks throughout the dayTo migrate to Airflow were deprecating our Luigi solution on two fronts: cross-database replication and task orchestration Were using Amazons Database Migration Service (DMS) to replace our Luigi-implemented replication solution and re-building all other Luigi workflows in Airflow Well dive into each of these pieces below to explain how Airflow mediated this transitionWe used Luigi to extract and load source data from multiple internal databases into our Redshift data warehouse on an ongoing basis We recently adopted Amazons DMS for continuous cross-database replication to Redshift moving away from our internally-built solutionThe only downside of DMS is that we are not aware of how recent source data is in Redshift For example a task computing all of a prior days activity executed at midnight would be inaccurate if Redshift were missing data from DMS at midnight due to lagIn Luigi we knew when the data was pulled and only then would we trigger a task However in Airflow we reversed our thinking to embrace DMS using Airflows sensor operators to wait for rows to be pushed from DMS before carrying on with dependent tasksWhile Airflow doesnt claim to be highly available out of the box we built an infrastructure to get as close as possible Were running Airflows database on Amazons Relational Database Service and using Amazons Elasticache for Redis queuing Both of these solutions come with high availability and automatic failover as add-ons Amazon provides Additionally we always deploy multiple baseline Airflow workers in case one fails in which case we use automated deploys to stand up any part of the Airflow cluster on new hardwareThere is still one single point of failure left in our Airflow architecture though: the scheduler While we may implement a hot-standby backup in the future we simply accept it as a known risk and set our monitoring system to notify a team member of any deviancesSince our processing needs fluctuate throughout the day we were paying for computing power we didnt actually need during non-peak times on a single machine as shown in our Luigi servers loadDistributed workers used with Amazons Auto Scaling Groups allow us to automatically add and remove workers based on outstanding tasks in our queues Effectively this means maintaining only a baseline level of workers throughout the day and scaling up during peaks when our workload increasesAirflow queues allow us to designate certain tasks to run on particular hardware (eg CPU optimized) to further reduce costs We found just a few hardware type queues to be effective For instance tasks that saturate CPU are best run on a compute optimized worker with concurrency set to the number of cores Non-CPU intensive tasks (eg polling a database) can run on higher concurrency per CPU core to save overall resourcesAirflow tasks that pass data to each other can run on different machines presenting a new challenge versus running everything on a single machine For example one Airflow task may write a file and a subsequent task may need to email the file from the dependent task ran on another machine To implement this pattern we use Amazon S3 as a persistent storage tierFortunately Airflow already maintains a wide selection of hooks to work with remote sources such as S3 While S3 is great for production its a little difficult to work with in development and testing where we prefer to use the local filesystem We implemented a local fallback mixin for Airflow maintained hooks that uses the local filesystem for development and testing deferring to the actual hooks remote functionality only on productionWe mimic our production cluster as closely as possible for development & testing to identify any issues that may arise with multiple workers This is why we adopted Docker to run a production-like Airflow cluster from the ground up on our development machines We use containers to simulate multiple physical worker machines that connect to officially maintained local Redis and PostgreSQL containersDevelopment and testing also require us to stand up the Airflow database with predefined objects such as connections and pools for the code under test to function properly To solve this programmatically we adopted Alembicdatabase migrations to manage these objects through code allowing us to keep our development testing and production Airflow databases consistentUpon each deploy we use Ansible to launch new worker instances and terminate existing workers But what happens when our workers are busy with other work during a deploy? We dont want to terminate workers while theyre finishing something up and instead want them to terminate after the work is done (not accepting new work in the interim)Fortunately Celery supports this shutdown behavior and will stop accepting new work after receiving an initial TERM signal letting old work finish up We use Upstart to define all Airflow services and simply wrap the TERM behavior in our workers post-stop script sending the TERM signal first waiting until we see the Celery process stopped then finally poweroff the machineThe path to building a highly available data processing service was not straightforward requiring us to build a few specific but critical additions to Airflow Investing the time to run Airflow as a cluster versus a single machine allows us to run work in a more elastic manner saving costs and using optimized hardware for particular jobs Implementing a local fallback for remote hooks made our code much more testable and easier to work with locally while still allowing us to run with Airflow-maintained functionality in productionWhile migrating from Luigi to Airflow is not yet complete Airflow has already offered us a solid foundation We look forward to continuing to build upon Airflow and contributing back to the communityThis article is part of Engineering at BettermentOriginally published at wwwbettermentcom on August 14 2017
fwSKxXMWiBEE6gpLEnfupV,Talk about data driven company which means people should use data to prove their argument so the data itself must be rich enough Rich data can be measured by its size throughput (amount per time) or complexity of its value Distributed machine framework or simply we call it big data technology can be used to handle that Data engineers in my office uses Hadoop family ecosystem such as HBase Hive Impala HDFS etc Hadoop is convenient enough for us to handle large data the implementation of partition to address which data on which physical nodes makes us able to keep performance of query trough nearly infinite data set Its distributed architecture and replication makes it have high availability feature so we less to worry about data loss But replication itself comes with disadvantage the most disadvantage is increasing its storage size as much as number of replication that we setOn the other hand data value will most likely decrease over time especially for time based data like system log user behavior events etc Storing large amount of data in distributed manner (Hadoop) is so costly since most likely we implement replication factor to meet high standard of availability By default HDFS replication factor is 3 so our data multiplied by 3 and distributed among available nodes For some reasons this is too much especially for data which is no longer valuable ie user behavior data gathered last year So we need to find solution or technology that meet our needs such as able to handle large or nearly infinite data set since data is always increasing trough time have good performance when querying and efficient disk consumption Its not as simple as reducing replication factor because if we reduce replication factor to lets say 1 we cant achieve its high standard availability anymore since data that stored in dead node will be unreachableApache Hive has great solution for it which we can store our data on AWS S3 assuming that AWS S3 availability is beyond our standard In the context of persistence in my case(data engineers at my office) AWS S3 provide better persistence than HDFS since its premium and managed service so data will be safe even we failed to spin up our self hosted service So its wiser for data engineers to archive old less valuable data to S3 instead on HDFS In the context of technology when we pick to use S3 instead of HDFS on Hive it become external table So if we use S3 we just need to deploy as minimum as one node just for Hive related instances (minimum node specification still applied)In the context of cost data persistence and effort to maintain S3 is better than HDFS subjectively for our case But what about performance? When we need to consume data that stored in S3 by Hive it will be downloaded first decompress if any compression method is applied before it is ready to be used More or less it will affect the read performance different from HDFS which data is ready to be consumed every time So storing data on HDFS will provide better read throughput than S3 in the context of Hive Another issue is computation service different than Hive as a warehouse service If we need to consume large data set of course we still needs to deploy multiple nodes and uses computation service like Spark By that advantages and disadvantages then how we decide when to use S3 and when to use HDFS?To make it easy we differentiate data by its value first is the data will be frequently consumed or not how big the data is etc For example we can classified data by group such asSome of the data may have more than one type ie products stock we can put it as dynamic data on its own RDBMS but we can also gather its time based log or past activity to analyze it later Talk about big data the most relevant data stored to distributed system on my case is time based But occasionally dynamic and static data may be stored to distributed system as well such as HBase to leverage some specific needs (may discussed in different article)For time based data we should have the information about how people will use it how old is past data should be kept in HDFS etc After we understand how the behavior of the data is then we decide where to store it whether on HDFS or S3 how old we should keep on HDFS etcIn my case whether to use S3 or HDFS for storage is very flexible and highly influenced by its use case As a data engineer we wont restrict people ideas to implement his own flows to achieve better performance or value Some data are stored to HDFS first and S3 later some of them are directly to HDFS and S3 and some of them only on S3 without HDFS All is possible and have its own advantages and disadvantages the most important part is engineer who used it must be understand and responsible whats best for their caseFor data engineers who uses big data technology to do data warehousing most likely most of expensive data is time based ie users behaviors events logs or systems logs which are very expensive both on size and on the context of its high throughput To determine how to store some data at first we gather informations from the people who needed it and how those data will be used for to determine do we need to store it on HDFS and how long this data will be stored on HDFS Also we determine when this data is ready to store to S3 to leverage efficiency Some of the data stored in HDFS will eventually be deleted to eliminate storage costs
XtYdtvd9J6rFMH7SwZ3Gcj,New York City has 59 Community Boards each representing the residents living within a geographic area of the city The members of these boards also live within its bounds and play an integral role connecting citizens with City agencies which they do in many ways One way Community Boards connect with City agencies is by what we call Community Board Budget RequestsCommunity Board Budget Requests well call them Budget Requests for short are submitted by each Community Board every October and contain a list of that communitys priority capital and programmatic investments intended to better that neighborhood These lists are submitted to the Office of Management and Budget who then distributes these Budget Requests to the respective City agency The City agencies then take these Budget Requests into consideration when developing the upcoming Capital BudgetBy sharing the neighborhoods priority needs for services and infrastructure with City agencies the Community Boards via Budget Requests can influence and shape investments across the cityExplore: New York Citys 59 Community BoardsHistorically these Budget Requests have been shared with City agencies via spreadsheets which is not always the easiest way to consume this information City agencies know where their assets are so seeing the locations of the requests on a map is a more intuitive and helpful way for them to explore the requests Additionally by viewing the requests spatially agencies can better understand what requests are being made within a given community what is being requested of them across the city and what is being requested of partner agenciesOur problem was that we couldnt create the maps since the submitted Budget Requests do not contain spatial data so we had to generate our ownTo geolocate these Budget Requests we used three methods: automated georeferencing fuzzy string matching and manual data creationAfter Community Boards submit the Budget Requests our colleagues on the Planning Coordination team receive the requests in Excel spreadsheets The requests occasionally include addresses and often include descriptive information that can be matched to a location but again no spatial dataFor records with free form address intersection or on-to-from information (eg on 5th Avenue between 42nd and 48th Street) we leveraged City Plannings geocoder whose web API is called Geoclient Geoclient returns the latitude and longitude (lat/long) of an address or intersection and the beginning and ending lat/long for any street segment (Unfortunately we werent able to link the information returned by the Geoclient API to LION the citys street centerline database but thats another blog post)Using the lat/long data returned by the Geoclient API we made spatial data! This method got some requests mapped but there were more requests to goThis method may be the most funFor records that couldnt automatically be georeferenced via the method described above we had to get creative since we needed to decipher location information from the description of the requested projectFor context here are some sample descriptions: Now you (maybe with the help of Google) can read Handicap Accessibility in Front of the 46th Precinct and know where that project should be mapped to The trick is getting a computer to know to map Handicap Accessibility in Front of the 46th Precinct to NYPDs 46th Precinct station house Inconsistent site name patterns such as 46 PCT vs 46th Precinct added to the challenge and complexity of automating the mapping of projects based on descriptionGiven these factors we developed a series of SQL LIKE statements and Fuzzy string matching algorithms that matched words in the descriptions to names of places in other spatial datasets The two reference datasets we used to compare against were City Plannings Facilities Database which maps +36k government facilities or program sites throughout NYC and NYC Parks Properties which has polygon geometries for all 1969 NYC parks These two datasets capture the majority of the Citys building or park based fixed assetsThis process mapped a fair number of projects by matching Build the Green Outlook Riverside Park (DPR) to Riverside Park in the Park Properties dataset and Upgrade the FDNY Engine 307 kitchen to Eng 307 Lad 154 in the Facilities Database for example Though it didnt map everything…Without over engineering our solution we could not automatically map all requests such as this one: So we had to do some manual workBudget Requests are classified into two categories: Capital and Expense Generally Capital requests are for large scale investments in infrastructure and impact the built environment while Expense requests are for funding programs and other repeating costs of government servicesAdditionally Budget Requests are grouped into two impact types: Site specific and Non-site specific Non-site specific requests are for projects that are not necessarily tied to a discrete fixed asset such as: Whereas Site specific requests can be associated with a known location like: We focused our manual mapping efforts on Site specific Capital requestsTo map these records we used a simple Leaflet Draw GeoJSON creator built by our colleague Chris Whong which outputs code for a geojson file The tool is simple enough for most people to create spatial data regardless of skill in GISIn the end we mapped 95% of all site specific records which we published on an online map currently accessible to City employees Using this map Planners from all City agencies can filter and search requests to explore projects requested by Community Boards across the cityOur goal is that by making Community Board Budget Requests more accessible and easier to explore through a map City agencies will discover synergies between existing capital projects and these Budget Requests so that in the end more Budget Requests come to fruitionWe plan to make the data and map public after City agencies have reviewed and responded to all Community Board Budget Requests
JvT3qFqkpb679mvs3rxJ5g,These days docker and Cloudera are garnering a lot of attention I have written this blog on how to deploy the Cloudera Quickstart VM using Docker on Mac OSInstall Docker Desktop using these official steps: https://docsdockercom/docker-for-mac/install/$ docker version$ docker pull cloudera/quickstart:latestShow the list of repositories available in docker$ docker images$ hostnameExecute the `docker run` command with the following arguments: hostname  quickstartcloudera and interactive mode-p describes a process running on a given port$ docker run --hostname=quickstartcloudera --privileged=true -t -i -p 8888:8888 -p 10000:10000 -p 10020:10020 -p 11000:11000 -p 18080:18080 -p 18081:18081 -p 18088:18088 -p 19888:19888 -p 21000:21000 -p 21050:21050 -p 2181:2181 -p 25000:25000 -p 25010:25010 -p 25020:25020 -p 50010:50010 -p 50030:50030 -p 50060:50060 -p 50070:50070 -p 50075:50075 -p 50090:50090 -p 60000:60000 -p 60010:60010 -p 60020:60020 -p 60030:60030 -p 7180:7180 -p 7183:7183 -p 7187:7187 -p 80:80 -p 8020:8020 -p 8032:8032 -p 802:8042 -p 8088:8088 -p 8983:8983 -p 9083:9083 4239cd2958c6 /usr/bin/docker-quickstart$ docker ps -aRun a command in a running container$ docker exec -it 0628fb260d35 /bin/bashNote: 0628fb260d35 is the CONTAINER ID (You can find this Container ID in the above List all Containers on the Host System section)$ jpsCongratulations you have successfully created a container that can run Cloudera Quickstart services!To manage the Cloudera cluster with Cloudera Manager it will give you a UI to manage your cluster and configuration$ sudo /home/cloudera/cloudera-manager --expressGo to: http://localhost:7180/Use the following Usernames and Passwords : 1) Full Administrator: cloudera and cloudera2) Full Administrator: admin and adminNow the Cloudera Manager is running Keep it up!Home > (Username) Right top cornerHome > Support > About::: BONUS ::: $ service cloudera-scm-server status$ service cloudera-scm-agent status$ service cloudera-scm-server start$ service cloudera-scm-agent start$ docker stop  express=60 <Container ID> | <Name>eg $ docker stop --time=60 0628fb260d35 Note: 0628fb260d35 is a Container ID$ docker start 0628fb260d35
